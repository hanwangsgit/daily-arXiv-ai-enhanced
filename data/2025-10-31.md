<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 70]
- [cs.LG](#cs.LG) [Total: 101]
- [cs.AI](#cs.AI) [Total: 45]
- [eess.SP](#eess.SP) [Total: 11]
- [eess.IV](#eess.IV) [Total: 9]
- [cs.IT](#cs.IT) [Total: 6]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [Enhancing Underwater Object Detection through Spatio-Temporal Analysis and Spatial Attention Networks](https://arxiv.org/abs/2510.25797)
*Sai Likhith Karri,Ansh Saxena*

Main category: cs.CV

TL;DR: This study evaluates spatio-temporal modeling and spatial attention mechanisms in deep learning for underwater object detection, comparing YOLOv5, T-YOLOv5, and T-YOLOv5 with CBAM.


<details>
  <summary>Details</summary>
Motivation: To improve underwater object detection accuracy in dynamic marine environments with challenges like sudden movements, partial occlusions, and gradual motion.

Method: Two-phase approach: first evaluates temporal-enhanced T-YOLOv5 vs standard YOLOv5, then develops augmented T-YOLOv5 with Convolutional Block Attention Module (CBAM).

Result: T-YOLOv5 (mAP@50-95: 0.813) and T-YOLOv5 with CBAM (mAP@50-95: 0.811) significantly outperformed standard YOLOv5 (mAP@50-95: 0.563). CBAM improved performance in challenging scenarios but showed accuracy loss in simpler cases.

Conclusion: Temporal modeling significantly enhances detection reliability, and CBAM further improves performance in complex scenarios, though with trade-offs in simpler conditions.

Abstract: This study examines the effectiveness of spatio-temporal modeling and the
integration of spatial attention mechanisms in deep learning models for
underwater object detection. Specifically, in the first phase, the performance
of temporal-enhanced YOLOv5 variant T-YOLOv5 is evaluated, in comparison with
the standard YOLOv5. For the second phase, an augmented version of T-YOLOv5 is
developed, through the addition of a Convolutional Block Attention Module
(CBAM). By examining the effectiveness of the already pre-existing YOLOv5 and
T-YOLOv5 models and of the newly developed T-YOLOv5 with CBAM. With CBAM, the
research highlights how temporal modeling improves detection accuracy in
dynamic marine environments, particularly under conditions of sudden movements,
partial occlusions, and gradual motion. The testing results showed that YOLOv5
achieved a mAP@50-95 of 0.563, while T-YOLOv5 and T-YOLOv5 with CBAM
outperformed with mAP@50-95 scores of 0.813 and 0.811, respectively,
highlighting their superior accuracy and generalization in detecting complex
objects. The findings demonstrate that T-YOLOv5 significantly enhances
detection reliability compared to the standard model, while T-YOLOv5 with CBAM
further improves performance in challenging scenarios, although there is a loss
of accuracy when it comes to simpler scenarios.

</details>


### [2] [MIRO: MultI-Reward cOnditioned pretraining improves T2I quality and efficiency](https://arxiv.org/abs/2510.25897)
*Nicolas Dufour,Lucas Degeorge,Arijit Ghosh,Vicky Kalogeiton,David Picard*

Main category: cs.CV

TL;DR: MIRO conditions text-to-image models on multiple reward models during training to directly learn user preferences, improving visual quality and training speed while maintaining diversity.


<details>
  <summary>Details</summary>
Motivation: Current text-to-image models trained on uncurated datasets don't align well with user preferences, and post-hoc reward-based selection methods harm diversity, semantic fidelity, and efficiency.

Method: Instead of post-processing, MIRO conditions the model on multiple reward models during training to directly learn user preferences.

Result: MIRO dramatically improves visual quality, significantly speeds up training, and achieves state-of-the-art performance on GenEval benchmark and user-preference scores (PickAScore, ImageReward, HPSv2).

Conclusion: Conditioning on multiple reward models during training is more effective than post-hoc selection for aligning text-to-image generation with user preferences while maintaining diversity and efficiency.

Abstract: Current text-to-image generative models are trained on large uncurated
datasets to enable diverse generation capabilities. However, this does not
align well with user preferences. Recently, reward models have been
specifically designed to perform post-hoc selection of generated images and
align them to a reward, typically user preference. This discarding of
informative data together with the optimizing for a single reward tend to harm
diversity, semantic fidelity and efficiency. Instead of this post-processing,
we propose to condition the model on multiple reward models during training to
let the model learn user preferences directly. We show that this not only
dramatically improves the visual quality of the generated images but it also
significantly speeds up the training. Our proposed method, called MIRO,
achieves state-of-the-art performances on the GenEval compositional benchmark
and user-preference scores (PickAScore, ImageReward, HPSv2).

</details>


### [3] [BikeScenes: Online LiDAR Semantic Segmentation for Bicycles](https://arxiv.org/abs/2510.25901)
*Denniz Goren,Holger Caesar*

Main category: cs.CV

TL;DR: This paper develops a 3D LiDAR segmentation approach for bicycle safety using a custom 'SenseBike' platform and introduces the BikeScenes-lidarseg dataset to bridge the automotive-to-bicycle domain gap.


<details>
  <summary>Details</summary>
Motivation: The vulnerability of cyclists, especially with the rising popularity of faster e-bikes, motivates adapting automotive perception technologies for bicycle safety.

Method: Used a multi-sensor 'SenseBike' research platform to develop 3D LiDAR segmentation, created the BikeScenes-lidarseg dataset with 3021 LiDAR scans annotated for 29 classes, and performed fine-tuning on this domain-specific dataset.

Result: Fine-tuning on the BikeScenes dataset achieved a mean Intersection-over-Union (mIoU) of 63.6%, significantly outperforming the 13.8% obtained with SemanticKITTI pre-training alone.

Conclusion: Domain-specific training is necessary and effective for bicycle-mounted perception systems, and the BikeScenes dataset serves as a valuable resource for advancing cyclist-centric LiDAR segmentation research.

Abstract: The vulnerability of cyclists, exacerbated by the rising popularity of faster
e-bikes, motivates adapting automotive perception technologies for bicycle
safety. We use our multi-sensor 'SenseBike' research platform to develop and
evaluate a 3D LiDAR segmentation approach tailored to bicycles. To bridge the
automotive-to-bicycle domain gap, we introduce the novel BikeScenes-lidarseg
Dataset, comprising 3021 consecutive LiDAR scans around the university campus
of the TU Delft, semantically annotated for 29 dynamic and static classes. By
evaluating model performance, we demonstrate that fine-tuning on our BikeScenes
dataset achieves a mean Intersection-over-Union (mIoU) of 63.6%, significantly
outperforming the 13.8% obtained with SemanticKITTI pre-training alone. This
result underscores the necessity and effectiveness of domain-specific training.
We highlight key challenges specific to bicycle-mounted, hardware-constrained
perception systems and contribute the BikeScenes dataset as a resource for
advancing research in cyclist-centric LiDAR segmentation.

</details>


### [4] [Generative Image Restoration and Super-Resolution using Physics-Informed Synthetic Data for Scanning Tunneling Microscopy](https://arxiv.org/abs/2510.25921)
*Nikola L. Kolev,Tommaso Rodani,Neil J. Curson,Taylor J. Z. Stock,Alberto Cazzaniga*

Main category: cs.CV

TL;DR: Machine learning approach using physics-informed synthetic data enables effective STM image restoration and super-resolution, reducing acquisition time 2-4x and decreasing need for tip conditioning.


<details>
  <summary>Details</summary>
Motivation: STM imaging is limited by tip degradation and slow serial data acquisition, requiring frequent tip conditioning procedures that reduce experimental throughput.

Method: Used physics-informed synthetic data generation pipeline to train flow-matching and diffusion models on only 36 pristine experimental images of Si(001):H.

Result: Models effectively restore images and achieve 2-4x reduction in image acquisition time by accurately reconstructing from sparsely sampled data, validated by CLIP Maximum Mean Discrepancy and structural similarity metrics.

Conclusion: The framework can significantly increase STM experimental throughput by reducing tip-conditioning frequency and enhancing frame rates in existing high-speed STM systems.

Abstract: Scanning tunnelling microscopy (STM) enables atomic-resolution imaging and
atom manipulation, but its utility is often limited by tip degradation and slow
serial data acquisition. Fabrication adds another layer of complexity since the
tip is often subjected to large voltages, which may alter the shape of its
apex, requiring it to be conditioned. Here, we propose a machine learning (ML)
approach for image repair and super-resolution to alleviate both challenges.
Using a dataset of only 36 pristine experimental images of Si(001):H, we
demonstrate that a physics-informed synthetic data generation pipeline can be
used to train several state-of-the-art flow-matching and diffusion models.
Quantitative evaluation with metrics such as the CLIP Maximum Mean Discrepancy
(CMMD) score and structural similarity demonstrates that our models are able to
effectively restore images and offer a two- to fourfold reduction in image
acquisition time by accurately reconstructing images from sparsely sampled
data. Our framework has the potential to significantly increase STM
experimental throughput by offering a route to reducing the frequency of
tip-conditioning procedures and to enhancing frame rates in existing high-speed
STM systems.

</details>


### [5] [SplitFlow: Flow Decomposition for Inversion-Free Text-to-Image Editing](https://arxiv.org/abs/2510.25970)
*Sung-Hoon Yoon,Minghan Li,Gaspard Beaudouin,Congcong Wen,Muhammad Rafay Azhar,Mengyu Wang*

Main category: cs.CV

TL;DR: Proposes a flow decomposition-and-aggregation framework for image editing that semantically decomposes target prompts into sub-prompts, computes independent flows for each, and aggregates them with adaptive weighting to address inversion inaccuracy and gradient entanglement issues in rectified flow models.


<details>
  <summary>Details</summary>
Motivation: Rectified flow models have limitations in image editing tasks due to inaccurate inversion processes and gradient entanglement issues, leading to outputs that don't faithfully reflect target prompts. Existing ODE-based approaches without inversion still yield suboptimal editing quality.

Method: Semantically decomposes target prompt into multiple sub-prompts, computes independent flow for each, and aggregates them using a projection and soft-aggregation mechanism that adaptively weights sub-target velocity fields to suppress semantic redundancy while emphasizing distinct directions.

Result: Outperforms existing zero-shot editing approaches in terms of semantic fidelity and attribute disentanglement. The method enhances diversity in target space while maintaining consistent guidance toward the full target prompt.

Conclusion: The proposed flow decomposition-and-aggregation framework effectively addresses limitations of rectified flow models in image editing by preserving both diversity and consistency in edited outputs through adaptive weighting of semantic components.

Abstract: Rectified flow models have become a de facto standard in image generation due
to their stable sampling trajectories and high-fidelity outputs. Despite their
strong generative capabilities, they face critical limitations in image editing
tasks: inaccurate inversion processes for mapping real images back into the
latent space, and gradient entanglement issues during editing often result in
outputs that do not faithfully reflect the target prompt. Recent efforts have
attempted to directly map source and target distributions via ODE-based
approaches without inversion; however,these methods still yield suboptimal
editing quality. In this work, we propose a flow decomposition-and-aggregation
framework built upon an inversion-free formulation to address these
limitations. Specifically, we semantically decompose the target prompt into
multiple sub-prompts, compute an independent flow for each, and aggregate them
to form a unified editing trajectory. While we empirically observe that
decomposing the original flow enhances diversity in the target space,
generating semantically aligned outputs still requires consistent guidance
toward the full target prompt. To this end, we design a projection and
soft-aggregation mechanism for flow, inspired by gradient conflict resolution
in multi-task learning. This approach adaptively weights the sub-target
velocity fields, suppressing semantic redundancy while emphasizing distinct
directions, thereby preserving both diversity and consistency in the final
edited output. Experimental results demonstrate that our method outperforms
existing zero-shot editing approaches in terms of semantic fidelity and
attribute disentanglement. The code is available at
https://github.com/Harvard-AI-and-Robotics-Lab/SplitFlow.

</details>


### [6] [Brain-IT: Image Reconstruction from fMRI via Brain-Interaction Transformer](https://arxiv.org/abs/2510.25976)
*Roman Beliy,Amit Zalcher,Jonathan Kogman,Navve Wasserman,Michal Irani*

Main category: cs.CV

TL;DR: Brain-IT uses a Brain Interaction Transformer to reconstruct images from fMRI data by enabling interactions between brain-voxel clusters and predicting complementary semantic and structural image features.


<details>
  <summary>Details</summary>
Motivation: Current methods for reconstructing images from fMRI brain recordings often lack faithfulness to the actual seen images, despite progress with diffusion models.

Method: Brain-IT employs a Brain Interaction Transformer (BIT) that allows interactions between clusters of functionally-similar brain-voxels, predicting both high-level semantic features and low-level structural features to guide diffusion model-based image reconstruction.

Result: The method achieves faithful image reconstructions from fMRI that surpass current state-of-the-art approaches both visually and by objective metrics, and achieves comparable results with only 1-hour of fMRI data versus 40-hour recordings required by current methods.

Conclusion: Brain-IT's brain-inspired approach with functional-cluster interactions and complementary feature prediction enables efficient and faithful image reconstruction from fMRI data with significantly reduced training data requirements.

Abstract: Reconstructing images seen by people from their fMRI brain recordings
provides a non-invasive window into the human brain. Despite recent progress
enabled by diffusion models, current methods often lack faithfulness to the
actual seen images. We present "Brain-IT", a brain-inspired approach that
addresses this challenge through a Brain Interaction Transformer (BIT),
allowing effective interactions between clusters of functionally-similar
brain-voxels. These functional-clusters are shared by all subjects, serving as
building blocks for integrating information both within and across brains. All
model components are shared by all clusters & subjects, allowing efficient
training with a limited amount of data. To guide the image reconstruction, BIT
predicts two complementary localized patch-level image features: (i)high-level
semantic features which steer the diffusion model toward the correct semantic
content of the image; and (ii)low-level structural features which help to
initialize the diffusion process with the correct coarse layout of the image.
BIT's design enables direct flow of information from brain-voxel clusters to
localized image features. Through these principles, our method achieves image
reconstructions from fMRI that faithfully reconstruct the seen images, and
surpass current SotA approaches both visually and by standard objective
metrics. Moreover, with only 1-hour of fMRI data from a new subject, we achieve
results comparable to current methods trained on full 40-hour recordings.

</details>


### [7] [CYPRESS: Crop Yield Prediction via Regression on Prithvi's Encoder for Satellite Sensing](https://arxiv.org/abs/2510.26609)
*Shayan Nejadshamsi,Yuanyuan Zhang,Shadi Zaki,Brock Porth,Lysa Porth,Vahab Khoshdel*

Main category: cs.CV

TL;DR: CYPRESS is a deep learning model that uses pre-trained geospatial foundation models to predict canola yields at high resolution from satellite imagery, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: Traditional crop yield prediction methods lack scalability and granularity needed for precision farming, creating a need for more detailed and actionable agricultural monitoring tools.

Method: Fine-tunes the Prithvi-EO-2.0-600M geospatial foundation model for continuous regression, transforming multi-temporal satellite imagery into dense, pixel-level yield maps.

Result: Superior performance over existing deep learning-based yield prediction models on Canadian Prairies dataset, providing continuous high-resolution output.

Conclusion: Validates an effective approach for bridging large-scale Earth observation with on-farm decision-making, offering scalable solutions for precision agriculture.

Abstract: Accurate and timely crop yield prediction is crucial for global food security
and modern agricultural management. Traditional methods often lack the
scalability and granularity required for precision farming. This paper
introduces CYPRESS (Crop Yield Prediction via Regression on Prithvi's Encoder
for Satellite Sensing), a deep learning model designed for high-resolution,
intra-field canola yield prediction. CYPRESS leverages a pre-trained,
large-scale geospatial foundation model (Prithvi-EO-2.0-600M) and adapts it for
a continuous regression task, transforming multi-temporal satellite imagery
into dense, pixel-level yield maps. Evaluated on a comprehensive dataset from
the Canadian Prairies, CYPRESS demonstrates superior performance over existing
deep learning-based yield prediction models, highlighting the effectiveness of
fine-tuning foundation models for specialized agricultural applications. By
providing a continuous, high-resolution output, CYPRESS offers a more
actionable tool for precision agriculture than conventional classification or
county-level aggregation methods. This work validates a novel approach that
bridges the gap between large-scale Earth observation and on-farm
decision-making, offering a scalable solution for detailed agricultural
monitoring.

</details>


### [8] [Fine-tuning Segment Anything for Real-Time Tumor Tracking in Cine-MRI](https://arxiv.org/abs/2510.25990)
*Valentin Boussot,Cédric Hémon,Jean-Claude Nunes,Jean-Louis Dillenseger*

Main category: cs.CV

TL;DR: This paper presents a real-time tumor tracking method for cine-MRI using SAM 2.1 foundation model with mask-based prompts, achieving 0.8794 Dice score in the TrackRAD2025 challenge.


<details>
  <summary>Details</summary>
Motivation: Address the TrackRAD2025 challenge of real-time tumor tracking in cine-MRI sequences under strong data scarcity constraints and one-second runtime requirements.

Method: Used SAM 2.1 foundation model with mask-based prompts from first annotated slice, fine-tuned on small labeled dataset. Applied balanced Dice + IoU loss, 1024x1024 patches, standard augmentations, and uniform low learning rate across all modules.

Result: Achieved Dice score of 0.8794 on hidden test set, ranking 6th overall in TrackRAD2025 challenge. The method maintained real-time performance within one-second constraint.

Conclusion: Foundation models like SAM 2.1 show strong potential for accurate and real-time tumor tracking in MRI-guided radiotherapy, even under data scarcity conditions.

Abstract: In this work, we address the TrackRAD2025 challenge of real-time tumor
tracking in cine-MRI sequences of the thoracic and abdominal regions under
strong data scarcity constraints. Two complementary strategies were explored:
(i) unsupervised registration with the IMPACT similarity metric and (ii)
foundation model-based segmentation leveraging SAM 2.1 and its recent variants
through prompt-based interaction. Due to the one-second runtime constraint, the
SAM-based method was ultimately selected. The final configuration used SAM2.1
b+ with mask-based prompts from the first annotated slice, fine-tuned solely on
the small labeled subset from TrackRAD2025. Training was configured to minimize
overfitting, using 1024x1024 patches (batch size 1), standard augmentations,
and a balanced Dice + IoU loss. A low uniform learning rate (0.0001) was
applied to all modules (prompt encoder, decoder, Hiera backbone) to preserve
generalization while adapting to annotator-specific styles. Training lasted 300
epochs (~12h on RTX A6000, 48GB). The same inference strategy was consistently
applied across all anatomical sites and MRI field strengths. Test-time
augmentation was considered but ultimately discarded due to negligible
performance gains. The final model was selected based on the highest Dice
Similarity Coefficient achieved on the validation set after fine-tuning. On the
hidden test set, the model reached a Dice score of 0.8794, ranking 6th overall
in the TrackRAD2025 challenge. These results highlight the strong potential of
foundation models for accurate and real-time tumor tracking in MRI-guided
radiotherapy.

</details>


### [9] [Surpassing state of the art on AMD area estimation from RGB fundus images through careful selection of U-Net architectures and loss functions for class imbalance](https://arxiv.org/abs/2510.26778)
*Valentyna Starodub,Mantas Lukoševičius*

Main category: cs.CV

TL;DR: This paper presents an improved AMD lesion detection framework using semantic segmentation on RGB fundus images, achieving state-of-the-art performance on the ADAM challenge benchmark.


<details>
  <summary>Details</summary>
Motivation: AMD is a leading cause of irreversible vision impairment in elderly people, and there's a need for effective detection methods using non-invasive, cost-effective RGB fundus imaging.

Method: Used U-Net as base architecture and evaluated various improvements including pre-processing techniques, different encoder backbones of varying complexity, and specialized loss functions to handle class imbalances at image and pixel levels.

Result: The final framework configuration outperformed all prior ADAM challenge submissions for multi-class segmentation of different AMD lesion types in RGB fundus images.

Conclusion: The research successfully developed an advanced AMD detection framework that sets a new benchmark for AMD lesion segmentation, with source code made publicly available.

Abstract: Age-related macular degeneration (AMD) is one of the leading causes of
irreversible vision impairment in people over the age of 60. This research
focuses on semantic segmentation for AMD lesion detection in RGB fundus images,
a non-invasive and cost-effective imaging technique. The results of the ADAM
challenge - the most comprehensive AMD detection from RGB fundus images
research competition and open dataset to date - serve as a benchmark for our
evaluation. Taking the U-Net connectivity as a base of our framework, we
evaluate and compare several approaches to improve the segmentation model's
architecture and training pipeline, including pre-processing techniques,
encoder (backbone) deep network types of varying complexity, and specialized
loss functions to mitigate class imbalances on image and pixel levels. The main
outcome of this research is the final configuration of the AMD detection
framework, which outperforms all the prior ADAM challenge submissions on the
multi-class segmentation of different AMD lesion types in non-invasive RGB
fundus images. The source code used to conduct the experiments presented in
this paper is made freely available.

</details>


### [10] [Larger Hausdorff Dimension in Scanning Pattern Facilitates Mamba-Based Methods in Low-Light Image Enhancement](https://arxiv.org/abs/2510.26001)
*Xinhua Wang,Caibo Feng,Xiangjun Fu,Chunxiao Liu*

Main category: cs.CV

TL;DR: Enhanced Mamba framework with Hilbert Selective Scan mechanism that increases Hausdorff dimension for better feature space exploration and fine-scale detail capture in low-light image enhancement.


<details>
  <summary>Details</summary>
Motivation: To address information inconsistencies and improve spatial locality in Mamba-based methods while maintaining long-range dependency handling capabilities.

Method: Proposed Hilbert Selective Scan mechanism that increases the Hausdorff dimension of Mamba's scanning pattern for more effective feature space exploration.

Result: Significantly improved quantitative metrics and qualitative visual fidelity in low-light image enhancement benchmarks, with reduced computational resources and shorter inference time.

Conclusion: The refined strategy advances state-of-the-art in low-light image enhancement and shows promise for broader Mamba-based applications.

Abstract: We propose an innovative enhancement to the Mamba framework by increasing the
Hausdorff dimension of its scanning pattern through a novel Hilbert Selective
Scan mechanism. This mechanism explores the feature space more effectively,
capturing intricate fine-scale details and improving overall coverage. As a
result, it mitigates information inconsistencies while refining spatial
locality to better capture subtle local interactions without sacrificing the
model's ability to handle long-range dependencies. Extensive experiments on
publicly available benchmarks demonstrate that our approach significantly
improves both the quantitative metrics and qualitative visual fidelity of
existing Mamba-based low-light image enhancement methods, all while reducing
computational resource consumption and shortening inference time. We believe
that this refined strategy not only advances the state-of-the-art in low-light
image enhancement but also holds promise for broader applications in fields
that leverage Mamba-based techniques.

</details>


### [11] [CAVE: Detecting and Explaining Commonsense Anomalies in Visual Environments](https://arxiv.org/abs/2510.26006)
*Rishika Bhagwatkar,Syrielle Montariol,Angelika Romanou,Beatriz Borges,Irina Rish,Antoine Bosselut*

Main category: cs.CV

TL;DR: CAVE is the first benchmark for real-world visual anomalies with tasks for description, explanation, and justification, showing that current VLMs struggle with anomaly perception and reasoning.


<details>
  <summary>Details</summary>
Motivation: Existing anomaly detection in computer vision is limited to industrial defects or synthetic anomalies, failing to capture real-world complexity and unpredictability.

Method: Introduces CAVE benchmark with fine-grained annotations for visual grounding and categorization based on visual manifestations, complexity, severity, and commonness, drawing from cognitive science research.

Result: State-of-the-art Vision-Language Models struggle with visual anomaly perception and commonsense reasoning, even with advanced prompting strategies.

Conclusion: CAVE provides a realistic and cognitively grounded benchmark to advance research in anomaly detection and commonsense reasoning in VLMs.

Abstract: Humans can naturally identify, reason about, and explain anomalies in their
environment. In computer vision, this long-standing challenge remains limited
to industrial defects or unrealistic, synthetically generated anomalies,
failing to capture the richness and unpredictability of real-world anomalies.
In this work, we introduce CAVE, the first benchmark of real-world visual
anomalies. CAVE supports three open-ended tasks: anomaly description,
explanation, and justification; with fine-grained annotations for visual
grounding and categorizing anomalies based on their visual manifestations,
their complexity, severity, and commonness. These annotations draw inspiration
from cognitive science research on how humans identify and resolve anomalies,
providing a comprehensive framework for evaluating Vision-Language Models
(VLMs) in detecting and understanding anomalies. We show that state-of-the-art
VLMs struggle with visual anomaly perception and commonsense reasoning, even
with advanced prompting strategies. By offering a realistic and cognitively
grounded benchmark, CAVE serves as a valuable resource for advancing research
in anomaly detection and commonsense reasoning in VLMs.

</details>


### [12] [Climate Adaptation-Aware Flood Prediction for Coastal Cities Using Deep Learning](https://arxiv.org/abs/2510.26017)
*Bilal Hassan,Areg Karapetyan,Aaron Chung Hin Chow,Samer Madanat*

Main category: cs.CV

TL;DR: A lightweight CNN model for predicting coastal flooding under sea-level rise scenarios, achieving 20% lower MAE than state-of-the-art methods and generalizing across Abu Dhabi and San Francisco regions.


<details>
  <summary>Details</summary>
Motivation: Traditional physics-based hydrodynamic simulators are computationally expensive for city-scale coastal planning, while existing deep learning methods face data scarcity and high-dimensional output challenges.

Method: Developed a novel lightweight CNN-based model using a vision-based, low-resource DL framework to predict coastal flooding under variable SLR projections and shoreline adaptation scenarios.

Result: The model significantly outperforms state-of-the-art methods, reducing mean absolute error in predicted flood depth maps by nearly 20% on average, and demonstrates generalization across diverse geographical contexts.

Conclusion: The approach serves as a scalable and practical tool for coastal flood management, empowering decision-makers to develop effective mitigation strategies against climate change impacts.

Abstract: Climate change and sea-level rise (SLR) pose escalating threats to coastal
cities, intensifying the need for efficient and accurate methods to predict
potential flood hazards. Traditional physics-based hydrodynamic simulators,
although precise, are computationally expensive and impractical for city-scale
coastal planning applications. Deep Learning (DL) techniques offer promising
alternatives, however, they are often constrained by challenges such as data
scarcity and high-dimensional output requirements. Leveraging a recently
proposed vision-based, low-resource DL framework, we develop a novel,
lightweight Convolutional Neural Network (CNN)-based model designed to predict
coastal flooding under variable SLR projections and shoreline adaptation
scenarios. Furthermore, we demonstrate the ability of the model to generalize
across diverse geographical contexts by utilizing datasets from two distinct
regions: Abu Dhabi and San Francisco. Our findings demonstrate that the
proposed model significantly outperforms state-of-the-art methods, reducing the
mean absolute error (MAE) in predicted flood depth maps on average by nearly
20%. These results highlight the potential of our approach to serve as a
scalable and practical tool for coastal flood management, empowering
decision-makers to develop effective mitigation strategies in response to the
growing impacts of climate change. Project Page: https://caspiannet.github.io/

</details>


### [13] [Enhancing Temporal Understanding in Video-LLMs through Stacked Temporal Attention in Vision Encoders](https://arxiv.org/abs/2510.26027)
*Ali Rasekh,Erfan Bagheri Soula,Omid Daliran,Simon Gottschalk,Mohsen Fayyaz*

Main category: cs.CV

TL;DR: The paper proposes a Video-LLM architecture with stacked temporal attention modules in the vision encoder to improve temporal understanding in videos, achieving up to +5.5% improvement on video QA benchmarks.


<details>
  <summary>Details</summary>
Motivation: Current Video-LLMs struggle with understanding complex temporal dynamics in videos, particularly in comprehending action sequences and temporal progression.

Method: Introduces stacked temporal attention modules directly within the vision encoder to capture action progression and frame relationships before passing visual tokens to the LLM.

Result: Significantly improves temporal reasoning and outperforms existing models in video question answering tasks, with up to +5.5% improvement on VITATECS, MVBench, and Video-MME benchmarks.

Conclusion: Enhancing the vision encoder with temporal structure addresses a critical gap in video understanding for Video-LLMs.

Abstract: Despite significant advances in Multimodal Large Language Models (MLLMs),
understanding complex temporal dynamics in videos remains a major challenge.
Our experiments show that current Video Large Language Model (Video-LLM)
architectures have critical limitations in temporal understanding, struggling
with tasks that require detailed comprehension of action sequences and temporal
progression. In this work, we propose a Video-LLM architecture that introduces
stacked temporal attention modules directly within the vision encoder. This
design incorporates a temporal attention in vision encoder, enabling the model
to better capture the progression of actions and the relationships between
frames before passing visual tokens to the LLM. Our results show that this
approach significantly improves temporal reasoning and outperforms existing
models in video question answering tasks, specifically in action recognition.
We improve on benchmarks including VITATECS, MVBench, and Video-MME by up to
+5.5%. By enhancing the vision encoder with temporal structure, we address a
critical gap in video understanding for Video-LLMs. Project page and code are
available at: https://alirasekh.github.io/STAVEQ2/.

</details>


### [14] [FlexICL: A Flexible Visual In-context Learning Framework for Elbow and Wrist Ultrasound Segmentation](https://arxiv.org/abs/2510.26049)
*Yuyue Zhou,Jessica Knight,Shrimanti Ghosh,Banafshe Felfeliyan,Jacob L. Jaremko,Abhilash R. Hareendranathan*

Main category: cs.CV

TL;DR: FlexICL is a flexible in-context learning framework for segmenting bony regions in ultrasound images, achieving robust performance with only 5% labeled data across four wrist and elbow datasets.


<details>
  <summary>Details</summary>
Motivation: Automatic segmentation of musculoskeletal structures in ultrasound can improve diagnostic accuracy for pediatric elbow and wrist fractures, but pixel-wise expert annotations are time-consuming and costly.

Method: Proposed FlexICL framework for intra-video segmentation using in-context learning, with novel image concatenation techniques and training strategies that integrate multiple augmentation strategies.

Result: Outperforms state-of-the-art visual ICL models (Painter, MAE-VQGAN) and conventional segmentation models (U-Net, TransUNet) by 1-27% Dice coefficient on 1,252 US sweeps.

Conclusion: FlexICL shows potential as an efficient and scalable solution for ultrasound image segmentation in medical imaging where labeled data is scarce.

Abstract: Elbow and wrist fractures are the most common fractures in pediatric
populations. Automatic segmentation of musculoskeletal structures in ultrasound
(US) can improve diagnostic accuracy and treatment planning. Fractures appear
as cortical defects but require expert interpretation. Deep learning (DL) can
provide real-time feedback and highlight key structures, helping lightly
trained users perform exams more confidently. However, pixel-wise expert
annotations for training remain time-consuming and costly. To address this
challenge, we propose FlexICL, a novel and flexible in-context learning (ICL)
framework for segmenting bony regions in US images. We apply it to an
intra-video segmentation setting, where experts annotate only a small subset of
frames, and the model segments unseen frames. We systematically investigate
various image concatenation techniques and training strategies for visual ICL
and introduce novel concatenation methods that significantly enhance model
performance with limited labeled data. By integrating multiple augmentation
strategies, FlexICL achieves robust segmentation performance across four wrist
and elbow US datasets while requiring only 5% of the training images. It
outperforms state-of-the-art visual ICL models like Painter, MAE-VQGAN, and
conventional segmentation models like U-Net and TransUNet by 1-27% Dice
coefficient on 1,252 US sweeps. These initial results highlight the potential
of FlexICL as an efficient and scalable solution for US image segmentation well
suited for medical imaging use cases where labeled data is scarce.

</details>


### [15] [Dynamic VLM-Guided Negative Prompting for Diffusion Models](https://arxiv.org/abs/2510.26052)
*Hoyeon Chang,Seungjin Kim,Yoonseok Choi*

Main category: cs.CV

TL;DR: Dynamic negative prompting using VLMs to generate adaptive negative prompts during diffusion model denoising, improving text-image alignment.


<details>
  <summary>Details</summary>
Motivation: Traditional negative prompting uses fixed prompts, which may not be contextually appropriate throughout the denoising process.

Method: Generate intermediate image predictions at specific denoising steps and query a Vision-Language Model to produce contextually appropriate negative prompts.

Result: Evaluated on benchmark datasets, showing trade-offs between negative guidance strength and text-image alignment.

Conclusion: The proposed dynamic negative prompting approach effectively improves diffusion model performance by adapting negative prompts during generation.

Abstract: We propose a novel approach for dynamic negative prompting in diffusion
models that leverages Vision-Language Models (VLMs) to adaptively generate
negative prompts during the denoising process. Unlike traditional Negative
Prompting methods that use fixed negative prompts, our method generates
intermediate image predictions at specific denoising steps and queries a VLM to
produce contextually appropriate negative prompts. We evaluate our approach on
various benchmark datasets and demonstrate the trade-offs between negative
guidance strength and text-image alignment.

</details>


### [16] [Security Risk of Misalignment between Text and Image in Multi-modal Model](https://arxiv.org/abs/2510.26105)
*Xiaosen Wang,Zhijin Ge,Shaokang Wang*

Main category: cs.CV

TL;DR: PReMA is a novel adversarial attack that manipulates multi-modal diffusion models by modifying input images while keeping prompts fixed, enabling generation of inappropriate content without prompt changes.


<details>
  <summary>Details</summary>
Motivation: Existing multi-modal diffusion models have inadequate alignment between text and image modalities, creating security risks for generating NSFW content that need to be addressed.

Method: Proposed Prompt-Restricted Multi-modal Attack (PReMA) creates adversarial images that manipulate model outputs when combined with any specified prompt, without modifying the prompt itself.

Result: Comprehensive evaluations on image inpainting and style transfer tasks across various models demonstrate PReMA's potent efficacy in manipulating generated content.

Conclusion: PReMA poses a novel threat to multi-modal diffusion model integrity, especially in image-editing applications with fixed prompts, highlighting critical security vulnerabilities.

Abstract: Despite the notable advancements and versatility of multi-modal diffusion
models, such as text-to-image models, their susceptibility to adversarial
inputs remains underexplored. Contrary to expectations, our investigations
reveal that the alignment between textual and Image modalities in existing
diffusion models is inadequate. This misalignment presents significant risks,
especially in the generation of inappropriate or Not-Safe-For-Work (NSFW)
content. To this end, we propose a novel attack called Prompt-Restricted
Multi-modal Attack (PReMA) to manipulate the generated content by modifying the
input image in conjunction with any specified prompt, without altering the
prompt itself. PReMA is the first attack that manipulates model outputs by
solely creating adversarial images, distinguishing itself from prior methods
that primarily generate adversarial prompts to produce NSFW content.
Consequently, PReMA poses a novel threat to the integrity of multi-modal
diffusion models, particularly in image-editing applications that operate with
fixed prompts. Comprehensive evaluations conducted on image inpainting and
style transfer tasks across various models confirm the potent efficacy of
PReMA.

</details>


### [17] [EgoExo-Con: Exploring View-Invariant Video Temporal Understanding](https://arxiv.org/abs/2510.26113)
*Minjoon Jung,Junbin Xiao,Junghyun Kim,Byoung-Tak Zhang,Angela Yao*

Main category: cs.CV

TL;DR: EgoExo-Con benchmark evaluates Video-LLMs' temporal consistency across egocentric and exocentric viewpoints, revealing limitations in cross-view consistency and proposing View-GRPO framework for improvement.


<details>
  <summary>Details</summary>
Motivation: To study whether Video-LLMs can achieve consistent temporal understanding when videos capture the same event from different viewpoints (egocentric vs exocentric).

Method: Introduces EgoExo-Con benchmark with synchronized video pairs and human-refined queries, focusing on Temporal Verification and Temporal Grounding tasks. Proposes View-GRPO reinforcement learning framework to strengthen view-specific reasoning while encouraging cross-view consistency.

Result: Analysis reveals existing Video-LLMs fail to maintain consistency across viewpoints, with performance worse than single-view. Naive finetuning with synchronized videos improves consistency but underperforms single-view training. View-GRPO demonstrates superiority over SFT and GRPO for improving cross-view consistency.

Conclusion: Video-LLMs have significant limitations in maintaining temporal consistency across different viewpoints, and the proposed View-GRPO framework effectively addresses this issue by strengthening both view-specific reasoning and cross-view consistency.

Abstract: Can Video-LLMs achieve consistent temporal understanding when videos capture
the same event from different viewpoints? To study this, we introduce
EgoExo-Con (Consistency), a benchmark of comprehensively synchronized
egocentric and exocentric video pairs with human-refined queries in natural
language. EgoExo-Con emphasizes two temporal understanding tasks: Temporal
Verification and Temporal Grounding. It evaluates not only correctness but
consistency across viewpoints. Our analysis reveals two critical limitations of
existing Video-LLMs: (1) models often fail to maintain consistency, with
results far worse than their single-view performances. (2) When naively
finetuned with synchronized videos of both viewpoints, the models show improved
consistency but often underperform those trained on a single view. For
improvements, we propose View-GRPO, a novel reinforcement learning framework
that effectively strengthens view-specific temporal reasoning while encouraging
consistent comprehension across viewpoints. Our method demonstrates its
superiority over naive SFT and GRPO, especially for improving cross-view
consistency. All resources will be made publicly available.

</details>


### [18] [OracleAgent: A Multimodal Reasoning Agent for Oracle Bone Script Research](https://arxiv.org/abs/2510.26114)
*Caoshuo Li,Zengmao Ding,Xiaobin Hu,Bang Li,Donghao Luo,Xu Peng,Taisong Jin,Yongge Liu,Shengwei Han,Jing Yang,Xiaoping He,Feng Gao,AndyPian Wu,SevenShu,Chaoyang Wang,Chengjie Wang*

Main category: cs.CV

TL;DR: OracleAgent is the first agent system for Oracle Bone Script (OBS) research, integrating LLM-powered analysis tools with a comprehensive multimodal knowledge base to address workflow complexity and retrieval efficiency challenges.


<details>
  <summary>Details</summary>
Motivation: Current OBS research faces challenges: (1) complex interpretation workflow with multiple serial/parallel sub-tasks, and (2) inefficient information organization/retrieval where scholars spend substantial effort searching and managing resources.

Method: Developed OracleAgent system that integrates multiple OBS analysis tools powered by LLMs, with flexible orchestration. Built a comprehensive domain-specific multimodal knowledge base through multi-year data collection, cleaning, and expert annotation containing 1.4M single-character rubbing images and 80K interpretation texts.

Result: OracleAgent achieves superior performance across multimodal reasoning and generation tasks, surpassing leading MLLMs like GPT-4o. Case studies show it effectively assists domain experts, significantly reducing OBS research time cost.

Conclusion: OracleAgent represents a significant step toward practical deployment of OBS-assisted research and automated interpretation systems, demonstrating effective integration of multimodal tools and domain knowledge.

Abstract: As one of the earliest writing systems, Oracle Bone Script (OBS) preserves
the cultural and intellectual heritage of ancient civilizations. However,
current OBS research faces two major challenges: (1) the interpretation of OBS
involves a complex workflow comprising multiple serial and parallel sub-tasks,
and (2) the efficiency of OBS information organization and retrieval remains a
critical bottleneck, as scholars often spend substantial effort searching for,
compiling, and managing relevant resources. To address these challenges, we
present OracleAgent, the first agent system designed for the structured
management and retrieval of OBS-related information. OracleAgent seamlessly
integrates multiple OBS analysis tools, empowered by large language models
(LLMs), and can flexibly orchestrate these components. Additionally, we
construct a comprehensive domain-specific multimodal knowledge base for OBS,
which is built through a rigorous multi-year process of data collection,
cleaning, and expert annotation. The knowledge base comprises over 1.4M
single-character rubbing images and 80K interpretation texts. OracleAgent
leverages this resource through its multimodal tools to assist experts in
retrieval tasks of character, document, interpretation text, and rubbing image.
Extensive experiments demonstrate that OracleAgent achieves superior
performance across a range of multimodal reasoning and generation tasks,
surpassing leading mainstream multimodal large language models (MLLMs) (e.g.,
GPT-4o). Furthermore, our case study illustrates that OracleAgent can
effectively assist domain experts, significantly reducing the time cost of OBS
research. These results highlight OracleAgent as a significant step toward the
practical deployment of OBS-assisted research and automated interpretation
systems.

</details>


### [19] [JOGS: Joint Optimization of Pose Estimation and 3D Gaussian Splatting](https://arxiv.org/abs/2510.26117)
*Yuxuan Li,Tao Wang,Xianben Yang*

Main category: cs.CV

TL;DR: A unified framework that jointly optimizes 3D Gaussian points and camera poses without requiring pre-calibrated camera inputs, eliminating the need for external pose estimation tools like COLMAP.


<details>
  <summary>Details</summary>
Motivation: Traditional novel view synthesis methods rely on external camera pose estimation tools (e.g., COLMAP), which introduce computational bottlenecks and propagate errors. This creates challenges in scenarios with large viewpoint variations and sparse feature distributions.

Method: Joint optimization framework with interleaved phases: 1) Update 3D Gaussian parameters via differentiable rendering with fixed poses, 2) Refine camera poses using customized 3D optical flow algorithm incorporating geometric and photometric constraints. Progressive reduction of projection errors through iterative co-optimization.

Result: Significantly outperforms existing COLMAP-free techniques in reconstruction quality, and surpasses standard COLMAP-based baseline in general performance across multiple datasets.

Conclusion: The proposed unified framework successfully eliminates dependency on external pose estimation tools while achieving superior scene reconstruction fidelity and pose accuracy through joint optimization of 3D Gaussians and camera poses.

Abstract: Traditional novel view synthesis methods heavily rely on external camera pose
estimation tools such as COLMAP, which often introduce computational
bottlenecks and propagate errors. To address these challenges, we propose a
unified framework that jointly optimizes 3D Gaussian points and camera poses
without requiring pre-calibrated inputs. Our approach iteratively refines 3D
Gaussian parameters and updates camera poses through a novel co-optimization
strategy, ensuring simultaneous improvements in scene reconstruction fidelity
and pose accuracy. The key innovation lies in decoupling the joint optimization
into two interleaved phases: first, updating 3D Gaussian parameters via
differentiable rendering with fixed poses, and second, refining camera poses
using a customized 3D optical flow algorithm that incorporates geometric and
photometric constraints. This formulation progressively reduces projection
errors, particularly in challenging scenarios with large viewpoint variations
and sparse feature distributions, where traditional methods struggle. Extensive
evaluations on multiple datasets demonstrate that our approach significantly
outperforms existing COLMAP-free techniques in reconstruction quality, and also
surpasses the standard COLMAP-based baseline in general.

</details>


### [20] [WOD-E2E: Waymo Open Dataset for End-to-End Driving in Challenging Long-tail Scenarios](https://arxiv.org/abs/2510.26125)
*Runsheng Xu,Hubert Lin,Wonseok Jeon,Hao Feng,Yuliang Zou,Liting Sun,John Gorman,Kate Tolstaya,Sarah Tang,Brandyn White,Ben Sapp,Mingxing Tan,Jyh-Jing Hwang,Drago Anguelov*

Main category: cs.CV

TL;DR: The paper introduces WOD-E2E, a new benchmark dataset for end-to-end driving focused on challenging long-tail scenarios, along with a novel evaluation metric called Rater Feedback Score (RFS) that measures trajectory preference alignment rather than just waypoint distance.


<details>
  <summary>Details</summary>
Motivation: Current E2E driving benchmarks primarily test nominal scenarios and existing metrics fail to adequately evaluate performance in rare, challenging long-tail situations, which is crucial for developing robust autonomous driving systems.

Method: Created WOD-E2E dataset with 4,021 driving segments (12 hours) specifically curated for long-tail scenarios occurring less than 0.03% frequency. Includes routing info, ego states, and 360-degree camera views. Proposed RFS metric that evaluates trajectory predictions against human rater preference labels.

Result: Released a comprehensive dataset for challenging driving scenarios and a novel evaluation framework that better captures driving performance in complex situations. The dataset and challenge are publicly available.

Conclusion: WOD-E2E aims to advance research in generalizable, robust, and safe end-to-end autonomous driving by providing better evaluation tools for handling complex real-world situations, particularly rare long-tail scenarios.

Abstract: Vision-based end-to-end (E2E) driving has garnered significant interest in
the research community due to its scalability and synergy with multimodal large
language models (MLLMs). However, current E2E driving benchmarks primarily
feature nominal scenarios, failing to adequately test the true potential of
these systems. Furthermore, existing open-loop evaluation metrics often fall
short in capturing the multi-modal nature of driving or effectively evaluating
performance in long-tail scenarios. To address these gaps, we introduce the
Waymo Open Dataset for End-to-End Driving (WOD-E2E). WOD-E2E contains 4,021
driving segments (approximately 12 hours), specifically curated for challenging
long-tail scenarios that that are rare in daily life with an occurring
frequency of less than 0.03%. Concretely, each segment in WOD-E2E includes the
high-level routing information, ego states, and 360-degree camera views from 8
surrounding cameras. To evaluate the E2E driving performance on these long-tail
situations, we propose a novel open-loop evaluation metric: Rater Feedback
Score (RFS). Unlike conventional metrics that measure the distance between
predicted way points and the logs, RFS measures how closely the predicted
trajectory matches rater-annotated trajectory preference labels. We have
released rater preference labels for all WOD-E2E validation set segments, while
the held out test set labels have been used for the 2025 WOD-E2E Challenge.
Through our work, we aim to foster state of the art research into
generalizable, robust, and safe end-to-end autonomous driving agents capable of
handling complex real-world situations.

</details>


### [21] [Exploring Object-Aware Attention Guided Frame Association for RGB-D SLAM](https://arxiv.org/abs/2510.26131)
*Ali Caglayan,Nevrez Imamoglu,Oguzhan Guclu,Ali Osman Serhatoglu,Ahmet Burak Can,Ryosuke Nakamura*

Main category: cs.CV

TL;DR: The paper proposes integrating gradient-based attention information with CNN features to improve RGB-D indoor SLAM performance, showing enhanced frame association especially in large environments.


<details>
  <summary>Details</summary>
Motivation: Current CNN representations don't fully utilize gradient-based attention information for semantic object understanding, which could benefit visual tasks like SLAM where attentive object locations can enhance performance.

Method: Integrate layer-wise attention information derived from network gradients with CNN feature representations to improve frame association in RGB-D indoor SLAM.

Result: Experimental results show improved performance compared to baseline methods, with particular benefits in large environments.

Conclusion: Integrating task-specific network attention with CNN features effectively enhances SLAM performance, demonstrating the value of gradient-based attention for visual understanding tasks.

Abstract: Attention models have recently emerged as a powerful approach, demonstrating
significant progress in various fields. Visualization techniques, such as class
activation mapping, provide visual insights into the reasoning of convolutional
neural networks (CNNs). Using network gradients, it is possible to identify
regions where the network pays attention during image recognition tasks.
Furthermore, these gradients can be combined with CNN features to localize more
generalizable, task-specific attentive (salient) regions within scenes.
However, explicit use of this gradient-based attention information integrated
directly into CNN representations for semantic object understanding remains
limited. Such integration is particularly beneficial for visual tasks like
simultaneous localization and mapping (SLAM), where CNN representations
enriched with spatially attentive object locations can enhance performance. In
this work, we propose utilizing task-specific network attention for RGB-D
indoor SLAM. Specifically, we integrate layer-wise attention information
derived from network gradients with CNN feature representations to improve
frame association performance. Experimental results indicate improved
performance compared to baseline methods, particularly for large environments.

</details>


### [22] [FullPart: Generating each 3D Part at Full Resolution](https://arxiv.org/abs/2510.26140)
*Lihe Ding,Shaocong Dong,Yaokun Li,Chenjian Gao,Xiao Chen,Rui Han,Yihao Kuang,Hong Zhang,Bo Huang,Zhanpeng Huang,Zibin Wang,Dan Xu,Tianfan Xue*

Main category: cs.CV

TL;DR: FullPart combines implicit and explicit paradigms for 3D part generation, using implicit diffusion for bounding box layout and full-resolution voxel grids for detailed part generation, achieving state-of-the-art results.


<details>
  <summary>Details</summary>
Motivation: Previous part generators using implicit vector-set tokens lack geometric details, while explicit voxel approaches with shared global grids degrade small part quality due to insufficient voxel allocation.

Method: Uses implicit box vector-set diffusion for bounding box layout, then generates each part in its own fixed full-resolution voxel grid. Introduces center-point encoding to handle part size variations and maintain global coherence.

Result: Achieves state-of-the-art results in 3D part generation, enabling synthesis of intricate details even for small parts by generating them at full resolution.

Conclusion: FullPart effectively combines implicit and explicit approaches, with full-resolution part generation and center-point encoding addressing previous limitations. The PartVerse-XL dataset supports training and future research.

Abstract: Part-based 3D generation holds great potential for various applications.
Previous part generators that represent parts using implicit vector-set tokens
often suffer from insufficient geometric details. Another line of work adopts
an explicit voxel representation but shares a global voxel grid among all
parts; this often causes small parts to occupy too few voxels, leading to
degraded quality. In this paper, we propose FullPart, a novel framework that
combines both implicit and explicit paradigms. It first derives the bounding
box layout through an implicit box vector-set diffusion process, a task that
implicit diffusion handles effectively since box tokens contain little
geometric detail. Then, it generates detailed parts, each within its own fixed
full-resolution voxel grid. Instead of sharing a global low-resolution space,
each part in our method - even small ones - is generated at full resolution,
enabling the synthesis of intricate details. We further introduce a
center-point encoding strategy to address the misalignment issue when
exchanging information between parts of different actual sizes, thereby
maintaining global coherence. Moreover, to tackle the scarcity of reliable part
data, we present PartVerse-XL, the largest human-annotated 3D part dataset to
date with 40K objects and 320K parts. Extensive experiments demonstrate that
FullPart achieves state-of-the-art results in 3D part generation. We will
release all code, data, and model to benefit future research in 3D part
generation.

</details>


### [23] [BasicAVSR: Arbitrary-Scale Video Super-Resolution via Image Priors and Enhanced Motion Compensation](https://arxiv.org/abs/2510.26149)
*Wei Shang,Wanying Zhang,Shuhang Gu,Pengfei Zhu,Qinghua Hu,Dongwei Ren*

Main category: cs.CV

TL;DR: BasicAVSR is a strong baseline for arbitrary-scale video super-resolution that integrates adaptive multi-scale frequency priors, flow-guided propagation, second-order motion compensation, and hyper-upsampling units to achieve superior performance across different scenarios.


<details>
  <summary>Details</summary>
Motivation: Arbitrary-scale video super-resolution faces challenges in spatial detail reproduction, temporal consistency, and computational complexity. The paper aims to create a versatile solution that can handle various scaling factors while maintaining high quality and efficiency.

Method: The method integrates four key components: 1) adaptive multi-scale frequency priors from image Laplacian pyramids, 2) flow-guided propagation unit for spatiotemporal information aggregation, 3) second-order motion compensation for accurate frame alignment, and 4) hyper-upsampling unit for scale-aware upsampling kernels. Three propagation variants are instantiated for different application scenarios.

Result: Experimental results show BasicAVSR significantly outperforms existing methods in super-resolution quality, generalization ability, and inference speed. The model demonstrates effectiveness and adaptability across different scenarios including online inference, limited lookahead, and offline processing.

Conclusion: BasicAVSR advances the state-of-the-art in arbitrary-scale video super-resolution and extends its core components to multiple frameworks for diverse scenarios, providing a strong baseline solution with superior performance and flexibility.

Abstract: Arbitrary-scale video super-resolution (AVSR) aims to enhance the resolution
of video frames, potentially at various scaling factors, which presents several
challenges regarding spatial detail reproduction, temporal consistency, and
computational complexity. In this paper, we propose a strong baseline BasicAVSR
for AVSR by integrating four key components: 1) adaptive multi-scale frequency
priors generated from image Laplacian pyramids, 2) a flow-guided propagation
unit to aggregate spatiotemporal information from adjacent frames, 3) a
second-order motion compensation unit for more accurate spatial alignment of
adjacent frames, and 4) a hyper-upsampling unit to generate scale-aware and
content-independent upsampling kernels. To meet diverse application demands, we
instantiate three propagation variants: (i) a unidirectional RNN unit for
strictly online inference, (ii) a unidirectional RNN unit empowered with a
limited lookahead that tolerates a small output delay, and (iii) a
bidirectional RNN unit designed for offline tasks where computational resources
are less constrained. Experimental results demonstrate the effectiveness and
adaptability of our model across these different scenarios. Through extensive
experiments, we show that BasicAVSR significantly outperforms existing methods
in terms of super-resolution quality, generalization ability, and inference
speed. Our work not only advances the state-of-the-art in AVSR but also extends
its core components to multiple frameworks for diverse scenarios. The code is
available at https://github.com/shangwei5/BasicAVSR.

</details>


### [24] [MV-MLM: Bridging Multi-View Mammography and Language for Breast Cancer Diagnosis and Risk Prediction](https://arxiv.org/abs/2510.26151)
*Shunjie-Fabian Zheng,Hyeonjun Lee,Thijs Kooi,Ali Diba*

Main category: cs.CV

TL;DR: A novel Multi-View Mammography and Language Model (MV-MLM) that uses cross-modal self-supervision with synthetic radiology reports to achieve state-of-the-art performance in breast cancer classification and risk prediction tasks.


<details>
  <summary>Details</summary>
Motivation: Large annotated datasets for breast cancer CAD models are costly and time-consuming to acquire. Vision-Language Models offer a promising solution by enhancing robustness and data efficiency in medical imaging.

Method: Leverages multi-view supervision with paired mammogram images and synthetic radiology reports, using cross-modal self-supervision across image-text pairs and a joint visual-textual learning strategy.

Result: Achieves state-of-the-art performance in malignancy classification, subtype classification, and image-based cancer risk prediction on both private and public datasets, with strong data efficiency outperforming fully supervised or VLM baselines.

Conclusion: The proposed MV-MLM demonstrates effective use of synthetic text reports and multi-view supervision for robust breast cancer analysis without requiring actual radiology reports.

Abstract: Large annotated datasets are essential for training robust Computer-Aided
Diagnosis (CAD) models for breast cancer detection or risk prediction. However,
acquiring such datasets with fine-detailed annotation is both costly and
time-consuming. Vision-Language Models (VLMs), such as CLIP, which are
pre-trained on large image-text pairs, offer a promising solution by enhancing
robustness and data efficiency in medical imaging tasks. This paper introduces
a novel Multi-View Mammography and Language Model for breast cancer
classification and risk prediction, trained on a dataset of paired mammogram
images and synthetic radiology reports. Our MV-MLM leverages multi-view
supervision to learn rich representations from extensive radiology data by
employing cross-modal self-supervision across image-text pairs. This includes
multiple views and the corresponding pseudo-radiology reports. We propose a
novel joint visual-textual learning strategy to enhance generalization and
accuracy performance over different data types and tasks to distinguish breast
tissues or cancer characteristics(calcification, mass) and utilize these
patterns to understand mammography images and predict cancer risk. We evaluated
our method on both private and publicly available datasets, demonstrating that
the proposed model achieves state-of-the-art performance in three
classification tasks: (1) malignancy classification, (2) subtype
classification, and (3) image-based cancer risk prediction. Furthermore, the
model exhibits strong data efficiency, outperforming existing fully supervised
or VLM baselines while trained on synthetic text reports and without the need
for actual radiology reports.

</details>


### [25] [Detecting Unauthorized Vehicles using Deep Learning for Smart Cities: A Case Study on Bangladesh](https://arxiv.org/abs/2510.26154)
*Sudipto Das Sukanto,Diponker Roy,Fahim Shakil,Nirjhar Singha,Abdullah Asik,Aniket Joarder,Mridha Md Nafis Fuad,Muhammad Ibrahim*

Main category: cs.CV

TL;DR: A machine learning approach using YOLOv8 for real-time auto-rickshaw detection in Bangladesh traffic images, achieving 83.447% mAP50 and over 78% precision/recall.


<details>
  <summary>Details</summary>
Motivation: Auto-rickshaws in South Asian countries need monitoring due to traffic restrictions, but existing surveillance struggles with distinguishing them from similar vehicles and manual analysis is inefficient.

Method: Used YOLOv8 model for real-time object detection, trained on 1,730 annotated images captured under various traffic conditions.

Result: Model achieved mAP50 of 83.447% and binary precision/recall values above 78%, performing well in both dense and sparse traffic scenarios.

Conclusion: The proposed system effectively automates auto-rickshaw detection and the dataset has been publicly released for further research.

Abstract: Modes of transportation vary across countries depending on geographical
location and cultural context. In South Asian countries rickshaws are among the
most common means of local transport. Based on their mode of operation,
rickshaws in cities across Bangladesh can be broadly classified into non-auto
(pedal-powered) and auto-rickshaws (motorized). Monitoring the movement of
auto-rickshaws is necessary as traffic rules often restrict auto-rickshaws from
accessing certain routes. However, existing surveillance systems make it quite
difficult to monitor them due to their similarity to other vehicles, especially
non-auto rickshaws whereas manual video analysis is too time-consuming. This
paper presents a machine learning-based approach to automatically detect
auto-rickshaws in traffic images. In this system, we used real-time object
detection using the YOLOv8 model. For training purposes, we prepared a set of
1,730 annotated images that were captured under various traffic conditions. The
results show that our proposed model performs well in real-time auto-rickshaw
detection and offers an mAP50 of 83.447% and binary precision and recall values
above 78%, demonstrating its effectiveness in handling both dense and sparse
traffic scenarios. The dataset has been publicly released for further research.

</details>


### [26] [CRAG-MM: Multi-modal Multi-turn Comprehensive RAG Benchmark](https://arxiv.org/abs/2510.26160)
*Jiaqi Wang,Xiao Yang,Kai Sun,Parth Suresh,Sanat Sharma,Adam Czyzewski,Derek Andersen,Surya Appini,Arkav Banerjee,Sajal Choudhary,Shervin Ghasemlou,Ziqiang Guan,Akil Iyer,Haidar Khan,Lingkun Kong,Roy Luo,Tiffany Ma,Zhen Qiao,David Tran,Wenfang Xu,Skyler Yeatman,Chen Zhou,Gunveer Gujral,Yinglong Xia,Shane Moon,Nicolas Scheffer,Nirav Shah,Eun Chang,Yue Liu,Florian Metze,Tammy Stark,Zhaleh Feizollahi,Andrea Jessee,Mangesh Pujari,Ahmed Aly,Babak Damavandi,Rakesh Wanga,Anuj Kumar,Rohit Patel,Wen-tau Yih,Xin Luna Dong*

Main category: cs.CV

TL;DR: CRAG-MM is a comprehensive benchmark for multi-modal RAG with 6.5K image-question-answer triplets and 2K multi-turn conversations across 13 domains, designed to evaluate wearable device scenarios with realistic challenges.


<details>
  <summary>Details</summary>
Motivation: There is no comprehensive benchmark for multi-modal RAG tasks, especially for wearable device scenarios where users interact with visual entities in their surroundings.

Method: Created CRAG-MM benchmark with diverse datasets including 6.2K egocentric images, carefully constructed questions reflecting real-world challenges (image quality issues, question types, entity popularity, information dynamism), and designed three evaluation tasks with retrieval corpora and APIs.

Result: Baseline RAG approaches achieved only 32% truthfulness on single-turn QA and 43% on multi-turn QA, while state-of-the-art industry solutions showed similar performance (32%/45%), indicating significant room for improvement.

Conclusion: CRAG-MM successfully highlights the limitations of current multi-modal RAG systems and has already impacted the field through KDD Cup 2025, where winning solutions improved baseline performance by 28%.

Abstract: Wearable devices such as smart glasses are transforming the way people
interact with their surroundings, enabling users to seek information regarding
entities in their view. Multi-Modal Retrieval-Augmented Generation (MM-RAG)
plays a key role in supporting such questions, yet there is still no
comprehensive benchmark for this task, especially regarding wearables
scenarios. To fill this gap, we present CRAG-MM -- a Comprehensive RAG
benchmark for Multi-modal Multi-turn conversations. CRAG-MM contains a diverse
set of 6.5K (image, question, answer) triplets and 2K visual-based multi-turn
conversations across 13 domains, including 6.2K egocentric images designed to
mimic captures from wearable devices. We carefully constructed the questions to
reflect real-world scenarios and challenges, including five types of
image-quality issues, six question types, varying entity popularity, differing
information dynamism, and different conversation turns. We design three tasks:
single-source augmentation, multi-source augmentation, and multi-turn
conversations -- each paired with an associated retrieval corpus and APIs for
both image-KG retrieval and webpage retrieval. Our evaluation shows that
straightforward RAG approaches achieve only 32% and 43% truthfulness on CRAG-MM
single- and multi-turn QA, respectively, whereas state-of-the-art industry
solutions have similar quality (32%/45%), underscoring ample room for
improvement. The benchmark has hosted KDD Cup 2025, attracting about 1K
participants and 5K submissions, with winning solutions improving baseline
performance by 28%, highlighting its early impact on advancing the field.

</details>


### [27] [MoTDiff: High-resolution Motion Trajectory estimation from a single blurred image using Diffusion models](https://arxiv.org/abs/2510.26173)
*Wontae Choi,Jaelin Lee,Hyung Sup Yun,Byeungwoo Jeon,Il Yong Chun*

Main category: cs.CV

TL;DR: MoTDiff is a high-resolution motion trajectory estimation framework using diffusion models that extracts fine-grained motion information from single motion-blurred images, outperforming state-of-the-art methods in blind image deblurring and coded exposure photography.


<details>
  <summary>Details</summary>
Motivation: Existing motion representations from single blurred images are often low quality - coarse-grained and inaccurate. There's a need for high-resolution, high-quality motion trajectory estimation to improve computational imaging and computer vision applications.

Method: Proposes MoTDiff framework with two key components: 1) conditional diffusion framework using multi-scale feature maps from blurred images as condition, and 2) training method promoting precise identification of fine-grained motion trajectory, consistent shape/position estimation, and pixel connectivity along trajectories.

Result: MoTDiff outperforms state-of-the-art methods in both blind image deblurring and coded exposure photography applications, demonstrating superior motion trajectory estimation capabilities.

Conclusion: The proposed MoTDiff framework successfully achieves high-resolution motion trajectory estimation from single motion-blurred images, representing a significant advancement over existing low-quality motion representations.

Abstract: Accurate estimation of motion information is crucial in diverse computational
imaging and computer vision applications. Researchers have investigated various
methods to extract motion information from a single blurred image, including
blur kernels and optical flow. However, existing motion representations are
often of low quality, i.e., coarse-grained and inaccurate. In this paper, we
propose the first high-resolution (HR) Motion Trajectory estimation framework
using Diffusion models (MoTDiff). Different from existing motion
representations, we aim to estimate an HR motion trajectory with high-quality
from a single motion-blurred image. The proposed MoTDiff consists of two key
components: 1) a new conditional diffusion framework that uses multi-scale
feature maps extracted from a single blurred image as a condition, and 2) a new
training method that can promote precise identification of a fine-grained
motion trajectory, consistent estimation of overall shape and position of a
motion path, and pixel connectivity along a motion trajectory. Our experiments
demonstrate that the proposed MoTDiff can outperform state-of-the-art methods
in both blind image deblurring and coded exposure photography applications.

</details>


### [28] [ConceptScope: Characterizing Dataset Bias via Disentangled Visual Concepts](https://arxiv.org/abs/2510.26186)
*Jinho Choi,Hyesu Lim,Steffen Schneider,Jaegul Choo*

Main category: cs.CV

TL;DR: ConceptScope is an automated framework that uses Sparse Autoencoders on vision foundation models to discover and quantify human-interpretable concepts in datasets, enabling systematic bias identification and dataset analysis without requiring fine-grained annotations.


<details>
  <summary>Details</summary>
Motivation: Dataset bias is common in machine learning but challenging to identify without costly fine-grained annotations. There's a need for scalable, automated methods to systematically discover and quantify biases in visual datasets.

Method: Uses Sparse Autoencoders trained on representations from vision foundation models to discover interpretable concepts. Categorizes concepts into target, context, and bias types based on semantic relevance and statistical correlation to class labels. Enables class-level dataset characterization and concept-based subgrouping.

Result: Validated to capture diverse visual concepts (objects, textures, backgrounds, facial attributes, emotions, actions). Concept activations produce spatial attributions aligned with meaningful image regions. Successfully detected known biases (background bias in Waterbirds) and uncovered new ones (co-occurring objects in ImageNet).

Conclusion: ConceptScope provides a practical tool for automated dataset auditing and model diagnostics, offering scalable bias detection without requiring expensive annotations.

Abstract: Dataset bias, where data points are skewed to certain concepts, is ubiquitous
in machine learning datasets. Yet, systematically identifying these biases is
challenging without costly, fine-grained attribute annotations. We present
ConceptScope, a scalable and automated framework for analyzing visual datasets
by discovering and quantifying human-interpretable concepts using Sparse
Autoencoders trained on representations from vision foundation models.
ConceptScope categorizes concepts into target, context, and bias types based on
their semantic relevance and statistical correlation to class labels, enabling
class-level dataset characterization, bias identification, and robustness
evaluation through concept-based subgrouping. We validate that ConceptScope
captures a wide range of visual concepts, including objects, textures,
backgrounds, facial attributes, emotions, and actions, through comparisons with
annotated datasets. Furthermore, we show that concept activations produce
spatial attributions that align with semantically meaningful image regions.
ConceptScope reliably detects known biases (e.g., background bias in
Waterbirds) and uncovers previously unannotated ones (e.g, co-occurring objects
in ImageNet), offering a practical tool for dataset auditing and model
diagnostics.

</details>


### [29] [Sketch2PoseNet: Efficient and Generalized Sketch to 3D Human Pose Prediction](https://arxiv.org/abs/2510.26196)
*Li Wang,Yiyu Zhuang,Yanwen Wang,Xun Cao,Chuan Guo,Xinxin Zuo,Hao Zhu*

Main category: cs.CV

TL;DR: A novel approach for 3D human pose estimation from sketches using a 'learn from synthesis' strategy that creates synthetic sketch-3D pose pairs and combines diffusion models with neural networks for efficient estimation.


<details>
  <summary>Details</summary>
Motivation: Traditional sketch-to-pose methods face challenges due to abstract and disproportionate sketch nature, lack of large-scale annotations, and reliance on time-consuming optimization with limited generalizability.

Method: Train diffusion model to synthesize sketches from 2D poses, create SKEP-120K synthetic dataset, combine 2D pose detectors and diffusion priors with feed-forward neural network, and use heuristic loss functions for geometric coherence.

Result: Model substantially surpasses previous methods in both estimation accuracy and speed for sketch-to-pose tasks, as shown by qualitative, quantitative, and subjective evaluations.

Conclusion: The proposed data-driven framework effectively addresses sketch-to-pose challenges through synthetic data generation and achieves superior performance compared to previous optimization-based approaches.

Abstract: 3D human pose estimation from sketches has broad applications in computer
animation and film production. Unlike traditional human pose estimation, this
task presents unique challenges due to the abstract and disproportionate nature
of sketches. Previous sketch-to-pose methods, constrained by the lack of
large-scale sketch-3D pose annotations, primarily relied on optimization with
heuristic rules-an approach that is both time-consuming and limited in
generalizability. To address these challenges, we propose a novel approach
leveraging a "learn from synthesis" strategy. First, a diffusion model is
trained to synthesize sketch images from 2D poses projected from 3D human
poses, mimicking disproportionate human structures in sketches. This process
enables the creation of a synthetic dataset, SKEP-120K, consisting of 120k
accurate sketch-3D pose annotation pairs across various sketch styles. Building
on this synthetic dataset, we introduce an end-to-end data-driven framework for
estimating human poses and shapes from diverse sketch styles. Our framework
combines existing 2D pose detectors and generative diffusion priors for sketch
feature extraction with a feed-forward neural network for efficient 2D pose
estimation. Multiple heuristic loss functions are incorporated to guarantee
geometric coherence between the derived 3D poses and the detected 2D poses
while preserving accurate self-contacts. Qualitative, quantitative, and
subjective evaluations collectively show that our model substantially surpasses
previous ones in both estimation accuracy and speed for sketch-to-pose tasks.

</details>


### [30] [Developing a Multi-task Ensemble Geometric Deep Network for Supply Chain Sustainability and Risk Management](https://arxiv.org/abs/2510.26203)
*Mehdi Khaleghi,Nastaran Khaleghi,Sobhan Sheykhivand,Sebelan Danishvar*

Main category: cs.CV

TL;DR: The paper proposes a Chebyshev ensemble geometric network (Ch-EGN) for supply chain sustainability, achieving high accuracy in risk management (98.95%), product classification (100%), and relationship classification (98.07%).


<details>
  <summary>Details</summary>
Motivation: Supply chain sustainability and risk management are crucial for optimal performance. Proper product classification and leveraging deep learning advancements can enhance supply chain efficiency and sustainability.

Method: A novel hybrid Chebyshev ensemble geometric network (Ch-EGN) combining convolutional and geometric deep learning to analyze information dependencies in supply chain data from SupplyGraph Dataset and DataCo.

Result: Achieved 98.95% accuracy for risk management, 100% for 5-product group classification, 98.07% for 4-product relation classification, and 92.37% for 25-company relation classification, outperforming state-of-the-art methods.

Conclusion: The proposed Ch-EGN method effectively enhances supply chain sustainability and risk management through superior classification accuracy and outperforms existing approaches.

Abstract: The sustainability of supply chain plays a key role in achieving optimal
performance in controlling the supply chain. The management of risks that occur
in a supply chain is a fundamental problem for the purpose of developing the
sustainability of the network and elevating the performance efficiency of the
supply chain. The correct classification of products is another essential
element in a sustainable supply chain. Acknowledging recent breakthroughs in
the context of deep networks, several architectural options have been deployed
to analyze supply chain datasets. A novel geometric deep network is used to
propose an ensemble deep network. The proposed Chebyshev ensemble geometric
network (Ch-EGN) is a hybrid convolutional and geometric deep learning. This
network is proposed to leverage the information dependencies in supply chain to
derive invisible states of samples in the database. The functionality of the
proposed deep network is assessed on the two different databases. The
SupplyGraph Dataset and DataCo are considered in this research. The prediction
of delivery status of DataCo supply chain is done for risk administration. The
product classification and edge classification are performed using the
SupplyGraph database to enhance the sustainability of the supply network. An
average accuracy of 98.95% is obtained for the ensemble network for risk
management. The average accuracy of 100% and 98.07% are obtained for
sustainable supply chain in terms of 5 product group classification and 4
product relation classification, respectively. The average accuracy of 92.37%
is attained for 25 company relation classification. The results confirm an
average improvement and efficiency of the proposed method compared to the
state-of-the-art approaches.

</details>


### [31] [OmniLayout: Enabling Coarse-to-Fine Learning with LLMs for Universal Document Layout Generation](https://arxiv.org/abs/2510.26213)
*Hengrui Kang,Zhuangcheng Gu,Zhiyuan Zhao,Zichen Wen,Bin Wang,Weijia Li,Conghui He*

Main category: cs.CV

TL;DR: The paper introduces OmniLayout-1M, a million-scale dataset for diverse document layout generation, and OmniLayout-LLM, a 0.5B model with a coarse-to-fine learning paradigm that achieves state-of-the-art performance across multiple domains.


<details>
  <summary>Details</summary>
Motivation: Document layout generation remains underexplored compared to document layout analysis, with existing datasets dominated by academic papers and lacking diversity in real-world document types like newspapers and magazines.

Method: Proposes OmniLayout-LLM, a 0.5B model with a two-stage coarse-to-fine learning paradigm: 1) learning universal layout principles from the OmniLayout-1M dataset with coarse categories, and 2) transferring knowledge to specific domains with fine-grained annotations.

Result: The approach achieves strong performance on multiple domains in the M$^{6}$Doc dataset, substantially surpassing both existing layout generation experts and several latest general-purpose LLMs.

Conclusion: The work addresses the scarcity of diverse layout data and introduces an effective method for document layout generation that outperforms existing approaches, with code, models, and dataset to be publicly released.

Abstract: Document AI has advanced rapidly and is attracting increasing attention. Yet,
while most efforts have focused on document layout analysis (DLA), its
generative counterpart, document layout generation, remains underexplored. A
major obstacle lies in the scarcity of diverse layouts: academic papers with
Manhattan-style structures dominate existing studies, while open-world genres
such as newspapers and magazines remain severely underrepresented. To address
this gap, we curate OmniLayout-1M, the first million-scale dataset of diverse
document layouts, covering six common document types and comprising
contemporary layouts collected from multiple sources. Moreover, since existing
methods struggle in complex domains and often fail to arrange long sequences
coherently, we introduce OmniLayout-LLM, a 0.5B model with designed two-stage
Coarse-to-Fine learning paradigm: 1) learning universal layout principles from
OmniLayout-1M with coarse category definitions, and 2) transferring the
knowledge to a specific domain with fine-grained annotations. Extensive
experiments demonstrate that our approach achieves strong performance on
multiple domains in M$^{6}$Doc dataset, substantially surpassing both existing
layout generation experts and several latest general-purpose LLMs. Our code,
models, and dataset will be publicly released.

</details>


### [32] [Which Way Does Time Flow? A Psychophysics-Grounded Evaluation for Vision-Language Models](https://arxiv.org/abs/2510.26241)
*Shiho Matta,Lis Kanashiro Pereira,Peitao Han,Fei Cheng,Shigeru Kitazawa*

Main category: cs.CV

TL;DR: Vision-language models struggle with temporal reasoning, performing poorly on judging whether videos are played forward or backward, especially on physically irreversible processes and causal actions where humans excel.


<details>
  <summary>Details</summary>
Motivation: To evaluate and expose the weak temporal understanding in modern vision-language models, particularly their inability to grasp temporal direction in videos compared to human capabilities.

Method: Created AoT-PsyPhyBENCH benchmark with psychophysically validated stimuli to test VLMs' ability to judge arrow of time (forward vs backward video playback) using the same behavioral baselines established for human studies.

Result: Most VLMs perform near chance level, with even the best models lagging far behind human accuracy on physically irreversible processes (free fall, diffusion/explosion) and causal manual actions that humans recognize almost instantly.

Conclusion: Current multimodal systems lack inductive biases for temporal continuity and causal understanding, highlighting a fundamental gap in their temporal reasoning capabilities despite capturing rich visual-semantic correlations.

Abstract: Modern vision-language models (VLMs) excel at many multimodal tasks, yet
their grasp of temporal information in video remains weak and, crucially,
under-evaluated. We probe this gap with a deceptively simple but revealing
challenge: judging the arrow of time (AoT)-whether a short clip is played
forward or backward. We introduce AoT-PsyPhyBENCH, a psychophysically validated
benchmark that tests whether VLMs can infer temporal direction in natural
videos using the same stimuli and behavioral baselines established for humans.
Our comprehensive evaluation of open-weight and proprietary, reasoning and
non-reasoning VLMs reveals that most models perform near chance, and even the
best lag far behind human accuracy on physically irreversible processes (e.g.,
free fall, diffusion/explosion) and causal manual actions (division/addition)
that humans recognize almost instantly. These results highlight a fundamental
gap in current multimodal systems: while they capture rich visual-semantic
correlations, they lack the inductive biases required for temporal continuity
and causal understanding. We release the code and data for AoT-PsyPhyBENCH to
encourage further progress in the physical and temporal reasoning capabilities
of VLMs.

</details>


### [33] [Revisiting Generative Infrared and Visible Image Fusion Based on Human Cognitive Laws](https://arxiv.org/abs/2510.26268)
*Lin Guo,Xiaoqing Luo,Wei Xie,Zhancheng Zhang,Hui Li,Rui Wang,Zhenhua Feng,Xiaoning Song*

Main category: cs.CV

TL;DR: HCLFuse is a novel infrared and visible image fusion method inspired by human cognitive laws, using multi-scale mask-regulated variational bottleneck encoder and diffusion model with physical guidance to achieve state-of-the-art fusion performance.


<details>
  <summary>Details</summary>
Motivation: Existing fusion methods struggle with balancing modal information and lack interpretability in modal selection, affecting reliability in complex scenarios. Generative methods have limited capabilities.

Method: Proposes HCLFuse with multi-scale mask-regulated variational bottleneck encoder for information decomposition, and integrates diffusion model with physical laws for time-varying guidance during generation.

Result: Achieves state-of-the-art fusion performance in qualitative and quantitative evaluations across multiple datasets, significantly improving semantic segmentation metrics.

Conclusion: The method demonstrates advantages in enhancing structural consistency and detail quality by drawing inspiration from human cognition for generative image fusion.

Abstract: Existing infrared and visible image fusion methods often face the dilemma of
balancing modal information. Generative fusion methods reconstruct fused images
by learning from data distributions, but their generative capabilities remain
limited. Moreover, the lack of interpretability in modal information selection
further affects the reliability and consistency of fusion results in complex
scenarios. This manuscript revisits the essence of generative image fusion
under the inspiration of human cognitive laws and proposes a novel infrared and
visible image fusion method, termed HCLFuse. First, HCLFuse investigates the
quantification theory of information mapping in unsupervised fusion networks,
which leads to the design of a multi-scale mask-regulated variational
bottleneck encoder. This encoder applies posterior probability modeling and
information decomposition to extract accurate and concise low-level modal
information, thereby supporting the generation of high-fidelity structural
details. Furthermore, the probabilistic generative capability of the diffusion
model is integrated with physical laws, forming a time-varying physical
guidance mechanism that adaptively regulates the generation process at
different stages, thereby enhancing the ability of the model to perceive the
intrinsic structure of data and reducing dependence on data quality.
Experimental results show that the proposed method achieves state-of-the-art
fusion performance in qualitative and quantitative evaluations across multiple
datasets and significantly improves semantic segmentation metrics. This fully
demonstrates the advantages of this generative image fusion method, drawing
inspiration from human cognition, in enhancing structural consistency and
detail quality.

</details>


### [34] [Exploring Complementarity and Explainability in CNNs for Periocular Verification Across Acquisition Distances](https://arxiv.org/abs/2510.26282)
*Fernando Alonso-Fernandez,Kevin Hernandez Diaz,Jose M. Buades,Kiran Raja,Josef Bigun*

Main category: cs.CV

TL;DR: This paper analyzes the complementarity of three CNN architectures (SqueezeNet, MobileNetv2, ResNet50) for periocular verification at different distances, showing that fusion of all three networks provides substantial performance gains and achieves state-of-the-art results on UBIPr database.


<details>
  <summary>Details</summary>
Motivation: To study how different CNN architectures with varying complexity levels complement each other for periocular verification, particularly at different distances, and to understand their attention patterns through explainable AI methods.

Method: Trained three CNN architectures (SqueezeNet, MobileNetv2, ResNet50) on eye crops from VGGFace2, analyzed performance with cosine and chi2 metrics, compared network initializations, applied score-level fusion via logistic regression, and used LIME heatmaps and Jensen-Shannon divergence to compare attention patterns.

Result: ResNet50 performed best individually, but fusion of all three networks provided substantial gains. LIME heatmaps revealed that networks focus on distinct regions of images, explaining their complementarity. The method significantly outperformed previous works on UBIPr database.

Conclusion: The complementarity of different CNN architectures for periocular verification can be effectively leveraged through fusion techniques, with attention analysis showing that networks focus on different image regions, leading to state-of-the-art performance when combined.

Abstract: We study the complementarity of different CNNs for periocular verification at
different distances on the UBIPr database. We train three architectures of
increasing complexity (SqueezeNet, MobileNetv2, and ResNet50) on a large set of
eye crops from VGGFace2. We analyse performance with cosine and chi2 metrics,
compare different network initialisations, and apply score-level fusion via
logistic regression. In addition, we use LIME heatmaps and Jensen-Shannon
divergence to compare attention patterns of the CNNs. While ResNet50
consistently performs best individually, the fusion provides substantial gains,
especially when combining all three networks. Heatmaps show that networks
usually focus on distinct regions of a given image, which explains their
complementarity. Our method significantly outperforms previous works on UBIPr,
achieving a new state-of-the-art.

</details>


### [35] [Beyond Imitation: Constraint-Aware Trajectory Generation with Flow Matching For End-to-End Autonomous Driving](https://arxiv.org/abs/2510.26292)
*Lin Liu,Guanyi Yu,Ziying Song,Junqiao Li,Caiyan Jia,Feiyang Jia,Peiliang Wu,Yandan Luo*

Main category: cs.CV

TL;DR: CATG is a novel autonomous driving planning framework that uses Constrained Flow Matching to generate diverse, safe trajectories while incorporating driving aggressiveness as a control signal, achieving 2nd place in NavSim v2 challenge.


<details>
  <summary>Details</summary>
Motivation: To address mode collapse in imitation learning methods and the inability of existing generative approaches to directly incorporate safety and physical constraints during generation, requiring additional optimization stages.

Method: Uses Constrained Flow Matching to explicitly model the flow matching process, imposing explicit constraints directly within the generative process for safety and kinematic rules, and parameterizing driving aggressiveness as a control signal.

Result: Achieved 2nd place in NavSim v2 challenge with EPDMS score of 51.31 and received Innovation Award.

Conclusion: CATG successfully addresses mode collapse and constraint incorporation issues in autonomous driving planning through constrained flow matching, enabling diverse and safe trajectory generation with controllable driving style.

Abstract: Planning is a critical component of end-to-end autonomous driving. However,
prevailing imitation learning methods often suffer from mode collapse, failing
to produce diverse trajectory hypotheses. Meanwhile, existing generative
approaches struggle to incorporate crucial safety and physical constraints
directly into the generative process, necessitating an additional optimization
stage to refine their outputs. To address these limitations, we propose CATG, a
novel planning framework that leverages Constrained Flow Matching. Concretely,
CATG explicitly models the flow matching process, which inherently mitigates
mode collapse and allows for flexible guidance from various conditioning
signals. Our primary contribution is the novel imposition of explicit
constraints directly within the flow matching process, ensuring that the
generated trajectories adhere to vital safety and kinematic rules. Secondly,
CATG parameterizes driving aggressiveness as a control signal during
generation, enabling precise manipulation of trajectory style. Notably, on the
NavSim v2 challenge, CATG achieved 2nd place with an EPDMS score of 51.31 and
was honored with the Innovation Award.

</details>


### [36] [Leveraging Large-Scale Face Datasets for Deep Periocular Recognition via Ocular Cropping](https://arxiv.org/abs/2510.26294)
*Fernando Alonso-Fernandez,Kevin Hernandez-Diaz,Jose Maria Buades Rubio,Josef Bigun*

Main category: cs.CV

TL;DR: This paper evaluates three CNN architectures for periocular biometric recognition using large-scale training data from VGGFace2, achieving state-of-the-art performance on the UFPR-Periocular dataset with 1-2% EER.


<details>
  <summary>Details</summary>
Motivation: To leverage the periocular region's high discrimination capability and minimal acquisition constraints for biometric recognition, and address the limitation of small-scale datasets in existing works.

Method: Three CNN architectures of varying depth/complexity trained on 1,907,572 ocular crops from VGGFace2 database, evaluated on VGGFace2-Pose and UFPR-Periocular datasets.

Result: VGGFace2-Pose: 9-15% EER (higher than full-face 3-6% EER); UFPR-Periocular: 1-2% EER (state-of-the-art performance due to better image quality and consistent acquisition).

Conclusion: Large-scale training data significantly improves periocular recognition performance, with controlled acquisition conditions yielding much better results (1-2% EER) compared to uncontrolled environments (9-15% EER).

Abstract: We focus on ocular biometrics, specifically the periocular region (the area
around the eye), which offers high discrimination and minimal acquisition
constraints. We evaluate three Convolutional Neural Network architectures of
varying depth and complexity to assess their effectiveness for periocular
recognition. The networks are trained on 1,907,572 ocular crops extracted from
the large-scale VGGFace2 database. This significantly contrasts with existing
works, which typically rely on small-scale periocular datasets for training
having only a few thousand images. Experiments are conducted with ocular images
from VGGFace2-Pose, a subset of VGGFace2 containing in-the-wild face images,
and the UFPR-Periocular database, which consists of selfies captured via mobile
devices with user guidance on the screen. Due to the uncontrolled conditions of
VGGFace2, the Equal Error Rates (EERs) obtained with ocular crops range from
9-15%, noticeably higher than the 3-6% EERs achieved using full-face images. In
contrast, UFPR-Periocular yields significantly better performance (EERs of
1-2%), thanks to higher image quality and more consistent acquisition
protocols. To the best of our knowledge, these are the lowest reported EERs on
the UFPR dataset to date.

</details>


### [37] [Towards Realistic Earth-Observation Constellation Scheduling: Benchmark and Methodology](https://arxiv.org/abs/2510.26297)
*Luting Wang,Yinghao Xiang,Hongliang Huang,Dongjun Li,Chen Gao,Si Liu*

Main category: cs.CV

TL;DR: AEOS-Bench is the first large-scale benchmark for Agile Earth Observation Satellites constellation scheduling, featuring 16,410 realistic scenarios. AEOS-Former, a Transformer-based model with constraint-aware attention, outperforms baselines in task completion and energy efficiency.


<details>
  <summary>Details</summary>
Motivation: Existing AEOS scheduling methods simplify real-world complexities, limiting performance in large-scale, dynamic environments with stringent constraints.

Method: Created AEOS-Bench benchmark with 16,410 scenarios generated via high-fidelity simulation, and developed AEOS-Former - a Transformer model with constraint-aware attention and dedicated internal constraint module for physical/operational limits.

Result: AEOS-Former outperforms baseline models in both task completion and energy efficiency, with ablation studies confirming the importance of each component.

Conclusion: The unified framework provides a robust solution for AEOS constellation scheduling, with the benchmark enabling standardized evaluation and the model offering superior performance in realistic scenarios.

Abstract: Agile Earth Observation Satellites (AEOSs) constellations offer unprecedented
flexibility for monitoring the Earth's surface, but their scheduling remains
challenging under large-scale scenarios, dynamic environments, and stringent
constraints. Existing methods often simplify these complexities, limiting their
real-world performance. We address this gap with a unified framework
integrating a standardized benchmark suite and a novel scheduling model. Our
benchmark suite, AEOS-Bench, contains $3,907$ finely tuned satellite assets and
$16,410$ scenarios. Each scenario features $1$ to $50$ satellites and $50$ to
$300$ imaging tasks. These scenarios are generated via a high-fidelity
simulation platform, ensuring realistic satellite behavior such as orbital
dynamics and resource constraints. Ground truth scheduling annotations are
provided for each scenario. To our knowledge, AEOS-Bench is the first
large-scale benchmark suite tailored for realistic constellation scheduling.
Building upon this benchmark, we introduce AEOS-Former, a Transformer-based
scheduling model that incorporates a constraint-aware attention mechanism. A
dedicated internal constraint module explicitly models the physical and
operational limits of each satellite. Through simulation-based iterative
learning, AEOS-Former adapts to diverse scenarios, offering a robust solution
for AEOS constellation scheduling. Experimental results demonstrate that
AEOS-Former outperforms baseline models in task completion and energy
efficiency, with ablation studies highlighting the contribution of each
component. Code and data are provided in
https://github.com/buaa-colalab/AEOSBench.

</details>


### [38] [Exploring the correlation between the type of music and the emotions evoked: A study using subjective questionnaires and EEG](https://arxiv.org/abs/2510.26304)
*Jelizaveta Jankowska,Bożena Kostek,Fernando Alonso-Fernandez,Prayag Tiwari*

Main category: cs.CV

TL;DR: Study examines how different music genres affect human emotions using EEG measurements and surveys, showing connections between brain activity and emotional responses.


<details>
  <summary>Details</summary>
Motivation: To understand the impact of different music genres on human emotions and demonstrate measurable connections between music listening and brain activity patterns.

Method: Used EEG helmet to measure brain activity while participants listened to different music genres, combined with subjective surveys about emotional responses. Involved diverse participants with different genders and musical preferences.

Result: Analysis revealed connections between emotions reported in questionnaires and observed brain activity patterns measured by EEG. Captured wide range of emotional responses to different music types.

Conclusion: Different music genres have measurable impacts on human emotions, with EEG signals showing correlations with subjective emotional experiences during music listening.

Abstract: The subject of this work is to check how different types of music affect
human emotions. While listening to music, a subjective survey and brain
activity measurements were carried out using an EEG helmet. The aim is to
demonstrate the impact of different music genres on emotions. The research
involved a diverse group of participants of different gender and musical
preferences. This had the effect of capturing a wide range of emotional
responses to music. After the experiment, a relationship analysis of the
respondents' questionnaires with EEG signals was performed. The analysis
revealed connections between emotions and observed brain activity.

</details>


### [39] [A Hybrid Framework Bridging CNN and ViT based on Theory of Evidence for Diabetic Retinopathy Grading](https://arxiv.org/abs/2510.26315)
*Junlai Qiu,Yunzhu Chen,Hao Zheng,Yawen Huang,Yuexiang Li*

Main category: cs.CV

TL;DR: A novel evidential fusion paradigm that combines CNN and ViT backbones for diabetic retinopathy grading, leveraging local and global feature strengths through deep evidential networks to improve accuracy and interpretability.


<details>
  <summary>Details</summary>
Motivation: Diabetic retinopathy is a major cause of vision loss, and existing automated diagnosis systems using single-type backbones (CNN or ViT) have performance limitations due to their inherent shortcomings. Combining different backbones can leverage CNN's local feature extraction and ViT's global feature capturing capabilities.

Method: Proposed an evidential fusion paradigm that transforms features from different backbones into supporting evidences via deep evidential networks. This allows adaptive fusion pattern tuning between CNN and ViT backbones based on aggregated opinions formed from the supporting evidences.

Result: Evaluated on two publicly available DR grading datasets, the hybrid model demonstrated improved accuracy compared to state-of-the-art frameworks and provided excellent interpretability for feature fusion and decision-making.

Conclusion: The evidential fusion approach effectively integrates CNN and ViT backbones, overcoming limitations of single-type backbones and achieving both performance improvements and enhanced interpretability in diabetic retinopathy grading.

Abstract: Diabetic retinopathy (DR) is a leading cause of vision loss among middle-aged
and elderly people, which significantly impacts their daily lives and mental
health. To improve the efficiency of clinical screening and enable the early
detection of DR, a variety of automated DR diagnosis systems have been recently
established based on convolutional neural network (CNN) or vision Transformer
(ViT). However, due to the own shortages of CNN / ViT, the performance of
existing methods using single-type backbone has reached a bottleneck. One
potential way for the further improvements is integrating different kinds of
backbones, which can fully leverage the respective strengths of them
(\emph{i.e.,} the local feature extraction capability of CNN and the global
feature capturing ability of ViT). To this end, we propose a novel paradigm to
effectively fuse the features extracted by different backbones based on the
theory of evidence. Specifically, the proposed evidential fusion paradigm
transforms the features from different backbones into supporting evidences via
a set of deep evidential networks. With the supporting evidences, the
aggregated opinion can be accordingly formed, which can be used to adaptively
tune the fusion pattern between different backbones and accordingly boost the
performance of our hybrid model. We evaluated our method on two publicly
available DR grading datasets. The experimental results demonstrate that our
hybrid model not only improves the accuracy of DR grading, compared to the
state-of-the-art frameworks, but also provides the excellent interpretability
for feature fusion and decision-making.

</details>


### [40] [GLYPH-SR: Can We Achieve Both High-Quality Image Super-Resolution and High-Fidelity Text Recovery via VLM-guided Latent Diffusion Model?](https://arxiv.org/abs/2510.26339)
*Mingyu Sung,Seungjae Ham,Kangwoo Kim,Yeokyoung Yoon,Sangseok Yun,Il-Min Kim,Jae-Mo Kang*

Main category: cs.CV

TL;DR: GLYPH-SR is a vision-language-guided diffusion framework for image super-resolution that specifically optimizes for both text legibility and perceptual quality in natural scenes.


<details>
  <summary>Details</summary>
Motivation: Traditional super-resolution methods focus on distortion or perceptual metrics that are insensitive to character-level errors, causing OCR failures even when images appear sharp. Scene-text carries crucial actionable information but is often treated as generic texture.

Method: Uses a Text-SR Fusion ControlNet guided by OCR data and a ping-pong scheduler that alternates between text- and scene-centric guidance. Trains on synthetic corpus while keeping main SR branch frozen.

Result: Improves OCR F1 by up to +15.18 percentage points over diffusion/GAN baselines (SVT x8, OpenOCR) while maintaining competitive perceptual quality metrics (MANIQA, CLIP-IQA, MUSIQ).

Conclusion: GLYPH-SR successfully delivers super-resolution that achieves both high readability and high visual realism, making it effective for practical applications where text recognition is critical.

Abstract: Image super-resolution(SR) is fundamental to many vision system-from
surveillance and autonomy to document analysis and retail analytics-because
recovering high-frequency details, especially scene-text, enables reliable
downstream perception. Scene-text, i.e., text embedded in natural images such
as signs, product labels, and storefronts, often carries the most actionable
information; when characters are blurred or hallucinated, optical character
recognition(OCR) and subsequent decisions fail even if the rest of the image
appears sharp. Yet previous SR research has often been tuned to distortion
(PSNR/SSIM) or learned perceptual metrics (LIPIS, MANIQA, CLIP-IQA, MUSIQ) that
are largely insensitive to character-level errors. Furthermore, studies that do
address text SR often focus on simplified benchmarks with isolated characters,
overlooking the challenges of text within complex natural scenes. As a result,
scene-text is effectively treated as generic texture. For SR to be effective in
practical deployments, it is therefore essential to explicitly optimize for
both text legibility and perceptual quality. We present GLYPH-SR, a
vision-language-guided diffusion framework that aims to achieve both objectives
jointly. GLYPH-SR utilizes a Text-SR Fusion ControlNet(TS-ControlNet) guided by
OCR data, and a ping-pong scheduler that alternates between text- and
scene-centric guidance. To enable targeted text restoration, we train these
components on a synthetic corpus while keeping the main SR branch frozen.
Across SVT, SCUT-CTW1500, and CUTE80 at x4, and x8, GLYPH-SR improves OCR F1 by
up to +15.18 percentage points over diffusion/GAN baseline (SVT x8, OpenOCR)
while maintaining competitive MANIQA, CLIP-IQA, and MUSIQ. GLYPH-SR is designed
to satisfy both objectives simultaneously-high readability and high visual
realism-delivering SR that looks right and reds right.

</details>


### [41] [EEG-Driven Image Reconstruction with Saliency-Guided Diffusion Models](https://arxiv.org/abs/2510.26391)
*Igor Abramov,Ilya Makarov*

Main category: cs.CV

TL;DR: A dual-conditioning framework combining EEG embeddings with spatial saliency maps improves EEG-driven image reconstruction by enhancing fidelity and semantic coherence through attention mechanisms.


<details>
  <summary>Details</summary>
Motivation: Existing EEG-driven image reconstruction methods overlook spatial attention mechanisms, limiting fidelity and semantic coherence in generated images.

Method: Proposes a dual-conditioning framework using Adaptive Thinking Mapper (ATM) for EEG feature extraction, fine-tunes Stable Diffusion 2.1 via LoRA for neural-visual alignment, and adds a ControlNet branch for spatial conditioning via saliency maps.

Result: Achieves significant improvement in quality of low- and high-level image features over existing approaches on THINGS-EEG dataset, with strong alignment to human visual attention.

Conclusion: Attentional priors resolve EEG ambiguities, enabling high-fidelity reconstructions with applications in medical diagnostics and neuroadaptive interfaces, advancing neural decoding through efficient adaptation of pre-trained diffusion models.

Abstract: Existing EEG-driven image reconstruction methods often overlook spatial
attention mechanisms, limiting fidelity and semantic coherence. To address
this, we propose a dual-conditioning framework that combines EEG embeddings
with spatial saliency maps to enhance image generation. Our approach leverages
the Adaptive Thinking Mapper (ATM) for EEG feature extraction and fine-tunes
Stable Diffusion 2.1 via Low-Rank Adaptation (LoRA) to align neural signals
with visual semantics, while a ControlNet branch conditions generation on
saliency maps for spatial control. Evaluated on THINGS-EEG, our method achieves
a significant improvement in the quality of low- and high-level image features
over existing approaches. Simultaneously, strongly aligning with human visual
attention. The results demonstrate that attentional priors resolve EEG
ambiguities, enabling high-fidelity reconstructions with applications in
medical diagnostics and neuroadaptive interfaces, advancing neural decoding
through efficient adaptation of pre-trained diffusion models.

</details>


### [42] [LoCoT2V-Bench: A Benchmark for Long-Form and Complex Text-to-Video Generation](https://arxiv.org/abs/2510.26412)
*Xiangqing Zheng,Chengyue Wu,Kehai Chen,Min Zhang*

Main category: cs.CV

TL;DR: LoCoT2V-Bench is a new benchmark for evaluating long video generation models using complex prompts, featuring multi-dimensional metrics including event alignment, temporal consistency, and narrative coherence.


<details>
  <summary>Details</summary>
Motivation: Existing benchmarks for text-to-video generation focus on simplified prompts and low-level metrics, overlooking fine-grained alignment with complex prompts and abstract dimensions like narrative coherence and thematic expression.

Method: Proposed LoCoT2V-Bench benchmark based on real-world videos with realistic complex prompts containing scene transitions and event dynamics. Introduced multi-dimensional evaluation framework with new metrics: event-level alignment, fine-grained temporal consistency, content clarity, and HERD (Human Expectation Realization Degree) for narrative flow and emotional response.

Result: Comprehensive evaluation of nine LVG models showed current methods perform well on basic visual and temporal aspects but struggle with inter-event consistency, fine-grained alignment, and high-level thematic adherence.

Conclusion: LoCoT2V-Bench provides a comprehensive platform for evaluating long-form complex text-to-video generation and identifies critical improvement directions for future methods.

Abstract: Recently text-to-video generation has made impressive progress in producing
short, high-quality clips, but evaluating long-form outputs remains a major
challenge especially when processing complex prompts. Existing benchmarks
mostly rely on simplified prompts and focus on low-level metrics, overlooking
fine-grained alignment with prompts and abstract dimensions such as narrative
coherence and thematic expression. To address these gaps, we propose
LoCoT2V-Bench, a benchmark specifically designed for long video generation
(LVG) under complex input conditions. Based on various real-world videos,
LoCoT2V-Bench introduces a suite of realistic and complex prompts incorporating
elements like scene transitions and event dynamics. Moreover, it constructs a
multi-dimensional evaluation framework that includes our newly proposed metrics
such as event-level alignment, fine-grained temporal consistency, content
clarity, and the Human Expectation Realization Degree (HERD) that focuses on
more abstract attributes like narrative flow, emotional response, and character
development. Using this framework, we conduct a comprehensive evaluation of
nine representative LVG models, finding that while current methods perform well
on basic visual and temporal aspects, they struggle with inter-event
consistency, fine-grained alignment, and high-level thematic adherence, etc.
Overall, LoCoT2V-Bench provides a comprehensive and reliable platform for
evaluating long-form complex text-to-video generation and highlights critical
directions for future method improvement.

</details>


### [43] [A-TPT: Angular Diversity Calibration Properties for Test-Time Prompt Tuning of Vision-Language Models](https://arxiv.org/abs/2510.26441)
*Shihab Aaqil Ahamed,Udaya S. K. P. Miriya Thanthrige,Ranga Rodrigo,Muhammad Haris Khan*

Main category: cs.CV

TL;DR: A-TPT is a test-time prompt tuning framework that improves vision-language model calibration by maximizing angular diversity between class-wise textual features on the unit hypersphere.


<details>
  <summary>Details</summary>
Motivation: Current TPT methods lack optimal angular separation between class-wise textual features, which hurts calibration performance and raises concerns about VLMs' reliability and safety.

Method: Introduces angular diversity by maximizing the minimum pairwise angular distance between normalized textual features on the unit hypersphere, encouraging uniformity in feature distribution.

Result: Consistently surpasses state-of-the-art TPT methods in reducing aggregate average calibration error while maintaining comparable accuracy, with superior zero-shot calibration on distribution shifts and medical datasets.

Conclusion: Promoting angular diversity achieves well-dispersed textual features, significantly improving VLM calibration during test-time adaptation.

Abstract: Test-time prompt tuning (TPT) has emerged as a promising technique for
adapting large vision-language models (VLMs) to unseen tasks without relying on
labeled data. However, the lack of dispersion between textual features can hurt
calibration performance, which raises concerns about VLMs' reliability,
trustworthiness, and safety. Current TPT approaches primarily focus on
improving prompt calibration by either maximizing average textual feature
dispersion or enforcing orthogonality constraints to encourage angular
separation. However, these methods may not always have optimal angular
separation between class-wise textual features, which implies overlooking the
critical role of angular diversity. To address this, we propose A-TPT, a novel
TPT framework that introduces angular diversity to encourage uniformity in the
distribution of normalized textual features induced by corresponding learnable
prompts. This uniformity is achieved by maximizing the minimum pairwise angular
distance between features on the unit hypersphere. We show that our approach
consistently surpasses state-of-the-art TPT methods in reducing the aggregate
average calibration error while maintaining comparable accuracy through
extensive experiments with various backbones on different datasets. Notably,
our approach exhibits superior zero-shot calibration performance on natural
distribution shifts and generalizes well to medical datasets. We provide
extensive analyses, including theoretical aspects, to establish the grounding
of A-TPT. These results highlight the potency of promoting angular diversity to
achieve well-dispersed textual features, significantly improving VLM
calibration during test-time adaptation. Our code will be made publicly
available.

</details>


### [44] [PointSt3R: Point Tracking through 3D Grounded Correspondence](https://arxiv.org/abs/2510.26443)
*Rhodri Guerrier,Adam W. Harley,Dima Damen*

Main category: cs.CV

TL;DR: This paper adapts foundational 3D reconstruction models (DUSt3R and MASt3R) for point tracking by combining reconstruction loss with dynamic correspondence training and a visibility head, achieving competitive results on multiple datasets.


<details>
  <summary>Details</summary>
Motivation: To leverage recent advances in foundational 3D reconstruction models for point tracking tasks, particularly addressing both static and dynamic point correspondence in scenes.

Method: Fine-tune MASt3R for point tracking using synthetic data, combining reconstruction loss with dynamic correspondence training and adding a visibility head. Training and evaluation are done on pairs of frames containing query points without temporal context.

Result: Achieved competitive or superior performance on four datasets: TAP-Vid-DAVIS (73.8 δ_avg / 85.8% occlusion accuracy), significantly outperformed CoTracker3 on EgoPoints (61.3 vs 54.2) and RGB-S (87.0 vs 82.8), and showed +33.5% improvement on EgoPoints vs CoTracker2.

Conclusion: The adapted 3D reconstruction models are effective for point tracking tasks, demonstrating strong performance on both static and dynamic point correspondence across multiple benchmarks.

Abstract: Recent advances in foundational 3D reconstruction models, such as DUSt3R and
MASt3R, have shown great potential in 2D and 3D correspondence in static
scenes. In this paper, we propose to adapt them for the task of point tracking
through 3D grounded correspondence. We first demonstrate that these models are
competitive point trackers when focusing on static points, present in current
point tracking benchmarks ($+33.5\%$ on EgoPoints vs. CoTracker2). We propose
to combine the reconstruction loss with training for dynamic correspondence
along with a visibility head, and fine-tuning MASt3R for point tracking using a
relatively small amount of synthetic data. Importantly, we only train and
evaluate on pairs of frames where one contains the query point, effectively
removing any temporal context. Using a mix of dynamic and static point
correspondences, we achieve competitive or superior point tracking results on
four datasets (e.g. competitive on TAP-Vid-DAVIS 73.8 $\delta_{avg}$ / 85.8\%
occlusion acc. for PointSt3R compared to 75.7 / 88.3\% for CoTracker2; and
significantly outperform CoTracker3 on EgoPoints 61.3 vs 54.2 and RGB-S 87.0 vs
82.8). We also present results on 3D point tracking along with several
ablations on training datasets and percentage of dynamic correspondences.

</details>


### [45] [Towards Fine-Grained Vision-Language Alignment for Few-Shot Anomaly Detection](https://arxiv.org/abs/2510.26464)
*Yuanting Fan,Jun Liu,Xiaochen Chen,Bin-Bin Gao,Jian Li,Yong Liu,Jinlong Peng,Chengjie Wang*

Main category: cs.CV

TL;DR: FineGrainedAD improves few-shot anomaly detection by introducing multi-level fine-grained semantic captions and alignment mechanisms to address semantic misalignment between image descriptions and patch-level visual anomalies.


<details>
  <summary>Details</summary>
Motivation: Existing FSAD methods rely on pre-trained VLMs but suffer from semantic misalignment due to lack of detailed textual descriptions, leading to sub-optimal localization performance.

Method: Proposes MFSC for multi-level fine-grained textual descriptions, MLLP for fine-grained semantics in prompts, and MLSA with region aggregation and multi-level alignment training.

Result: Achieves superior overall performance in few-shot settings on MVTec-AD and VisA datasets.

Conclusion: The proposed FineGrainedAD framework effectively addresses semantic misalignment issues and improves anomaly localization performance in few-shot settings.

Abstract: Few-shot anomaly detection (FSAD) methods identify anomalous regions with few
known normal samples. Most existing methods rely on the generalization ability
of pre-trained vision-language models (VLMs) to recognize potentially anomalous
regions through feature similarity between text descriptions and images.
However, due to the lack of detailed textual descriptions, these methods can
only pre-define image-level descriptions to match each visual patch token to
identify potential anomalous regions, which leads to the semantic misalignment
between image descriptions and patch-level visual anomalies, achieving
sub-optimal localization performance. To address the above issues, we propose
the Multi-Level Fine-Grained Semantic Caption (MFSC) to provide multi-level and
fine-grained textual descriptions for existing anomaly detection datasets with
automatic construction pipeline. Based on the MFSC, we propose a novel
framework named FineGrainedAD to improve anomaly localization performance,
which consists of two components: Multi-Level Learnable Prompt (MLLP) and
Multi-Level Semantic Alignment (MLSA). MLLP introduces fine-grained semantics
into multi-level learnable prompts through automatic replacement and
concatenation mechanism, while MLSA designs region aggregation strategy and
multi-level alignment training to facilitate learnable prompts better align
with corresponding visual regions. Experiments demonstrate that the proposed
FineGrainedAD achieves superior overall performance in few-shot settings on
MVTec-AD and VisA datasets.

</details>


### [46] [Representation-Level Counterfactual Calibration for Debiased Zero-Shot Recognition](https://arxiv.org/abs/2510.26466)
*Pei Peng,MingKun Xie,Hang Hao,Tong Jin,ShengJun Huang*

Main category: cs.CV

TL;DR: A causal inference approach to address object-context shortcuts in vision-language models by synthesizing counterfactual embeddings and estimating Total Direct Effect to improve zero-shot reliability.


<details>
  <summary>Details</summary>
Motivation: Object-context shortcuts undermine zero-shot reliability when test scenes differ from training co-occurrences, requiring debiased multimodal reasoning.

Method: Estimate object and background expectations in CLIP's representation space, synthesize counterfactual embeddings by recombining object features with alternative contexts, and use Total Direct Effect to subtract background-only activation.

Result: Substantially improves worst-group and average accuracy on context-sensitive benchmarks without retraining or prompt design, achieving new zero-shot state of the art.

Conclusion: Provides a lightweight representation-level counterfactual approach for debiased and reliable multimodal reasoning through practical causal inference.

Abstract: Object-context shortcuts remain a persistent challenge in vision-language
models, undermining zero-shot reliability when test-time scenes differ from
familiar training co-occurrences. We recast this issue as a causal inference
problem and ask: Would the prediction remain if the object appeared in a
different environment? To answer this at inference time, we estimate object and
background expectations within CLIP's representation space, and synthesize
counterfactual embeddings by recombining object features with diverse
alternative contexts sampled from external datasets, batch neighbors, or
text-derived descriptions. By estimating the Total Direct Effect and simulating
intervention, we further subtract background-only activation, preserving
beneficial object-context interactions while mitigating hallucinated scores.
Without retraining or prompt design, our method substantially improves both
worst-group and average accuracy on context-sensitive benchmarks, establishing
a new zero-shot state of the art. Beyond performance, our framework provides a
lightweight representation-level counterfactual approach, offering a practical
causal avenue for debiased and reliable multimodal reasoning.

</details>


### [47] [Counteracting Matthew Effect in Self-Improvement of LVLMs through Head-Tail Re-balancing](https://arxiv.org/abs/2510.26474)
*Xin Guo,Zhiheng Xi,Yiwen Ding,Yitao Zhai,Xiaowei Shi,Xunliang Cai,Tao Gui,Qi Zhang,Xuanjing Huang*

Main category: cs.CV

TL;DR: The paper addresses the Matthew effect in self-improvement of LVLMs, where models focus on simple queries and neglect complex ones, leading to performance bottlenecks. It proposes distribution-reshaping and trajectory-resampling strategies to re-balance learning.


<details>
  <summary>Details</summary>
Motivation: Self-improvement in LVLMs suffers from imbalanced optimization where models excel at simple queries but struggle with complex ones, creating a Matthew effect that hinders overall improvement.

Method: Four efficient strategies from two perspectives: distribution-reshaping and trajectory-resampling, designed to achieve head-tail re-balancing during the self-improvement process.

Result: Extensive experiments on Qwen2-VL-7B-Instruct and InternVL2.5-4B models show consistent improvement in visual reasoning capabilities, outperforming vanilla self-improvement by 3.86 points on average.

Conclusion: The proposed head-tail re-balancing strategies effectively counteract the Matthew effect in self-improvement, enabling better performance on complex reasoning tasks and overcoming performance bottlenecks.

Abstract: Self-improvement has emerged as a mainstream paradigm for advancing the
reasoning capabilities of large vision-language models (LVLMs), where models
explore and learn from successful trajectories iteratively. However, we
identify a critical issue during this process: the model excels at generating
high-quality trajectories for simple queries (i.e., head data) but struggles
with more complex ones (i.e., tail data). This leads to an imbalanced
optimization that drives the model to prioritize simple reasoning skills, while
hindering its ability to tackle more complex reasoning tasks. Over iterations,
this imbalance becomes increasingly pronounced--a dynamic we term the "Matthew
effect"--which ultimately hinders further model improvement and leads to
performance bottlenecks. To counteract this challenge, we introduce four
efficient strategies from two perspectives: distribution-reshaping and
trajectory-resampling, to achieve head-tail re-balancing during the
exploration-and-learning self-improvement process. Extensive experiments on
Qwen2-VL-7B-Instruct and InternVL2.5-4B models across visual reasoning tasks
demonstrate that our methods consistently improve visual reasoning
capabilities, outperforming vanilla self-improvement by 3.86 points on average.

</details>


### [48] [Analysis of the Robustness of an Edge Detector Based on Cellular Automata Optimized by Particle Swarm](https://arxiv.org/abs/2510.26509)
*Vinícius Ferraria,Eurico Ruivo*

Main category: cs.CV

TL;DR: An adaptable edge detector using cellular automata and meta-heuristic optimization was developed, but expanding the search space didn't improve results and transfer learning showed no significant benefits.


<details>
  <summary>Details</summary>
Motivation: To address weaknesses in edge detection like difficulty detecting loose edges and lack of context, and to create a detector that adapts to image properties.

Method: Used two-dimensional cellular automaton optimized by meta-heuristic combined with transfer learning techniques, analyzing search space expansion and adaptability.

Result: Expanding the search space was ineffective for the chosen image set, and the model adapted to input regardless of validation, with transfer learning showing no significant improvements.

Conclusion: The proposed adaptable edge detector approach did not yield expected benefits from search space expansion or transfer learning techniques for the tested image sets.

Abstract: The edge detection task is essential in image processing aiming to extract
relevant information from an image. One recurring problem in this task is the
weaknesses found in some detectors, such as the difficulty in detecting loose
edges and the lack of context to extract relevant information from specific
problems. To address these weaknesses and adapt the detector to the properties
of an image, an adaptable detector described by two-dimensional cellular
automaton and optimized by meta-heuristic combined with transfer learning
techniques was developed. This study aims to analyze the impact of expanding
the search space of the optimization phase and the robustness of the
adaptability of the detector in identifying edges of a set of natural images
and specialized subsets extracted from the same image set. The results obtained
prove that expanding the search space of the optimization phase was not
effective for the chosen image set. The study also analyzed the adaptability of
the model through a series of experiments and validation techniques and found
that, regardless of the validation, the model was able to adapt to the input
and the transfer learning techniques applied to the model showed no significant
improvements.

</details>


### [49] [SA$^{2}$Net: Scale-Adaptive Structure-Affinity Transformation for Spine Segmentation from Ultrasound Volume Projection Imaging](https://arxiv.org/abs/2510.26568)
*Hao Xie,Zixun Huang,Yushen Zuo,Yakun Ju,Frank H. F. Leung,N. F. Law,Kin-Man Lam,Yong-Ping Zheng,Sai Ho Ling*

Main category: cs.CV

TL;DR: SA²Net is a scale-adaptive structure-aware network for spine segmentation in ultrasound volume projection imaging, addressing challenges in learning global contextual knowledge and encoding structural bone features for scoliosis diagnosis.


<details>
  <summary>Details</summary>
Motivation: Spine segmentation in ultrasound VPI faces challenges: global contextual knowledge may not be well-learned due to neglected spatial correlation of bone features, and rich structural knowledge about bone shapes and positions needs encoding into segmentation.

Method: Proposes SA²Net with: 1) scale-adaptive complementary strategy for cross-dimensional long-distance correlation features, 2) structure-affinity transformation using multi-head self-attention consistency with semantic level affinity combined with Transformer decoder, 3) feature mixing loss aggregation for enhanced training.

Result: SA²Net achieves superior segmentation performance compared to state-of-the-art methods, demonstrating improved robustness and accuracy in spine segmentation.

Conclusion: SA²Net shows promising potential as an advanced tool for scoliosis diagnosis through intelligent spinal image analysis, with adaptability to various backbones enhancing its utility.

Abstract: Spine segmentation, based on ultrasound volume projection imaging (VPI),
plays a vital role for intelligent scoliosis diagnosis in clinical
applications. However, this task faces several significant challenges. Firstly,
the global contextual knowledge of spines may not be well-learned if we neglect
the high spatial correlation of different bone features. Secondly, the spine
bones contain rich structural knowledge regarding their shapes and positions,
which deserves to be encoded into the segmentation process. To address these
challenges, we propose a novel scale-adaptive structure-aware network
(SA$^{2}$Net) for effective spine segmentation. First, we propose a
scale-adaptive complementary strategy to learn the cross-dimensional
long-distance correlation features for spinal images. Second, motivated by the
consistency between multi-head self-attention in Transformers and semantic
level affinity, we propose structure-affinity transformation to transform
semantic features with class-specific affinity and combine it with a
Transformer decoder for structure-aware reasoning. In addition, we adopt a
feature mixing loss aggregation method to enhance model training. This method
improves the robustness and accuracy of the segmentation process. The
experimental results demonstrate that our SA$^{2}$Net achieves superior
segmentation performance compared to other state-of-the-art methods. Moreover,
the adaptability of SA$^{2}$Net to various backbones enhances its potential as
a promising tool for advanced scoliosis diagnosis using intelligent spinal
image analysis. The code and experimental demo are available at
https://github.com/taetiseo09/SA2Net.

</details>


### [50] [AdSum: Two-stream Audio-visual Summarization for Automated Video Advertisement Clipping](https://arxiv.org/abs/2510.26569)
*Wen Xie,Yanjun Zhu,Gijs Overgoor,Yakov Bart,Agata Lapedriza Garcia,Sarah Ostadabbas*

Main category: cs.CV

TL;DR: Automated video ad clipping framework using audio-visual fusion for shot selection, outperforming existing methods on multiple metrics.


<details>
  <summary>Details</summary>
Motivation: Manual creation of multiple ad versions is labor-intensive; need automated solution tailored for advertising with audio emphasis.

Method: Two-stream audio-visual fusion model predicting frame importance, treating clipping as shot selection problem with novel AdSum204 dataset.

Result: Model outperforms state-of-the-art methods across Average Precision, AUC, Spearman, and Kendall metrics.

Conclusion: Audio-visual fusion approach effectively automates video ad clipping, demonstrating superior performance over visual-only methods.

Abstract: Advertisers commonly need multiple versions of the same advertisement (ad) at
varying durations for a single campaign. The traditional approach involves
manually selecting and re-editing shots from longer video ads to create shorter
versions, which is labor-intensive and time-consuming. In this paper, we
introduce a framework for automated video ad clipping using video summarization
techniques. We are the first to frame video clipping as a shot selection
problem, tailored specifically for advertising. Unlike existing general video
summarization methods that primarily focus on visual content, our approach
emphasizes the critical role of audio in advertising. To achieve this, we
develop a two-stream audio-visual fusion model that predicts the importance of
video frames, where importance is defined as the likelihood of a frame being
selected in the firm-produced short ad. To address the lack of ad-specific
datasets, we present AdSum204, a novel dataset comprising 102 pairs of
30-second and 15-second ads from real advertising campaigns. Extensive
experiments demonstrate that our model outperforms state-of-the-art methods
across various metrics, including Average Precision, Area Under Curve,
Spearman, and Kendall.

</details>


### [51] [Dynamic Context-Aware Scene Reasoning Using Vision-Language Alignment in Zero-Shot Real-World Scenarios](https://arxiv.org/abs/2510.26580)
*Manjunath Prasad Holenarasipura Rajiv,B. M. Vidyavathi*

Main category: cs.CV

TL;DR: A Dynamic Context-Aware Scene Reasoning framework that uses Vision-Language Alignment for zero-shot scene understanding in unfamiliar real-world environments without labeled data.


<details>
  <summary>Details</summary>
Motivation: AI systems struggle with unfamiliar scenarios lacking labeled data, limiting deployment in dynamic, unstructured settings. Conventional models cannot generalize across unseen contexts.

Method: Integrates pre-trained vision transformers and large language models to align visual semantics with natural language descriptions. Includes dynamic reasoning module that combines global scene cues and object-level interactions guided by linguistic priors.

Result: 18% improvement in scene understanding accuracy on zero-shot benchmarks (COCO, Visual Genome, Open Images) over baseline models. Robust performance in ambiguous or cluttered scenes due to vision-language fusion.

Conclusion: The framework provides scalable and interpretable context-aware reasoning, advancing zero-shot generalization in dynamic real-world settings.

Abstract: In real-world environments, AI systems often face unfamiliar scenarios
without labeled data, creating a major challenge for conventional scene
understanding models. The inability to generalize across unseen contexts limits
the deployment of vision-based applications in dynamic, unstructured settings.
This work introduces a Dynamic Context-Aware Scene Reasoning framework that
leverages Vision-Language Alignment to address zero-shot real-world scenarios.
The goal is to enable intelligent systems to infer and adapt to new
environments without prior task-specific training. The proposed approach
integrates pre-trained vision transformers and large language models to align
visual semantics with natural language descriptions, enhancing contextual
comprehension. A dynamic reasoning module refines predictions by combining
global scene cues and object-level interactions guided by linguistic priors.
Extensive experiments on zero-shot benchmarks such as COCO, Visual Genome, and
Open Images demonstrate up to 18% improvement in scene understanding accuracy
over baseline models in complex and unseen environments. Results also show
robust performance in ambiguous or cluttered scenes due to the synergistic
fusion of vision and language. This framework offers a scalable and
interpretable approach for context-aware reasoning, advancing zero-shot
generalization in dynamic real-world settings.

</details>


### [52] [CATCH: A Modular Cross-domain Adaptive Template with Hook](https://arxiv.org/abs/2510.26582)
*Xinjin Li,Yulie Lu,Jinghan Cao,Yu Ma,Zhenglin Li,Yeyang Zhou*

Main category: cs.CV

TL;DR: CATCH is a plug-and-play framework that improves VQA model generalization across domains using lightweight domain classification and dual adapters, achieving performance gains without retraining the backbone model.


<details>
  <summary>Details</summary>
Motivation: VQA models like LLaVA perform well on natural images but degrade significantly in out-of-domain scenarios (remote sensing, medical imaging, math diagrams) due to distributional shifts and lack of effective domain adaptation mechanisms.

Method: Decouples visual and linguistic adaptation using two lightweight modules: a domain classifier to identify input image type, and dual adapters (Prompt Adapter for language modulation and Visual Adapter for vision feature adjustment) dynamically injected via unified hook interface.

Result: Achieved consistent performance gains across four domain-specific VQA benchmarks: +2.3 BLEU on MathVQA, +2.6 VQA on MedVQA-RAD, and +3.1 ROUGE on ChartQA without retraining the backbone model.

Conclusion: CATCH provides a scalable and extensible approach to multi-domain VQA, enabling practical deployment across diverse application domains with minimal architectural changes.

Abstract: Recent advances in Visual Question Answering (VQA) have demonstrated
impressive performance in natural image domains, with models like LLaVA
leveraging large language models (LLMs) for open-ended reasoning. However,
their generalization degrades significantly when transferred to out-of-domain
scenarios such as remote sensing, medical imaging, or math diagrams, due to
large distributional shifts and the lack of effective domain adaptation
mechanisms. Existing approaches typically rely on per-domain fine-tuning or
bespoke pipelines, which are costly, inflexible, and not scalable across
diverse tasks. In this paper, we propose CATCH, a plug-and-play framework for
cross-domain adaptation that improves the generalization of VQA models while
requiring minimal changes to their core architecture. Our key idea is to
decouple visual and linguistic adaptation by introducing two lightweight
modules: a domain classifier to identify the input image type, and a dual
adapter mechanism comprising a Prompt Adapter for language modulation and a
Visual Adapter for vision feature adjustment. Both modules are dynamically
injected via a unified hook interface, requiring no retraining of the backbone
model. Experimental results across four domain-specific VQA benchmarks
demonstrate that our framework achieves consistent performance gains without
retraining the backbone model, including +2.3 BLEU on MathVQA, +2.6 VQA on
MedVQA-RAD, and +3.1 ROUGE on ChartQA. These results highlight that CATCH
provides a scalable and extensible approach to multi-domain VQA, enabling
practical deployment across diverse application domains.

</details>


### [53] [Emu3.5: Native Multimodal Models are World Learners](https://arxiv.org/abs/2510.26583)
*Yufeng Cui,Honghao Chen,Haoge Deng,Xu Huang,Xinghang Li,Jirong Liu,Yang Liu,Zhuoyan Luo,Jinsheng Wang,Wenxuan Wang,Yueze Wang,Chengyuan Wang,Fan Zhang,Yingli Zhao,Ting Pan,Xianduo Li,Zecheng Hao,Wenxuan Ma,Zhuo Chen,Yulong Ao,Tiejun Huang,Zhongyuan Wang,Xinlong Wang*

Main category: cs.CV

TL;DR: Emu3.5 is a large-scale multimodal world model that predicts next states across vision and language using unified next-token prediction, achieving strong performance in multimodal generation tasks with 20x inference acceleration.


<details>
  <summary>Details</summary>
Motivation: To create a unified multimodal world model that can naturally handle interleaved vision-language inputs and outputs, enabling complex multimodal reasoning and generation capabilities.

Method: Pre-trained end-to-end with next-token prediction on 10T+ tokens of vision-language data, post-trained with reinforcement learning, and uses Discrete Diffusion Adaptation (DiDA) for 20x inference acceleration.

Result: Achieves performance comparable to Gemini 2.5 Flash Image on generation tasks, superior results on interleaved generation, and demonstrates spatiotemporally consistent world exploration and open-world embodied manipulation.

Conclusion: Emu3.5 represents a significant advancement in multimodal world modeling with strong native capabilities and efficient inference, supporting community research through open-source release.

Abstract: We introduce Emu3.5, a large-scale multimodal world model that natively
predicts the next state across vision and language. Emu3.5 is pre-trained
end-to-end with a unified next-token prediction objective on a corpus of
vision-language interleaved data containing over 10 trillion tokens, primarily
derived from sequential frames and transcripts of internet videos. The model
naturally accepts interleaved vision-language inputs and generates interleaved
vision-language outputs. Emu3.5 is further post-trained with large-scale
reinforcement learning to enhance multimodal reasoning and generation. To
improve inference efficiency, we propose Discrete Diffusion Adaptation (DiDA),
which converts token-by-token decoding into bidirectional parallel prediction,
accelerating per-image inference by about 20x without sacrificing performance.
Emu3.5 exhibits strong native multimodal capabilities, including long-horizon
vision-language generation, any-to-image (X2I) generation, and complex
text-rich image generation. It also exhibits generalizable world-modeling
abilities, enabling spatiotemporally consistent world exploration and
open-world embodied manipulation across diverse scenarios and tasks. For
comparison, Emu3.5 achieves performance comparable to Gemini 2.5 Flash Image
(Nano Banana) on image generation and editing tasks and demonstrates superior
results on a suite of interleaved generation tasks. We open-source Emu3.5 at
https://github.com/baaivision/Emu3.5 to support community research.

</details>


### [54] [ResMatching: Noise-Resilient Computational Super-Resolution via Guided Conditional Flow Matching](https://arxiv.org/abs/2510.26601)
*Anirban Ray,Vera Galinova,Florian Jug*

Main category: cs.CV

TL;DR: ResMatching is a novel computational super-resolution method using guided conditional flow matching to learn strong data priors, achieving competitive results with the best trade-off between data fidelity and perceptual realism.


<details>
  <summary>Details</summary>
Motivation: Computational super-resolution in fluorescence microscopy is an ill-posed problem that requires strong priors to extrapolate missing frequencies. With better data-driven machine learning techniques, stronger priors can be learned to improve CSR results.

Method: ResMatching uses guided conditional flow matching to learn improved data priors for computational super-resolution. It can sample from an implicitly learned posterior distribution and provides pixel-wise data uncertainty.

Result: ResMatching achieves competitive results on 4 diverse biological structures from BioSR dataset against 7 baselines, showing the best trade-off between data fidelity and perceptual realism. It performs particularly well with noisy low-resolution images and provides calibrated uncertainty estimates.

Conclusion: ResMatching is an effective CSR method that learns strong data priors through flow matching, delivering high-quality super-resolution with uncertainty quantification, especially beneficial for challenging cases with noisy inputs.

Abstract: Computational Super-Resolution (CSR) in fluorescence microscopy has, despite
being an ill-posed problem, a long history. At its very core, CSR is about
finding a prior that can be used to extrapolate frequencies in a micrograph
that have never been imaged by the image-generating microscope. It stands to
reason that, with the advent of better data-driven machine learning techniques,
stronger prior can be learned and hence CSR can lead to better results. Here,
we present ResMatching, a novel CSR method that uses guided conditional flow
matching to learn such improved data-priors. We evaluate ResMatching on 4
diverse biological structures from the BioSR dataset and compare its results
against 7 baselines. ResMatching consistently achieves competitive results,
demonstrating in all cases the best trade-off between data fidelity and
perceptual realism. We observe that CSR using ResMatching is particularly
effective in cases where a strong prior is hard to learn, e.g. when the given
low-resolution images contain a lot of noise. Additionally, we show that
ResMatching can be used to sample from an implicitly learned posterior
distribution and that this distribution is calibrated for all tested use-cases,
enabling our method to deliver a pixel-wise data-uncertainty term that can
guide future users to reject uncertain predictions.

</details>


### [55] [Spiking Patches: Asynchronous, Sparse, and Efficient Tokens for Event Cameras](https://arxiv.org/abs/2510.26614)
*Christoffer Koo Øhrstrøm,Ronja Güldenring,Lazaros Nalpantidis*

Main category: cs.CV

TL;DR: Spiking Patches tokenizer preserves event camera properties (asynchronous, sparse) while matching/surpassing frame/voxel accuracy with 3.4-10.4x faster inference.


<details>
  <summary>Details</summary>
Motivation: Existing event representations (frames, voxels) lose the asynchronous and spatially sparse properties of event cameras, which are their key advantages.

Method: Propose Spiking Patches tokenizer that converts asynchronous event streams into tokens while preserving sparsity and asynchronicity. Evaluated with GNN, PCN, and Transformer on gesture recognition and object detection.

Result: 3.4x faster inference than voxels, 10.4x faster than frames. Matches accuracy and achieves absolute improvements up to 3.8% for gestures and 1.4% for object detection.

Conclusion: Tokenization is a novel direction for event-based vision that preserves event camera properties without sacrificing accuracy, enabling faster inference.

Abstract: We propose tokenization of events and present a tokenizer, Spiking Patches,
specifically designed for event cameras. Given a stream of asynchronous and
spatially sparse events, our goal is to discover an event representation that
preserves these properties. Prior works have represented events as frames or as
voxels. However, while these representations yield high accuracy, both frames
and voxels are synchronous and decrease the spatial sparsity. Spiking Patches
gives the means to preserve the unique properties of event cameras and we show
in our experiments that this comes without sacrificing accuracy. We evaluate
our tokenizer using a GNN, PCN, and a Transformer on gesture recognition and
object detection. Tokens from Spiking Patches yield inference times that are up
to 3.4x faster than voxel-based tokens and up to 10.4x faster than frames. We
achieve this while matching their accuracy and even surpassing in some cases
with absolute improvements up to 3.8 for gesture recognition and up to 1.4 for
object detection. Thus, tokenization constitutes a novel direction in
event-based vision and marks a step towards methods that preserve the
properties of event cameras.

</details>


### [56] [PT-DETR: Small Target Detection Based on Partially-Aware Detail Focus](https://arxiv.org/abs/2510.26630)
*Bingcong Huo,Zhiming Wang*

Main category: cs.CV

TL;DR: PT-DETR improves UAV object detection by enhancing small object feature extraction through PADF and MFFF modules, and using Focaler-SIoU for better bounding box matching, achieving 1.6-1.7% mAP gains over RT-DETR with lower complexity.


<details>
  <summary>Details</summary>
Motivation: To address challenges in UAV object detection including complex backgrounds, severe occlusion, dense small objects, and varying lighting conditions.

Method: Based on RT-DETR, introduces Partially-Aware Detail Focus (PADF) Module for enhanced small object feature extraction, Median-Frequency Feature Fusion (MFFF) module for better detail and context capture, and Focaler-SIoU for improved bounding box matching.

Result: Achieves mAP improvements of 1.6% and 1.7% on VisDrone2019 dataset compared to RT-DETR, with lower computational complexity and fewer parameters.

Conclusion: PT-DETR demonstrates robustness and feasibility for small-object detection tasks in UAV imagery.

Abstract: To address the challenges in UAV object detection, such as complex
backgrounds, severe occlusion, dense small objects, and varying lighting
conditions,this paper proposes PT-DETR based on RT-DETR, a novel detection
algorithm specifically designed for small objects in UAV imagery. In the
backbone network, we introduce the Partially-Aware Detail Focus (PADF) Module
to enhance feature extraction for small objects. Additionally,we design the
Median-Frequency Feature Fusion (MFFF) module,which effectively improves the
model's ability to capture small-object details and contextual information.
Furthermore,we incorporate Focaler-SIoU to strengthen the model's bounding box
matching capability and increase its sensitivity to small-object features,
thereby further enhancing detection accuracy and robustness. Compared with
RT-DETR, our PT-DETR achieves mAP improvements of 1.6% and 1.7% on the
VisDrone2019 dataset with lower computational complexity and fewer parameters,
demonstrating its robustness and feasibility for small-object detection tasks.

</details>


### [57] [All You Need for Object Detection: From Pixels, Points, and Prompts to Next-Gen Fusion and Multimodal LLMs/VLMs in Autonomous Vehicles](https://arxiv.org/abs/2510.26641)
*Sayed Pedram Haeri Boroujeni,Niloufar Mehrabi,Hazim Alzorgan,Ahmad Sarlak,Mahlagha Fazeli,Abolfazl Razi*

Main category: cs.CV

TL;DR: This survey provides a forward-looking analysis of object detection in Autonomous Vehicles, focusing on emerging AI paradigms like Vision-Language Models, Large Language Models, and Generative AI rather than outdated techniques.


<details>
  <summary>Details</summary>
Motivation: The field of autonomous vehicles faces fragmented knowledge across multimodal perception, contextual reasoning, and cooperative intelligence, creating a need to bridge these gaps with a comprehensive survey.

Method: Systematic review of AV sensors and fusion strategies, structured categorization of datasets (ego-vehicle, infrastructure-based, cooperative), and analysis of cutting-edge detection methodologies including 2D/3D pipelines and transformer-driven approaches.

Result: The survey synthesizes current capabilities in object detection for AVs, highlighting the integration potential of sensors with LLM/VLM-driven perception frameworks and providing cross-analysis of dataset characteristics.

Conclusion: The paper delivers a clear roadmap of current capabilities, open challenges, and future opportunities in object detection for autonomous vehicles, emphasizing emerging AI technologies.

Abstract: Autonomous Vehicles (AVs) are transforming the future of transportation
through advances in intelligent perception, decision-making, and control
systems. However, their success is tied to one core capability, reliable object
detection in complex and multimodal environments. While recent breakthroughs in
Computer Vision (CV) and Artificial Intelligence (AI) have driven remarkable
progress, the field still faces a critical challenge as knowledge remains
fragmented across multimodal perception, contextual reasoning, and cooperative
intelligence. This survey bridges that gap by delivering a forward-looking
analysis of object detection in AVs, emphasizing emerging paradigms such as
Vision-Language Models (VLMs), Large Language Models (LLMs), and Generative AI
rather than re-examining outdated techniques. We begin by systematically
reviewing the fundamental spectrum of AV sensors (camera, ultrasonic, LiDAR,
and Radar) and their fusion strategies, highlighting not only their
capabilities and limitations in dynamic driving environments but also their
potential to integrate with recent advances in LLM/VLM-driven perception
frameworks. Next, we introduce a structured categorization of AV datasets that
moves beyond simple collections, positioning ego-vehicle, infrastructure-based,
and cooperative datasets (e.g., V2V, V2I, V2X, I2I), followed by a
cross-analysis of data structures and characteristics. Ultimately, we analyze
cutting-edge detection methodologies, ranging from 2D and 3D pipelines to
hybrid sensor fusion, with particular attention to emerging transformer-driven
approaches powered by Vision Transformers (ViTs), Large and Small Language
Models (SLMs), and VLMs. By synthesizing these perspectives, our survey
delivers a clear roadmap of current capabilities, open challenges, and future
opportunities.

</details>


### [58] [Towards Reliable Sea Ice Drift Estimation in the Arctic Deep Learning Optical Flow on RADARSAT-2](https://arxiv.org/abs/2510.26653)
*Daniela Martin,Joseph Gallego*

Main category: cs.CV

TL;DR: Deep learning optical flow models achieve sub-kilometer accuracy for sea ice drift estimation from SAR imagery, outperforming classical methods and enabling continuous motion mapping for Arctic navigation and climate research.


<details>
  <summary>Details</summary>
Motivation: Accurate sea ice drift estimation is critical for Arctic navigation, climate research, and operational forecasting, but existing methods have limitations in complex scenarios.

Method: First large-scale benchmark of 48 deep learning optical flow models on RADARSAT-2 ScanSAR sea ice imagery, evaluated against GNSS-tracked buoys using endpoint error (EPE) and Fl all metrics.

Result: Several models achieved sub-kilometer accuracy (EPE 6-8 pixels, 300-400m), capable of capturing consistent regional drift patterns and providing spatially continuous drift fields.

Conclusion: Deep learning optical flow methods can be effectively transferred to polar remote sensing, offering new opportunities for navigation and climate modeling through continuous motion estimation.

Abstract: Accurate estimation of sea ice drift is critical for Arctic navigation,
climate research, and operational forecasting. While optical flow, a computer
vision technique for estimating pixel wise motion between consecutive images,
has advanced rapidly in computer vision, its applicability to geophysical
problems and to satellite SAR imagery remains underexplored. Classical optical
flow methods rely on mathematical models and strong assumptions about motion,
which limit their accuracy in complex scenarios. Recent deep learning based
approaches have substantially improved performance and are now the standard in
computer vision, motivating their application to sea ice drift estimation. We
present the first large scale benchmark of 48 deep learning optical flow models
on RADARSAT 2 ScanSAR sea ice imagery, evaluated with endpoint error (EPE) and
Fl all metrics against GNSS tracked buoys. Several models achieve sub kilometer
accuracy (EPE 6 to 8 pixels, 300 to 400 m), a small error relative to the
spatial scales of sea ice motion and typical navigation requirements in the
Arctic. Our results demonstrate that the models are capable of capturing
consistent regional drift patterns and that recent deep learning based optical
flow methods, which have substantially improved motion estimation accuracy
compared to classical methods, can be effectively transferred to polar remote
sensing. Optical flow produces spatially continuous drift fields, providing
motion estimates for every image pixel rather than at sparse buoy locations,
offering new opportunities for navigation and climate modeling.

</details>


### [59] [Improving Classification of Occluded Objects through Scene Context](https://arxiv.org/abs/2510.26681)
*Courtney M. King,Daniel D. Leeds,Damian Lyons,George Kalaitzis*

Main category: cs.CV

TL;DR: The paper proposes two scene-based information fusion techniques to improve object detection under occlusion conditions in RPN-DCNN networks, showing improved recall and precision.


<details>
  <summary>Details</summary>
Motivation: Occlusions pose significant challenges to object recognition algorithms, and scene context can provide valuable additional information to reduce errors caused by occlusions.

Method: Two distinct scene-based fusion techniques: 1) pre-prediction method that selects custom object networks based on identified background scenes, and 2) post-detection method that fuses scene knowledge into initial object scores from RPN.

Result: Demonstrated on challenging datasets with partial occlusions, showing overall improvement in both recall and precision against baseline methods. Training on combination of occluded and unoccluded images performed best.

Conclusion: The method is interpretable, easily adaptable to other datasets, and offers promising directions for future research and practical applications in occlusion handling.

Abstract: The presence of occlusions has provided substantial challenges to
typically-powerful object recognition algorithms. Additional sources of
information can be extremely valuable to reduce errors caused by occlusions.
Scene context is known to aid in object recognition in biological vision. In
this work, we attempt to add robustness into existing Region Proposal
Network-Deep Convolutional Neural Network (RPN-DCNN) object detection networks
through two distinct scene-based information fusion techniques. We present one
algorithm under each methodology: the first operates prior to prediction,
selecting a custom object network to use based on the identified background
scene, and the second operates after detection, fusing scene knowledge into
initial object scores output by the RPN. We demonstrate our algorithms on
challenging datasets featuring partial occlusions, which show overall
improvement in both recall and precision against baseline methods. In addition,
our experiments contrast multiple training methodologies for occlusion
handling, finding that training on a combination of both occluded and
unoccluded images demonstrates an improvement over the others. Our method is
interpretable and can easily be adapted to other datasets, offering many future
directions for research and practical applications.

</details>


### [60] [Process Integrated Computer Vision for Real-Time Failure Prediction in Steel Rolling Mill](https://arxiv.org/abs/2510.26684)
*Vaibhav Kurrey,Sivakalyan Pujari,Gagan Raj Gupta*

Main category: cs.CV

TL;DR: A machine vision-based anomaly detection system was deployed in a steel rolling mill to predict equipment failures using industrial cameras and deep learning, reducing unplanned breakdown costs through proactive maintenance.


<details>
  <summary>Details</summary>
Motivation: To reduce unplanned breakdown costs and improve operational reliability in industrial manufacturing by predicting equipment failures before they occur.

Method: Integration of industrial cameras to monitor equipment operation, alignment, and hot bar motion in real time, with live video streams processed on a centralized video server using deep learning models. Joint analysis of sensor data and visual inputs to identify failure locations and root causes.

Result: The system enables early prediction of equipment failures and process interruptions, reduces computational load on industrial process control systems, and provides actionable insights for proactive maintenance.

Conclusion: The integrated machine vision and sensor data approach enhances operational reliability, productivity, and profitability in industrial manufacturing environments through scalable deployment and proactive failure prediction.

Abstract: We present a long-term deployment study of a machine vision-based anomaly
detection system for failure prediction in a steel rolling mill. The system
integrates industrial cameras to monitor equipment operation, alignment, and
hot bar motion in real time along the process line. Live video streams are
processed on a centralized video server using deep learning models, enabling
early prediction of equipment failures and process interruptions, thereby
reducing unplanned breakdown costs. Server-based inference minimizes the
computational load on industrial process control systems (PLCs), supporting
scalable deployment across production lines with minimal additional resources.
By jointly analyzing sensor data from data acquisition systems and visual
inputs, the system identifies the location and probable root causes of
failures, providing actionable insights for proactive maintenance. This
integrated approach enhances operational reliability, productivity, and
profitability in industrial manufacturing environments.

</details>


### [61] [The Impact and Outlook of 3D Gaussian Splatting](https://arxiv.org/abs/2510.26694)
*Bernhard Kerbl*

Main category: cs.CV

TL;DR: 3D Gaussian Splatting (3DGS) has evolved from a breakthrough 3D scene representation into a versatile foundational tool, with advances in efficiency, dynamic representations, mathematical foundations, mobile/VR deployment, massive-scale environments, and near-instant reconstruction.


<details>
  <summary>Details</summary>
Motivation: To provide an overview of the key research directions and developments that have emerged following the introduction of 3DGS, highlighting its transformation of the 3D scene representation landscape.

Method: Survey and analysis of follow-up research including efficiency improvements, dynamic 4D representations, mathematical exploration, mobile/VR platform adaptation, massive-scale environment extension, and fast reconstruction techniques.

Result: 3DGS has inspired extensive research leading to enhanced efficiency, scalability, real-world applicability, and evolution into a foundational tool for 3D vision and graphics.

Conclusion: 3DGS has successfully evolved from an initial breakthrough into a versatile and foundational technology that continues to drive innovation across multiple domains of 3D representation and rendering.

Abstract: Since its introduction, 3D Gaussian Splatting (3DGS) has rapidly transformed
the landscape of 3D scene representations, inspiring an extensive body of
associated research. Follow-up work includes analyses and contributions that
enhance the efficiency, scalability, and real-world applicability of 3DGS. In
this summary, we present an overview of several key directions that have
emerged in the wake of 3DGS. We highlight advances enabling resource-efficient
training and rendering, the evolution toward dynamic (or four-dimensional,
4DGS) representations, and deeper exploration of the mathematical foundations
underlying its appearance modeling and rendering process. Furthermore, we
examine efforts to bring 3DGS to mobile and virtual reality platforms, its
extension to massive-scale environments, and recent progress toward
near-instant radiance field reconstruction via feed-forward or distributed
computation. Collectively, these developments illustrate how 3DGS has evolved
from a breakthrough representation into a versatile and foundational tool for
3D vision and graphics.

</details>


### [62] [SteerVLM: Robust Model Control through Lightweight Activation Steering for Vision Language Models](https://arxiv.org/abs/2510.26769)
*Anushka Sivakumar,Andrew Zhang,Zaber Hakim,Chris Thomas*

Main category: cs.CV

TL;DR: SteerVLM is a lightweight steering module that guides Vision-Language Models to better follow instructions by dynamically adjusting activations between language and image modalities, requiring only 0.14% of the original model's parameters.


<details>
  <summary>Details</summary>
Motivation: To enable fine-grained control over VLM outputs during inference without modifying model weights, while preserving performance on other tasks and avoiding manual intervention requirements.

Method: Learns from latent embeddings of paired prompts to dynamically adjust activations connecting language modality with image context, using dimension-wise activation modulation and adaptive steering across layers without static vectors or manual tuning.

Result: Outperforms existing intervention techniques on steering and hallucination mitigation benchmarks, achieves model control with minimal parameter overhead (0.14% of VLM size), and introduces the VNIA dataset for evaluation.

Conclusion: Provides a robust solution for multimodal model control through activation engineering, enabling inference-time control over complex output semantics while maintaining off-task performance.

Abstract: This work introduces SteerVLM, a lightweight steering module designed to
guide Vision-Language Models (VLMs) towards outputs that better adhere to
desired instructions. Our approach learns from the latent embeddings of paired
prompts encoding target and converse behaviors to dynamically adjust
activations connecting the language modality with image context. This allows
for fine-grained, inference-time control over complex output semantics without
modifying model weights while preserving performance on off-target tasks. Our
steering module requires learning parameters equal to 0.14% of the original
VLM's size. Our steering module gains model control through dimension-wise
activation modulation and adaptive steering across layers without requiring
pre-extracted static vectors or manual tuning of intervention points.
Furthermore, we introduce VNIA (Visual Narrative Intent Alignment), a
multimodal dataset specifically created to facilitate the development and
evaluation of VLM steering techniques. Our method outperforms existing
intervention techniques on steering and hallucination mitigation benchmarks for
VLMs and proposes a robust solution for multimodal model control through
activation engineering.

</details>


### [63] [ChartAB: A Benchmark for Chart Grounding & Dense Alignment](https://arxiv.org/abs/2510.26781)
*Aniruddh Bansal,Davit Soselia,Dang Nguyen,Tianyi Zhou*

Main category: cs.CV

TL;DR: The paper introduces ChartAlign Benchmark (ChartAB) to evaluate vision-language models' chart grounding capabilities, including data extraction, element localization, and attribute recognition.


<details>
  <summary>Details</summary>
Motivation: Existing VLMs lack accurate perception of details and struggle with fine-grained structure extraction from charts, limiting their ability to compare and reason over multiple charts.

Method: Developed a comprehensive benchmark with JSON templates for evaluation metrics, and introduced a two-stage inference workflow to assess chart alignment and comparison capabilities.

Result: Evaluation of recent VLMs revealed perception biases, weaknesses, robustness issues, and hallucinations in chart understanding, highlighting fine-grained discrepancies among models.

Conclusion: The findings identify specific skills that need strengthening in current VLMs for better chart understanding and point to areas for model improvement.

Abstract: Charts play an important role in visualization, reasoning, data analysis, and
the exchange of ideas among humans. However, existing vision-language models
(VLMs) still lack accurate perception of details and struggle to extract
fine-grained structures from charts. Such limitations in chart grounding also
hinder their ability to compare multiple charts and reason over them. In this
paper, we introduce a novel "ChartAlign Benchmark (ChartAB)" to provide a
comprehensive evaluation of VLMs in chart grounding tasks, i.e., extracting
tabular data, localizing visualization elements, and recognizing various
attributes from charts of diverse types and complexities. We design a JSON
template to facilitate the calculation of evaluation metrics specifically
tailored for each grounding task. By incorporating a novel two-stage inference
workflow, the benchmark can further evaluate VLMs' capability to align and
compare elements/attributes across two charts. Our analysis of evaluations on
several recent VLMs reveals new insights into their perception biases,
weaknesses, robustness, and hallucinations in chart understanding. These
findings highlight the fine-grained discrepancies among VLMs in chart
understanding tasks and point to specific skills that need to be strengthened
in current models.

</details>


### [64] [HEIR: Learning Graph-Based Motion Hierarchies](https://arxiv.org/abs/2510.26786)
*Cheng Zheng,William Koch,Baiang Li,Felix Heide*

Main category: cs.CV

TL;DR: A general hierarchical motion modeling method that learns structured, interpretable motion relationships from data using graph-based hierarchies and differentiable graph learning.


<details>
  <summary>Details</summary>
Motivation: Existing methods rely on manually-defined or heuristic hierarchies with fixed motion primitives, limiting generalizability across different tasks.

Method: Represents motions using graph-based hierarchies, decomposing global motions into parent-inherited patterns and local residuals. Formulates hierarchy inference as differentiable graph learning with vertices representing elemental motions and directed edges capturing parent-child dependencies through graph neural networks.

Result: Successfully reconstructs intrinsic motion hierarchy in 1D and 2D cases, and produces more realistic and interpretable deformations compared to baseline on dynamic 3D Gaussian splatting scenes.

Conclusion: Provides an adaptable, data-driven hierarchical modeling paradigm applicable to a broad range of motion-centric tasks.

Abstract: Hierarchical structures of motion exist across research fields, including
computer vision, graphics, and robotics, where complex dynamics typically arise
from coordinated interactions among simpler motion components. Existing methods
to model such dynamics typically rely on manually-defined or heuristic
hierarchies with fixed motion primitives, limiting their generalizability
across different tasks. In this work, we propose a general hierarchical motion
modeling method that learns structured, interpretable motion relationships
directly from data. Our method represents observed motions using graph-based
hierarchies, explicitly decomposing global absolute motions into
parent-inherited patterns and local motion residuals. We formulate hierarchy
inference as a differentiable graph learning problem, where vertices represent
elemental motions and directed edges capture learned parent-child dependencies
through graph neural networks. We evaluate our hierarchical reconstruction
approach on three examples: 1D translational motion, 2D rotational motion, and
dynamic 3D scene deformation via Gaussian splatting. Experimental results show
that our method reconstructs the intrinsic motion hierarchy in 1D and 2D cases,
and produces more realistic and interpretable deformations compared to the
baseline on dynamic 3D Gaussian splatting scenes. By providing an adaptable,
data-driven hierarchical modeling paradigm, our method offers a formulation
applicable to a broad range of motion-centric tasks. Project Page:
https://light.princeton.edu/HEIR/

</details>


### [65] [The Quest for Generalizable Motion Generation: Data, Model, and Evaluation](https://arxiv.org/abs/2510.26794)
*Jing Lin,Ruisi Wang,Junzhe Lu,Ziqi Huang,Guorui Song,Ailing Zeng,Xian Liu,Chen Wei,Wanqi Yin,Qingping Sun,Zhongang Cai,Lei Yang,Ziwei Liu*

Main category: cs.CV

TL;DR: This paper presents a comprehensive framework that transfers knowledge from video generation (ViGen) to 3D human motion generation (MoGen) through data, modeling, and evaluation improvements, achieving superior generalization capabilities.


<details>
  <summary>Details</summary>
Motivation: Existing 3D human motion generation models face fundamental bottlenecks in generalization capability, while video generation models have demonstrated remarkable generalization in modeling human behaviors, suggesting transferable insights that MoGen can leverage.

Method: 1) Created ViMoGen-228K dataset with 228,000 high-quality motion samples integrating MoCap data, web videos, and synthesized samples; 2) Proposed ViMoGen - a flow-matching-based diffusion transformer with gated multimodal conditioning; 3) Developed ViMoGen-light as a distilled variant; 4) Introduced MBench hierarchical benchmark for fine-grained evaluation.

Result: Extensive experiments show the framework significantly outperforms existing approaches in both automatic and human evaluations, demonstrating superior generalization capabilities.

Conclusion: The proposed framework successfully transfers knowledge from video generation to motion generation across data, modeling, and evaluation pillars, achieving state-of-the-art performance and enhanced generalization in 3D human motion generation.

Abstract: Despite recent advances in 3D human motion generation (MoGen) on standard
benchmarks, existing models still face a fundamental bottleneck in their
generalization capability. In contrast, adjacent generative fields, most
notably video generation (ViGen), have demonstrated remarkable generalization
in modeling human behaviors, highlighting transferable insights that MoGen can
leverage. Motivated by this observation, we present a comprehensive framework
that systematically transfers knowledge from ViGen to MoGen across three key
pillars: data, modeling, and evaluation. First, we introduce ViMoGen-228K, a
large-scale dataset comprising 228,000 high-quality motion samples that
integrates high-fidelity optical MoCap data with semantically annotated motions
from web videos and synthesized samples generated by state-of-the-art ViGen
models. The dataset includes both text-motion pairs and text-video-motion
triplets, substantially expanding semantic diversity. Second, we propose
ViMoGen, a flow-matching-based diffusion transformer that unifies priors from
MoCap data and ViGen models through gated multimodal conditioning. To enhance
efficiency, we further develop ViMoGen-light, a distilled variant that
eliminates video generation dependencies while preserving strong
generalization. Finally, we present MBench, a hierarchical benchmark designed
for fine-grained evaluation across motion quality, prompt fidelity, and
generalization ability. Extensive experiments show that our framework
significantly outperforms existing approaches in both automatic and human
evaluations. The code, data, and benchmark will be made publicly available.

</details>


### [66] [Scaling Image Geo-Localization to Continent Level](https://arxiv.org/abs/2510.26795)
*Philipp Lindenberger,Paul-Edouard Sarlin,Jan Hosang,Matteo Balice,Marc Pollefeys,Simon Lynen,Eduard Trulls*

Main category: cs.CV

TL;DR: A hybrid approach for fine-grained image geo-localization at continental scale using proxy classification and aerial imagery embeddings to achieve precise localization within 200m for over 68% of queries.


<details>
  <summary>Details</summary>
Motivation: Standard image retrieval techniques are inefficient for global scale and fail with insufficient coverage, while existing scalable solutions either provide coarse results (10+ km) or suffer from domain gaps in cross-view retrieval.

Method: Uses proxy classification during training to learn rich feature representations encoding precise location, combined with aerial imagery embeddings to increase robustness to sparse ground-level data, enabling direct fine-grained retrieval across multiple countries.

Result: Achieves localization within 200m for more than 68% of queries on a dataset covering large parts of Europe, demonstrating effective fine-grained geo-localization at continental scale.

Conclusion: The hybrid approach successfully enables precise, fine-grained geo-localization across large geographic areas by combining learned prototypes with aerial imagery embeddings, overcoming limitations of existing methods.

Abstract: Determining the precise geographic location of an image at a global scale
remains an unsolved challenge. Standard image retrieval techniques are
inefficient due to the sheer volume of images (>100M) and fail when coverage is
insufficient. Scalable solutions, however, involve a trade-off: global
classification typically yields coarse results (10+ kilometers), while
cross-view retrieval between ground and aerial imagery suffers from a domain
gap and has been primarily studied on smaller regions. This paper introduces a
hybrid approach that achieves fine-grained geo-localization across a large
geographic expanse the size of a continent. We leverage a proxy classification
task during training to learn rich feature representations that implicitly
encode precise location information. We combine these learned prototypes with
embeddings of aerial imagery to increase robustness to the sparsity of
ground-level data. This enables direct, fine-grained retrieval over areas
spanning multiple countries. Our extensive evaluation demonstrates that our
approach can localize within 200m more than 68\% of queries of a dataset
covering a large part of Europe. The code is publicly available at
https://scaling-geoloc.github.io.

</details>


### [67] [SEE4D: Pose-Free 4D Generation via Auto-Regressive Video Inpainting](https://arxiv.org/abs/2510.26796)
*Dongyue Lu,Ao Liang,Tianxin Huang,Xiao Fu,Yuyang Zhao,Baorui Ma,Liang Pan,Wei Yin,Lingdong Kong,Wei Tsang Ooi,Ziwei Liu*

Main category: cs.CV

TL;DR: SEE4D is a pose-free video-to-4D framework that uses virtual cameras and view-conditional inpainting to synthesize spatiotemporal content from casual videos without 3D supervision, outperforming trajectory-based methods.


<details>
  <summary>Details</summary>
Motivation: Existing video-to-4D methods require manually annotated camera poses which are labor-intensive and brittle for in-the-wild footage. Warp-then-inpaint approaches still entangle camera motion with scene dynamics, complicating modeling.

Method: Replaces trajectory prediction with rendering to fixed virtual cameras, separating camera control from scene modeling. Uses view-conditional video inpainting trained to denoise warped images and inpaint missing regions across viewpoints. Features spatiotemporal autoregressive inference with virtual-camera splines and overlapping windows.

Result: Achieves superior generalization and improved performance on cross-view video generation and sparse reconstruction benchmarks compared to pose- or trajectory-conditioned baselines, both quantitatively and qualitatively.

Conclusion: SEE4D advances practical 4D world modeling from casual videos by eliminating the need for explicit 3D annotations and providing a more robust framework for synthesizing spatiotemporal content.

Abstract: Immersive applications call for synthesizing spatiotemporal 4D content from
casual videos without costly 3D supervision. Existing video-to-4D methods
typically rely on manually annotated camera poses, which are labor-intensive
and brittle for in-the-wild footage. Recent warp-then-inpaint approaches
mitigate the need for pose labels by warping input frames along a novel camera
trajectory and using an inpainting model to fill missing regions, thereby
depicting the 4D scene from diverse viewpoints. However, this
trajectory-to-trajectory formulation often entangles camera motion with scene
dynamics and complicates both modeling and inference. We introduce SEE4D, a
pose-free, trajectory-to-camera framework that replaces explicit trajectory
prediction with rendering to a bank of fixed virtual cameras, thereby
separating camera control from scene modeling. A view-conditional video
inpainting model is trained to learn a robust geometry prior by denoising
realistically synthesized warped images and to inpaint occluded or missing
regions across virtual viewpoints, eliminating the need for explicit 3D
annotations. Building on this inpainting core, we design a spatiotemporal
autoregressive inference pipeline that traverses virtual-camera splines and
extends videos with overlapping windows, enabling coherent generation at
bounded per-step complexity. We validate See4D on cross-view video generation
and sparse reconstruction benchmarks. Across quantitative metrics and
qualitative assessments, our method achieves superior generalization and
improved performance relative to pose- or trajectory-conditioned baselines,
advancing practical 4D world modeling from casual videos.

</details>


### [68] [Masked Diffusion Captioning for Visual Feature Learning](https://arxiv.org/abs/2510.26799)
*Chao Feng,Zihao Wei,Andrew Owens*

Main category: cs.CV

TL;DR: A masked diffusion captioning (MDC) method learns visual features by reconstructing masked text tokens from images, achieving competitive performance with autoregressive and contrastive approaches.


<details>
  <summary>Details</summary>
Motivation: To develop a more effective visual feature learning method that doesn't depend on token position like autoregressive approaches, reducing the need for auxiliary objectives.

Method: Train an image-conditioned masked diffusion language model where text tokens in image-caption pairs are randomly masked, and a visual-conditioned decoder reconstructs the original text.

Result: Linear probing experiments show the learned visual features are competitive with those from autoregressive and contrastive approaches across various models and datasets.

Conclusion: Masked diffusion captioning provides an effective alternative for visual feature learning that reduces position dependency and auxiliary objective requirements while maintaining competitive performance.

Abstract: We learn visual features by captioning images with an image-conditioned
masked diffusion language model, a formulation we call masked diffusion
captioning (MDC). During training, text tokens in each image-caption pair are
masked at a randomly chosen ratio, and a decoder conditioned on visual features
is trained to reconstruct the original text. After training, the learned visual
features can be applied to downstream vision tasks. Unlike autoregressive
captioning, the strength of the visual learning signal in MDC does not depend
on each token's position in the sequence, reducing the need for auxiliary
objectives. Linear probing experiments across a variety of academic-scale
models and datasets show that the learned visual features are competitive with
those produced by autoregressive and contrastive approaches.

</details>


### [69] [OmniX: From Unified Panoramic Generation and Perception to Graphics-Ready 3D Scenes](https://arxiv.org/abs/2510.26800)
*Yukun Huang,Jiwen Yu,Yanning Zhou,Jianan Wang,Xintao Wang,Pengfei Wan,Xihui Liu*

Main category: cs.CV

TL;DR: OmniX is a unified framework that repurposes 2D generative models for panoramic perception of geometry, textures, and PBR materials to generate graphics-ready 3D scenes suitable for physically based rendering, relighting, and simulation.


<details>
  <summary>Details</summary>
Motivation: To advance panorama-based 2D lifting techniques beyond appearance generation and enable perception of intrinsic properties for creating physically realistic 3D environments.

Method: Uses a lightweight cross-modal adapter structure to reuse 2D generative priors for panoramic vision tasks including perception, generation, and completion. Constructs a large-scale synthetic panorama dataset with multimodal panoramas from diverse scenes.

Result: Extensive experiments demonstrate effectiveness in panoramic visual perception and graphics-ready 3D scene generation, enabling physically realistic virtual world creation.

Conclusion: OmniX opens new possibilities for immersive and physically realistic virtual world generation by unifying panoramic perception and generation capabilities.

Abstract: There are two prevalent ways to constructing 3D scenes: procedural generation
and 2D lifting. Among them, panorama-based 2D lifting has emerged as a
promising technique, leveraging powerful 2D generative priors to produce
immersive, realistic, and diverse 3D environments. In this work, we advance
this technique to generate graphics-ready 3D scenes suitable for physically
based rendering (PBR), relighting, and simulation. Our key insight is to
repurpose 2D generative models for panoramic perception of geometry, textures,
and PBR materials. Unlike existing 2D lifting approaches that emphasize
appearance generation and ignore the perception of intrinsic properties, we
present OmniX, a versatile and unified framework. Based on a lightweight and
efficient cross-modal adapter structure, OmniX reuses 2D generative priors for
a broad range of panoramic vision tasks, including panoramic perception,
generation, and completion. Furthermore, we construct a large-scale synthetic
panorama dataset containing high-quality multimodal panoramas from diverse
indoor and outdoor scenes. Extensive experiments demonstrate the effectiveness
of our model in panoramic visual perception and graphics-ready 3D scene
generation, opening new possibilities for immersive and physically realistic
virtual world generation.

</details>


### [70] [Are Video Models Ready as Zero-Shot Reasoners? An Empirical Study with the MME-CoF Benchmark](https://arxiv.org/abs/2510.26802)
*Ziyu Guo,Xinyan Chen,Renrui Zhang,Ruichuan An,Yu Qi,Dongzhi Jiang,Xiangtai Li,Manyuan Zhang,Hongsheng Li,Pheng-Ann Heng*

Main category: cs.CV

TL;DR: Video generation models like Veo-3 show promising visual reasoning capabilities but are not yet reliable as standalone zero-shot reasoners, exhibiting strengths in short-horizon spatial coherence but limitations in long-horizon causal reasoning.


<details>
  <summary>Details</summary>
Motivation: To investigate whether current video generation models can serve as zero-shot reasoners in challenging visual reasoning scenarios, given their demonstrated capabilities in producing high-fidelity, temporally coherent videos.

Method: Conducted an empirical study evaluating Veo-3's reasoning behavior across 12 dimensions including spatial, geometric, physical, temporal, and embodied logic, using the curated MME-CoF benchmark for Chain-of-Frame reasoning assessment.

Result: Video models demonstrate promising reasoning patterns on short-horizon spatial coherence, fine-grained grounding, and locally consistent dynamics, but remain limited in long-horizon causal reasoning, strict geometric constraints, and abstract logic.

Conclusion: Current video models are not yet reliable as standalone zero-shot reasoners but exhibit encouraging signs as complementary visual engines alongside dedicated reasoning models.

Abstract: Recent video generation models can produce high-fidelity, temporally coherent
videos, indicating that they may encode substantial world knowledge. Beyond
realistic synthesis, they also exhibit emerging behaviors indicative of visual
perception, modeling, and manipulation. Yet, an important question still
remains: Are video models ready to serve as zero-shot reasoners in challenging
visual reasoning scenarios? In this work, we conduct an empirical study to
comprehensively investigate this question, focusing on the leading and popular
Veo-3. We evaluate its reasoning behavior across 12 dimensions, including
spatial, geometric, physical, temporal, and embodied logic, systematically
characterizing both its strengths and failure modes. To standardize this study,
we curate the evaluation data into MME-CoF, a compact benchmark that enables
in-depth and thorough assessment of Chain-of-Frame (CoF) reasoning. Our
findings reveal that while current video models demonstrate promising reasoning
patterns on short-horizon spatial coherence, fine-grained grounding, and
locally consistent dynamics, they remain limited in long-horizon causal
reasoning, strict geometric constraints, and abstract logic. Overall, they are
not yet reliable as standalone zero-shot reasoners, but exhibit encouraging
signs as complementary visual engines alongside dedicated reasoning models.
Project page: https://video-cof.github.io

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [71] [A Practitioner's Guide to Kolmogorov-Arnold Networks](https://arxiv.org/abs/2510.25781)
*Amir Noorizadegan,Sifan Wang,Leevan Ling*

Main category: cs.LG

TL;DR: This paper provides a comprehensive review of Kolmogorov-Arnold Networks (KANs), covering their theoretical foundations, architectural variants, implementation strategies, and practical guidance for selection and use.


<details>
  <summary>Details</summary>
Motivation: KANs have emerged as a promising alternative to MLPs with enhanced expressivity and interpretability, but the rapidly expanding landscape requires systematic organization and synthesis to guide researchers and practitioners.

Method: The review systematically categorizes KAN implementations, analyzes basis function choices (B-splines, polynomials, ReLU, etc.), and structures advancements into a roadmap covering accuracy, efficiency, and regularization techniques.

Result: The paper establishes formal equivalence between KANs and MLPs, highlights KANs' superior parameter efficiency, and provides a practical guide for selecting appropriate KAN architectures based on application needs.

Conclusion: The review maps the vibrant KAN ecosystem, identifies current research gaps, and provides a structured reference through an associated GitHub repository to support ongoing KAN research and practical implementation.

Abstract: Kolmogorov-Arnold Networks (KANs) have recently emerged as a promising
alternative to traditional Multilayer Perceptrons (MLPs), inspired by the
Kolmogorov-Arnold representation theorem. Unlike MLPs, which use fixed
activation functions on nodes, KANs employ learnable univariate basis functions
on edges, offering enhanced expressivity and interpretability. This review
provides a systematic and comprehensive overview of the rapidly expanding KAN
landscape, moving beyond simple performance comparisons to offer a structured
synthesis of theoretical foundations, architectural variants, and practical
implementation strategies. By collecting and categorizing a vast array of
open-source implementations, we map the vibrant ecosystem supporting KAN
development. We begin by bridging the conceptual gap between KANs and MLPs,
establishing their formal equivalence and highlighting the superior parameter
efficiency of the KAN formulation. A central theme of our review is the
critical role of the basis function; we survey a wide array of choices,
including B-splines, Chebyshev and Jacobi polynomials, ReLU compositions,
Gaussian RBFs, and Fourier series, and analyze their respective trade-offs in
terms of smoothness, locality, and computational cost. We then categorize
recent advancements into a clear roadmap, covering techniques for improving
accuracy, efficiency, and regularization. Key topics include physics-informed
loss design, adaptive sampling, domain decomposition, hybrid architectures, and
specialized methods for handling discontinuities. Finally, we provide a
practical "Choose-Your-KAN" guide to help practitioners select appropriate
architectures, and we conclude by identifying current research gaps. The
associated GitHub repository https://github.com/AmirNoori68/kan-review
complements this paper and serves as a structured reference for ongoing KAN
research.

</details>


### [72] [HiMAE: Hierarchical Masked Autoencoders Discover Resolution-Specific Structure in Wearable Time Series](https://arxiv.org/abs/2510.25785)
*Simon A. Lee,Cyrus Tanade,Hao Zhou,Juhyeon Lee,Megha Thukral,Minji Han,Rachel Choi,Md Sazzad Hissain Khan,Baiying Lu,Migyeong Gwak,Mehrab Bin Morshed,Viswam Nathan,Md Mahbubur Rahman,Li Zhu,Subramaniam Venkatraman,Sharanya Arcot Desai*

Main category: cs.LG

TL;DR: HiMAE is a hierarchical masked autoencoder that learns multi-resolution embeddings from wearable sensor data, outperforming state-of-the-art models while being compact enough for edge inference on smartwatches.


<details>
  <summary>Details</summary>
Motivation: To understand how temporal resolution affects predictive utility in wearable sensor data, hypothesizing that different clinical outcomes rely on structure at distinct temporal scales.

Method: HiMAE combines masked autoencoding with a hierarchical convolutional encoder-decoder to produce multi-resolution embeddings, treating resolution as an interpretability probe rather than a hyperparameter.

Result: HiMAE consistently outperforms state-of-the-art foundation models across classification, regression, and generative benchmarks while being orders of magnitude smaller and achieving sub-millisecond inference on smartwatch CPUs.

Conclusion: HiMAE serves as both an efficient self-supervised learning method and a discovery tool for identifying scale-sensitive structure in wearable health data.

Abstract: Wearable sensors provide abundant physiological time series, yet the
principles governing their predictive utility remain unclear. We hypothesize
that temporal resolution is a fundamental axis of representation learning, with
different clinical and behavioral outcomes relying on structure at distinct
scales. To test this resolution hypothesis, we introduce HiMAE (Hierarchical
Masked Autoencoder), a self supervised framework that combines masked
autoencoding with a hierarchical convolutional encoder decoder. HiMAE produces
multi resolution embeddings that enable systematic evaluation of which temporal
scales carry predictive signal, transforming resolution from a hyperparameter
into a probe for interpretability. Across classification, regression, and
generative benchmarks, HiMAE consistently outperforms state of the art
foundation models that collapse scale, while being orders of magnitude smaller.
HiMAE is an efficient representation learner compact enough to run entirely on
watch, achieving sub millisecond inference on smartwatch class CPUs for true
edge inference. Together, these contributions position HiMAE as both an
efficient self supervised learning method and a discovery tool for scale
sensitive structure in wearable health.

</details>


### [73] [SHA-256 Infused Embedding-Driven Generative Modeling of High-Energy Molecules in Low-Data Regimes](https://arxiv.org/abs/2510.25788)
*Siddharth Verma,Alankar Alankar*

Main category: cs.LG

TL;DR: This paper presents a novel approach for discovering high-energy materials using LSTM networks for molecular generation and attentive GNNs for property prediction, achieving 67.5% validity and 37.5% novelty in generated molecules.


<details>
  <summary>Details</summary>
Motivation: High-energy materials discovery is constrained by limited experimental data and restricted access to testing facilities, creating a need for computational approaches to accelerate discovery.

Method: Combines LSTM networks for molecular generation with attentive Graph Neural Networks for property predictions, using a transformative embedding space construction strategy that integrates fixed SHA-256 embeddings with partially trainable representations.

Result: Achieved 67.5% validity and 37.5% novelty in generated molecules, with a mean Tanimoto coefficient of 0.214 indicating diverse chemical space generation. Identified 37 new super explosives with predicted detonation velocity higher than 9 km/s.

Conclusion: The proposed framework successfully generates diverse and novel high-energy molecules without pretraining, demonstrating potential for accelerating discovery in propulsion and defense domains.

Abstract: High-energy materials (HEMs) are critical for propulsion and defense domains,
yet their discovery remains constrained by experimental data and restricted
access to testing facilities. This work presents a novel approach toward
high-energy molecules by combining Long Short-Term Memory (LSTM) networks for
molecular generation and Attentive Graph Neural Networks (GNN) for property
predictions. We propose a transformative embedding space construction strategy
that integrates fixed SHA-256 embeddings with partially trainable
representations. Unlike conventional regularization techniques, this changes
the representational basis itself, reshaping the molecular input space before
learning begins. Without recourse to pretraining, the generator achieves 67.5%
validity and 37.5% novelty. The generated library exhibits a mean Tanimoto
coefficient of 0.214 relative to training set signifying the ability of
framework to generate a diverse chemical space. We identified 37 new super
explosives higher than 9 km/s predicted detonation velocity.

</details>


### [74] [The Kinetics of Reasoning: How Chain-of-Thought Shapes Learning in Transformers?](https://arxiv.org/abs/2510.25791)
*Zihan Pengmei,Costas Mavromatis,Zhengyuan Shen,Yunyi Zhang,Vassilis N. Ioannidis,Huzefa Rangwala*

Main category: cs.LG

TL;DR: Chain-of-thought supervision improves transformer performance but learning mechanisms are unclear. Study shows CoT accelerates generalization but doesn't overcome high algorithmic complexity tasks, reveals transient trace unfaithfulness phase, and alters internal computation.


<details>
  <summary>Details</summary>
Motivation: To understand how transformers learn to follow and benefit from chain-of-thought supervision, particularly the learning dynamics and mechanisms behind CoT's effectiveness.

Method: Pretrained transformers on symbolic reasoning tasks with tunable algorithmic complexity and controllable data composition. Compared two settings: final answers only vs. explicit CoT traces before answering. Used three-parameter logistic curve to model learning dynamics.

Result: CoT generally improves performance but benefits depend on task complexity. CoT accelerates generalization but cannot overcome high algorithmic complexity tasks. Models show transient trace unfaithfulness early in training (correct answers with incorrect reasoning) before aligning reasoning with answers.

Conclusion: CoT supervision alters transformer learning dynamics mechanistically, accelerates generalization, and trace faithfulness emerges as a dynamic property during training, but CoT cannot overcome fundamental algorithmic complexity limitations.

Abstract: Chain-of-thought (CoT) supervision can substantially improve transformer
performance, yet the mechanisms by which models learn to follow and benefit
from CoT remain poorly understood. We investigate these learning dynamics
through the lens of grokking by pretraining transformers on symbolic reasoning
tasks with tunable algorithmic complexity and controllable data composition to
study their generalization. Models were trained under two settings: (i)
producing only final answers, and (ii) emitting explicit CoT traces before
answering. Our results show that while CoT generally improves task performance,
its benefits depend on task complexity. To quantify these effects, we model the
accuracy of the logarithmic training steps with a three-parameter logistic
curve, revealing how the learning speed and shape vary with task complexity,
data distribution, and the presence of CoT supervision. We also uncover a
transient trace unfaithfulness phase: early in training, models often produce
correct answers while skipping or contradicting CoT steps, before later
aligning their reasoning traces with answers. Empirically, we (1) demonstrate
that CoT accelerates generalization but does not overcome tasks with higher
algorithmic complexity, such as finding list intersections; (2) introduce a
kinetic modeling framework for understanding transformer learning; (3)
characterize trace faithfulness as a dynamic property that emerges over
training; and (4) show CoT alters internal transformer computation
mechanistically.

</details>


### [75] [Optimal Information Combining for Multi-Agent Systems Using Adaptive Bias Learning](https://arxiv.org/abs/2510.25793)
*Siavash M. Alamouti,Fay Arjomandi*

Main category: cs.LG

TL;DR: The paper develops a theoretical framework to determine when bias learning in multi-agent systems is worthwhile, introducing the learnability ratio concept and presenting the ABLOC algorithm that achieves performance improvements bounded by this ratio.


<details>
  <summary>Details</summary>
Motivation: Multi-agent systems suffer performance degradation from systematic biases that vary with environmental conditions, leading to inaccurate monitoring, unreliable predictions, and flawed human judgment aggregation. Current approaches either ignore biases or require expensive calibration.

Method: Developed a theoretical framework decomposing biases into learnable systematic and irreducible stochastic components, introduced learnability ratio concept, and created the ABLOC algorithm that iteratively learns bias-correcting transformations while optimizing combination weights through closed-form solutions.

Result: Systems with high learnability ratios achieved 40%-70% of theoretical maximum performance improvement, while those with low learnability showed minimal benefit, validating the diagnostic criteria for deployment decisions.

Conclusion: The learnability ratio fundamentally bounds achievable performance improvement, providing quantitative guidance for system designers on when to invest in bias learning versus simpler approaches.

Abstract: Modern multi-agent systems ranging from sensor networks monitoring critical
infrastructure to crowdsourcing platforms aggregating human intelligence can
suffer significant performance degradation due to systematic biases that vary
with environmental conditions. Current approaches either ignore these biases,
leading to suboptimal decisions, or require expensive calibration procedures
that are often infeasible in practice. This performance gap has real
consequences: inaccurate environmental monitoring, unreliable financial
predictions, and flawed aggregation of human judgments. This paper addresses
the fundamental question: when can we learn and correct for these unknown
biases to recover near-optimal performance, and when is such learning futile?
We develop a theoretical framework that decomposes biases into learnable
systematic components and irreducible stochastic components, introducing the
concept of learnability ratio as the fraction of bias variance predictable from
observable covariates. This ratio determines whether bias learning is
worthwhile for a given system. We prove that the achievable performance
improvement is fundamentally bounded by this learnability ratio, providing
system designers with quantitative guidance on when to invest in bias learning
versus simpler approaches. We present the Adaptive Bias Learning and Optimal
Combining (ABLOC) algorithm, which iteratively learns bias-correcting
transformations while optimizing combination weights through closedform
solutions, guaranteeing convergence to these theoretical bounds. Experimental
validation demonstrates that systems with high learnability ratios can recover
significant performance (we achieved 40%-70% of theoretical maximum improvement
in our examples), while those with low learnability show minimal benefit,
validating our diagnostic criteria for practical deployment decisions.

</details>


### [76] [Non-myopic Matching and Rebalancing in Large-Scale On-Demand Ride-Pooling Systems Using Simulation-Informed Reinforcement Learning](https://arxiv.org/abs/2510.25796)
*Farnoosh Namdarpour,Joseph Y. J. Chow*

Main category: cs.LG

TL;DR: The paper proposes a simulation-informed reinforcement learning approach to address myopic decision-making in ride-pooling systems, extending a ride-hailing framework to enable non-myopic dispatch decisions and vehicle rebalancing.


<details>
  <summary>Details</summary>
Motivation: Ride-pooling services can reduce costs and environmental impacts but suffer from myopic decision-making that overlooks long-term effects of dispatch decisions.

Method: Extends a learning and planning framework from ride-hailing to ride-pooling by embedding a ride-pooling simulation within the RL mechanism, using n-step temporal difference learning on simulated experiences to derive spatiotemporal state values.

Result: Non-myopic policy increases service rate by up to 8.4%, reduces wait and in-vehicle times, and can decrease fleet size by over 25% while maintaining performance. Adding rebalancing further cuts wait time by 27.3%, in-vehicle time by 12.5%, and raises service rate by 15.1%.

Conclusion: The proposed simulation-informed RL framework enables effective non-myopic decision-making in ride-pooling systems, offering significant performance improvements and cost savings for operators while maintaining service quality.

Abstract: Ride-pooling, also known as ride-sharing, shared ride-hailing, or
microtransit, is a service wherein passengers share rides. This service can
reduce costs for both passengers and operators and reduce congestion and
environmental impacts. A key limitation, however, is its myopic
decision-making, which overlooks long-term effects of dispatch decisions. To
address this, we propose a simulation-informed reinforcement learning (RL)
approach. While RL has been widely studied in the context of ride-hailing
systems, its application in ride-pooling systems has been less explored. In
this study, we extend the learning and planning framework of Xu et al. (2018)
from ride-hailing to ride-pooling by embedding a ride-pooling simulation within
the learning mechanism to enable non-myopic decision-making. In addition, we
propose a complementary policy for rebalancing idle vehicles. By employing
n-step temporal difference learning on simulated experiences, we derive
spatiotemporal state values and subsequently evaluate the effectiveness of the
non-myopic policy using NYC taxi request data. Results demonstrate that the
non-myopic policy for matching can increase the service rate by up to 8.4%
versus a myopic policy while reducing both in-vehicle and wait times for
passengers. Furthermore, the proposed non-myopic policy can decrease fleet size
by over 25% compared to a myopic policy, while maintaining the same level of
performance, thereby offering significant cost savings for operators.
Incorporating rebalancing operations into the proposed framework cuts wait time
by up to 27.3%, in-vehicle time by 12.5%, and raises service rate by 15.1%
compared to using the framework for matching decisions alone at the cost of
increased vehicle minutes traveled per passenger.

</details>


### [77] [MemEIC: A Step Toward Continual and Compositional Knowledge Editing](https://arxiv.org/abs/2510.25798)
*Jin Seong,Jiyun Park,Wencke Liermann,Hongseok Choi,Yoonji Nam,Hyun Kim,Soojong Lim,Namhoon Lee*

Main category: cs.LG

TL;DR: MemEIC is a novel method for Continual and Compositional Knowledge Editing (CCKE) in large vision-language models that enables sequential editing of both visual and textual knowledge through a hybrid external-internal editor with dual memory and LoRA adapters.


<details>
  <summary>Details</summary>
Motivation: Current knowledge editing techniques focus on single modalities in isolation, neglecting the multimodality of LVLMs and continuous nature of knowledge updates, leading to suboptimal editing outcomes.

Method: Uses a hybrid external-internal editor with dual external memory for cross-modal evidence retrieval and dual LoRA adapters for disentangled parameter updates per modality, plus a brain-inspired knowledge connector for compositional reasoning.

Result: MemEIC significantly improves performance on complex multimodal questions and effectively preserves prior edits, setting a new benchmark for CCKE in LVLMs.

Conclusion: The proposed method successfully addresses the limitations of existing single-modality editing approaches by enabling continual and compositional knowledge editing across both visual and textual modalities.

Abstract: The dynamic nature of information necessitates continuously updating large
vision-language models (LVLMs). While recent knowledge editing techniques hint
at promising directions, they often focus on editing a single modality (vision
or language) in isolation. This prevalent practice neglects the inherent
multimodality of LVLMs and the continuous nature of knowledge updates,
potentially leading to suboptimal editing outcomes when considering the
interplay between modalities and the need for ongoing knowledge refinement. To
address these limitations, we propose MemEIC, a novel method for Continual and
Compositional Knowledge Editing (CCKE) in LVLMs. MemEIC enables compositional
editing of both visual and textual knowledge sequentially. Our approach employs
a hybrid external-internal editor featuring a dual external memory for
cross-modal evidence retrieval and dual LoRA adapters that facilitate
disentangled parameter updates for each modality. A key component is a
brain-inspired knowledge connector, activated selectively for compositional
reasoning, that integrates information across different modalities. Experiments
demonstrate that MemEIC significantly improves performance on complex
multimodal questions and effectively preserves prior edits, setting a new
benchmark for CCKE in LVLMs.

</details>


### [78] [FreIE: Low-Frequency Spectral Bias in Neural Networks for Time-Series Tasks](https://arxiv.org/abs/2510.25800)
*Jialong Sun,Xinpeng Ling,Jiaxuan Zou,Jiawen Kang,Kejia Zhang*

Main category: cs.LG

TL;DR: The paper identifies spectral bias as a universal phenomenon in time series prediction models where they prioritize fitting low-frequency signals over high-frequency ones, and proposes FreLE algorithm to mitigate this through frequency regularization.


<details>
  <summary>Details</summary>
Motivation: To unify understanding of spectral bias phenomenon in long-term time series prediction and address the challenge that existing models tend to fit low-frequency signals before high-frequency ones, limiting their performance.

Method: Conducted extensive empirical experiments to measure spectral bias across mainstream models, then proposed FreLE (Frequency Loss Enhancement) algorithm - a plug-and-play loss function unit that enhances model generalization through explicit and implicit frequency regularization.

Result: Found that virtually all existing models exhibit spectral bias phenomenon. Experiments demonstrated that FreLE achieves superior performance in mitigating spectral bias and improving prediction accuracy.

Conclusion: Spectral bias is a universal characteristic across time series prediction models, and the proposed FreLE algorithm effectively addresses this limitation through frequency regularization, providing a practical solution for improved long-term time series forecasting.

Abstract: The inherent autocorrelation of time series data presents an ongoing
challenge to multivariate time series prediction. Recently, a widely adopted
approach has been the incorporation of frequency domain information to assist
in long-term prediction tasks. Many researchers have independently observed the
spectral bias phenomenon in neural networks, where models tend to fit
low-frequency signals before high-frequency ones. However, these observations
have often been attributed to the specific architectures designed by the
researchers, rather than recognizing the phenomenon as a universal
characteristic across models. To unify the understanding of the spectral bias
phenomenon in long-term time series prediction, we conducted extensive
empirical experiments to measure spectral bias in existing mainstream models.
Our findings reveal that virtually all models exhibit this phenomenon. To
mitigate the impact of spectral bias, we propose the FreLE (Frequency Loss
Enhancement) algorithm, which enhances model generalization through both
explicit and implicit frequency regularization. This is a plug-and-play model
loss function unit. A large number of experiments have proven the superior
performance of FreLE. Code is available at
https://github.com/Chenxing-Xuan/FreLE.

</details>


### [79] [Metis-SPECS: Decoupling Multimodal Learning via Self-distilled Preference-based Cold Start](https://arxiv.org/abs/2510.25801)
*Kun Chen,Peng Shi,Haibo Qiu,Zhixiong Zeng,Siqi Yang,Wenji Mao,Lin Ma*

Main category: cs.LG

TL;DR: SPECS is a self-distilled, preference-based cold start framework that decouples multimodal learning into surface-form preference training and deep reasoning RL, outperforming SFT-based approaches.


<details>
  <summary>Details</summary>
Motivation: SFT-based cold start methods cause instruction-style overfitting and weaken out-of-distribution generalization, which negatively impacts downstream reinforcement learning performance.

Method: SPECS framework: (1) generates introspective preference data pairs via self-distillation, (2) performs preference-based training focusing on shallow, transferable surface-form criteria, and (3) hands off to RL with verifiable rewards for deep reasoning.

Result: Consistent performance gains across multimodal benchmarks: MEGA-Bench improved by 4.1% and MathVista by 12.2%. Reduces in-distribution "stuckness," improves exploration, stabilizes training, and raises performance ceiling.

Conclusion: Preference-based training methods generalize better than SFT-based methods for cold start, and the decoupled learning framework of SPECS effectively separates surface-form learning from deep reasoning.

Abstract: Reinforcement learning (RL) with verifiable rewards has recently catalyzed a
wave of "MLLM-r1" approaches that bring RL to vision language models. Most
representative paradigms begin with a cold start, typically employing
supervised fine-tuning (SFT), to initialize the policy before RL. However,
SFT-based cold start adopts the reasoning paradigm intertwined with task
solution and output format, which may induce instruction-style overfitting,
weakens out-of-distribution generalization, and ultimately affects downstream
RL. We revisit the cold start along two views, its training method and data
construction, and introduce the Generalization Factor (GF) coefficient to
quantify the generalization capability under different methods. Our empirical
study finds that preference-based training methods (e.g. DPO) generalizes
better than SFT-based methods in cold start. Motivated by this, we propose
SPECS-a Self-distilled, Preference-based Cold Start framework that decouples
multimodal learning: (1) generates introspective preference data pairs via
self-distillation, avoiding reliance on larger teachers or manual annotation;
(2) performs preference-based training to learn, focusing on shallow,
transferable surface-form criteria (format, structure, style) rather than
memorizing content; and (3) hands off to RL with verifiable rewards for deep
reasoning results. Experimental results across multiple multimodal benchmarks
show that our decoupling learning framework yields consistent performance gains
over strong baselines, improving MEGA-Bench by 4.1% and MathVista by 12.2%.
Additional experiments indicate that SPECS contributes to reducing
in-distribution "stuckness," improving exploration, stabilizing training, and
raising the performance ceiling.

</details>


### [80] [Contrastive Predictive Coding Done Right for Mutual Information Estimation](https://arxiv.org/abs/2510.25983)
*J. Jon Ryu,Pavan Yeddanapudi,Xiangxiang Xu,Gregory W. Wornell*

Main category: cs.LG

TL;DR: InfoNCE is not a valid mutual information estimator. The paper introduces InfoNCE-anchor, a modified version that reduces bias for accurate MI estimation, and generalizes the framework using proper scoring rules.


<details>
  <summary>Details</summary>
Motivation: InfoNCE has been widely used for mutual information estimation despite its indirect connection to MI. The authors aim to demonstrate why InfoNCE is not a valid MI estimator and propose a better alternative.

Method: Introduces InfoNCE-anchor with an auxiliary anchor class for consistent density ratio estimation. Generalizes the framework using proper scoring rules, unifying various contrastive objectives including NCE, InfoNCE, and f-divergence variants.

Result: InfoNCE-anchor with log score achieves the most accurate MI estimates. However, in self-supervised representation learning, the anchor modification does not improve downstream task performance.

Conclusion: Contrastive representation learning benefits from learning structured density ratios rather than accurate mutual information estimation per se.

Abstract: The InfoNCE objective, originally introduced for contrastive representation
learning, has become a popular choice for mutual information (MI) estimation,
despite its indirect connection to MI. In this paper, we demonstrate why
InfoNCE should not be regarded as a valid MI estimator, and we introduce a
simple modification, which we refer to as InfoNCE-anchor, for accurate MI
estimation. Our modification introduces an auxiliary anchor class, enabling
consistent density ratio estimation and yielding a plug-in MI estimator with
significantly reduced bias. Beyond this, we generalize our framework using
proper scoring rules, which recover InfoNCE-anchor as a special case when the
log score is employed. This formulation unifies a broad spectrum of contrastive
objectives, including NCE, InfoNCE, and $f$-divergence variants, under a single
principled framework. Empirically, we find that InfoNCE-anchor with the log
score achieves the most accurate MI estimates; however, in self-supervised
representation learning experiments, we find that the anchor does not improve
the downstream task performance. These findings corroborate that contrastive
representation learning benefits not from accurate MI estimation per se, but
from the learning of structured density ratios.

</details>


### [81] [Mixture-of-Experts Operator Transformer for Large-Scale PDE Pre-Training](https://arxiv.org/abs/2510.25803)
*Hong Wang,Haiyang Xin,Jie Wang,Xuanze Yang,Fei Zha,Huanshuo Dong,Yan Jiang*

Main category: cs.LG

TL;DR: MoE-POT is a sparse-activated pre-training operator transformer that uses mixture-of-experts architecture to efficiently scale parameters while controlling inference costs for solving PDE problems.


<details>
  <summary>Details</summary>
Motivation: Address challenges in PDE neural operator pre-training including dataset heterogeneity causing high errors in mixed training, and dense pre-training models having high inference costs.

Method: Propose Mixture-of-Experts Pre-training Operator Transformer (MoE-POT) with layer-wise router-gating network that dynamically selects 4 routed experts from 16 expert networks during inference, plus 2 shared experts to capture common PDE properties.

Result: Pre-trained models from 30M to 0.5B parameters on 6 PDE datasets; 90M activated parameter model achieves up to 40% reduction in zero-shot error compared to existing 120M parameter models; interpretability analysis shows dataset types can be inferred from router decisions.

Conclusion: MoE-POT effectively addresses PDE dataset heterogeneity and inference cost issues through sparse activation, demonstrating superior performance and interpretability in neural operator pre-training.

Abstract: Pre-training has proven effective in addressing data scarcity and performance
limitations in solving PDE problems with neural operators. However, challenges
remain due to the heterogeneity of PDE datasets in equation types, which leads
to high errors in mixed training. Additionally, dense pre-training models that
scale parameters by increasing network width or depth incur significant
inference costs. To tackle these challenges, we propose a novel
Mixture-of-Experts Pre-training Operator Transformer (MoE-POT), a
sparse-activated architecture that scales parameters efficiently while
controlling inference costs. Specifically, our model adopts a layer-wise
router-gating network to dynamically select 4 routed experts from 16 expert
networks during inference, enabling the model to focus on equation-specific
features. Meanwhile, we also integrate 2 shared experts, aiming to capture
common properties of PDE and reduce redundancy among routed experts. The final
output is computed as the weighted average of the results from all activated
experts. We pre-train models with parameters from 30M to 0.5B on 6 public PDE
datasets. Our model with 90M activated parameters achieves up to a 40%
reduction in zero-shot error compared with existing models with 120M activated
parameters. Additionally, we conduct interpretability analysis, showing that
dataset types can be inferred from router-gating network decisions, which
validates the rationality and effectiveness of the MoE architecture.

</details>


### [82] [PRESTO: Preimage-Informed Instruction Optimization for Prompting Black-Box LLMs](https://arxiv.org/abs/2510.25808)
*Jaewon Chu,Seunghun Lee,Hyunwoo J. Kim*

Main category: cs.LG

TL;DR: PRESTO is a novel framework that leverages the preimage structure of soft prompts to optimize instructions for black-box LLMs more efficiently by sharing evaluation scores, using preimage-based initialization, and enforcing score consistency.


<details>
  <summary>Details</summary>
Motivation: Existing methods for optimizing instructions for black-box LLMs suffer from redundant queries due to white-box LLMs mapping different soft prompts to the same instruction, which previous approaches viewed as a hindrance rather than an opportunity.

Method: PRESTO uses three key components: score sharing (sharing evaluation scores across all soft prompts in a preimage), preimage-based initialization (selecting initial points to maximize search space coverage), and score consistency regularization (enforcing prediction consistency within each preimage).

Result: PRESTO achieves the effect of obtaining 14 times more scored data under the same query budget and demonstrates superior performance on 33 instruction optimization tasks.

Conclusion: The preimage structure, previously viewed as a hindrance, can be leveraged as useful prior knowledge to significantly accelerate instruction optimization for black-box LLMs.

Abstract: Large language models (LLMs) have achieved remarkable success across diverse
domains, due to their strong instruction-following capabilities. This has led
to increasing interest in optimizing instructions for black-box LLMs, whose
internal parameters are inaccessible but widely used due to their strong
performance. To optimize instructions for black-box LLMs, recent methods employ
white-box LLMs to generate candidate instructions from optimized soft prompts.
However, white-box LLMs often map different soft prompts to the same
instruction, leading to redundant queries. While previous studies regarded this
many-to-one mapping as a structure that hinders optimization efficiency, we
reinterpret it as a useful prior knowledge that can accelerate the
optimization. To this end, we introduce PREimage-informed inSTruction
Optimization (PRESTO), a novel framework that leverages the preimage structure
of soft prompts for efficient optimization. PRESTO consists of three key
components: (1) score sharing, which shares the evaluation score with all soft
prompts in a preimage; (2) preimage-based initialization, which selects initial
data points that maximize search space coverage using preimage information; and
(3) score consistency regularization, which enforces prediction consistency
within each preimage. By leveraging preimages, PRESTO achieves the effect of
effectively obtaining 14 times more scored data under the same query budget,
resulting in more efficient optimization. Experimental results on 33
instruction optimization tasks demonstrate the superior performance of PRESTO.
Code is available at https://github.com/mlvlab/PRESTO

</details>


### [83] [ScaleDiff: Higher-Resolution Image Synthesis via Efficient and Model-Agnostic Diffusion](https://arxiv.org/abs/2510.25818)
*Sungho Koh,SeungJu Cha,Hyunwoo Oh,Kwanyoung Lee,Dong-Jin Kim*

Main category: cs.LG

TL;DR: ScaleDiff is a training-free framework that extends the resolution of pretrained diffusion models using Neighborhood Patch Attention, Latent Frequency Mixing, and Structure Guidance to improve image quality and speed.


<details>
  <summary>Details</summary>
Motivation: Text-to-image diffusion models perform poorly beyond their training resolution, and existing training-free methods are computationally expensive or incompatible with Diffusion Transformer models.

Method: Proposes ScaleDiff with Neighborhood Patch Attention (NPA) to reduce computational redundancy in self-attention, integrates NPA into SDEdit pipeline with Latent Frequency Mixing (LFM) for fine details, and applies Structure Guidance for global structure enhancement.

Result: ScaleDiff achieves state-of-the-art performance among training-free methods in both image quality and inference speed on both U-Net and Diffusion Transformer architectures.

Conclusion: ScaleDiff provides an efficient, model-agnostic solution for resolution extension of pretrained diffusion models without additional training.

Abstract: Text-to-image diffusion models often exhibit degraded performance when
generating images beyond their training resolution. Recent training-free
methods can mitigate this limitation, but they often require substantial
computation or are incompatible with recent Diffusion Transformer models. In
this paper, we propose ScaleDiff, a model-agnostic and highly efficient
framework for extending the resolution of pretrained diffusion models without
any additional training. A core component of our framework is Neighborhood
Patch Attention (NPA), an efficient mechanism that reduces computational
redundancy in the self-attention layer with non-overlapping patches. We
integrate NPA into an SDEdit pipeline and introduce Latent Frequency Mixing
(LFM) to better generate fine details. Furthermore, we apply Structure Guidance
to enhance global structure during the denoising process. Experimental results
demonstrate that ScaleDiff achieves state-of-the-art performance among
training-free methods in terms of both image quality and inference speed on
both U-Net and Diffusion Transformer architectures.

</details>


### [84] [MedVLSynther: Synthesizing High-Quality Visual Question Answering from Medical Documents with Generator-Verifier LMMs](https://arxiv.org/abs/2510.25867)
*Xiaoke Huang,Ningsen Wang,Hui Liu,Xianfeng Tang,Yuyin Zhou*

Main category: cs.LG

TL;DR: MedVLSynther is a framework that automatically generates high-quality medical VQA questions from biomedical literature, creating the MedSynVQA dataset which improves LMM performance on medical benchmarks.


<details>
  <summary>Details</summary>
Motivation: Training general medical VQA systems is hindered by the lack of large, openly usable, high-quality datasets for joint reasoning over medical images and text.

Method: A rubric-guided generator-verifier framework that synthesizes multiple-choice VQA items from biomedical literature using figures, captions, and references, with multi-stage verification for quality control.

Result: Created MedSynVQA dataset with 13,087 questions over 14,803 images spanning 13 modalities and 28 anatomical regions. Training LMMs with this data achieved up to 77.57 on VQA-RAD and 67.76 on PathVQA, outperforming existing medical LMMs.

Conclusion: MedVLSynther provides an auditable, reproducible, and privacy-preserving approach to scalable medical VQA training data generation using open literature and open-weight models.

Abstract: Large Multimodal Models (LMMs) are increasingly capable of answering medical
questions that require joint reasoning over images and text, yet training
general medical VQA systems is impeded by the lack of large, openly usable,
high-quality corpora. We present MedVLSynther, a rubric-guided
generator-verifier framework that synthesizes high-quality multiple-choice VQA
items directly from open biomedical literature by conditioning on figures,
captions, and in-text references. The generator produces self-contained stems
and parallel, mutually exclusive options under a machine-checkable JSON schema;
a multi-stage verifier enforces essential gates (self-containment, single
correct answer, clinical validity, image-text consistency), awards fine-grained
positive points, and penalizes common failure modes before acceptance. Applying
this pipeline to PubMed Central yields MedSynVQA: 13,087 audited questions over
14,803 images spanning 13 imaging modalities and 28 anatomical regions.
Training open-weight LMMs with reinforcement learning using verifiable rewards
improves accuracy across six medical VQA benchmarks, achieving averages of
55.85 (3B) and 58.15 (7B), with up to 77.57 on VQA-RAD and 67.76 on PathVQA,
outperforming strong medical LMMs. A Ablations verify that both generation and
verification are necessary and that more verified data consistently helps, and
a targeted contamination analysis detects no leakage from evaluation suites. By
operating entirely on open literature and open-weight models, MedVLSynther
offers an auditable, reproducible, and privacy-preserving path to scalable
medical VQA training data.

</details>


### [85] [$π_\texttt{RL}$: Online RL Fine-tuning for Flow-based Vision-Language-Action Models](https://arxiv.org/abs/2510.25889)
*Kang Chen,Zhihao Liu,Tonghe Zhang,Zhen Guo,Si Xu,Hao Lin,Hongzhi Zang,Quanlu Zhang,Zhaofei Yu,Guoliang Fan,Tiejun Huang,Yu Wang,Chao Yu*

Main category: cs.LG

TL;DR: π_RL is a framework for training flow-based Vision-Language-Action models using reinforcement learning, overcoming challenges with intractable action log-likelihoods through two novel RL algorithms.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of applying large-scale RL to flow-based VLAs due to intractable action log-likelihoods from iterative denoising processes.

Method: Developed π_RL framework with two RL algorithms: Flow-Noise (models denoising as discrete-time MDP with learnable noise network) and Flow-SDE (integrates denoising with agent-environment interaction using ODE-to-SDE conversion).

Result: Significant performance improvements: on LIBERO, boosted π_0 from 57.6% to 97.6% and π_0.5 from 77.1% to 98.3%; on ManiSkill, improved π_0 from 41.6% to 85.7% and π_0.5 from 40.0% to 84.8% across 4352 tasks using 320 parallel environments.

Conclusion: π_RL achieves substantial performance gains and stronger generalization over supervised fine-tuning models, validating the effectiveness of online RL for flow-based VLAs.

Abstract: Vision-Language-Action (VLA) models enable robots to understand and perform
complex tasks from multimodal input. Although recent work explores using
reinforcement learning (RL) to automate the laborious data collection process
in scaling supervised fine-tuning (SFT), applying large-scale RL to flow-based
VLAs (e.g., $\pi_0$, $\pi_{0.5}$) remains challenging due to intractable action
log-likelihoods from iterative denoising.
  We address this challenge with $\pi_{\text{RL}}$, an open-source framework
for training flow-based VLAs in parallel simulation. $\pi_{\text{RL}}$
implements two RL algorithms: (1) {Flow-Noise} models the denoising process as
a discrete-time MDP with a learnable noise network for exact log-likelihood
computation. (2) {Flow-SDE} integrates denoising with agent-environment
interaction, formulating a two-layer MDP that employs ODE-to-SDE conversion for
efficient RL exploration.
  We evaluate $\pi_{\text{RL}}$ on LIBERO and ManiSkill benchmarks. On LIBERO,
$\pi_{\text{RL}}$ boosts few-shot SFT models $\pi_0$ and $\pi_{0.5}$ from 57.6%
to 97.6% and from 77.1% to 98.3%, respectively. In ManiSkill, we train
$\pi_{\text{RL}}$ in 320 parallel environments, improving $\pi_0$ from 41.6% to
85.7% and $\pi_{0.5}$ from 40.0% to 84.8% across 4352 pick-and-place tasks,
demonstrating scalable multitask RL under heterogeneous simulation.
  Overall, $\pi_{\text{RL}}$ achieves significant performance gains and
stronger generalization over SFT-models, validating the effectiveness of online
RL for flow-based VLAs.

</details>


### [86] [Topology-Aware Active Learning on Graphs](https://arxiv.org/abs/2510.25892)
*Harris Hardiman-Mostow,Jack Mauro,Adrien Weihs,Andrea L. Bertozzi*

Main category: cs.LG

TL;DR: A graph-topological approach for active learning using Balanced Forman Curvature to guide exploration-exploitation trade-off with coreset construction and dynamic switching, plus localized graph rewiring for better label propagation.


<details>
  <summary>Details</summary>
Motivation: Address the core challenge of exploration versus exploitation under scarce label budgets in active learning, replacing hand-tuned heuristics with data-driven methods.

Method: Uses Balanced Forman Curvature for coreset construction to select representative initial labels, includes data-driven stopping criterion for exploration, and employs localized graph rewiring to incorporate multiscale information while preserving sparsity.

Result: Experiments on benchmark classification tasks show consistent outperformance of existing graph-based semi-supervised baselines at low label rates.

Conclusion: The proposed graph-topological approach effectively handles exploration-exploitation trade-off in active learning with limited labels through curvature-based methods and localized rewiring.

Abstract: We propose a graph-topological approach to active learning that directly
targets the core challenge of exploration versus exploitation under scarce
label budgets. To guide exploration, we introduce a coreset construction
algorithm based on Balanced Forman Curvature (BFC), which selects
representative initial labels that reflect the graph's cluster structure. This
method includes a data-driven stopping criterion that signals when the graph
has been sufficiently explored. We further use BFC to dynamically trigger the
shift from exploration to exploitation within active learning routines,
replacing hand-tuned heuristics. To improve exploitation, we introduce a
localized graph rewiring strategy that efficiently incorporates multiscale
information around labeled nodes, enhancing label propagation while preserving
sparsity. Experiments on benchmark classification tasks show that our methods
consistently outperform existing graph-based semi-supervised baselines at low
label rates.

</details>


### [87] [Non-Convex Over-the-Air Heterogeneous Federated Learning: A Bias-Variance Trade-off](https://arxiv.org/abs/2510.26722)
*Muhammad Faraz Ul Abrar,Nicolò Michelusi*

Main category: cs.LG

TL;DR: This paper proposes a novel OTA-FL framework that allows structured bias in model updates to optimize the bias-variance trade-off under heterogeneous wireless conditions, using an SCA algorithm for power control.


<details>
  <summary>Details</summary>
Motivation: Existing OTA-FL designs enforce zero-bias updates, which are constrained by weakest devices in heterogeneous wireless scenarios and inflate update variance. Prior analyses also mainly address convex objectives while modern AI models are non-convex.

Method: Developed OTA-FL SGD updates allowing structured time-invariant model bias with reduced variance. Derived finite-time stationarity bound revealing bias-variance trade-off. Proposed non-convex joint OTA power-control design solved via efficient SCA algorithm using statistical CSI.

Result: Experiments on non-convex image classification show the SCA-based design accelerates convergence through optimized bias and improves generalization over prior OTA-FL baselines.

Conclusion: The proposed biased OTA-FL framework effectively addresses wireless heterogeneity by optimizing the bias-variance trade-off, enabling faster convergence and better performance for non-convex objectives.

Abstract: Over-the-air (OTA) federated learning (FL) has been well recognized as a
scalable paradigm that exploits the waveform superposition of the wireless
multiple-access channel to aggregate model updates in a single use. Existing
OTA-FL designs largely enforce zero-bias model updates by either assuming
\emph{homogeneous} wireless conditions (equal path loss across devices) or
forcing zero-bias updates to guarantee convergence. Under \emph{heterogeneous}
wireless scenarios, however, such designs are constrained by the weakest device
and inflate the update variance. Moreover, prior analyses of biased OTA-FL
largely address convex objectives, while most modern AI models are highly
non-convex. Motivated by these gaps, we study OTA-FL with stochastic gradient
descent (SGD) for general smooth non-convex objectives under wireless
heterogeneity. We develop novel OTA-FL SGD updates that allow a structured,
time-invariant model bias while facilitating reduced variance updates. We
derive a finite-time stationarity bound (expected time average squared gradient
norm) that explicitly reveals a bias-variance trade-off. To optimize this
trade-off, we pose a non-convex joint OTA power-control design and develop an
efficient successive convex approximation (SCA) algorithm that requires only
statistical CSI at the base station. Experiments on a non-convex image
classification task validate the approach: the SCA-based design accelerates
convergence via an optimized bias and improves generalization over prior OTA-FL
baselines.

</details>


### [88] [Transferring Causal Effects using Proxies](https://arxiv.org/abs/2510.25924)
*Manuel Iglesias-Alonso,Felix Schur,Julius von Kügelgen,Jonas Peters*

Main category: cs.LG

TL;DR: Methodology for estimating causal effects in multi-domain settings with unobserved confounders using proxy variables, with identifiability proofs and estimation techniques.


<details>
  <summary>Details</summary>
Motivation: Address the challenge of estimating causal effects when confounders are unobserved and effects vary across domains, using proxy variables as a practical solution.

Method: Proposed methodology using proxy variables for hidden confounders, with two estimation techniques for discrete/categorical variables, proving identifiability even for continuous treatments/responses.

Result: Proved identifiability under specified conditions, introduced consistent estimation methods with confidence intervals, validated through simulations and real-world application on website rankings' effect on consumer choices.

Conclusion: The approach successfully enables causal effect estimation in target domains with unobserved confounders using proxy variables, with theoretical guarantees and practical validation.

Abstract: We consider the problem of estimating a causal effect in a multi-domain
setting. The causal effect of interest is confounded by an unobserved
confounder and can change between the different domains. We assume that we have
access to a proxy of the hidden confounder and that all variables are discrete
or categorical. We propose methodology to estimate the causal effect in the
target domain, where we assume to observe only the proxy variable. Under these
conditions, we prove identifiability (even when treatment and response
variables are continuous). We introduce two estimation techniques, prove
consistency, and derive confidence intervals. The theoretical results are
supported by simulation studies and a real-world example studying the causal
effect of website rankings on consumer choices.

</details>


### [89] [Active Learning with Task-Driven Representations for Messy Pools](https://arxiv.org/abs/2510.25926)
*Kianoosh Ashouritaklimi,Tom Rainforth*

Main category: cs.LG

TL;DR: Active learning with task-driven representations that update during the process outperforms fixed unsupervised representations for messy data pools.


<details>
  <summary>Details</summary>
Motivation: Standard active learning approaches use fixed unsupervised representations which fail to capture task-relevant information in messy, uncurated data pools.

Method: Proposed two strategies: learning semi-supervised representations directly, and supervised fine-tuning of initial unsupervised representations, both updated periodically during active learning.

Result: Both proposed strategies significantly improved empirical performance compared to using unsupervised or pretrained representations.

Conclusion: Task-driven representations that evolve during active learning are more effective than fixed representations for handling messy data pools.

Abstract: Active learning has the potential to be especially useful for messy,
uncurated pools where datapoints vary in relevance to the target task. However,
state-of-the-art approaches to this problem currently rely on using fixed,
unsupervised representations of the pool, focusing on modifying the acquisition
function instead. We show that this model setup can undermine their
effectiveness at dealing with messy pools, as such representations can fail to
capture important information relevant to the task. To address this, we propose
using task-driven representations that are periodically updated during the
active learning process using the previously collected labels. We introduce two
specific strategies for learning these representations, one based on directly
learning semi-supervised representations and the other based on supervised
fine-tuning of an initial unsupervised representation. We find that both
significantly improve empirical performance over using unsupervised or
pretrained representations.

</details>


### [90] [Robust GNN Watermarking via Implicit Perception of Topological Invariants](https://arxiv.org/abs/2510.25934)
*Jipeng Li,Yannning Shen*

Main category: cs.LG

TL;DR: InvGNN-WM is a trigger-free watermarking method for Graph Neural Networks that ties ownership to graph invariant perception, providing robust black-box verification with minimal task performance impact.


<details>
  <summary>Details</summary>
Motivation: Existing GNN watermarks using backdoor triggers are vulnerable to common model edits and create ownership ambiguity, necessitating a more robust and trigger-free approach.

Method: Uses a lightweight head to predict normalized algebraic connectivity on owner-private carrier sets, with sign-sensitive bit decoding and calibrated thresholding for false-positive control.

Result: Achieves comparable clean accuracy while providing higher watermark accuracy than trigger-based and compression-based baselines across diverse datasets and backbones, remaining robust under pruning, fine-tuning, and quantization.

Conclusion: The method offers theoretical guarantees for imperceptibility and robustness, proves exact removal is NP-complete, and shows that watermark loss during knowledge distillation can restore weakened marks.

Abstract: Graph Neural Networks (GNNs) are valuable intellectual property, yet many
watermarks rely on backdoor triggers that break under common model edits and
create ownership ambiguity. We present InvGNN-WM, which ties ownership to a
model's implicit perception of a graph invariant, enabling trigger-free,
black-box verification with negligible task impact. A lightweight head predicts
normalized algebraic connectivity on an owner-private carrier set; a
sign-sensitive decoder outputs bits, and a calibrated threshold controls the
false-positive rate. Across diverse node and graph classification datasets and
backbones, InvGNN-WM matches clean accuracy while yielding higher watermark
accuracy than trigger- and compression-based baselines. It remains strong under
unstructured pruning, fine-tuning, and post-training quantization; plain
knowledge distillation (KD) weakens the mark, while KD with a watermark loss
(KD+WM) restores it. We provide guarantees for imperceptibility and robustness,
and we prove that exact removal is NP-complete.

</details>


### [91] [Modular Linear Tokenization (MLT)](https://arxiv.org/abs/2510.25952)
*Tcharlies Schmitz*

Main category: cs.LG

TL;DR: MLT is a reversible, deterministic technique for encoding high-cardinality categorical identifiers into compact numerical vectors using modular arithmetic and invertible linear transformations.


<details>
  <summary>Details</summary>
Motivation: Traditional methods like hashing or one-hot encodings lack bijective mappings and reversibility for categorical identifiers, especially with millions of items.

Method: Leverages modular arithmetic over finite fields and invertible linear transformations to create deterministic, reversible encodings with explicit control of dimensionality.

Result: On MovieLens 20M dataset, MLT achieves comparable predictive performance to supervised embeddings with significantly fewer parameters and lower training cost.

Conclusion: MLT provides an efficient, scalable alternative to traditional categorical encoding methods while maintaining full reversibility and competitive performance.

Abstract: This paper introduces Modular Linear Tokenization (MLT), a reversible and
deterministic technique for encoding high-cardinality categorical identifiers
into compact numerical vectors. Unlike traditional hashing or one-hot
encodings, MLT preserves bijective mappings by leveraging modular arithmetic
over finite fields and invertible linear transformations. The method offers
explicit control of dimensionality and computational scalability while
maintaining full reversibility, even for millions of identifiers. Experimental
results on the MovieLens 20M dataset show that MLT achieves comparable
predictive performance to supervised embeddings while requiring significantly
fewer parameters and lower training cost. An open-source implementation of MLT
is available on PyPI (https://pypi.org/project/light-mlt/) and GitHub
(https://github.com/tcharliesschmitz/light-mlt).

</details>


### [92] [Application and Validation of Geospatial Foundation Model Data for the Prediction of Health Facility Programmatic Outputs -- A Case Study in Malawi](https://arxiv.org/abs/2510.25954)
*Lynn Metz,Rachel Haggard,Michael Moszczynski,Samer Asbah,Chris Mwase,Patricia Khomani,Tyler Smith,Hannah Cooper,Annie Mwale,Arbaaz Muslim,Gautam Prasad,Mimi Sun,Tomer Shekel,Joydeep Paul,Anna Carter,Shravya Shetty,Dylan Green*

Main category: cs.LG

TL;DR: GeoFMs improve health data prediction in LMICs, with multi-source integration showing best performance for indicators like HIV cases and vaccinations.


<details>
  <summary>Details</summary>
Motivation: Routine health data in LMICs suffers from reporting delays and incomplete coverage, requiring novel data sources and analytics.

Method: Evaluated three GeoFM embedding sources (Google PDFM, AlphaEarth, mobile CDR) using XGBoost models on 552 health catchment areas with 5-fold cross-validation.

Result: Embedding-based approaches outperformed traditional methods in 13/15 indicators. Multi-GeoFM model achieved R2 values up to 0.68 for HIV cases and 0.55 for vaccinations.

Conclusion: Integration of multiple GeoFM sources is an efficient tool for supplementing constrained health information systems in LMICs.

Abstract: The reliability of routine health data in low and middle-income countries
(LMICs) is often constrained by reporting delays and incomplete coverage,
necessitating the exploration of novel data sources and analytics. Geospatial
Foundation Models (GeoFMs) offer a promising avenue by synthesizing diverse
spatial, temporal, and behavioral data into mathematical embeddings that can be
efficiently used for downstream prediction tasks. This study evaluated the
predictive performance of three GeoFM embedding sources - Google Population
Dynamics Foundation Model (PDFM), Google AlphaEarth (derived from satellite
imagery), and mobile phone call detail records (CDR) - for modeling 15 routine
health programmatic outputs in Malawi, and compared their utility to
traditional geospatial interpolation methods. We used XGBoost models on data
from 552 health catchment areas (January 2021-May 2023), assessing performance
with R2, and using an 80/20 training and test data split with 5-fold
cross-validation used in training. While predictive performance was mixed, the
embedding-based approaches improved upon baseline geostatistical methods in 13
of 15 (87%) indicators tested. A Multi-GeoFM model integrating all three
embedding sources produced the most robust predictions, achieving average
5-fold cross validated R2 values for indicators like population density (0.63),
new HIV cases (0.57), and child vaccinations (0.47) and test set R2 of 0.64,
0.68, and 0.55, respectively. Prediction was poor for prediction targets with
low primary data availability, such as TB and malnutrition cases. These results
demonstrate that GeoFM embeddings imbue a modest predictive improvement for
select health and demographic outcomes in an LMIC context. We conclude that the
integration of multiple GeoFM sources is an efficient and valuable tool for
supplementing and strengthening constrained routine health information systems.

</details>


### [93] [On the Dataless Training of Neural Networks](https://arxiv.org/abs/2510.25962)
*Alvaro Velasquez,Susmit Jha,Ismail R. Alkhouri*

Main category: cs.LG

TL;DR: Survey of neural networks for optimization in training-data-free settings, categorizing methods into architecture-agnostic and architecture-specific approaches.


<details>
  <summary>Details</summary>
Motivation: Data-driven learning approaches are underdeveloped in some domains like combinatorial optimization, and training data is inherently limited in applications like medical image reconstruction.

Method: Re-parameterizing optimization problems using neural network architectures (MLP, convolutional, graph, quadratic) and categorizing into architecture-agnostic vs architecture-specific methods based on how problem instances are encoded.

Result: Promising results across diverse applications including combinatorial optimization, inverse problems, and partial differential equations.

Conclusion: Defines dataless neural network setting and clarifies distinctions from related concepts like zero-shot learning, lifting in optimization, and over-parameterization.

Abstract: This paper surveys studies on the use of neural networks for optimization in
the training-data-free setting. Specifically, we examine the dataless
application of neural network architectures in optimization by
re-parameterizing problems using fully connected (or MLP), convolutional,
graph, and quadratic neural networks. Although MLPs have been used to solve
linear programs a few decades ago, this approach has recently gained increasing
attention due to its promising results across diverse applications, including
those based on combinatorial optimization, inverse problems, and partial
differential equations. The motivation for this setting stems from two key
(possibly over-lapping) factors: (i) data-driven learning approaches are still
underdeveloped and have yet to demonstrate strong results, as seen in
combinatorial optimization, and (ii) the availability of training data is
inherently limited, such as in medical image reconstruction and other
scientific applications. In this paper, we define the dataless setting and
categorize it into two variants based on how a problem instance -- defined by a
single datum -- is encoded onto the neural network: (i) architecture-agnostic
methods and (ii) architecture-specific methods. Additionally, we discuss
similarities and clarify distinctions between the dataless neural network (dNN)
settings and related concepts such as zero-shot learning, one-shot learning,
lifting in optimization, and over-parameterization.

</details>


### [94] [A General and Streamlined Differentiable Optimization Framework](https://arxiv.org/abs/2510.25986)
*Andrew W. Rosemberg,Joaquim Dias Garcia,François Pacaud,Robert B. Parker,Benoît Legat,Kaarthik Sundar,Russell Bent,Pascal Van Hentenryck*

Main category: cs.LG

TL;DR: DiffOpt.jl is a Julia framework that enables automatic differentiation through constrained optimization problems, providing forward- and reverse-mode sensitivities while maintaining standard JuMP modeling practices.


<details>
  <summary>Details</summary>
Motivation: There's a growing need to differentiate through optimization problems for learning, control, and decision-making systems, but practical integration is challenging due to solver specialization and interface mismatches.

Method: The framework computes sensitivities by differentiating the KKT system under standard regularity assumptions, using a parameter-centric API that allows declaring named parameters and obtaining derivatives directly with respect to them.

Result: The framework successfully handles convex and nonconvex models including economic dispatch, portfolio selection, and robot inverse kinematics, and enables gradient-based methods for energy market bidding and training optimization proxies.

Conclusion: Differentiable optimization can be deployed as a routine tool for experimentation, learning, calibration, and design while maintaining standard modeling practices and access to solver ecosystems.

Abstract: Differentiating through constrained optimization problems is increasingly
central to learning, control, and large-scale decision-making systems, yet
practical integration remains challenging due to solver specialization and
interface mismatches. This paper presents a general and streamlined
framework-an updated DiffOpt.jl-that unifies modeling and differentiation
within the Julia optimization stack. The framework computes forward - and
reverse-mode solution and objective sensitivities for smooth, potentially
nonconvex programs by differentiating the KKT system under standard regularity
assumptions. A first-class, JuMP-native parameter-centric API allows users to
declare named parameters and obtain derivatives directly with respect to them -
even when a parameter appears in multiple constraints and objectives -
eliminating brittle bookkeeping from coefficient-level interfaces. We
illustrate these capabilities on convex and nonconvex models, including
economic dispatch, mean-variance portfolio selection with conic risk
constraints, and nonlinear robot inverse kinematics. Two companion studies
further demonstrate impact at scale: gradient-based iterative methods for
strategic bidding in energy markets and Sobolev-style training of end-to-end
optimization proxies using solver-accurate sensitivities. Together, these
results demonstrate that differentiable optimization can be deployed as a
routine tool for experimentation, learning, calibration, and design-without
deviating from standard JuMP modeling practices and while retaining access to a
broad ecosystem of solvers.

</details>


### [95] [Efficient Online Learning with Predictive Coding Networks: Exploiting Temporal Correlations](https://arxiv.org/abs/2510.25993)
*Darius Masoum Zadeh-Jousdani,Elvin Hajizada,Eyke Hüllermeier*

Main category: cs.LG

TL;DR: PCN-TA is a biologically plausible predictive coding network that reduces computational overhead by preserving latent states across temporal frames, achieving 50% fewer inference steps than baseline PC and 10% fewer weight updates than backpropagation.


<details>
  <summary>Details</summary>
Motivation: Need for efficient online learning algorithms for robotic edge systems that can continuously adapt to changing environments while being biologically plausible and suitable for neuromorphic hardware.

Method: Predictive Coding Network with Temporal Amortization (PCN-TA) that preserves latent states across temporal frames to leverage temporal correlations and reduce computational demands.

Result: On COIL-20 dataset: 10% fewer weight updates than backpropagation, 50% fewer inference steps than baseline PC networks, with maintained learning performance.

Conclusion: PCN-TA enables efficient online learning for resource-constrained robotic systems and is promising for neuromorphic hardware implementation.

Abstract: Robotic systems operating at the edge require efficient online learning
algorithms that can continuously adapt to changing environments while
processing streaming sensory data. Traditional backpropagation, while
effective, conflicts with biological plausibility principles and may be
suboptimal for continuous adaptation scenarios. The Predictive Coding (PC)
framework offers a biologically plausible alternative with local, Hebbian-like
update rules, making it suitable for neuromorphic hardware implementation.
However, PC's main limitation is its computational overhead due to multiple
inference iterations during training. We present Predictive Coding Network with
Temporal Amortization (PCN-TA), which preserves latent states across temporal
frames. By leveraging temporal correlations, PCN-TA significantly reduces
computational demands while maintaining learning performance. Our experiments
on the COIL-20 robotic perception dataset demonstrate that PCN-TA achieves 10%
fewer weight updates compared to backpropagation and requires 50% fewer
inference steps than baseline PC networks. These efficiency gains directly
translate to reduced computational overhead for moving another step toward edge
deployment and real-time adaptation support in resource-constrained robotic
systems. The biologically-inspired nature of our approach also makes it a
promising candidate for future neuromorphic hardware implementations, enabling
efficient online learning at the edge.

</details>


### [96] [Infrequent Exploration in Linear Bandits](https://arxiv.org/abs/2510.26000)
*Harin Lee,Min-hwan Oh*

Main category: cs.LG

TL;DR: INFEX is a framework for infrequent exploration in linear bandits that combines greedy actions with scheduled exploratory steps, achieving efficient regret while reducing exploration frequency and computational costs.


<details>
  <summary>Details</summary>
Motivation: Address the gap between fully adaptive exploration methods (which explore too frequently) and purely greedy approaches (which require strong diversity assumptions), particularly for safety-critical or costly domains where continuous exploration is impractical.

Method: INFEX executes a base exploratory policy according to a predetermined schedule while using greedy actions between exploration steps. It's a modular framework that can integrate any adaptive exploration method.

Result: Theoretical analysis shows INFEX achieves instance-dependent regret matching standard efficient algorithms when exploration frequency exceeds a logarithmic threshold. Empirical evaluations confirm state-of-the-art regret performance and runtime improvements.

Conclusion: INFEX successfully bridges the extremes of continuous exploration and purely greedy approaches, providing a practical solution for infrequent exploration that maintains theoretical guarantees while improving computational efficiency.

Abstract: We study the problem of infrequent exploration in linear bandits, addressing
a significant yet overlooked gap between fully adaptive exploratory methods
(e.g., UCB and Thompson Sampling), which explore potentially at every time
step, and purely greedy approaches, which require stringent diversity
assumptions to succeed. Continuous exploration can be impractical or unethical
in safety-critical or costly domains, while purely greedy strategies typically
fail without adequate contextual diversity. To bridge these extremes, we
introduce a simple and practical framework, INFEX, explicitly designed for
infrequent exploration. INFEX executes a base exploratory policy according to a
given schedule while predominantly choosing greedy actions in between. Despite
its simplicity, our theoretical analysis demonstrates that INFEX achieves
instance-dependent regret matching standard provably efficient algorithms,
provided the exploration frequency exceeds a logarithmic threshold.
Additionally, INFEX is a general, modular framework that allows seamless
integration of any fully adaptive exploration method, enabling wide
applicability and ease of adoption. By restricting intensive exploratory
computations to infrequent intervals, our approach can also enhance
computational efficiency. Empirical evaluations confirm our theoretical
findings, showing state-of-the-art regret performance and runtime improvements
over existing methods.

</details>


### [97] [Dual Mixture-of-Experts Framework for Discrete-Time Survival Analysis](https://arxiv.org/abs/2510.26014)
*Hyeonjun Lee,Hyungseob Shin,Gunhee Nam,Hyeonsoo Lee*

Main category: cs.LG

TL;DR: A dual mixture-of-experts framework for discrete-time survival analysis that combines feature-encoder MoE for subgroup-aware representation learning with hazard MoE for temporal dynamics, improving performance on breast cancer datasets.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of modeling patient heterogeneity while adapting risk predictions to individual characteristics and temporal dynamics in survival analysis.

Method: Proposes a dual mixture-of-experts framework with feature-encoder MoE for subgroup-aware representation learning and hazard MoE that uses patient features and time embeddings to capture temporal dynamics.

Result: Consistently improves performance on METABRIC and GBSG breast cancer datasets, boosting time-dependent C-index up to 0.04 on test sets, with further gains when integrated into Consurv framework.

Conclusion: The dual-MoE design effectively integrates with existing deep learning survival pipelines and provides flexible modeling of patient heterogeneity and temporal dynamics.

Abstract: Survival analysis is a task to model the time until an event of interest
occurs, widely used in clinical and biomedical research. A key challenge is to
model patient heterogeneity while also adapting risk predictions to both
individual characteristics and temporal dynamics. We propose a dual
mixture-of-experts (MoE) framework for discrete-time survival analysis. Our
approach combines a feature-encoder MoE for subgroup-aware representation
learning with a hazard MoE that leverages patient features and time embeddings
to capture temporal dynamics. This dual-MoE design flexibly integrates with
existing deep learning based survival pipelines. On METABRIC and GBSG breast
cancer datasets, our method consistently improves performance, boosting the
time-dependent C-index up to 0.04 on the test sets, and yields further gains
when incorporated into the Consurv framework.

</details>


### [98] [Exploring Human-AI Conceptual Alignment through the Prism of Chess](https://arxiv.org/abs/2510.26025)
*Semyon Lomaso,Judah Goldfeder,Mehmet Hamza Erol,Matthew So,Yao Yan,Addison Howard,Nathan Kutz,Ravid Shwartz Ziv*

Main category: cs.LG

TL;DR: AI chess models achieve grandmaster-level play but develop alien representations that diverge from human strategic concepts, revealing reliance on memorized patterns rather than abstract understanding.


<details>
  <summary>Details</summary>
Motivation: To investigate whether AI systems truly understand human concepts or merely mimic surface patterns, using chess as a domain where human creativity meets precise strategic concepts.

Method: Analyzed a 270M-parameter transformer chess model, introduced Chess960 dataset with randomized starting positions to eliminate opening theory, and conducted layer-wise analysis of concept representations.

Result: Early layers encode human concepts with 85% accuracy, but deeper layers drop to 50-65% accuracy despite superior performance. Concept recognition drops 10-20% when opening theory is eliminated, showing reliance on memorization.

Conclusion: Current AI architectures face a fundamental tension: representations that optimize for performance diverge from human-aligned thinking, suggesting AI develops increasingly alien intelligence as it optimizes, posing challenges for genuine human-AI collaboration.

Abstract: Do AI systems truly understand human concepts or merely mimic surface
patterns? We investigate this through chess, where human creativity meets
precise strategic concepts. Analyzing a 270M-parameter transformer that
achieves grandmaster-level play, we uncover a striking paradox: while early
layers encode human concepts like center control and knight outposts with up to
85\% accuracy, deeper layers, despite driving superior performance, drift
toward alien representations, dropping to 50-65\% accuracy. To test conceptual
robustness beyond memorization, we introduce the first Chess960 dataset: 240
expert-annotated positions across 6 strategic concepts. When opening theory is
eliminated through randomized starting positions, concept recognition drops
10-20\% across all methods, revealing the model's reliance on memorized
patterns rather than abstract understanding. Our layer-wise analysis exposes a
fundamental tension in current architectures: the representations that win
games diverge from those that align with human thinking. These findings suggest
that as AI systems optimize for performance, they develop increasingly alien
intelligence, a critical challenge for creative AI applications requiring
genuine human-AI collaboration. Dataset and code are available at:
https://github.com/slomasov/ChessConceptsLLM.

</details>


### [99] [Do Students Debias Like Teachers? On the Distillability of Bias Mitigation Methods](https://arxiv.org/abs/2510.26038)
*Jiali Cheng,Chirag Agarwal,Hadi Amiri*

Main category: cs.LG

TL;DR: Knowledge distillation undermines model debiasing capabilities, with significant variations across bias types, but three solutions are proposed to improve distillability.


<details>
  <summary>Details</summary>
Motivation: To investigate how knowledge distillation affects model robustness against spurious correlations and debiasing capabilities, which remains underexplored.

Method: Extensive experiments on natural language inference and image classification tasks, analyzing attention patterns and circuits post-distillation.

Result: KD generally undermines debiasing capabilities, training debiased models doesn't benefit from teacher knowledge, robustness varies across bias types, and specific internal patterns cause distinct behavior.

Conclusion: First study on KD's effect on debiasing at scale, providing insights for designing better debiasing methods and proposing three solutions: high-quality data augmentation, iterative KD, and teacher-initialized student models.

Abstract: Knowledge distillation (KD) is an effective method for model compression and
transferring knowledge between models. However, its effect on model's
robustness against spurious correlations that degrade performance on
out-of-distribution data remains underexplored. This study investigates the
effect of knowledge distillation on the transferability of ``debiasing''
capabilities from teacher models to student models on natural language
inference (NLI) and image classification tasks. Through extensive experiments,
we illustrate several key findings: (i) overall the debiasing capability of a
model is undermined post-KD; (ii) training a debiased model does not benefit
from injecting teacher knowledge; (iii) although the overall robustness of a
model may remain stable post-distillation, significant variations can occur
across different types of biases; and (iv) we pin-point the internal attention
pattern and circuit that causes the distinct behavior post-KD. Given the above
findings, we propose three effective solutions to improve the distillability of
debiasing methods: developing high quality data for augmentation, implementing
iterative knowledge distillation, and initializing student models with weights
obtained from teacher models. To the best of our knowledge, this is the first
study on the effect of KD on debiasing and its interenal mechanism at scale.
Our findings provide understandings on how KD works and how to design better
debiasing methods.

</details>


### [100] [Towards Scaling Laws for Symbolic Regression](https://arxiv.org/abs/2510.26064)
*David Otte,Jörg K. H. Franke,Frank Hutter*

Main category: cs.LG

TL;DR: This paper investigates scaling laws in symbolic regression, showing that both validation loss and solved rate follow power-law trends with compute, similar to language models.


<details>
  <summary>Details</summary>
Motivation: To understand the role of scale in symbolic regression, which has remained largely unexplored despite recent advances in deep learning-based approaches.

Method: Used a scalable end-to-end transformer pipeline with carefully generated training data, testing five different model sizes across three orders of magnitude in compute.

Result: Found clear power-law scaling trends for both validation loss and solved rate with compute, and identified compute-optimal hyperparameter scaling relationships.

Conclusion: Symbolic regression performance is largely predictable from compute, providing important insights for training next-generation SR models.

Abstract: Symbolic regression (SR) aims to discover the underlying mathematical
expressions that explain observed data. This holds promise for both gaining
scientific insight and for producing inherently interpretable and generalizable
models for tabular data. In this work we focus on the basics of SR. Deep
learning-based SR has recently become competitive with genetic programming
approaches, but the role of scale has remained largely unexplored. Inspired by
scaling laws in language modeling, we present the first systematic
investigation of scaling in SR, using a scalable end-to-end transformer
pipeline and carefully generated training data. Across five different model
sizes and spanning three orders of magnitude in compute, we find that both
validation loss and solved rate follow clear power-law trends with compute. We
further identify compute-optimal hyperparameter scaling: optimal batch size and
learning rate grow with model size, and a token-to-parameter ratio of
$\approx$15 is optimal in our regime, with a slight upward trend as compute
increases. These results demonstrate that SR performance is largely predictable
from compute and offer important insights for training the next generation of
SR models.

</details>


### [101] [Learning Geometry: A Framework for Building Adaptive Manifold Models through Metric Optimization](https://arxiv.org/abs/2510.26068)
*Di Zhang*

Main category: cs.LG

TL;DR: A novel machine learning paradigm that treats models as malleable geometric entities by optimizing metric tensor fields on manifolds, using a variational framework that balances data fidelity with geometric complexity.


<details>
  <summary>Details</summary>
Motivation: To move beyond traditional parameter optimization by dynamically shaping the geometric structure of model spaces, enabling more expressive models than those with fixed geometry.

Method: A variational framework that optimizes metric tensor fields on manifolds with predefined topology, discretized into triangular meshes with edge-length parameterization for efficient optimization using automatic differentiation.

Result: Theoretical analysis reveals an analogy with Einstein-Hilbert action in general relativity, showing metric optimization provides greater expressive power than fixed geometry models even with fixed topology.

Conclusion: This work establishes foundations for dynamic meta-learners that can autonomously evolve geometry and topology, with broad applications in scientific model discovery and robust representation learning.

Abstract: This paper proposes a novel paradigm for machine learning that moves beyond
traditional parameter optimization. Unlike conventional approaches that search
for optimal parameters within a fixed geometric space, our core idea is to
treat the model itself as a malleable geometric entity. Specifically, we
optimize the metric tensor field on a manifold with a predefined topology,
thereby dynamically shaping the geometric structure of the model space. To
achieve this, we construct a variational framework whose loss function
carefully balances data fidelity against the intrinsic geometric complexity of
the manifold. The former ensures the model effectively explains observed data,
while the latter acts as a regularizer, penalizing overly curved or irregular
geometries to encourage simpler models and prevent overfitting. To address the
computational challenges of this infinite-dimensional optimization problem, we
introduce a practical method based on discrete differential geometry: the
continuous manifold is discretized into a triangular mesh, and the metric
tensor is parameterized by edge lengths, enabling efficient optimization using
automatic differentiation tools. Theoretical analysis reveals a profound
analogy between our framework and the Einstein-Hilbert action in general
relativity, providing an elegant physical interpretation for the concept of
"data-driven geometry". We further argue that even with fixed topology, metric
optimization offers significantly greater expressive power than models with
fixed geometry. This work lays a solid foundation for constructing fully
dynamic "meta-learners" capable of autonomously evolving their geometry and
topology, and it points to broad application prospects in areas such as
scientific model discovery and robust representation learning.

</details>


### [102] [New Money: A Systematic Review of Synthetic Data Generation for Finance](https://arxiv.org/abs/2510.26076)
*James Meldrum,Basem Suleiman,Fethi Rabhi,Muhammad Johan Alibasa*

Main category: cs.LG

TL;DR: This systematic review analyzes 72 studies on synthetic financial data generation using GANs and VAEs, finding GANs dominate for market and credit data but privacy evaluation remains inadequate.


<details>
  <summary>Details</summary>
Motivation: To address challenges of using sensitive financial data in ML by creating artificial datasets that preserve statistical properties while mitigating privacy risks and regulatory constraints.

Method: Systematic review of 72 studies published since 2018, categorizing types of financial information synthesized, generative methods used, and evaluation strategies for data utility and privacy.

Result: GAN-based approaches dominate the literature, particularly for time-series market data and tabular credit data. While innovative techniques show potential, there's a notable lack of rigorous privacy safeguard evaluation across studies.

Conclusion: The review provides an integrated overview highlighting critical research gaps and offers guidance for developing robust, privacy-preserving synthetic data solutions in finance.

Abstract: Synthetic data generation has emerged as a promising approach to address the
challenges of using sensitive financial data in machine learning applications.
By leveraging generative models, such as Generative Adversarial Networks (GANs)
and Variational Autoencoders (VAEs), it is possible to create artificial
datasets that preserve the statistical properties of real financial records
while mitigating privacy risks and regulatory constraints. Despite the rapid
growth of this field, a comprehensive synthesis of the current research
landscape has been lacking. This systematic review consolidates and analyses 72
studies published since 2018 that focus on synthetic financial data generation.
We categorise the types of financial information synthesised, the generative
methods employed, and the evaluation strategies used to assess data utility and
privacy. The findings indicate that GAN-based approaches dominate the
literature, particularly for generating time-series market data and tabular
credit data. While several innovative techniques demonstrate potential for
improved realism and privacy preservation, there remains a notable lack of
rigorous evaluation of privacy safeguards across studies. By providing an
integrated overview of generative techniques, applications, and evaluation
methods, this review highlights critical research gaps and offers guidance for
future work aimed at developing robust, privacy-preserving synthetic data
solutions for the financial domain.

</details>


### [103] [Nirvana: A Specialized Generalist Model With Task-Aware Memory Mechanism](https://arxiv.org/abs/2510.26083)
*Yuhua Jiang,Shuang Cheng,Yihao Liu,Ermo Hua,Che Jiang,Weigao Sun,Yu Cheng,Feifei Gao,Biqing Qi,Bowen Zhou*

Main category: cs.LG

TL;DR: Nirvana is a Specialized Generalist Model with specialized memory mechanism, linear time complexity, and test-time task information extraction. It uses Task-Aware Memory Trigger and Specialized Memory Updater to adapt to domain shifts and achieve expert-level performance while preserving broad capabilities.


<details>
  <summary>Details</summary>
Motivation: Traditional LLM structures lack specialized memory mechanisms guided by task information, limiting their ability to achieve expert-level performance in target domains while maintaining broad capabilities.

Method: Propose Nirvana with Task-Aware Memory Trigger that treats each sample as self-supervised fine-tuning task, and Specialized Memory Updater that dynamically memorizes context guided by Trigger. Uses linear time complexity and test-time task information extraction.

Result: Achieves competitive/superior results on natural language modeling benchmarks. On MRI reconstruction task, achieves higher-quality reconstruction compared to conventional MRI models and traditional LLM backbones, and generates accurate preliminary clinical reports.

Conclusion: Nirvana effectively bridges the gap between general capabilities and specialized performance through its adaptive memory mechanism, demonstrating strong performance across both general language tasks and specialized medical domains.

Abstract: Specialized Generalist Models (SGMs) aim to preserve broad capabilities while
achieving expert-level performance in target domains. However, traditional LLM
structures including Transformer, Linear Attention, and hybrid models do not
employ specialized memory mechanism guided by task information. In this paper,
we present Nirvana, an SGM with specialized memory mechanism, linear time
complexity, and test-time task information extraction. Besides, we propose the
Task-Aware Memory Trigger ($\textit{Trigger}$) that flexibly adjusts memory
mechanism based on the current task's requirements. In Trigger, each incoming
sample is treated as a self-supervised fine-tuning task, enabling Nirvana to
adapt its task-related parameters on the fly to domain shifts. We also design
the Specialized Memory Updater ($\textit{Updater}$) that dynamically memorizes
the context guided by Trigger. We conduct experiments on both general language
tasks and specialized medical tasks. On a variety of natural language modeling
benchmarks, Nirvana achieves competitive or superior results compared to the
existing LLM structures. To prove the effectiveness of Trigger on specialized
tasks, we test Nirvana's performance on a challenging medical task, i.e.,
Magnetic Resonance Imaging (MRI). We post-train frozen Nirvana backbone with
lightweight codecs on paired electromagnetic signals and MRI images. Despite
the frozen Nirvana backbone, Trigger guides the model to adapt to the MRI
domain with the change of task-related parameters. Nirvana achieves
higher-quality MRI reconstruction compared to conventional MRI models as well
as the models with traditional LLMs' backbone, and can also generate accurate
preliminary clinical reports accordingly.

</details>


### [104] [LLMBisect: Breaking Barriers in Bug Bisection with A Comparative Analysis Pipeline](https://arxiv.org/abs/2510.26086)
*Zheng Zhang,Haonan Li,Xingyu Li,Hang Zhang,Zhiyun Qian*

Main category: cs.LG

TL;DR: A multi-stage LLM pipeline for bug bisection that significantly outperforms traditional methods by comprehensively analyzing patches and commits.


<details>
  <summary>Details</summary>
Motivation: Traditional patch-based bisection methods have limitations: they assume bug-inducing and patch commits modify same functions, ignore commit message information, and rely on simple heuristics without logical vulnerability analysis.

Method: Proposes a comprehensive multi-stage pipeline using LLMs to: (1) fully utilize patch information, (2) compare multiple candidate commits in context, and (3) progressively narrow down candidates through down-selection steps.

Result: Achieves 38% better accuracy than state-of-the-art solution and 60% improvement over baseline LLM-based bisection method.

Conclusion: LLMs are well-positioned to break barriers of existing bug bisection solutions, and the comprehensive multi-stage pipeline is essential for achieving high accuracy.

Abstract: Bug bisection has been an important security task that aims to understand the
range of software versions impacted by a bug, i.e., identifying the commit that
introduced the bug. However, traditional patch-based bisection methods are
faced with several significant barriers: For example, they assume that the
bug-inducing commit (BIC) and the patch commit modify the same functions, which
is not always true. They often rely solely on code changes, while the commit
message frequently contains a wealth of vulnerability-related information. They
are also based on simple heuristics (e.g., assuming the BIC initializes lines
deleted in the patch) and lack any logical analysis of the vulnerability.
  In this paper, we make the observation that Large Language Models (LLMs) are
well-positioned to break the barriers of existing solutions, e.g., comprehend
both textual data and code in patches and commits. Unlike previous BIC
identification approaches, which yield poor results, we propose a comprehensive
multi-stage pipeline that leverages LLMs to: (1) fully utilize patch
information, (2) compare multiple candidate commits in context, and (3)
progressively narrow down the candidates through a series of down-selection
steps. In our evaluation, we demonstrate that our approach achieves
significantly better accuracy than the state-of-the-art solution by more than
38\%. Our results further confirm that the comprehensive multi-stage pipeline
is essential, as it improves accuracy by 60\% over a baseline LLM-based
bisection method.

</details>


### [105] [Network-Constrained Policy Optimization for Adaptive Multi-agent Vehicle Routing](https://arxiv.org/abs/2510.26089)
*Fazel Arasteh,Arian Haghparast,Manos Papagelis*

Main category: cs.LG

TL;DR: The paper proposes two multi-agent reinforcement learning approaches for dynamic vehicle routing: Adaptive Navigation (AN) for decentralized intersection-level coordination, and Hierarchical Hub-based Adaptive Navigation (HHAN) for scalable routing in large networks using hub-based coordination with centralized training.


<details>
  <summary>Details</summary>
Motivation: Traditional SPF algorithms perform poorly in dynamic multi-vehicle settings by routing all vehicles along identical paths, worsening congestion. There's a need for coordinated, network-aware fleet navigation that can handle dynamic traffic conditions.

Method: AN uses decentralized MARL with intersection agents providing routing guidance using local traffic and neighborhood state via Graph Attention Networks. HHAN extends this with hierarchical hub-based coordination where agents control hub-to-hub routing while SPF handles micro-routing within hub regions, using CTDE with Attentive Q-Mixing for coordination.

Result: AN reduces average travel time versus SPF and learning baselines while maintaining 100% routing success. HHAN scales to networks with hundreds of intersections, achieving up to 15.9% improvement under heavy traffic conditions.

Conclusion: Network-constrained MARL enables scalable, coordinated, and congestion-aware routing in intelligent transportation systems, with hierarchical approaches providing effective solutions for large urban networks.

Abstract: Traffic congestion in urban road networks leads to longer trip times and
higher emissions, especially during peak periods. While the Shortest Path First
(SPF) algorithm is optimal for a single vehicle in a static network, it
performs poorly in dynamic, multi-vehicle settings, often worsening congestion
by routing all vehicles along identical paths. We address dynamic vehicle
routing through a multi-agent reinforcement learning (MARL) framework for
coordinated, network-aware fleet navigation. We first propose Adaptive
Navigation (AN), a decentralized MARL model where each intersection agent
provides routing guidance based on (i) local traffic and (ii) neighborhood
state modeled using Graph Attention Networks (GAT). To improve scalability in
large networks, we further propose Hierarchical Hub-based Adaptive Navigation
(HHAN), an extension of AN that assigns agents only to key intersections
(hubs). Vehicles are routed hub-to-hub under agent control, while SPF handles
micro-routing within each hub region. For hub coordination, HHAN adopts
centralized training with decentralized execution (CTDE) under the Attentive
Q-Mixing (A-QMIX) framework, which aggregates asynchronous vehicle decisions
via attention. Hub agents use flow-aware state features that combine local
congestion and predictive dynamics for proactive routing. Experiments on
synthetic grids and real urban maps (Toronto, Manhattan) show that AN reduces
average travel time versus SPF and learning baselines, maintaining 100% routing
success. HHAN scales to networks with hundreds of intersections, achieving up
to 15.9% improvement under heavy traffic. These findings highlight the
potential of network-constrained MARL for scalable, coordinated, and
congestion-aware routing in intelligent transportation systems.

</details>


### [106] [SAFE: A Novel Approach to AI Weather Evaluation through Stratified Assessments of Forecasts over Earth](https://arxiv.org/abs/2510.26099)
*Nick Masi,Randall Balestriero*

Main category: cs.LG

TL;DR: SAFE is a package for evaluating weather prediction models through stratified assessments across different geospatial attributes like countries, regions, income levels, and landcover, revealing performance disparities that global averages mask.


<details>
  <summary>Details</summary>
Motivation: Current ML evaluation averages performance globally, ignoring non-uniform human development and geography, which fails to capture where models perform best/worst and fairness issues.

Method: Developed SAFE package that integrates multiple data domains to stratify geospatial gridpoints by territory, global subregion, income, and landcover, enabling individual stratum performance analysis.

Result: Benchmarked state-of-the-art AI weather models and found all exhibit forecasting skill disparities across every attribute, enabling creation of fairness benchmarks through stratification.

Conclusion: SAFE enables moving beyond globally-averaged metrics to understand where models perform best/worst and which are most fair, with the package being open source for further research.

Abstract: The dominant paradigm in machine learning is to assess model performance
based on average loss across all samples in some test set. This amounts to
averaging performance geospatially across the Earth in weather and climate
settings, failing to account for the non-uniform distribution of human
development and geography. We introduce Stratified Assessments of Forecasts
over Earth (SAFE), a package for elucidating the stratified performance of a
set of predictions made over Earth. SAFE integrates various data domains to
stratify by different attributes associated with geospatial gridpoints:
territory (usually country), global subregion, income, and landcover (land or
water). This allows us to examine the performance of models for each individual
stratum of the different attributes (e.g., the accuracy in every individual
country). To demonstrate its importance, we utilize SAFE to benchmark a zoo of
state-of-the-art AI-based weather prediction models, finding that they all
exhibit disparities in forecasting skill across every attribute. We use this to
seed a benchmark of model forecast fairness through stratification at different
lead times for various climatic variables. By moving beyond globally-averaged
metrics, we for the first time ask: where do models perform best or worst, and
which models are most fair? To support further work in this direction, the SAFE
package is open source and available at https://github.com/N-Masi/safe

</details>


### [107] [Do Not Step Into the Same River Twice: Learning to Reason from Trial and Error](https://arxiv.org/abs/2510.26109)
*Chenming Tang,Hsiu-Yuan Huang,Weijie Liu,Saiyong Yang,Yunfang Wu*

Main category: cs.LG

TL;DR: LTE (Learning to reason from Trial and Error) is a novel RLVR approach that enhances LLM reasoning by using self-generated incorrect answers and overlong responses as hints, eliminating the need for external expert guidance.


<details>
  <summary>Details</summary>
Motivation: Existing RLVR approaches suffer from exploration stagnation due to being constrained by LLMs' initial capabilities and relying only on their own generated responses. Off-policy solutions require external expert guidance which has limited availability.

Method: LTE hints LLMs with their previously self-generated incorrect answers and problem of overlong responses, enabling learning from trial and error without external expert guidance.

Result: LTE outperforms normal group relative policy optimization (GRPO) by 6.38 in Pass@1 and 9.00 in Pass@k on average across six mathematics benchmarks for Qwen3-4B-Base. It successfully mitigates exploration stagnation and enhances both exploitation and exploration during training.

Conclusion: LTE provides an effective solution to exploration stagnation in RLVR by leveraging self-generated incorrect responses as learning opportunities, achieving significant performance improvements without requiring external expert guidance.

Abstract: Reinforcement learning with verifiable rewards (RLVR) has significantly
boosted the reasoning capability of large language models (LLMs) recently.
However, existing RLVR approaches merely train LLMs based on their own
generated responses and are constrained by the initial capability of LLMs, thus
prone to exploration stagnation, in which LLMs fail to solve more training
problems and cannot further learn from the training data. Some work tries to
address this by leveraging off-policy solutions to training problems but
requires external guidance from experts which suffers from limited
availability. In this work, we propose LTE (Learning to reason from Trial and
Error), an approach hinting LLMs with their previously self-generated incorrect
answers and problem of overlong responses, which does not require any external
expert guidance. Experiments validate the effectiveness of LTE, which
outperforms the normal group relative policy optimization (GRPO) by 6.38 in
Pass@1 and 9.00 in Pass@k on average across six mathematics benchmarks for
Qwen3-4B-Base. Further analysis confirms that LTE successfully mitigates the
problem of exploration stagnation and enhances both exploitation and
exploration during training.

</details>


### [108] [maxVSTAR: Maximally Adaptive Vision-Guided CSI Sensing with Closed-Loop Edge Model Adaptation for Robust Human Activity Recognition](https://arxiv.org/abs/2510.26146)
*Kexing Liu*

Main category: cs.LG

TL;DR: maxVSTAR is a vision-guided framework that autonomously adapts WiFi CSI-based activity recognition models to environmental changes using real-time visual supervision for edge deployment.


<details>
  <summary>Details</summary>
Motivation: Domain shift in WiFi CSI-based HAR severely degrades performance when deployed on edge devices under varying environmental conditions, limiting practical adoption.

Method: Closed-loop teacher-student architecture with YOLO vision model providing real-time activity labels to fine-tune lightweight CSI-based STAR model autonomously at the edge.

Result: Restored accuracy from 49.14% to 81.51% after single adaptation cycle, compared to baseline drop from 93.52% to 49.14% on uncalibrated hardware.

Conclusion: maxVSTAR enables dynamic, self-supervised model adaptation for privacy-conscious IoT environments, establishing scalable paradigm for long-term autonomous HAR at network edge.

Abstract: WiFi Channel State Information (CSI)-based human activity recognition (HAR)
provides a privacy-preserving, device-free sensing solution for smart
environments. However, its deployment on edge devices is severely constrained
by domain shift, where recognition performance deteriorates under varying
environmental and hardware conditions. This study presents maxVSTAR (maximally
adaptive Vision-guided Sensing Technology for Activity Recognition), a
closed-loop, vision-guided model adaptation framework that autonomously
mitigates domain shift for edge-deployed CSI sensing systems. The proposed
system integrates a cross-modal teacher-student architecture, where a
high-accuracy YOLO-based vision model serves as a dynamic supervisory signal,
delivering real-time activity labels for the CSI data stream. These labels
enable autonomous, online fine-tuning of a lightweight CSI-based HAR model,
termed Sensing Technology for Activity Recognition (STAR), directly at the
edge. This closed-loop retraining mechanism allows STAR to continuously adapt
to environmental changes without manual intervention. Extensive experiments
demonstrate the effectiveness of maxVSTAR. When deployed on uncalibrated
hardware, the baseline STAR model's recognition accuracy declined from 93.52%
to 49.14%. Following a single vision-guided adaptation cycle, maxVSTAR restored
the accuracy to 81.51%. These results confirm the system's capacity for
dynamic, self-supervised model adaptation in privacy-conscious IoT
environments, establishing a scalable and practical paradigm for long-term
autonomous HAR using CSI sensing at the network edge.

</details>


### [109] [STAR: A Privacy-Preserving, Energy-Efficient Edge AI Framework for Human Activity Recognition via Wi-Fi CSI in Mobile and Pervasive Computing Environments](https://arxiv.org/abs/2510.26148)
*Kexing Liu*

Main category: cs.LG

TL;DR: STAR is an edge-AI-optimized framework for Wi-Fi CSI-based human activity recognition that achieves 93.52% accuracy with a lightweight 97.6k-parameter model, enabling real-time processing on low-power embedded devices.


<details>
  <summary>Details</summary>
Motivation: Existing Wi-Fi CSI-based HAR methods suffer from computational inefficiency, high latency, and limited feasibility in resource-constrained mobile edge environments, requiring an optimized solution for practical deployment.

Method: STAR integrates a lightweight GRU-based neural network (33% smaller than LSTM), multi-stage signal processing (median filtering, Butterworth filtering, EMD), and hardware-aware co-optimization implemented on Rockchip RV1126 with NPU and ESP32-S3 CSI module.

Result: Achieved 93.52% mean accuracy across 7 activity classes and 99.11% for presence detection; INT8 quantization enabled 33 MHz processing speed with 8% CPU utilization, delivering 6x speed improvement over CPU execution.

Conclusion: STAR provides a practical, scalable solution for real-time, privacy-preserving HAR in mobile and pervasive computing environments with sub-second latency and low power consumption.

Abstract: Human Activity Recognition (HAR) via Wi-Fi Channel State Information (CSI)
presents a privacy-preserving, contactless sensing approach suitable for smart
homes, healthcare monitoring, and mobile IoT systems. However, existing methods
often encounter computational inefficiency, high latency, and limited
feasibility within resource-constrained, embedded mobile edge environments.
This paper proposes STAR (Sensing Technology for Activity Recognition), an
edge-AI-optimized framework that integrates a lightweight neural architecture,
adaptive signal processing, and hardware-aware co-optimization to enable
real-time, energy-efficient HAR on low-power embedded devices. STAR
incorporates a streamlined Gated Recurrent Unit (GRU)-based recurrent neural
network, reducing model parameters by 33% compared to conventional LSTM models
while maintaining effective temporal modeling capability. A multi-stage
pre-processing pipeline combining median filtering, 8th-order Butterworth
low-pass filtering, and Empirical Mode Decomposition (EMD) is employed to
denoise CSI amplitude data and extract spatial-temporal features. For on-device
deployment, STAR is implemented on a Rockchip RV1126 processor equipped with an
embedded Neural Processing Unit (NPU), interfaced with an ESP32-S3-based CSI
acquisition module. Experimental results demonstrate a mean recognition
accuracy of 93.52% across seven activity classes and 99.11% for human presence
detection, utilizing a compact 97.6k-parameter model. INT8 quantized inference
achieves a processing speed of 33 MHz with just 8% CPU utilization, delivering
sixfold speed improvements over CPU-based execution. With sub-second response
latency and low power consumption, the system ensures real-time,
privacy-preserving HAR, offering a practical, scalable solution for mobile and
pervasive computing environments.

</details>


### [110] [Bridging the Gap Between Molecule and Textual Descriptions via Substructure-aware Alignment](https://arxiv.org/abs/2510.26157)
*Hyuntae Park,Yeachan Kim,SangKeun Lee*

Main category: cs.LG

TL;DR: MolBridge is a novel molecule-text learning framework that uses substructure-aware alignments to capture fine-grained correspondences between molecular substructures and chemical phrases, outperforming state-of-the-art methods.


<details>
  <summary>Details</summary>
Motivation: Existing models struggle to capture subtle differences between molecules and their descriptions because they lack the ability to learn fine-grained alignments between molecular substructures and chemical phrases.

Method: MolBridge augments molecule-description pairs with alignment signals from molecular substructures and chemical phrases, using substructure-aware contrastive learning with a self-refinement mechanism to filter out noisy alignment signals.

Result: MolBridge effectively captures fine-grained correspondences and outperforms state-of-the-art baselines on a wide range of molecular benchmarks.

Conclusion: The results highlight the significance of substructure-aware alignment in molecule-text learning, demonstrating MolBridge's effectiveness in capturing detailed molecular-text relationships.

Abstract: Molecule and text representation learning has gained increasing interest due
to its potential for enhancing the understanding of chemical information.
However, existing models often struggle to capture subtle differences between
molecules and their descriptions, as they lack the ability to learn
fine-grained alignments between molecular substructures and chemical phrases.
To address this limitation, we introduce MolBridge, a novel molecule-text
learning framework based on substructure-aware alignments. Specifically, we
augment the original molecule-description pairs with additional alignment
signals derived from molecular substructures and chemical phrases. To
effectively learn from these enriched alignments, MolBridge employs
substructure-aware contrastive learning, coupled with a self-refinement
mechanism that filters out noisy alignment signals. Experimental results show
that MolBridge effectively captures fine-grained correspondences and
outperforms state-of-the-art baselines on a wide range of molecular benchmarks,
highlighting the significance of substructure-aware alignment in molecule-text
learning.

</details>


### [111] [Segmentation over Complexity: Evaluating Ensemble and Hybrid Approaches for Anomaly Detection in Industrial Time Series](https://arxiv.org/abs/2510.26159)
*Emilio Mastriani,Alessandro Costa,Federico Incardona,Kevin Munari,Sebastiano Spinello*

Main category: cs.LG

TL;DR: Complex feature engineering and hybrid models for anomaly detection in industrial time series underperformed compared to a simple Random Forest + XGBoost ensemble, which achieved superior performance with 0.976 AUC-ROC and 100% early detection.


<details>
  <summary>Details</summary>
Motivation: To investigate the effectiveness of advanced feature engineering and hybrid model architectures for anomaly detection in multivariate industrial time series, specifically for steam turbine systems.

Method: Evaluated change point-derived statistical features, clustering-based substructure representations, and hybrid learning strategies. Compared these against a simple Random Forest + XGBoost ensemble trained on segmented data.

Result: The ensemble achieved an AUC-ROC of 0.976, F1-score of 0.41, and 100% early detection within the defined time window, consistently outperforming more complex approaches.

Conclusion: In scenarios with highly imbalanced and temporally uncertain data, model simplicity combined with optimized segmentation can outperform sophisticated architectures, offering greater robustness, interpretability, and operational utility.

Abstract: In this study, we investigate the effectiveness of advanced feature
engineering and hybrid model architectures for anomaly detection in a
multivariate industrial time series, focusing on a steam turbine system. We
evaluate the impact of change point-derived statistical features,
clustering-based substructure representations, and hybrid learning strategies
on detection performance. Despite their theoretical appeal, these complex
approaches consistently underperformed compared to a simple Random Forest +
XGBoost ensemble trained on segmented data. The ensemble achieved an AUC-ROC of
0.976, F1-score of 0.41, and 100% early detection within the defined time
window. Our findings highlight that, in scenarios with highly imbalanced and
temporally uncertain data, model simplicity combined with optimized
segmentation can outperform more sophisticated architectures, offering greater
robustness, interpretability, and operational utility.

</details>


### [112] [A Game-Theoretic Spatio-Temporal Reinforcement Learning Framework for Collaborative Public Resource Allocation](https://arxiv.org/abs/2510.26184)
*Songxin Lei,Qiongyan Wang,Yanchen Zhu,Hanyu Yao,Sijie Ruan,Weilin Ruan,Yuyu Luo,Huaming Wu,Yuxuan Liang*

Main category: cs.LG

TL;DR: Proposes Collaborative Public Resource Allocation (CPRA) with capacity constraints and spatio-temporal dynamics, solved using Game-Theoretic Spatio-Temporal Reinforcement Learning (GSTRL) framework.


<details>
  <summary>Details</summary>
Motivation: Existing methods optimize individual resources independently without considering capacity constraints, limiting practical applicability in real-world scenarios.

Method: Formulates CPRA as a potential game and develops GSTRL framework that captures spatio-temporal dynamics to approximate Nash equilibrium of this NP-hard problem.

Result: GSTRL shows superior performance on two real-world datasets compared to existing methods.

Conclusion: The proposed CPRA problem and GSTRL framework provide a practical solution for collaborative resource allocation with capacity constraints, with solid theoretical foundation and empirical validation.

Abstract: Public resource allocation involves the efficient distribution of resources,
including urban infrastructure, energy, and transportation, to effectively meet
societal demands. However, existing methods focus on optimizing the movement of
individual resources independently, without considering their capacity
constraints. To address this limitation, we propose a novel and more practical
problem: Collaborative Public Resource Allocation (CPRA), which explicitly
incorporates capacity constraints and spatio-temporal dynamics in real-world
scenarios. We propose a new framework called Game-Theoretic Spatio-Temporal
Reinforcement Learning (GSTRL) for solving CPRA. Our contributions are twofold:
1) We formulate the CPRA problem as a potential game and demonstrate that there
is no gap between the potential function and the optimal target, laying a solid
theoretical foundation for approximating the Nash equilibrium of this NP-hard
problem; and 2) Our designed GSTRL framework effectively captures the
spatio-temporal dynamics of the overall system. We evaluate GSTRL on two
real-world datasets, where experiments show its superior performance. Our
source codes are available in the supplementary materials.

</details>


### [113] [Accumulative SGD Influence Estimation for Data Attribution](https://arxiv.org/abs/2510.26185)
*Yunxiao Shi,Shuo Yang,Yixin Su,Rui Zhang,Min Xu*

Main category: cs.LG

TL;DR: ACC-SGD-IE is a trajectory-aware influence estimation method that improves upon standard SGD-IE by accounting for cross-epoch compounding effects, leading to more accurate influence estimates and better data cleansing performance.


<details>
  <summary>Details</summary>
Motivation: Standard SGD-IE approximates leave-one-out effects by summing per-epoch surrogates but ignores cross-epoch compounding, which causes misranking of critical examples and inaccurate influence estimates.

Method: ACC-SGD-IE propagates leave-one-out perturbation across training and updates an accumulative influence state at each step, making it trajectory-aware rather than treating epochs independently.

Result: In smooth strongly convex settings, ACC-SGD-IE achieves geometric error contraction, and in smooth non-convex regimes it tightens error bounds. Empirically, it yields more accurate influence estimates, especially over long epochs, and more reliably flags noisy samples for data cleansing.

Conclusion: ACC-SGD-IE outperforms SGD-IE for influence estimation and data cleansing, producing better models when trained on data cleaned using ACC-SGD-IE influence estimates.

Abstract: Modern data-centric AI needs precise per-sample influence. Standard SGD-IE
approximates leave-one-out effects by summing per-epoch surrogates and ignores
cross-epoch compounding, which misranks critical examples. We propose
ACC-SGD-IE, a trajectory-aware estimator that propagates the leave-one-out
perturbation across training and updates an accumulative influence state at
each step. In smooth strongly convex settings it achieves geometric error
contraction and, in smooth non-convex regimes, it tightens error bounds; larger
mini-batches further reduce constants. Empirically, on Adult, 20 Newsgroups,
and MNIST under clean and corrupted data and both convex and non-convex
training, ACC-SGD-IE yields more accurate influence estimates, especially over
long epochs. For downstream data cleansing it more reliably flags noisy
samples, producing models trained on ACC-SGD-IE cleaned data that outperform
those cleaned with SGD-IE.

</details>


### [114] [Predicting All-Cause Hospital Readmissions from Medical Claims Data of Hospitalised Patients](https://arxiv.org/abs/2510.26188)
*Avinash Kadimisetty,Arun Rajagopalan,Vijendra SK*

Main category: cs.LG

TL;DR: Machine learning models (Random Forest, Logistic Regression, SVM) were used to predict hospital readmissions from health claims data, with Random Forest performing best. PCA was used for dimension reduction.


<details>
  <summary>Details</summary>
Motivation: Reducing preventable hospital readmissions is a national priority to improve healthcare quality and lower costs, as readmission rates are used as quality benchmarks.

Method: Used Logistic Regression, Random Forest, and Support Vector Machines on health claims data with Principal Component Analysis for dimension reduction. Models were evaluated using Area Under Curve (AUC) metric.

Result: Random Forest model achieved the highest performance, followed by Logistic Regression and Support Vector Machine models.

Conclusion: These machine learning models can identify crucial factors causing readmissions and help target patients to reduce readmission rates, ultimately lowering costs and improving healthcare quality.

Abstract: Reducing preventable hospital readmissions is a national priority for payers,
providers, and policymakers seeking to improve health care and lower costs. The
rate of readmission is being used as a benchmark to determine the quality of
healthcare provided by the hospitals. In thisproject, we have used machine
learning techniques like Logistic Regression, Random Forest and Support Vector
Machines to analyze the health claims data and identify demographic and medical
factors that play a crucial role in predicting all-cause readmissions. As the
health claims data is high dimensional, we have used Principal Component
Analysis as a dimension reduction technique and used the results for building
regression models. We compared and evaluated these models based on the Area
Under Curve (AUC) metric. Random Forest model gave the highest performance
followed by Logistic Regression and Support Vector Machine models. These models
can be used to identify the crucial factors causing readmissions and help
identify patients to focus on to reduce the chances of readmission, ultimately
bringing down the cost and increasing the quality of healthcare provided to the
patients.

</details>


### [115] [Test-Time Alignment of LLMs via Sampling-Based Optimal Control in pre-logit space](https://arxiv.org/abs/2510.26219)
*Sekitoshi Kanai,Tsukasa Yoshida,Hiroshi Takahashi,Haru Kuroki,Kazumune Hashimoto*

Main category: cs.LG

TL;DR: AISP is a test-time alignment method that applies Gaussian perturbation to pre-logits to maximize expected rewards, outperforming other methods in reward efficiency.


<details>
  <summary>Details</summary>
Motivation: Fine-tuning LLMs requires high computational costs, making test-time alignment methods more attractive for practical applications.

Method: Uses adaptive importance sampling on pre-logits with Gaussian perturbation, based on sampling-based model predictive control with stochastic control input.

Result: Outperforms best-of-n sampling in rewards per number of samples and achieves higher rewards than other reward-based test-time alignment methods.

Conclusion: AISP provides an effective test-time alignment approach that maximizes rewards while being computationally efficient compared to fine-tuning.

Abstract: Test-time alignment of large language models (LLMs) attracts attention
because fine-tuning LLMs requires high computational costs. In this paper, we
propose a new test-time alignment method called adaptive importance sampling on
pre-logits (AISP) on the basis of the sampling-based model predictive control
with the stochastic control input. AISP applies the Gaussian perturbation into
pre-logits, which are outputs of the penultimate layer, so as to maximize
expected rewards with respect to the mean of the perturbation. We demonstrate
that the optimal mean is obtained by importance sampling with sampled rewards.
AISP outperforms best-of-n sampling in terms of rewards over the number of used
samples and achieves higher rewards than other reward-based test-time alignment
methods.

</details>


### [116] [MPRU: Modular Projection-Redistribution Unlearning as Output Filter for Classification Pipelines](https://arxiv.org/abs/2510.26230)
*Minyi Peng,Darian Gunamardi,Ivan Tjuawinata,Kwok-Yan Lam*

Main category: cs.LG

TL;DR: A practical machine unlearning approach that treats classification training as a sequential process, enabling unlearning by reversing the last training sequence using a projection-redistribution layer, without requiring full access to original datasets or models.


<details>
  <summary>Details</summary>
Motivation: Existing machine unlearning methods face scalability issues and practical deployment challenges, requiring full access to original datasets and models, which limits their real-world applicability.

Method: Inductive approach treating classification as sequential learning, implementing unlearning by reversing the last training sequence through a projection-redistribution layer appended to the model, enabling modular deployment as an output filter.

Result: Experiments on CIFAR-10/100 (CNN) and Covertype (tree-based models) show similar performance to fully retrained models with significant computational cost reduction, demonstrating practical applicability and scalability.

Conclusion: The proposed approach provides a practical, scalable, and model-agnostic machine unlearning solution that maintains performance while enabling easy integration into existing classification pipelines with minimal modifications.

Abstract: As a new and promising approach, existing machine unlearning (MU) works
typically emphasize theoretical formulations or optimization objectives to
achieve knowledge removal. However, when deployed in real-world scenarios, such
solutions typically face scalability issues and have to address practical
requirements such as full access to original datasets and model. In contrast to
the existing approaches, we regard classification training as a sequential
process where classes are learned sequentially, which we call \emph{inductive
approach}. Unlearning can then be done by reversing the last training sequence.
This is implemented by appending a projection-redistribution layer in the end
of the model. Such an approach does not require full access to the original
dataset or the model, addressing the challenges of existing methods. This
enables modular and model-agnostic deployment as an output filter into existing
classification pipelines with minimal alterations. We conducted multiple
experiments across multiple datasets including image (CIFAR-10/100 using
CNN-based model) and tabular datasets (Covertype using tree-based model).
Experiment results show consistently similar output to a fully retrained model
with a high computational cost reduction. This demonstrates the applicability,
scalability, and system compatibility of our solution while maintaining the
performance of the output in a more practical setting.

</details>


### [117] [Angular Steering: Behavior Control via Rotation in Activation Space](https://arxiv.org/abs/2510.26243)
*Hieu M. Vu,Tan M. Nguyen*

Main category: cs.LG

TL;DR: Angular Steering is a novel method for controlling specific behaviors in LLMs by rotating activations in a 2D subspace, providing fine-grained control while maintaining general capabilities.


<details>
  <summary>Details</summary>
Motivation: Current steering methods are constrained to 2D subspaces, sensitive to parameters, and can affect unrelated features due to unintended activation space interactions.

Method: Formulates steering as geometric rotation toward/away from target behavior direction in fixed 2D subspace, with Adaptive variant rotating only aligned activations.

Result: Achieves robust behavioral control (refusal, compliance, emotion) while maintaining language modeling performance across multiple model families and sizes.

Conclusion: Angular Steering generalizes existing techniques under unified geometric framework, offering flexibility, generalization, and robustness compared to prior approaches.

Abstract: Controlling specific behaviors in large language models while preserving
their general capabilities is a central challenge for safe and reliable
artificial intelligence deployment. Current steering methods, such as vector
addition and directional ablation, are constrained within a two-dimensional
subspace defined by the activation and feature direction, making them sensitive
to chosen parameters and potentially affecting unrelated features due to
unintended interactions in activation space. We introduce Angular Steering, a
novel and flexible method for behavior modulation that operates by rotating
activations within a fixed two-dimensional subspace. By formulating steering as
a geometric rotation toward or away from a target behavior direction, Angular
Steering provides continuous, fine-grained control over behaviors such as
refusal and compliance. We demonstrate this method using refusal steering
emotion steering as use cases. Additionally, we propose Adaptive Angular
Steering, a selective variant that rotates only activations aligned with the
target feature, further enhancing stability and coherence. Angular Steering
generalizes existing addition and orthogonalization techniques under a unified
geometric rotation framework, simplifying parameter selection and maintaining
model stability across a broader range of adjustments. Experiments across
multiple model families and sizes show that Angular Steering achieves robust
behavioral control while maintaining general language modeling performance,
underscoring its flexibility, generalization, and robustness compared to prior
approaches. Code and artifacts are available at
https://github.com/lone17/angular-steering/.

</details>


### [118] [Likely Interpolants of Generative Models](https://arxiv.org/abs/2510.26266)
*Frederik Möbius Rygaard,Shen Zhu,Yinzhu Jin,Søren Hauberg,Tom Fletcher*

Main category: cs.LG

TL;DR: A general interpolation scheme for generative models that finds likely transition paths between data points, working as geodesics under a Riemannian metric without requiring additional training.


<details>
  <summary>Details</summary>
Motivation: Most generative models lack principled interpolation methods without restrictive assumptions on model or data dimensions, limiting controlled generation and model inspection capabilities.

Method: Developed a novel algorithm that computes interpolants analogous to geodesics constrained to data distributions, using suitable Riemannian metrics without additional training.

Result: The method quantitatively shows traversal through higher density regions than baseline approaches across various models and datasets.

Conclusion: The proposed interpolation scheme provides a principled approach for generative model interpolation that works effectively across different models and data distributions.

Abstract: Interpolation in generative models allows for controlled generation, model
inspection, and more. Unfortunately, most generative models lack a principal
notion of interpolants without restrictive assumptions on either the model or
data dimension. In this paper, we develop a general interpolation scheme that
targets likely transition paths compatible with different metrics and
probability distributions. We consider interpolants analogous to a geodesic
constrained to a suitable data distribution and derive a novel algorithm for
computing these curves, which requires no additional training. Theoretically,
we show that our method locally can be considered as a geodesic under a
suitable Riemannian metric. We quantitatively show that our interpolation
scheme traverses higher density regions than baselines across a range of models
and datasets.

</details>


### [119] [Distributional Multi-objective Black-box Optimization for Diffusion-model Inference-time Multi-Target Generation](https://arxiv.org/abs/2510.26278)
*Kim Yong Tan,Yueming Lyu,Ivor Tsang,Yew-Soon Ong*

Main category: cs.LG

TL;DR: IMG algorithm optimizes diffusion process at inference-time using weighted resampling to generate samples satisfying multiple objectives simultaneously, achieving higher efficiency than traditional methods.


<details>
  <summary>Details</summary>
Motivation: Existing approaches treat diffusion models as black-box refiners with external optimization loops, overlooking internal distribution transitions and limiting efficiency in multi-objective optimization.

Method: Performs weighted resampling during diffusion generation process according to expected aggregated multi-objective values, ensuring samples follow desired multi-target Boltzmann distribution.

Result: IMG achieves significantly higher hypervolume than baseline algorithms with only a single generation pass, compared to hundreds required by traditional methods.

Conclusion: IMG provides an optimized diffusion process that can be integrated into existing methods to improve performance, with the multi-target Boltzmann distribution being the optimal solution to distributional multi-objective optimization.

Abstract: Diffusion models have been successful in learning complex data distributions.
This capability has driven their application to high-dimensional
multi-objective black-box optimization problem. Existing approaches often
employ an external optimization loop, such as an evolutionary algorithm, to the
diffusion model. However, these approaches treat the diffusion model as a
black-box refiner, which overlooks the internal distribution transition of the
diffusion generation process, limiting their efficiency. To address these
challenges, we propose the Inference-time Multi-target Generation (IMG)
algorithm, which optimizes the diffusion process at inference-time to generate
samples that simultaneously satisfy multiple objectives. Specifically, our IMG
performs weighted resampling during the diffusion generation process according
to the expected aggregated multi-objective values. This weighted resampling
strategy ensures the diffusion-generated samples are distributed according to
our desired multi-target Boltzmann distribution. We further derive that the
multi-target Boltzmann distribution has an interesting log-likelihood
interpretation, where it is the optimal solution to the distributional
multi-objective optimization problem. We implemented IMG for a multi-objective
molecule generation task. Experiments show that IMG, requiring only a single
generation pass, achieves a significantly higher hypervolume than baseline
optimization algorithms that often require hundreds of diffusion generations.
Notably, our algorithm can be viewed as an optimized diffusion process and can
be integrated into existing methods to further improve their performance.

</details>


### [120] [Empirical Bayesian Multi-Bandit Learning](https://arxiv.org/abs/2510.26284)
*Xia Jiang,Rong J. B. Zhu*

Main category: cs.LG

TL;DR: A hierarchical Bayesian framework for multi-task contextual bandits that captures task heterogeneity and correlations through empirical Bayesian estimation of covariance structure, with two efficient algorithms (ebmTS and ebmUCB) showing superior performance.


<details>
  <summary>Details</summary>
Motivation: Multi-task learning in contextual bandits can enhance decision-making by leveraging shared structures across related tasks, but existing methods overlook learning the covariance structure across bandits, limiting practicality and flexibility.

Method: Proposed a hierarchical Bayesian framework with empirical Bayesian approach to estimate the covariance matrix of prior distribution, then developed ebmTS (Thompson Sampling) and ebmUCB (Upper Confidence Bound) algorithms that incorporate the estimated prior.

Result: Algorithms achieve lower cumulative regret compared to existing techniques, particularly in complex environments, with frequentist regret upper bounds provided to fill research gap in multi-bandit problems.

Conclusion: The hierarchical Bayesian framework with empirical covariance estimation enables effective information sharing while accommodating instance-specific variations, demonstrating superior performance in multi-bandit learning.

Abstract: Multi-task learning in contextual bandits has attracted significant research
interest due to its potential to enhance decision-making across multiple
related tasks by leveraging shared structures and task-specific heterogeneity.
In this article, we propose a novel hierarchical Bayesian framework for
learning in various bandit instances. This framework captures both the
heterogeneity and the correlations among different bandit instances through a
hierarchical Bayesian model, enabling effective information sharing while
accommodating instance-specific variations. Unlike previous methods that
overlook the learning of the covariance structure across bandits, we introduce
an empirical Bayesian approach to estimate the covariance matrix of the prior
distribution.This enhances both the practicality and flexibility of learning
across multi-bandits. Building on this approach, we develop two efficient
algorithms: ebmTS (Empirical Bayesian Multi-Bandit Thompson Sampling) and
ebmUCB (Empirical Bayesian Multi-Bandit Upper Confidence Bound), both of which
incorporate the estimated prior into the decision-making process. We provide
the frequentist regret upper bounds for the proposed algorithms, thereby
filling a research gap in the field of multi-bandit problems. Extensive
experiments on both synthetic and real-world datasets demonstrate the superior
performance of our algorithms, particularly in complex environments. Our
methods achieve lower cumulative regret compared to existing techniques,
highlighting their effectiveness in balancing exploration and exploitation
across multi-bandits.

</details>


### [121] [Offline Clustering of Preference Learning with Active-data Augmentation](https://arxiv.org/abs/2510.26301)
*Jingyuan Liu,Fatemeh Ghaffari,Xuchuang Wang,Mohammad Hajiesmaili,Carlee Joe-Wong*

Main category: cs.LG

TL;DR: The paper proposes offline clustering methods for preference learning to handle multiple users with different preferences and imbalanced data, with both pure offline and active-data augmentation approaches.


<details>
  <summary>Details</summary>
Motivation: Real-world preference learning faces challenges with limited user interactions, different user preferences, and imbalanced offline data across preference dimensions.

Method: Off-C²PL for pure offline setting with theoretical suboptimality analysis, and A²-Off-C²PL for active-data augmentation that selects samples targeting least-informative preference dimensions.

Result: Theoretical analysis shows tradeoff between sample noise and bias, and actively collected samples are proven more effective than offline ones. Simulations validate on synthetic and real-world datasets.

Conclusion: The proposed clustering framework effectively handles multi-user preference learning with imbalanced data, with active-data augmentation providing additional benefits for underrepresented preference dimensions.

Abstract: Preference learning from pairwise feedback is a widely adopted framework in
applications such as reinforcement learning with human feedback and
recommendations. In many practical settings, however, user interactions are
limited or costly, making offline preference learning necessary. Moreover,
real-world preference learning often involves users with different preferences.
For example, annotators from different backgrounds may rank the same responses
differently. This setting presents two central challenges: (1) identifying
similarity across users to effectively aggregate data, especially under
scenarios where offline data is imbalanced across dimensions, and (2) handling
the imbalanced offline data where some preference dimensions are
underrepresented. To address these challenges, we study the Offline Clustering
of Preference Learning problem, where the learner has access to fixed datasets
from multiple users with potentially different preferences and aims to maximize
utility for a test user. To tackle the first challenge, we first propose
Off-C$^2$PL for the pure offline setting, where the learner relies solely on
offline data. Our theoretical analysis provides a suboptimality bound that
explicitly captures the tradeoff between sample noise and bias. To address the
second challenge of inbalanced data, we extend our framework to the setting
with active-data augmentation where the learner is allowed to select a limited
number of additional active-data for the test user based on the cluster
structure learned by Off-C$^2$PL. In this setting, our second algorithm,
A$^2$-Off-C$^2$PL, actively selects samples that target the least-informative
dimensions of the test user's preference. We prove that these actively
collected samples contribute more effectively than offline ones. Finally, we
validate our theoretical results through simulations on synthetic and
real-world datasets.

</details>


### [122] [Understanding Hardness of Vision-Language Compositionality from A Token-level Causal Lens](https://arxiv.org/abs/2510.26302)
*Ziliang Chen,Tianang Xiao,Jusheng Zhang,Yongsen Zheng,Xipeng Chen*

Main category: cs.LG

TL;DR: CLIP fails at compositional reasoning despite strong cross-modal alignment. A token-aware causal framework reveals composition nonidentifiability as the core issue, explaining why CLIP behaves like a bag-of-words matcher and struggles with hard negatives.


<details>
  <summary>Details</summary>
Motivation: CLIP delivers strong cross-modal generalization but persistently fails at compositional reasoning over objects, attributes, and relations, behaving like a bag-of-words matcher. Prior causal accounts obscure token-level structure, leaving key phenomena unexplained.

Method: Token-aware causal representation learning framework with sequential language-token SCM. Extends block identifiability to tokenized text, proving CLIP's contrastive objective can recover modal-invariant latent variables under both sentence-level and token-level SCMs.

Result: Token granularity yields the first principled explanation of CLIP's compositional brittleness: composition nonidentifiability. Shows existence of pseudo-optimal text encoders that achieve perfect alignment but are insensitive to SWAP, REPLACE, and ADD operations over atomic concepts.

Conclusion: Language-side nonidentifiability links to visual-side failures via modality gap. Iterated composition operators compound hardness, motivating improved negative mining strategies for better compositional reasoning.

Abstract: Contrastive Language-Image Pre-training (CLIP) delivers strong cross modal
generalization by aligning images and texts in a shared embedding space, yet it
persistently fails at compositional reasoning over objects, attributes, and
relations often behaving like a bag-of-words matcher. Prior causal accounts
typically model text as a single vector, obscuring token-level structure and
leaving core phenomena-such as prompt sensitivity and failures on hard
negatives unexplained. We address this gap with a token-aware causal
representation learning (CRL) framework grounded in a sequential,
language-token SCM. Our theory extends block identifiability to tokenized text,
proving that CLIP's contrastive objective can recover the modal-invariant
latent variable under both sentence-level and token-level SCMs. Crucially,
token granularity yields the first principled explanation of CLIP's
compositional brittleness: composition nonidentifiability. We show the
existence of pseudo-optimal text encoders that achieve perfect modal-invariant
alignment yet are provably insensitive to SWAP, REPLACE, and ADD operations
over atomic concepts, thereby failing to distinguish correct captions from hard
negatives despite optimizing the same training objective as true-optimal
encoders. The analysis further links language-side nonidentifiability to
visual-side failures via the modality gap and shows how iterated composition
operators compound hardness, motivating improved negative mining strategies.

</details>


### [123] [Implicit Bias of Per-sample Adam on Separable Data: Departure from the Full-batch Regime](https://arxiv.org/abs/2510.26303)
*Beomhan Baek,Minhak Song,Chulhee Yun*

Main category: cs.LG

TL;DR: This paper analyzes the implicit bias of incremental Adam optimizer, showing it differs from full-batch Adam and can converge to ℓ₂-max-margin classifier, while Signum remains invariant to batch size.


<details>
  <summary>Details</summary>
Motivation: Adam is widely used but its theoretical understanding is limited, especially regarding how batch size affects its implicit bias. Prior analyses only covered full-batch regime.

Method: Analyzed incremental Adam (one sample per step) for logistic regression on separable data, developed proxy algorithm for β₂→1 limit, and compared with Signum optimizer.

Result: Found incremental Adam can converge to ℓ₂-max-margin classifier on structured datasets, unlike full-batch Adam's ℓ∞ bias. Signum converges to ℓ∞-max-margin for any batch size.

Conclusion: Adam's implicit bias depends on both batching scheme and dataset structure, while Signum's bias remains invariant to batch size.

Abstract: Adam [Kingma and Ba, 2015] is the de facto optimizer in deep learning, yet
its theoretical understanding remains limited. Prior analyses show that Adam
favors solutions aligned with $\ell_\infty$-geometry, but these results are
restricted to the full-batch regime. In this work, we study the implicit bias
of incremental Adam (using one sample per step) for logistic regression on
linearly separable data, and we show that its bias can deviate from the
full-batch behavior. To illustrate this, we construct a class of structured
datasets where incremental Adam provably converges to the $\ell_2$-max-margin
classifier, in contrast to the $\ell_\infty$-max-margin bias of full-batch
Adam. For general datasets, we develop a proxy algorithm that captures the
limiting behavior of incremental Adam as $\beta_2 \to 1$ and we characterize
its convergence direction via a data-dependent dual fixed-point formulation.
Finally, we prove that, unlike Adam, Signum [Bernstein et al., 2018] converges
to the $\ell_\infty$-max-margin classifier for any batch size by taking $\beta$
close enough to 1. Overall, our results highlight that the implicit bias of
Adam crucially depends on both the batching scheme and the dataset, while
Signum remains invariant.

</details>


### [124] [Model Inversion with Layer-Specific Modeling and Alignment for Data-Free Continual Learning](https://arxiv.org/abs/2510.26311)
*Ruilin Tong,Haodong Lu,Yuhang Liu,Dong Gong*

Main category: cs.LG

TL;DR: The paper proposes Per-layer Model Inversion (PMI) and feature modeling for data-free continual learning, enabling efficient generation of pseudo-data from pre-trained models without storing real samples.


<details>
  <summary>Details</summary>
Motivation: Data-free continual learning is needed when storing and replaying real data is infeasible due to privacy/security constraints or impractical for pre-trained models. Model inversion faces challenges with feature drift and computational expense.

Method: Proposes PMI for efficient initialization of full-model inversion, and models class-wise features via Gaussian distributions with contrastive learning to align synthetic and real features.

Result: The approach enables continual learning of new classes by generating pseudo-images from semantic-aware features, achieving strong effectiveness across multiple CL settings.

Conclusion: Combining PMI and feature modeling provides an effective solution for data-free continual learning that is efficient and compatible with various CL scenarios.

Abstract: Continual learning (CL) aims to incrementally train a model on a sequence of
tasks while retaining performance on prior ones. However, storing and replaying
data is often infeasible due to privacy or security constraints and impractical
for arbitrary pre-trained models. Data-free CL seeks to update models without
access to previous data. Beyond regularization, we employ model inversion to
synthesize data from the trained model, enabling replay without storing
samples. Yet, model inversion in predictive models faces two challenges: (1)
generating inputs solely from compressed output labels causes drift between
synthetic and real data, and replaying such data can erode prior knowledge; (2)
inversion is computationally expensive since each step backpropagates through
the full model. These issues are amplified in large pre-trained models such as
CLIP. To improve efficiency, we propose Per-layer Model Inversion (PMI),
inspired by faster convergence in single-layer optimization. PMI provides
strong initialization for full-model inversion, substantially reducing
iterations. To mitigate feature shift, we model class-wise features via
Gaussian distributions and contrastive model, ensuring alignment between
synthetic and real features. Combining PMI and feature modeling, our approach
enables continual learning of new classes by generating pseudo-images from
semantic-aware projected features, achieving strong effectiveness and
compatibility across multiple CL settings.

</details>


### [125] [On the Impact of Weight Discretization in QUBO-Based SVM Training](https://arxiv.org/abs/2510.26323)
*Sascha Mücke*

Main category: cs.LG

TL;DR: Quantum annealing for SVM training via QUBO formulation shows competitive performance even with low-precision parameter encoding, suggesting support vector selection matters more than precise weighting.


<details>
  <summary>Details</summary>
Motivation: To study how qubit count (parameter discretization level) affects SVM predictive performance and compare quantum annealing-based training with classical LIBSVM solver.

Method: Formulate SVM training as QUBO problem, use quantum annealing for optimization, vary discretization levels of dual weights (bit-depth per parameter), and compare with classical LIBSVM.

Result: Low-precision QUBO encodings (1 bit per parameter) yield competitive and sometimes superior accuracy compared to LIBSVM. Increased bit-depth enables larger regularization but doesn't always improve classification.

Conclusion: Support vector selection may be more important than precise weighting. Quantum annealing shows potential for efficient SVM training as quantum hardware scales, despite current size limitations.

Abstract: Training Support Vector Machines (SVMs) can be formulated as a QUBO problem,
enabling the use of quantum annealing for model optimization. In this work, we
study how the number of qubits - linked to the discretization level of dual
weights - affects predictive performance across datasets. We compare QUBO-based
SVM training to the classical LIBSVM solver and find that even low-precision
QUBO encodings (e.g., 1 bit per parameter) yield competitive, and sometimes
superior, accuracy. While increased bit-depth enables larger regularization
parameters, it does not always improve classification. Our findings suggest
that selecting the right support vectors may matter more than their precise
weighting. Although current hardware limits the size of solvable QUBOs, our
results highlight the potential of quantum annealing for efficient SVM training
as quantum devices scale.

</details>


### [126] [Posterior Sampling by Combining Diffusion Models with Annealed Langevin Dynamics](https://arxiv.org/abs/2510.26324)
*Zhiyang Xun,Shivam Gupta,Eric Price*

Main category: cs.LG

TL;DR: The paper presents a method for conditional sampling from posterior distributions in noisy linear inverse problems by combining diffusion models with annealed Langevin dynamics, achieving polynomial-time sampling with only L^4 bounds on score estimation error.


<details>
  <summary>Details</summary>
Motivation: Posterior sampling is crucial for tasks like inpainting, deblurring, and MRI reconstruction, but approximate posterior sampling is computationally intractable in general. Existing methods like Langevin dynamics are brittle to score estimation errors.

Method: The authors combine diffusion models with an annealed variant of Langevin dynamics, focusing on log-concave distributions. This approach leverages the robustness of diffusion models to score estimation errors while maintaining the conditional sampling capability.

Result: The method achieves conditional sampling in polynomial time with only an L^4 bound on the score estimation error, significantly improving over traditional Langevin dynamics which requires sub-exponential error bounds.

Conclusion: The proposed combination of diffusion models and annealed Langevin dynamics provides an efficient and robust framework for posterior sampling in noisy linear inverse problems, overcoming the brittleness of existing methods to score estimation errors.

Abstract: Given a noisy linear measurement $y = Ax + \xi$ of a distribution $p(x)$, and
a good approximation to the prior $p(x)$, when can we sample from the posterior
$p(x \mid y)$? Posterior sampling provides an accurate and fair framework for
tasks such as inpainting, deblurring, and MRI reconstruction, and several
heuristics attempt to approximate it. Unfortunately, approximate posterior
sampling is computationally intractable in general.
  To sidestep this hardness, we focus on (local or global) log-concave
distributions $p(x)$. In this regime, Langevin dynamics yields posterior
samples when the exact scores of $p(x)$ are available, but it is brittle to
score--estimation error, requiring an MGF bound (sub-exponential error). By
contrast, in the unconditional setting, diffusion models succeed with only an
$L^2$ bound on the score error. We prove that combining diffusion models with
an annealed variant of Langevin dynamics achieves conditional sampling in
polynomial time using merely an $L^4$ bound on the score error.

</details>


### [127] [Agent Skills Enable a New Class of Realistic and Trivially Simple Prompt Injections](https://arxiv.org/abs/2510.26328)
*David Schmotz,Sahar Abdelnabi,Maksym Andriushchenko*

Main category: cs.LG

TL;DR: Agent Skills framework in LLMs is fundamentally insecure and vulnerable to simple prompt injections that can exfiltrate sensitive data and bypass system guardrails.


<details>
  <summary>Details</summary>
Motivation: To demonstrate security vulnerabilities in the Agent Skills framework for LLMs, which enables continual learning but lacks proper security measures against prompt injection attacks.

Method: Demonstrated hiding malicious instructions in long Agent Skill files and referenced scripts, and showed how to bypass system-level guardrails through benign task approvals with "Don't ask again" options.

Result: Successfully exfiltrated sensitive data like internal files and passwords, and bypassed security guardrails of a popular coding agent, showing that closely related harmful actions can inherit approvals from benign tasks.

Conclusion: Despite ongoing research and scaling model capabilities, frontier LLMs remain vulnerable to simple prompt injections in realistic scenarios, highlighting fundamental security weaknesses in the Agent Skills framework.

Abstract: Enabling continual learning in LLMs remains a key unresolved research
challenge. In a recent announcement, a frontier LLM company made a step towards
this by introducing Agent Skills, a framework that equips agents with new
knowledge based on instructions stored in simple markdown files. Although Agent
Skills can be a very useful tool, we show that they are fundamentally insecure,
since they enable trivially simple prompt injections. We demonstrate how to
hide malicious instructions in long Agent Skill files and referenced scripts to
exfiltrate sensitive data, such as internal files or passwords. Importantly, we
show how to bypass system-level guardrails of a popular coding agent: a benign,
task-specific approval with the "Don't ask again" option can carry over to
closely related but harmful actions. Overall, we conclude that despite ongoing
research efforts and scaling model capabilities, frontier LLMs remain
vulnerable to very simple prompt injections in realistic scenarios. Our code is
available at https://github.com/aisa-group/promptinject-agent-skills.

</details>


### [128] [Linear Causal Discovery with Interventional Constraints](https://arxiv.org/abs/2510.26342)
*Zhigao Guo,Feng Dong*

Main category: cs.LG

TL;DR: The paper introduces interventional constraints as a novel concept in causal discovery that encodes high-level causal knowledge through inequality constraints on causal effects, bridging the gap between structural constraints and actual causal influences.


<details>
  <summary>Details</summary>
Motivation: Existing causal discovery methods can enforce structural constraints but may still produce incorrect causal conclusions that contradict known causal influences, such as learning opposite effects from established biological knowledge.

Method: Proposes a metric to quantify total causal effects for linear causal models and formulates the problem as a constrained optimization task, solved using a two-stage constrained optimization method.

Result: Evaluation on real-world datasets shows that integrating interventional constraints improves model accuracy, ensures consistency with established findings, makes models more explainable, and facilitates discovery of new causal relationships that would be costly to identify otherwise.

Conclusion: Interventional constraints provide a powerful framework for incorporating causal knowledge into discovery methods, bridging the gap between structural requirements and actual causal effects while enabling more accurate and explainable causal models.

Abstract: Incorporating causal knowledge and mechanisms is essential for refining
causal models and improving downstream tasks such as designing new treatments.
In this paper, we introduce a novel concept in causal discovery, termed
interventional constraints, which differs fundamentally from interventional
data. While interventional data require direct perturbations of variables,
interventional constraints encode high-level causal knowledge in the form of
inequality constraints on causal effects. For instance, in the Sachs dataset
(Sachs et al.\ 2005), Akt has been shown to be activated by PIP3, meaning PIP3
exerts a positive causal effect on Akt. Existing causal discovery methods allow
enforcing structural constraints (for example, requiring a causal path from
PIP3 to Akt), but they may still produce incorrect causal conclusions such as
learning that "PIP3 inhibits Akt". Interventional constraints bridge this gap
by explicitly constraining the total causal effect between variable pairs,
ensuring learned models respect known causal influences. To formalize
interventional constraints, we propose a metric to quantify total causal
effects for linear causal models and formulate the problem as a constrained
optimization task, solved using a two-stage constrained optimization method. We
evaluate our approach on real-world datasets and demonstrate that integrating
interventional constraints not only improves model accuracy and ensures
consistency with established findings, making models more explainable, but also
facilitates the discovery of new causal relationships that would otherwise be
costly to identify.

</details>


### [129] [Reinforcement Learning for Pollution Detection in a Randomized, Sparse and Nonstationary Environment with an Autonomous Underwater Vehicle](https://arxiv.org/abs/2510.26347)
*Sebastian Zieglmeier,Niklas Erdmann,Narada D. Warakagoda*

Main category: cs.LG

TL;DR: Modified Monte Carlo-based RL approach outperforms traditional Q-learning and exhaustive search in sparse, random, nonstationary environments like underwater pollution detection.


<details>
  <summary>Details</summary>
Motivation: RL algorithms struggle in random, nonstationary environments with sparse rewards, particularly in applications like AUV-based pollution cloud detection where actions often yield zero reward.

Method: Systematically modified classical RL approaches with hierarchical changes, multigoal learning, and location memory as external filter to prevent state revisits.

Result: Modified Monte Carlo-based approach significantly outperformed traditional Q-learning and two exhaustive search patterns.

Conclusion: RL approaches can be effectively adapted for random, nonstationary, and reward-sparse environments through systematic modifications.

Abstract: Reinforcement learning (RL) algorithms are designed to optimize
problem-solving by learning actions that maximize rewards, a task that becomes
particularly challenging in random and nonstationary environments. Even
advanced RL algorithms are often limited in their ability to solve problems in
these conditions. In applications such as searching for underwater pollution
clouds with autonomous underwater vehicles (AUVs), RL algorithms must navigate
reward-sparse environments, where actions frequently result in a zero reward.
This paper aims to address these challenges by revisiting and modifying
classical RL approaches to efficiently operate in sparse, randomized, and
nonstationary environments. We systematically study a large number of
modifications, including hierarchical algorithm changes, multigoal learning,
and the integration of a location memory as an external output filter to
prevent state revisits. Our results demonstrate that a modified Monte
Carlo-based approach significantly outperforms traditional Q-learning and two
exhaustive search patterns, illustrating its potential in adapting RL to
complex environments. These findings suggest that reinforcement learning
approaches can be effectively adapted for use in random, nonstationary, and
reward-sparse environments.

</details>


### [130] [UnifiedFL: A Dynamic Unified Learning Framework for Equitable Federation](https://arxiv.org/abs/2510.26350)
*Furkan Pala,Islem Rekik*

Main category: cs.LG

TL;DR: UnifiedFL is a dynamic federated learning framework that addresses architectural and statistical heterogeneity across clients with different neural architectures and non-IID data distributions using a shared GNN-based approach.


<details>
  <summary>Details</summary>
Motivation: Existing FL methods fail to support fundamentally different neural architectures (e.g., CNNs, GNNs, MLPs) and overlook domain-fracture problems where client data distributions differ from test environments, leading to poor generalization.

Method: Represents heterogeneous local networks as nodes/edges in a directed model graph optimized by a shared GNN, with distance-driven clustering and two-tier aggregation policy balancing convergence and diversity.

Result: Superior performance demonstrated on MedMNIST classification and hippocampus segmentation benchmarks compared to existing methods.

Conclusion: UnifiedFL effectively handles architectural heterogeneity, statistical heterogeneity, and domain-fracture problems in federated learning settings.

Abstract: Federated learning (FL) has emerged as a key paradigm for collaborative model
training across multiple clients without sharing raw data, enabling
privacy-preserving applications in areas such as radiology and pathology.
However, works on collaborative training across clients with fundamentally
different neural architectures and non-identically distributed datasets remain
scarce. Existing FL frameworks face several limitations. Despite claiming to
support architectural heterogeneity, most recent FL methods only tolerate
variants within a single model family (e.g., shallower, deeper, or wider CNNs),
still presuming a shared global architecture and failing to accommodate
federations where clients deploy fundamentally different network types (e.g.,
CNNs, GNNs, MLPs). Moreover, existing approaches often address only statistical
heterogeneity while overlooking the domain-fracture problem, where each
client's data distribution differs markedly from that faced at testing time,
undermining model generalizability. When clients use different architectures,
have non-identically distributed data, and encounter distinct test domains,
current methods perform poorly. To address these challenges, we propose
UnifiedFL, a dynamic federated learning framework that represents heterogeneous
local networks as nodes and edges in a directed model graph optimized by a
shared graph neural network (GNN). UnifiedFL introduces (i) a common GNN to
parameterize all architectures, (ii) distance-driven clustering via Euclidean
distances between clients' parameters, and (iii) a two-tier aggregation policy
balancing convergence and diversity. Experiments on MedMNIST classification and
hippocampus segmentation benchmarks demonstrate UnifiedFL's superior
performance. Code and data: https://github.com/basiralab/UnifiedFL

</details>


### [131] [Towards Explainable and Reliable AI in Finance](https://arxiv.org/abs/2510.26353)
*Albi Isufaj,Pablo Mollá,Helmut Prendinger*

Main category: cs.LG

TL;DR: The paper presents methods for explainable and reliable AI in financial forecasting, including Time-LLM for avoiding wrong directional forecasts, reliability estimators to filter unreliable predictions, and symbolic reasoning for transparent justification.


<details>
  <summary>Details</summary>
Motivation: Address the opacity challenges of large neural network models in financial forecasting for trust and regulatory compliance.

Method: Three approaches: Time-LLM foundation model with prompts to avoid wrong forecasts, combining foundation models with reliability estimators, and symbolic reasoning encoding domain rules.

Result: Experiments on equity and cryptocurrency data show reduced false positives and support for selective execution.

Conclusion: The framework advances transparent and auditable financial AI systems by integrating predictive performance with reliability estimation and rule-based reasoning.

Abstract: Financial forecasting increasingly uses large neural network models, but
their opacity raises challenges for trust and regulatory compliance. We present
several approaches to explainable and reliable AI in finance. \emph{First}, we
describe how Time-LLM, a time series foundation model, uses a prompt to avoid a
wrong directional forecast. \emph{Second}, we show that combining foundation
models for time series forecasting with a reliability estimator can filter our
unreliable predictions. \emph{Third}, we argue for symbolic reasoning encoding
domain rules for transparent justification. These approaches shift emphasize
executing only forecasts that are both reliable and explainable. Experiments on
equity and cryptocurrency data show that the architecture reduces false
positives and supports selective execution. By integrating predictive
performance with reliability estimation and rule-based reasoning, our framework
advances transparent and auditable financial AI systems.

</details>


### [132] [CorVS: Person Identification via Video Trajectory-Sensor Correspondence in a Real-World Warehouse](https://arxiv.org/abs/2510.26369)
*Kazuma Kano,Yuki Mori,Shin Katayama,Kenta Urano,Takuro Yonezawa,Nobuo Kawaguchi*

Main category: cs.LG

TL;DR: CorVS is a novel person identification method that matches visual tracking trajectories with wearable sensor measurements using deep learning to predict correspondence probabilities and reliabilities, then performs temporal matching for robust identification in real-world warehouse environments.


<details>
  <summary>Details</summary>
Motivation: Worker location data is crucial for productivity in industrial sites. While cameras offer valuable environmental context, visual-only identification is impractical. Existing methods that combine trajectories and sensor data break down under real-world conditions, necessitating a more robust solution.

Method: Proposed CorVS method: 1) Deep learning model predicts correspondence probabilities and reliabilities for every trajectory-sensor measurement pair, 2) Algorithm matches trajectories and sensor measurements over time using predicted probabilities and reliabilities.

Result: Developed a dataset with actual warehouse operations and demonstrated the method's effectiveness for real-world applications, showing improved performance over existing approaches.

Conclusion: CorVS provides a robust data-driven solution for person identification in industrial environments by effectively combining visual tracking trajectories with wearable sensor measurements, overcoming limitations of previous methods in real-world conditions.

Abstract: Worker location data is key to higher productivity in industrial sites.
Cameras are a promising tool for localization in logistics warehouses since
they also offer valuable environmental contexts such as package status.
However, identifying individuals with only visual data is often impractical.
Accordingly, several prior studies identified people in videos by comparing
their trajectories and wearable sensor measurements. While this approach has
advantages such as independence from appearance, the existing methods may break
down under real-world conditions. To overcome this challenge, we propose CorVS,
a novel data-driven person identification method based on correspondence
between visual tracking trajectories and sensor measurements. Firstly, our deep
learning model predicts correspondence probabilities and reliabilities for
every pair of a trajectory and sensor measurements. Secondly, our algorithm
matches the trajectories and sensor measurements over time using the predicted
probabilities and reliabilities. We developed a dataset with actual warehouse
operations and demonstrated the method's effectiveness for real-world
applications.

</details>


### [133] [Efficient Generative AI Boosts Probabilistic Forecasting of Sudden Stratospheric Warmings](https://arxiv.org/abs/2510.26376)
*Ningning Tao,Fei Xie,Baoxiang Pan,Hongyu Wang,Han Huang,Zhongpu Qiu,Ke Gui,Jiali Luo,Xiaosong Chen*

Main category: cs.LG

TL;DR: FM-Cast is a generative AI model using Flow Matching for efficient probabilistic forecasting of Sudden Stratospheric Warmings, achieving comparable accuracy to NWP systems while being 1000x faster.


<details>
  <summary>Details</summary>
Motivation: SSWs are crucial for subseasonal predictability but remain challenging to forecast accurately due to computational demands and physical limitations in NWP systems. Data-driven approaches for complex 3D SSW dynamics and probabilistic forecasting are underexplored.

Method: Developed a Flow Matching-based generative AI model (FM-Cast) for probabilistic forecasting of stratospheric circulation evolution, enabling efficient ensemble generation.

Result: FM-Cast successfully forecasted 10 out of 18 major SSW events up to 20 days in advance with ensemble accuracies above 50%. It performs comparably to leading NWP systems while requiring only 2 minutes for 50-member, 30-day forecasts on consumer GPUs.

Conclusion: FM-Cast establishes an efficient paradigm for probabilistic stratospheric forecasting and demonstrates generative AI's potential to enhance physical understanding of atmosphere-climate dynamics by linking SSW predictability to underlying physical drivers.

Abstract: Sudden Stratospheric Warmings (SSWs) are key sources of subseasonal
predictability and major drivers of extreme winter weather. Yet, their accurate
and efficient forecast remains a persistent challenge for numerical weather
prediction (NWP) systems due to limitations in physical representation,
initialization, and the immense computational demands of ensemble forecasts.
While data-driven forecasting is rapidly evolving, its application to the
complex, three-dimensional dynamics of SSWs, particularly for probabilistic
forecast, remains underexplored. Here, we bridge this gap by developing a Flow
Matching-based generative AI model (FM-Cast) for efficient and skillful
probabilistic forecasting of the spatiotemporal evolution of stratospheric
circulation. Evaluated across 18 major SSW events (1998-2024), FM-Cast
skillfully forecasts the onset, intensity, and morphology of 10 events up to 20
days in advance, achieving ensemble accuracies above 50%. Its performance is
comparable to or exceeds leading NWP systems while requiring only two minutes
for a 50-member, 30-day forecast on a consumer GPU. Furthermore, leveraging
FM-Cast as a scientific tool, we demonstrate through idealized experiments that
SSW predictability is fundamentally linked to its underlying physical drivers,
distinguishing between events forced from the troposphere and those driven by
internal stratospheric dynamics. Our work thus establishes a computationally
efficient paradigm for probabilistic forecasting stratospheric anomalies and
showcases generative AI's potential to deepen the physical understanding of
atmosphere-climate dynamics.

</details>


### [134] [Adaptive Context Length Optimization with Low-Frequency Truncation for Multi-Agent Reinforcement Learning](https://arxiv.org/abs/2510.26389)
*Wenchang Duan,Yaoliang Yu,Jiwan He,Yi Shi*

Main category: cs.LG

TL;DR: A novel MARL framework with adaptive context length optimization via temporal gradient analysis and Fourier-based low-frequency truncation for efficient exploration and redundant information filtering.


<details>
  <summary>Details</summary>
Motivation: Large fixed context lengths in deep MARL lead to limited exploration efficiency and redundant information, hindering performance in complex environments.

Method: Central agent dynamically optimizes context length using temporal gradient analysis, with Fourier-based low-frequency truncation for efficient input representation that extracts global temporal trends.

Result: Achieves state-of-the-art performance on long-term dependency tasks including PettingZoo, MiniGrid, Google Research Football, and StarCraft Multi-Agent Challenge v2.

Conclusion: The proposed adaptive context length optimization framework effectively enhances exploration and convergence in MARL while filtering redundant information through efficient temporal representation.

Abstract: Recently, deep multi-agent reinforcement learning (MARL) has demonstrated
promising performance for solving challenging tasks, such as long-term
dependencies and non-Markovian environments. Its success is partly attributed
to conditioning policies on large fixed context length. However, such large
fixed context lengths may lead to limited exploration efficiency and redundant
information. In this paper, we propose a novel MARL framework to obtain
adaptive and effective contextual information. Specifically, we design a
central agent that dynamically optimizes context length via temporal gradient
analysis, enhancing exploration to facilitate convergence to global optima in
MARL. Furthermore, to enhance the adaptive optimization capability of the
context length, we present an efficient input representation for the central
agent, which effectively filters redundant information. By leveraging a
Fourier-based low-frequency truncation method, we extract global temporal
trends across decentralized agents, providing an effective and efficient
representation of the MARL environment. Extensive experiments demonstrate that
the proposed method achieves state-of-the-art (SOTA) performance on long-term
dependency tasks, including PettingZoo, MiniGrid, Google Research Football
(GRF), and StarCraft Multi-Agent Challenge v2 (SMACv2).

</details>


### [135] [Multi-Task Learning Based on Support Vector Machines and Twin Support Vector Machines: A Comprehensive Survey](https://arxiv.org/abs/2510.26392)
*Fatemeh Bazikar,Hossein Moosaei,Atefeh Hemmati,Panos M. Pardalos*

Main category: cs.LG

TL;DR: Survey of multi-task learning approaches using Support Vector Machines (SVMs) and Twin SVMs (TWSVMs), comparing their theoretical properties, optimization strategies, and applications across various domains.


<details>
  <summary>Details</summary>
Motivation: To provide a comprehensive overview of SVM and TWSVM-based multi-task learning methods, highlighting their advantages in interpretability, theoretical rigor, and effectiveness with small datasets compared to deep learning approaches.

Method: Survey and analysis of existing MTL approaches based on SVM and TWSVM, focusing on shared representations, task regularization, and structural coupling strategies. Special attention given to emerging TWSVM extensions for multi-task settings.

Result: Comparison of models in terms of theoretical properties, optimization strategies, and empirical performance across applications in computer vision, natural language processing, and bioinformatics.

Conclusion: Identifies research gaps and outlines future directions for developing scalable, interpretable, and reliable margin-based MTL frameworks using SVM and TWSVM approaches.

Abstract: Multi-task learning (MTL) enables simultaneous training across related tasks,
leveraging shared information to improve generalization, efficiency, and
robustness, especially in data-scarce or high-dimensional scenarios. While deep
learning dominates recent MTL research, Support Vector Machines (SVMs) and Twin
SVMs (TWSVMs) remain relevant due to their interpretability, theoretical rigor,
and effectiveness with small datasets.
  This chapter surveys MTL approaches based on SVM and TWSVM, highlighting
shared representations, task regularization, and structural coupling
strategies. Special attention is given to emerging TWSVM extensions for
multi-task settings, which show promise but remain underexplored. We compare
these models in terms of theoretical properties, optimization strategies, and
empirical performance, and discuss applications in fields such as computer
vision, natural language processing, and bioinformatics.
  Finally, we identify research gaps and outline future directions for building
scalable, interpretable, and reliable margin-based MTL frameworks. This work
provides a comprehensive resource for researchers and practitioners interested
in SVM- and TWSVM-based multi-task learning.

</details>


### [136] [Co-Evolving Latent Action World Models](https://arxiv.org/abs/2510.26433)
*Yucen Wang,Fengming Zhang,De-Chuan Zhan,Li Zhao,Kaixin Wang,Jiang Bian*

Main category: cs.LG

TL;DR: CoLA-World enables joint training of latent action models and pre-trained world models through a warm-up phase, overcoming representational collapse and achieving better performance than two-stage approaches.


<details>
  <summary>Details</summary>
Motivation: Current two-stage approaches for adapting video generation models into controllable world models have redundant training and limited co-adaptation potential between latent action models and world models.

Method: Proposes CoLA-World with a critical warm-up phase that aligns representations between from-scratch LAM and pre-trained world model, enabling joint training and co-evolution cycle.

Result: Matches or outperforms prior two-stage methods in video simulation quality and downstream visual planning tasks.

Conclusion: Establishes a robust and efficient new paradigm for creating generalist world models through synergistic joint training.

Abstract: Adapting pre-trained video generation models into controllable world models
via latent actions is a promising step towards creating generalist world
models. The dominant paradigm adopts a two-stage approach that trains latent
action model (LAM) and the world model separately, resulting in redundant
training and limiting their potential for co-adaptation. A conceptually simple
and appealing idea is to directly replace the forward dynamic model in LAM with
a powerful world model and training them jointly, but it is non-trivial and
prone to representational collapse. In this work, we propose CoLA-World, which
for the first time successfully realizes this synergistic paradigm, resolving
the core challenge in joint learning through a critical warm-up phase that
effectively aligns the representations of the from-scratch LAM with the
pre-trained world model. This unlocks a co-evolution cycle: the world model
acts as a knowledgeable tutor, providing gradients to shape a high-quality LAM,
while the LAM offers a more precise and adaptable control interface to the
world model. Empirically, CoLA-World matches or outperforms prior two-stage
methods in both video simulation quality and downstream visual planning,
establishing a robust and efficient new paradigm for the field.

</details>


### [137] [Personalized Treatment Outcome Prediction from Scarce Data via Dual-Channel Knowledge Distillation and Adaptive Fusion](https://arxiv.org/abs/2510.26444)
*Wenjie Chen,Li Zhuang,Ziying Luo,Yu Liu,Jiahao Wu,Shengcai Liu*

Main category: cs.LG

TL;DR: CFKD-AFN uses low-fidelity simulation data to improve treatment outcome prediction for rare patient groups when trial data is scarce, achieving significant accuracy improvements.


<details>
  <summary>Details</summary>
Motivation: Personalized treatment prediction for small-sample and rare patient groups is crucial but limited by costly trial data availability.

Method: Cross-fidelity knowledge distillation with dual-channel distillation module and attention-guided fusion to integrate low-fidelity simulation data with high-fidelity trial data.

Result: 6.67% to 74.55% improvement in prediction accuracy over state-of-the-art methods, with strong robustness to varying high-fidelity dataset sizes.

Conclusion: CFKD-AFN effectively enhances prediction performance for rare patient groups and can be extended to provide interpretable insights for clinical decision-making.

Abstract: Personalized treatment outcome prediction based on trial data for
small-sample and rare patient groups is critical in precision medicine.
However, the costly trial data limit the prediction performance. To address
this issue, we propose a cross-fidelity knowledge distillation and adaptive
fusion network (CFKD-AFN), which leverages abundant but low-fidelity simulation
data to enhance predictions on scarce but high-fidelity trial data. CFKD-AFN
incorporates a dual-channel knowledge distillation module to extract
complementary knowledge from the low-fidelity model, along with an
attention-guided fusion module to dynamically integrate multi-source
information. Experiments on treatment outcome prediction for the chronic
obstructive pulmonary disease demonstrates significant improvements of CFKD-AFN
over state-of-the-art methods in prediction accuracy, ranging from 6.67\% to
74.55\%, and strong robustness to varying high-fidelity dataset sizes.
Furthermore, we extend CFKD-AFN to an interpretable variant, enabling the
exploration of latent medical semantics to support clinical decision-making.

</details>


### [138] [Robust Graph Condensation via Classification Complexity Mitigation](https://arxiv.org/abs/2510.26451)
*Jiayi Luo,Qingyun Sun,Beining Yang,Haonan Yuan,Xingcheng Fu,Yanbiao Ma,Jianxin Li,Philip S. Yu*

Main category: cs.LG

TL;DR: The paper proposes MRGC, a manifold-constrained robust graph condensation framework that addresses the vulnerability of graph condensation to adversarial attacks by preserving classification complexity reduction while ensuring robustness.


<details>
  <summary>Details</summary>
Motivation: Existing graph condensation methods overlook robustness when the original graph is corrupted, leading to significant performance deterioration, while current robust graph learning technologies offer limited effectiveness.

Method: Adopts a geometry perspective of graph data manifold with three learning modules that guide the condensed graph to lie within a smooth, low-dimensional manifold with minimal class ambiguity, preserving classification complexity reduction capability.

Result: Extensive experiments demonstrate the robustness of MRGC across diverse attack scenarios.

Conclusion: The proposed MRGC framework effectively tackles the vulnerability of graph condensation to adversarial perturbations by leveraging manifold constraints to ensure robust performance while maintaining the essential classification complexity reduction property.

Abstract: Graph condensation (GC) has gained significant attention for its ability to
synthesize smaller yet informative graphs. However, existing studies often
overlook the robustness of GC in scenarios where the original graph is
corrupted. In such cases, we observe that the performance of GC deteriorates
significantly, while existing robust graph learning technologies offer only
limited effectiveness. Through both empirical investigation and theoretical
analysis, we reveal that GC is inherently an intrinsic-dimension-reducing
process, synthesizing a condensed graph with lower classification complexity.
Although this property is critical for effective GC performance, it remains
highly vulnerable to adversarial perturbations. To tackle this vulnerability
and improve GC robustness, we adopt the geometry perspective of graph data
manifold and propose a novel Manifold-constrained Robust Graph Condensation
framework named MRGC. Specifically, we introduce three graph data manifold
learning modules that guide the condensed graph to lie within a smooth,
low-dimensional manifold with minimal class ambiguity, thereby preserving the
classification complexity reduction capability of GC and ensuring robust
performance under universal adversarial attacks. Extensive experiments
demonstrate the robustness of \ModelName\ across diverse attack scenarios.

</details>


### [139] [ReSpec: Towards Optimizing Speculative Decoding in Reinforcement Learning Systems](https://arxiv.org/abs/2510.26475)
*Qiaoling Chen,Zijun Liu,Peng Sun,Shenggui Li,Guoteng Wang,Ziming Liu,Yonggang Wen,Siyuan Feng,Tianwei Zhang*

Main category: cs.LG

TL;DR: ReSpec accelerates RL training of LLMs by adapting speculative decoding with dynamic configuration tuning, drafter evolution via distillation, and reward-weighted updates, achieving up to 4.5x speedup while maintaining training stability.


<details>
  <summary>Details</summary>
Motivation: RL training of LLMs is bottlenecked by generation time (over 75% of training), and while speculative decoding helps in serving, it has gaps when applied to RL systems including diminishing speedups, drafter staleness, and policy degradation.

Method: ReSpec adapts speculative decoding to RL through three mechanisms: dynamic SD configuration tuning, evolving drafter via knowledge distillation, and weighting updates by rollout rewards.

Result: On Qwen models (3B-14B), ReSpec achieves up to 4.5x speedup while preserving reward convergence and training stability.

Conclusion: ReSpec provides a practical solution for efficient RL-based LLM adaptation by successfully adapting speculative decoding to overcome RL-specific challenges.

Abstract: Adapting large language models (LLMs) via reinforcement learning (RL) is
often bottlenecked by the generation stage, which can consume over 75\% of the
training time. Speculative decoding (SD) accelerates autoregressive generation
in serving systems, but its behavior under RL training remains largely
unexplored. We identify three critical gaps that hinder the naive integration
of SD into RL systems: diminishing speedups at large batch sizes, drafter
staleness under continual actor updates, and drafter-induced policy
degradation.
  To address these gaps, we present ReSpec, a system that adapts SD to RL
through three complementary mechanisms: dynamically tuning SD configurations,
evolving the drafter via knowledge distillation, and weighting updates by
rollout rewards. On Qwen models (3B--14B), ReSpec achieves up to 4.5x speedup
while preserving reward convergence and training stability, providing a
practical solution for efficient RL-based LLM adaptation.

</details>


### [140] [Quantum Gated Recurrent GAN with Gaussian Uncertainty for Network Anomaly Detection](https://arxiv.org/abs/2510.26487)
*Wajdi Hammami,Soumaya Cherkaoui,Jean-Frederic Laprade,Ola Ahmad,Shengrui Wang*

Main category: cs.LG

TL;DR: A quantum-enhanced GAN using Quantum Gated Recurrent Unit (QGRU) with Successive Data Injection and multi-metric gating for network anomaly detection, achieving 89.43% TaF1 score and demonstrating practical feasibility on IBM quantum hardware.


<details>
  <summary>Details</summary>
Motivation: Address the challenge of anomaly detection in time-series data for network security, overcoming limitations of existing quantum machine learning approaches constrained by limited qubit counts.

Method: QGRU-based GAN with Successive Data Injection and multi-metric gating strategy; quantum-enhanced generator outputs Gaussian distribution parameters via reparameterization; uses Wasserstein critic for stable training; anomaly detection through gating mechanism combining Gaussian uncertainty estimates, critic scores, and reconstruction errors.

Result: Achieved high time-series aware F1 score (TaF1) of 89.43%, outperforming existing classical and quantum models; successfully deployed on real IBM Quantum hardware with retained high performance.

Conclusion: The proposed QGRU-WGAN demonstrates superior anomaly detection capability, robustness, and practical feasibility on current NISQ devices, confirming its effectiveness for network security applications.

Abstract: Anomaly detection in time-series data is a critical challenge with
significant implications for network security. Recent quantum machine learning
approaches, such as quantum kernel methods and variational quantum circuits,
have shown promise in capturing complex data distributions for anomaly
detection but remain constrained by limited qubit counts. We introduce in this
work a novel Quantum Gated Recurrent Unit (QGRU)-based Generative Adversarial
Network (GAN) employing Successive Data Injection (SuDaI) and a multi-metric
gating strategy for robust network anomaly detection. Our model uniquely
utilizes a quantum-enhanced generator that outputs parameters (mean and
log-variance) of a Gaussian distribution via reparameterization, combined with
a Wasserstein critic to stabilize adversarial training. Anomalies are
identified through a novel gating mechanism that initially flags potential
anomalies based on Gaussian uncertainty estimates and subsequently verifies
them using a composite of critic scores and reconstruction errors. Evaluated on
benchmark datasets, our method achieves a high time-series aware F1 score
(TaF1) of 89.43% demonstrating superior capability in detecting anomalies
accurately and promptly as compared to existing classical and quantum models.
Furthermore, the trained QGRU-WGAN was deployed on real IBM Quantum hardware,
where it retained high anomaly detection performance, confirming its robustness
and practical feasibility on current noisy intermediate-scale quantum (NISQ)
devices.

</details>


### [141] [Data-Efficient RLVR via Off-Policy Influence Guidance](https://arxiv.org/abs/2510.26491)
*Erle Zhu,Dazhi Jiang,Yuan Wang,Xujun Li,Jiale Cheng,Yuxian Gu,Yilin Niu,Aohan Zeng,Jie Tang,Minlie Huang,Hongning Wang*

Main category: cs.LG

TL;DR: CROPI is a multi-stage RL framework that uses influence functions and off-policy estimation to efficiently select the most influential data for training LLMs, achieving 2.66x acceleration with only 10% data per stage.


<details>
  <summary>Details</summary>
Motivation: Current data selection methods for RLVR are heuristic-based and lack theoretical guarantees, limiting their effectiveness and generalizability for enhancing LLM reasoning capabilities.

Method: Proposes CROPI framework using influence functions to estimate data contributions, off-policy influence estimation with pre-collected trajectories to avoid expensive rollouts, and sparse random projection for gradient dimensionality reduction.

Result: Experiments on models up to 7B parameters show CROPI significantly accelerates training - achieving 2.66x step-level acceleration on a 1.5B model while using only 10% of data per stage compared to full-dataset training.

Conclusion: Influence-based data selection shows substantial potential for efficient RLVR, providing a theoretically-grounded alternative to heuristic methods.

Abstract: Data selection is a critical aspect of Reinforcement Learning with Verifiable
Rewards (RLVR) for enhancing the reasoning capabilities of large language
models (LLMs). Current data selection methods are largely heuristic-based,
lacking theoretical guarantees and generalizability. This work proposes a
theoretically-grounded approach using influence functions to estimate the
contribution of each data point to the learning objective. To overcome the
prohibitive computational cost of policy rollouts required for online influence
estimation, we introduce an off-policy influence estimation method that
efficiently approximates data influence using pre-collected offline
trajectories. Furthermore, to manage the high-dimensional gradients of LLMs, we
employ sparse random projection to reduce dimensionality and improve storage
and computation efficiency. Leveraging these techniques, we develop
\textbf{C}urriculum \textbf{R}L with \textbf{O}ff-\textbf{P}olicy
\text{I}nfluence guidance (\textbf{CROPI}), a multi-stage RL framework that
iteratively selects the most influential data for the current policy.
Experiments on models up to 7B parameters demonstrate that CROPI significantly
accelerates training. On a 1.5B model, it achieves a 2.66x step-level
acceleration while using only 10\% of the data per stage compared to
full-dataset training. Our results highlight the substantial potential of
influence-based data selection for efficient RLVR.

</details>


### [142] [Enhancing ECG Classification Robustness with Lightweight Unsupervised Anomaly Detection Filters](https://arxiv.org/abs/2510.26501)
*Mustafa Fuad Rifet Ibrahim,Maurice Meijer,Alexander Schlaefer,Peer Stelldinger*

Main category: cs.LG

TL;DR: This paper proposes using optimized Unsupervised Anomaly Detection (UAD) filters as upstream safeguards for ECG analysis in wearables, demonstrating improved robustness against Out-of-Distribution data and noise.


<details>
  <summary>Details</summary>
Motivation: Deep learning models for ECG analysis face reliability issues with OOD data (unseen pathologies, noise) in resource-constrained wearable environments, leading to erroneous high-confidence predictions that compromise patient safety.

Method: Benchmarked six UAD approaches (Deep SVDD, reconstruction models, Masked Anomaly Detection, normalizing flows, diffusion models) optimized via Neural Architecture Search under strict resource constraints (≤512k parameters), evaluated on PTB-XL and BUT QDB datasets.

Result: Deep SVDD consistently achieved the best trade-off between detection and efficiency. Integration with diagnostic classifier improved accuracy by up to 21 percentage points over classifier-only baseline in deployment simulation.

Conclusion: Optimized UAD filters can safeguard automated ECG analysis, enabling safer and more reliable continuous cardiovascular monitoring on wearables by effectively detecting OOD data and noise-corrupted signals.

Abstract: Continuous electrocardiogram (ECG) monitoring via wearables offers
significant potential for early cardiovascular disease (CVD) detection.
However, deploying deep learning models for automated analysis in
resource-constrained environments faces reliability challenges due to
inevitable Out-of-Distribution (OOD) data. OOD inputs, such as unseen
pathologies or noisecorrupted signals, often cause erroneous, high-confidence
predictions by standard classifiers, compromising patient safety. Existing OOD
detection methods either neglect computational constraints or address noise and
unseen classes separately. This paper explores Unsupervised Anomaly Detection
(UAD) as an independent, upstream filtering mechanism to improve robustness. We
benchmark six UAD approaches, including Deep SVDD, reconstruction-based models,
Masked Anomaly Detection, normalizing flows, and diffusion models, optimized
via Neural Architecture Search (NAS) under strict resource constraints (at most
512k parameters). Evaluation on PTB-XL and BUT QDB datasets assessed detection
of OOD CVD classes and signals unsuitable for analysis due to noise. Results
show Deep SVDD consistently achieves the best trade-off between detection and
efficiency. In a realistic deployment simulation, integrating the optimized
Deep SVDD filter with a diagnostic classifier improved accuracy by up to 21
percentage points over a classifier-only baseline. This study demonstrates that
optimized UAD filters can safeguard automated ECG analysis, enabling safer,
more reliable continuous cardiovascular monitoring on wearables.

</details>


### [143] [LLMs as In-Context Meta-Learners for Model and Hyperparameter Selection](https://arxiv.org/abs/2510.26510)
*Youssef Attia El Hili,Albert Thomas,Malik Tiomoko,Abdelhakim Benechehab,Corentin Léger,Corinne Ancourt,Balázs Kégl*

Main category: cs.LG

TL;DR: LLMs can act as in-context meta-learners for model and hyperparameter selection by converting datasets into metadata and using zero-shot or meta-informed prompting strategies, achieving competitive performance without expensive search.


<details>
  <summary>Details</summary>
Motivation: Model and hyperparameter selection are critical but challenging in machine learning, typically requiring expert intuition or expensive automated search. The paper investigates whether LLMs can serve as lightweight, general-purpose assistants for this task.

Method: Two prompting strategies: (1) zero-shot mode relying solely on pretrained knowledge, and (2) meta-informed mode augmented with examples of models and their performance on past tasks. Datasets are converted into interpretable metadata for LLM analysis.

Result: Across synthetic and real-world benchmarks, LLMs can exploit dataset metadata to recommend competitive models and hyperparameters without search. Meta-informed prompting shows improvements, demonstrating LLMs' capacity for in-context meta-learning.

Conclusion: LLMs show promise as lightweight, general-purpose assistants for model selection and hyperparameter optimization, highlighting a new role for them in the machine learning workflow.

Abstract: Model and hyperparameter selection are critical but challenging in machine
learning, typically requiring expert intuition or expensive automated search.
We investigate whether large language models (LLMs) can act as in-context
meta-learners for this task. By converting each dataset into interpretable
metadata, we prompt an LLM to recommend both model families and
hyperparameters. We study two prompting strategies: (1) a zero-shot mode
relying solely on pretrained knowledge, and (2) a meta-informed mode augmented
with examples of models and their performance on past tasks. Across synthetic
and real-world benchmarks, we show that LLMs can exploit dataset metadata to
recommend competitive models and hyperparameters without search, and that
improvements from meta-informed prompting demonstrate their capacity for
in-context meta-learning. These results highlight a promising new role for LLMs
as lightweight, general-purpose assistants for model selection and
hyperparameter optimization.

</details>


### [144] [Think Outside the Policy: In-Context Steered Policy Optimization](https://arxiv.org/abs/2510.26519)
*Hsiu-Yuan Huang,Chenming Tang,Weijie Liu,Saiyong Yang,Yunfang Wu*

Main category: cs.LG

TL;DR: ICPO is a new RLVR framework that uses in-context learning to provide expert guidance without needing advanced model trajectories, improving exploration and training stability for large reasoning models.


<details>
  <summary>Details</summary>
Motivation: Existing RLVR methods like GRPO have limited exploration due to on-policy rollouts confined to current policy distributions, while approaches using expert models increase computational costs and require inaccessible advanced models.

Method: ICPO introduces Mixed-Policy GRPO with Implicit Expert Forcing to expand exploration, Expert Region Reject Sampling to filter unreliable trajectories, and Annealed Expert-Bonus Reward Shaping to balance expert guidance with autonomous improvement.

Result: ICPO consistently enhances reinforcement learning performance and training stability on mathematical reasoning benchmarks.

Conclusion: ICPO reveals a scalable and effective RLVR paradigm for large reasoning models that doesn't rely on advanced model trajectories.

Abstract: Existing Reinforcement Learning from Verifiable Rewards (RLVR) methods, such
as Group Relative Policy Optimization (GRPO), have achieved remarkable progress
in improving the reasoning capabilities of Large Reasoning Models (LRMs).
However, they exhibit limited exploration due to reliance on on-policy rollouts
where confined to the current policy's distribution, resulting in narrow
trajectory diversity. Recent approaches attempt to expand policy coverage by
incorporating trajectories generated from stronger expert models, yet this
reliance increases computational cost and such advaned models are often
inaccessible. To address these issues, we propose In-Context Steered Policy
Optimization (ICPO), a unified framework that leverages the inherent in-context
learning capability of LRMs to provide expert guidance using existing datasets.
ICPO introduces Mixed-Policy GRPO with Implicit Expert Forcing, which expands
exploration beyond the current policy distribution without requiring advanced
LRM trajectories. To further stabilize optimization, ICPO integrates Expert
Region Reject Sampling to filter unreliable off-policy trajectories and
Annealed Expert-Bonus Reward Shaping to balance early expert guidance with
later autonomous improvement. Results demonstrate that ICPO consistently
enhances reinforcement learning performance and training stability on
mathematical reasoning benchmarks, revealing a scalable and effective RLVR
paradigm for LRMs.

</details>


### [145] [Polybasic Speculative Decoding Through a Theoretical Perspective](https://arxiv.org/abs/2510.26527)
*Ruilin Wang,Huixia Li,Yuexiao Ma,Xiawu Zheng,Fei Chao,Xuefeng Xiao,Rongrong Ji*

Main category: cs.LG

TL;DR: Polybasic speculative decoding framework accelerates LLM inference with theoretical grounding, achieving 3.31-4.43x speedup while preserving output distribution.


<details>
  <summary>Details</summary>
Motivation: Inference latency is a critical bottleneck in LLM deployment, and existing speculative decoding methods lack rigorous theoretical analysis and are limited to dualistic draft-verify frameworks.

Method: Introduces a polybasic speculative decoding framework with comprehensive theoretical analysis, proving a fundamental theorem characterizing optimal inference time for multi-model systems, and optimizing the interplay between model capabilities, acceptance lengths, and computational cost.

Result: Achieved speedup ratios of 3.31-4.01x for LLaMA2-Chat 7B, up to 3.87x for LLaMA3-8B, up to 4.43x for Vicuna-7B, and up to 3.85x for Qwen2-7B while preserving original output distribution.

Conclusion: The polybasic framework provides theoretical grounding for speculative decoding, extends beyond dualistic approaches, and enables significant inference acceleration across multiple model families without compromising output quality.

Abstract: Inference latency stands as a critical bottleneck in the large-scale
deployment of Large Language Models (LLMs). Speculative decoding methods have
recently shown promise in accelerating inference without compromising the
output distribution. However, existing work typically relies on a dualistic
draft-verify framework and lacks rigorous theoretical grounding. In this paper,
we introduce a novel \emph{polybasic} speculative decoding framework,
underpinned by a comprehensive theoretical analysis. Specifically, we prove a
fundamental theorem that characterizes the optimal inference time for
multi-model speculative decoding systems, shedding light on how to extend
beyond the dualistic approach to a more general polybasic paradigm. Through our
theoretical investigation of multi-model token generation, we expose and
optimize the interplay between model capabilities, acceptance lengths, and
overall computational cost. Our framework supports both standalone
implementation and integration with existing speculative techniques, leading to
accelerated performance in practice. Experimental results across multiple model
families demonstrate that our approach yields speedup ratios ranging from
$3.31\times$ to $4.01\times$ for LLaMA2-Chat 7B, up to $3.87 \times$ for
LLaMA3-8B, up to $4.43 \times$ for Vicuna-7B and up to $3.85 \times$ for
Qwen2-7B -- all while preserving the original output distribution. We release
our theoretical proofs and implementation code to facilitate further
investigation into polybasic speculative decoding.

</details>


### [146] [Higher-Order Regularization Learning on Hypergraphs](https://arxiv.org/abs/2510.26533)
*Adrien Weihs,Andrea Bertozzi,Matthew Thorpe*

Main category: cs.LG

TL;DR: Higher-Order Hypergraph Learning (HOHL) is extended with theoretical guarantees for truncated versions and convergence rates, showing strong empirical performance in active learning and non-geometric datasets.


<details>
  <summary>Details</summary>
Motivation: To extend the theoretical foundation of HOHL beyond prior asymptotic consistency analysis, providing explicit convergence rates and demonstrating versatility across diverse learning settings.

Method: Proves consistency of truncated HOHL, derives explicit convergence rates when used as regularizer in supervised learning, and evaluates empirical performance in active learning and non-geometric datasets.

Result: Established theoretical guarantees for truncated HOHL with explicit convergence rates, demonstrated strong empirical performance in active learning and datasets without underlying geometric structure.

Conclusion: HOHL shows versatility and robustness across diverse learning settings with solid theoretical foundations and strong empirical performance.

Abstract: Higher-Order Hypergraph Learning (HOHL) was recently introduced as a
principled alternative to classical hypergraph regularization, enforcing
higher-order smoothness via powers of multiscale Laplacians induced by the
hypergraph structure. Prior work established the well- and ill-posedness of
HOHL through an asymptotic consistency analysis in geometric settings. We
extend this theoretical foundation by proving the consistency of a truncated
version of HOHL and deriving explicit convergence rates when HOHL is used as a
regularizer in fully supervised learning. We further demonstrate its strong
empirical performance in active learning and in datasets lacking an underlying
geometric structure, highlighting HOHL's versatility and robustness across
diverse learning settings.

</details>


### [147] [A Three-Stage Bayesian Transfer Learning Framework to Improve Predictions in Data-Scarce Domains](https://arxiv.org/abs/2510.26541)
*Aidan Furlong,Robert Salko,Xingang Zhao,Xu Wu*

Main category: cs.LG

TL;DR: This paper introduces staged B-DANN, a three-stage framework combining parameter transfer and domain adaptation to improve deep learning performance when transferring knowledge from data-rich source domains to data-scarce target domains, with applications in nuclear engineering.


<details>
  <summary>Details</summary>
Motivation: Deep neural networks require large datasets but experimental data are often sparse and noisy. Transfer learning helps but parameter transfer degrades under large domain shifts, and existing domain-adversarial methods lack stability and uncertainty quantification.

Method: Three-stage framework: 1) Train deterministic feature extractor on source domain, 2) Adversarially refine using DANN, 3) Build Bayesian neural network on adapted feature extractor for target domain fine-tuning with uncertainty quantification.

Result: Staged B-DANN significantly outperformed standard transfer techniques on synthetic benchmark and improved predictive accuracy and generalization for critical heat flux prediction in rectangular channels using tube experiment data as source domain.

Conclusion: The staged B-DANN method can improve predictive accuracy and generalization, potentially assisting other domains in nuclear engineering by providing better transfer learning with uncertainty quantification.

Abstract: The use of ML in engineering has grown steadily to support a wide array of
applications. Among these methods, deep neural networks have been widely
adopted due to their performance and accessibility, but they require large,
high-quality datasets. Experimental data are often sparse, noisy, or
insufficient to build resilient data-driven models. Transfer learning, which
leverages relevant data-abundant source domains to assist learning in
data-scarce target domains, has shown efficacy. Parameter transfer, where
pretrained weights are reused, is common but degrades under large domain
shifts. Domain-adversarial neural networks (DANNs) help address this issue by
learning domain-invariant representations, thereby improving transfer under
greater domain shifts in a semi-supervised setting. However, DANNs can be
unstable during training and lack a native means for uncertainty
quantification. This study introduces a fully-supervised three-stage framework,
the staged Bayesian domain-adversarial neural network (staged B-DANN), that
combines parameter transfer and shared latent space adaptation. In Stage 1, a
deterministic feature extractor is trained on the source domain. This feature
extractor is then adversarially refined using a DANN in Stage 2. In Stage 3, a
Bayesian neural network is built on the adapted feature extractor for
fine-tuning on the target domain to handle conditional shifts and yield
calibrated uncertainty estimates. This staged B-DANN approach was first
validated on a synthetic benchmark, where it was shown to significantly
outperform standard transfer techniques. It was then applied to the task of
predicting critical heat flux in rectangular channels, leveraging data from
tube experiments as the source domain. The results of this study show that the
staged B-DANN method can improve predictive accuracy and generalization,
potentially assisting other domains in nuclear engineering.

</details>


### [148] [Boosted Trees on a Diet: Compact Models for Resource-Constrained Devices](https://arxiv.org/abs/2510.26557)
*Jan Stenkamp,Nina Herrmann,Benjamin Karic,Stefan Oehmcke,Fabian Gieseke*

Main category: cs.LG

TL;DR: A compression scheme for boosted decision trees that reduces memory footprint by 4-16x while maintaining performance, enabling deployment on compute-constrained IoT devices.


<details>
  <summary>Details</summary>
Motivation: The growing need for lightweight machine learning models on compute-constrained IoT devices that can operate autonomously without constant communication or external power supply.

Method: Techniques for training compact boosted decision tree ensembles that reward feature and threshold reuse during training, using an adapted training process and alternative memory layout.

Result: Models achieved the same performance as LightGBM models with 4-16x compression ratio, enabling autonomous operation on IoT devices with minimal computing power and energy.

Conclusion: The compression scheme enables a wide range of IoT applications including remote monitoring, edge analytics, and real-time decision making in isolated or power-limited environments.

Abstract: Deploying machine learning models on compute-constrained devices has become a
key building block of modern IoT applications. In this work, we present a
compression scheme for boosted decision trees, addressing the growing need for
lightweight machine learning models. Specifically, we provide techniques for
training compact boosted decision tree ensembles that exhibit a reduced memory
footprint by rewarding, among other things, the reuse of features and
thresholds during training. Our experimental evaluation shows that models
achieved the same performance with a compression ratio of 4-16x compared to
LightGBM models using an adapted training process and an alternative memory
layout. Once deployed, the corresponding IoT devices can operate independently
of constant communication or external energy supply, and, thus, autonomously,
requiring only minimal computing power and energy. This capability opens the
door to a wide range of IoT applications, including remote monitoring, edge
analytics, and real-time decision making in isolated or power-limited
environments.

</details>


### [149] [On Measuring Localization of Shortcuts in Deep Networks](https://arxiv.org/abs/2510.26560)
*Nikita Tsoy,Nikola Konstantinov*

Main category: cs.LG

TL;DR: Shortcuts (spurious rules) in deep networks are distributed throughout all layers rather than localized, with shallow layers encoding spurious features and deeper layers forgetting core features.


<details>
  <summary>Details</summary>
Motivation: Shortcuts present a major challenge to deep network reliability, but their impact on feature representations remains understudied, obstructing principled mitigation methods.

Method: Novel experiment design quantifying layer-wise contribution to accuracy degradation through counterfactual training on clean and skewed datasets across multiple architectures (VGG, ResNet, DeiT, ConvNeXt) and datasets (CIFAR-10, Waterbirds, CelebA).

Result: Shortcut learning is distributed throughout networks - shallow layers predominantly encode spurious features while deeper layers predominantly forget core features. Analysis reveals principal axes of variation in localization.

Conclusion: Layer-wise shortcut-mitigation strategies are challenging to design generally, supporting dataset- and architecture-specific approaches instead.

Abstract: Shortcuts, spurious rules that perform well during training but fail to
generalize, present a major challenge to the reliability of deep networks
(Geirhos et al., 2020). However, the impact of shortcuts on feature
representations remains understudied, obstructing the design of principled
shortcut-mitigation methods. To overcome this limitation, we investigate the
layer-wise localization of shortcuts in deep models. Our novel experiment
design quantifies the layer-wise contribution to accuracy degradation caused by
a shortcut-inducing skew by counterfactual training on clean and skewed
datasets. We employ our design to study shortcuts on CIFAR-10, Waterbirds, and
CelebA datasets across VGG, ResNet, DeiT, and ConvNeXt architectures. We find
that shortcut learning is not localized in specific layers but distributed
throughout the network. Different network parts play different roles in this
process: shallow layers predominantly encode spurious features, while deeper
layers predominantly forget core features that are predictive on clean data. We
also analyze the differences in localization and describe its principal axes of
variation. Finally, our analysis of layer-wise shortcut-mitigation strategies
suggests the hardness of designing general methods, supporting dataset- and
architecture-specific approaches instead.

</details>


### [150] [Multiclass Local Calibration With the Jensen-Shannon Distance](https://arxiv.org/abs/2510.26566)
*Cesare Barbera,Lorenzo Perini,Giovanni De Toni,Andrea Passerini,Andrea Pugnana*

Main category: cs.LG

TL;DR: The paper introduces multiclass local calibration to address proximity bias in existing calibration methods by incorporating distance among inputs, especially important for sparse regions in high-stakes applications.


<details>
  <summary>Details</summary>
Motivation: Existing multiclass calibration methods lack distance consideration, making them vulnerable to proximity bias where sparse regions suffer systematic miscalibration, which is critical in high-stakes domains like healthcare.

Method: Proposes a practical method for enhancing local calibration in Neural Networks by enforcing alignment between predicted probabilities and local estimates of class frequencies using Jensen-Shannon distance.

Result: The approach is empirically validated against existing multiclass calibration techniques, showing improved performance in handling proximity bias.

Conclusion: Multiclass local calibration effectively addresses the limitations of existing methods by incorporating local perspectives, making ML models more trustworthy in critical applications.

Abstract: Developing trustworthy Machine Learning (ML) models requires their predicted
probabilities to be well-calibrated, meaning they should reflect true-class
frequencies. Among calibration notions in multiclass classification, strong
calibration is the most stringent, as it requires all predicted probabilities
to be simultaneously calibrated across all classes. However, existing
approaches to multiclass calibration lack a notion of distance among inputs,
which makes them vulnerable to proximity bias: predictions in sparse regions of
the feature space are systematically miscalibrated. This is especially relevant
in high-stakes settings, such as healthcare, where the sparse instances are
exactly those most at risk of biased treatment. In this work, we address this
main shortcoming by introducing a local perspective on multiclass calibration.
First, we formally define multiclass local calibration and establish its
relationship with strong calibration. Second, we theoretically analyze the
pitfalls of existing evaluation metrics when applied to multiclass local
calibration. Third, we propose a practical method for enhancing local
calibration in Neural Networks, which enforces alignment between predicted
probabilities and local estimates of class frequencies using the Jensen-Shannon
distance. Finally, we empirically validate our approach against existing
multiclass calibration techniques.

</details>


### [151] [Wasserstein Regression as a Variational Approximation of Probabilistic Trajectories through the Bernstein Basis](https://arxiv.org/abs/2510.26607)
*Maksim Maslov,Alexander Kugaevskikh,Matthew Ivanov*

Main category: cs.LG

TL;DR: A new method for regression over distributions that combines Bernstein basis parameterization with Wasserstein distance minimization, providing geometric accuracy and interpretability.


<details>
  <summary>Details</summary>
Motivation: Existing approaches for regression over distributions often ignore probability space geometry or are computationally expensive, limiting their practical application.

Method: Models conditional distributions as smooth probability trajectories using Bernstein polynomials to parameterize Gaussian components, with optimization via autodiff and Wasserstein distance minimization.

Result: Competitive performance on synthetic datasets with complex trajectories, showing better approximation quality in nonlinear cases, smooth trajectories, robustness, and high interpretability.

Conclusion: The method offers a balanced solution combining geometric accuracy, computational practicality, and interpretability, with potential extensions to non-Gaussian distributions and high-dimensional data.

Abstract: This paper considers the problem of regression over distributions, which is
becoming increasingly important in machine learning. Existing approaches often
ignore the geometry of the probability space or are computationally expensive.
To overcome these limitations, a new method is proposed that combines the
parameterization of probability trajectories using a Bernstein basis and the
minimization of the Wasserstein distance between distributions. The key idea is
to model a conditional distribution as a smooth probability trajectory defined
by a weighted sum of Gaussian components whose parameters -- the mean and
covariance -- are functions of the input variable constructed using Bernstein
polynomials. The loss function is the averaged squared Wasserstein distance
between the predicted Gaussian distributions and the empirical data, which
takes into account the geometry of the distributions. An autodiff-based
optimization method is used to train the model. Experiments on synthetic
datasets that include complex trajectories demonstrated that the proposed
method provides competitive approximation quality in terms of the Wasserstein
distance, Energy Distance, and RMSE metrics, especially in cases of pronounced
nonlinearity. The model demonstrates trajectory smoothness that is better than
or comparable to alternatives and robustness to changes in data structure,
while maintaining high interpretability due to explicit parameterization via
control points. The developed approach represents a balanced solution that
combines geometric accuracy, computational practicality, and interpretability.
Prospects for further research include extending the method to non-Gaussian
distributions, applying entropy regularization to speed up computations, and
adapting the approach to working with high-dimensional data for approximating
surfaces and more complex structures.

</details>


### [152] [Clone Deterministic 3D Worlds with Geometrically-Regularized World Models](https://arxiv.org/abs/2510.26782)
*Zaishuo Xia,Yukuan Lu,Xinyi Li,Yifan Xu,Yubei Chen*

Main category: cs.LG

TL;DR: GRWM improves world models by enforcing geometric regularization in latent space, leading to more stable long-horizon predictions without enlarging dynamics modules.


<details>
  <summary>Details</summary>
Motivation: Current world models degrade over long horizons due to poor representation quality from lossy or entangled latent representations, making dynamics learning unnecessarily difficult.

Method: Proposes Geometrically-Regularized World Models (GRWM) that enforce consecutive points along sensory trajectories to remain close in latent space, creating better-aligned representations with the environment's true topology.

Result: GRWM significantly improves rollout fidelity and stability across deterministic 3D settings and long-horizon prediction tasks, learning latent manifolds with superior geometric structure.

Conclusion: Improving representation learning through geometric regularization is a direct and effective path to building robust world models capable of reliable long-horizon predictions.

Abstract: A world model is an internal model that simulates how the world evolves.
Given past observations and actions, it predicts the future of both the
embodied agent and its environment. Accurate world models are essential for
enabling agents to think, plan, and reason effectively in complex, dynamic
settings. Despite rapid progress, current world models remain brittle and
degrade over long horizons. We argue that a central cause is representation
quality: exteroceptive inputs (e.g., images) are high-dimensional, and lossy or
entangled latents make dynamics learning unnecessarily hard. We therefore ask
whether improving representation learning alone can substantially improve
world-model performance. In this work, we take a step toward building a truly
accurate world model by addressing a fundamental yet open problem: constructing
a model that can fully clone and overfit to a deterministic 3D world. We
propose Geometrically-Regularized World Models (GRWM), which enforces that
consecutive points along a natural sensory trajectory remain close in latent
representation space. This approach yields significantly improved latent
representations that align closely with the true topology of the environment.
GRWM is plug-and-play, requires only minimal architectural modification, scales
with trajectory length, and is compatible with diverse latent generative
backbones. Across deterministic 3D settings and long-horizon prediction tasks,
GRWM significantly increases rollout fidelity and stability. Analyses show that
its benefits stem from learning a latent manifold with superior geometric
structure. These findings support a clear takeaway: improving representation
learning is a direct and useful path to robust world models, delivering
reliable long-horizon predictions without enlarging the dynamics module.

</details>


### [153] [Aeolus: A Multi-structural Flight Delay Dataset](https://arxiv.org/abs/2510.26616)
*Lin Xu,Xinyun Yuan,Yuxuan Liang,Suwan Yin,Yuankai Wu*

Main category: cs.LG

TL;DR: Aeolus is a large-scale multi-modal flight delay dataset that addresses limitations of existing datasets by providing three aligned modalities: tabular data, flight chains for delay propagation modeling, and flight network graphs for relational reasoning.


<details>
  <summary>Details</summary>
Motivation: Existing flight delay datasets are limited to flat tabular structures and fail to capture spatiotemporal dynamics of delay propagation, creating a need for more comprehensive multi-modal datasets.

Method: The dataset provides three modalities: (1) tabular dataset with operational, meteorological, and airport features for 50M+ flights, (2) flight chain module modeling sequential delay propagation, and (3) flight network graph encoding shared resource connections.

Result: Aeolus supports diverse tasks including regression, classification, temporal structure modeling, and graph learning, serving as a unified benchmark across tabular, sequential, and graph modalities.

Conclusion: Aeolus fills a key gap for both domain-specific flight delay modeling and general-purpose structured data research, with released baseline experiments and preprocessing tools to facilitate adoption.

Abstract: We introduce Aeolus, a large-scale Multi-modal Flight Delay Dataset designed
to advance research on flight delay prediction and support the development of
foundation models for tabular data. Existing datasets in this domain are
typically limited to flat tabular structures and fail to capture the
spatiotemporal dynamics inherent in delay propagation. Aeolus addresses this
limitation by providing three aligned modalities: (i) a tabular dataset with
rich operational, meteorological, and airportlevel features for over 50 million
flights; (ii) a flight chain module that models delay propagation along
sequential flight legs, capturing upstream and downstream dependencies; and
(iii) a flight network graph that encodes shared aircraft, crew, and airport
resource connections, enabling cross-flight relational reasoning. The dataset
is carefully constructed with temporal splits, comprehensive features, and
strict leakage prevention to support realistic and reproducible machine
learning evaluation. Aeolus supports a broad range of tasks, including
regression, classification, temporal structure modeling, and graph learning,
serving as a unified benchmark across tabular, sequential, and graph
modalities. We release baseline experiments and preprocessing tools to
facilitate adoption. Aeolus fills a key gap for both domain-specific modeling
and general-purpose structured data research.Our source code and data can be
accessed at https://github.com/Flnny/Delay-data

</details>


### [154] [Omnipresent Yet Overlooked: Heat Kernels in Combinatorial Bayesian Optimization](https://arxiv.org/abs/2510.26633)
*Colin Doumont,Victor Picheny,Viacheslav Borovitskiy,Henry Moss*

Main category: cs.LG

TL;DR: The paper develops a unifying framework using heat kernels to understand relationships between various combinatorial kernels in Bayesian Optimization, showing many successful kernels are related to heat kernels and demonstrating state-of-the-art performance with a simple heat kernel pipeline.


<details>
  <summary>Details</summary>
Motivation: Bayesian Optimization requires specialized kernels for combinatorial domains, but relationships between existing combinatorial kernels are not well understood, creating a gap in theoretical understanding.

Method: Developed a unifying framework based on heat kernels, derived systematically with simple closed-form expressions, and used this framework to analyze relationships between combinatorial kernels.

Result: Proved that many successful combinatorial kernels are related or equivalent to heat kernels, validated experimentally. Heat kernels are not sensitive to optima location unlike some algorithms, and a simple heat kernel pipeline achieved state-of-the-art results.

Conclusion: Heat kernels provide a unifying framework for combinatorial kernels in Bayesian Optimization, offering theoretical insights and practical advantages including robustness to optima location and competitive performance with simpler implementations.

Abstract: Bayesian Optimization (BO) has the potential to solve various combinatorial
tasks, ranging from materials science to neural architecture search. However,
BO requires specialized kernels to effectively model combinatorial domains.
Recent efforts have introduced several combinatorial kernels, but the
relationships among them are not well understood. To bridge this gap, we
develop a unifying framework based on heat kernels, which we derive in a
systematic way and express as simple closed-form expressions. Using this
framework, we prove that many successful combinatorial kernels are either
related or equivalent to heat kernels, and validate this theoretical claim in
our experiments. Moreover, our analysis confirms and extends the results
presented in Bounce: certain algorithms' performance decreases substantially
when the unknown optima of the function do not have a certain structure. In
contrast, heat kernels are not sensitive to the location of the optima. Lastly,
we show that a fast and simple pipeline, relying on heat kernels, is able to
achieve state-of-the-art results, matching or even outperforming certain slow
or complex algorithms.

</details>


### [155] [MSAD: A Deep Dive into Model Selection for Time series Anomaly Detection](https://arxiv.org/abs/2510.26643)
*Emmanouil Sylligardos,John Paparrizos,Themis Palpanas,Pierre Senellart,Paul Boniol*

Main category: cs.LG

TL;DR: Time series classification methods can effectively select the best anomaly detection methods for heterogeneous datasets, outperforming any single anomaly detection method while maintaining similar execution times.


<details>
  <summary>Details</summary>
Motivation: No single best anomaly detection method exists for diverse time series datasets, and existing AutoML solutions don't work for time series anomaly detection, creating a need for effective model selection approaches.

Method: Evaluated 234 model configurations from 16 base classifiers across 1980+ time series, using time series classification for model selection in anomaly detection.

Result: Model selection methods outperformed every single anomaly detection method while maintaining similar execution times.

Conclusion: Time series classification provides an accurate and efficient approach for model selection in anomaly detection, establishing a strong baseline for AutoML pipelines.

Abstract: Anomaly detection is a fundamental task for time series analytics with
important implications for the downstream performance of many applications.
Despite increasing academic interest and the large number of methods proposed
in the literature, recent benchmarks and evaluation studies demonstrated that
no overall best anomaly detection methods exist when applied to very
heterogeneous time series datasets. Therefore, the only scalable and viable
solution to solve anomaly detection over very different time series collected
from diverse domains is to propose a model selection method that will select,
based on time series characteristics, the best anomaly detection methods to
run. Existing AutoML solutions are, unfortunately, not directly applicable to
time series anomaly detection, and no evaluation of time series-based
approaches for model selection exists. Towards that direction, this paper
studies the performance of time series classification methods used as model
selection for anomaly detection. In total, we evaluate 234 model configurations
derived from 16 base classifiers across more than 1980 time series, and we
propose the first extensive experimental evaluation of time series
classification as model selection for anomaly detection. Our results
demonstrate that model selection methods outperform every single anomaly
detection method while being in the same order of magnitude regarding execution
time. This evaluation is the first step to demonstrate the accuracy and
efficiency of time series classification algorithms for anomaly detection, and
represents a strong baseline that can then be used to guide the model selection
step in general AutoML pipelines. Preprint version of an article accepted at
the VLDB Journal.

</details>


### [156] [Curly Flow Matching for Learning Non-gradient Field Dynamics](https://arxiv.org/abs/2510.26645)
*Katarina Petrović,Lazar Atanackovic,Viggo Moro,Kacper Kapuśniak,İsmail İlkan Ceylan,Michael Bronstein,Avishek Joey Bose,Alexander Tong*

Main category: cs.LG

TL;DR: Curly-FM is a novel flow matching approach that learns non-gradient field dynamics by solving a Schrödinger bridge problem with non-zero drift reference processes, enabling modeling of periodic behaviors in systems like cell cycles and fluid dynamics.


<details>
  <summary>Details</summary>
Motivation: Current flow matching methods assume gradient field dynamics based on least action principle, which cannot capture non-gradient periodic behaviors common in real-world systems like cell cycles and ocean currents.

Method: Curly-FM designs and solves a Schrödinger bridge problem using a non-zero drift reference process constructed from inferred velocities and population snapshot data, unlike typical zero-drift approaches.

Result: Curly-FM successfully learns trajectories that better match both reference processes and population marginals in single-cell RNA, computational fluid dynamics, and ocean current applications.

Conclusion: Curly-FM expands flow matching beyond population modeling to capture known periodic behaviors in physical systems, addressing limitations of current state-of-the-art methods.

Abstract: Modeling the transport dynamics of natural processes from population-level
observations is a ubiquitous problem in the natural sciences. Such models rely
on key assumptions about the underlying process in order to enable faithful
learning of governing dynamics that mimic the actual system behavior. The de
facto assumption in current approaches relies on the principle of least action
that results in gradient field dynamics and leads to trajectories minimizing an
energy functional between two probability measures. However, many real-world
systems, such as cell cycles in single-cell RNA, are known to exhibit
non-gradient, periodic behavior, which fundamentally cannot be captured by
current state-of-the-art methods such as flow and bridge matching. In this
paper, we introduce Curly Flow Matching (Curly-FM), a novel approach that is
capable of learning non-gradient field dynamics by designing and solving a
Schr\"odinger bridge problem with a non-zero drift reference process -- in
stark contrast to typical zero-drift reference processes -- which is
constructed using inferred velocities in addition to population snapshot data.
We showcase Curly-FM by solving the trajectory inference problems for single
cells, computational fluid dynamics, and ocean currents with approximate
velocities. We demonstrate that Curly-FM can learn trajectories that better
match both the reference process and population marginals. Curly-FM expands
flow matching models beyond the modeling of populations and towards the
modeling of known periodic behavior in physical systems. Our code repository is
accessible at: https://github.com/kpetrovicc/curly-flow-matching.git

</details>


### [157] [Tight Differentially Private PCA via Matrix Coherence](https://arxiv.org/abs/2510.26679)
*Tommaso d'Orsi,Gleb Novikov*

Main category: cs.LG

TL;DR: A simple SVD-based algorithm provides differentially private rank-r approximations with error depending only on rank-r coherence and spectral gap, outperforming prior methods and matching non-private guarantees in some cases.


<details>
  <summary>Details</summary>
Motivation: To address the problem of computing top singular vectors under differential privacy, resolving a question from Hardt and Roth about achieving better error bounds that depend on coherence rather than worst-case matrix properties.

Method: Uses singular value decomposition with standard perturbation mechanisms, specifically the Gaussian mechanism, to create private rank-r approximations. The approach leverages the observation that coherence doesn't increase under Gaussian perturbations.

Result: The algorithm achieves error bounds that depend only on rank-r coherence and spectral gap, significantly outperforming prior methods. In dense settings, it matches optimal non-private guarantees for single-spike PCA in the Wishart model.

Conclusion: The simple SVD-based approach with Gaussian perturbation effectively preserves coherence and provides strong privacy guarantees, with potential applications to graph problems like Max-Cut under low coherence assumptions.

Abstract: We revisit the task of computing the span of the top $r$ singular vectors
$u_1, \ldots, u_r$ of a matrix under differential privacy. We show that a
simple and efficient algorithm -- based on singular value decomposition and
standard perturbation mechanisms -- returns a private rank-$r$ approximation
whose error depends only on the \emph{rank-$r$ coherence} of $u_1, \ldots, u_r$
and the spectral gap $\sigma_r - \sigma_{r+1}$. This resolves a question posed
by Hardt and Roth~\cite{hardt2013beyond}. Our estimator outperforms the state
of the art -- significantly so in some regimes. In particular, we show that in
the dense setting, it achieves the same guarantees for single-spike PCA in the
Wishart model as those attained by optimal non-private algorithms, whereas
prior private algorithms failed to do so.
  In addition, we prove that (rank-$r$) coherence does not increase under
Gaussian perturbations. This implies that any estimator based on the Gaussian
mechanism -- including ours -- preserves the coherence of the input. We
conjecture that similar behavior holds for other structured models, including
planted problems in graphs.
  We also explore applications of coherence to graph problems. In particular,
we present a differentially private algorithm for Max-Cut and other constraint
satisfaction problems under low coherence assumptions.

</details>


### [158] [LoRAQuant: Mixed-Precision Quantization of LoRA to Ultra-Low Bits](https://arxiv.org/abs/2510.26690)
*Amir Reza Mirzaei,Yuqiao Wen,Yanshuai Cao,Lili Mou*

Main category: cs.LG

TL;DR: LoRAQuant is a mixed-precision post-training quantization method for LoRA adapters that uses SVD reparameterization to concentrate important information, enabling ultra-low bitwidth quantization while maintaining performance.


<details>
  <summary>Details</summary>
Motivation: Multiple LoRA adapters are used simultaneously for LLM customization, but their aggregate cost becomes substantial at scale despite each being lightweight individually.

Method: Reparameterizes each LoRA adapter using singular value decomposition (SVD) to concentrate important information into specific rows and columns, then applies mixed-precision quantization with higher precision for important components and ultra-low bitwidth for the rest.

Result: Experiments with LLaMA 2-7B, LLaMA 2-13B, and Mistral 7B on mathematical reasoning, coding, and summarization tasks show LoRAQuant uses significantly lower bits than other methods while achieving comparable or even higher performance.

Conclusion: LoRAQuant effectively reduces the aggregate cost of multiple LoRA adapters through intelligent mixed-precision quantization while maintaining or improving model performance.

Abstract: Low-Rank Adaptation (LoRA) has become a popular technique for
parameter-efficient fine-tuning of large language models (LLMs). In many
real-world scenarios, multiple adapters are loaded simultaneously to enable LLM
customization for personalized user experiences or to support a diverse range
of tasks. Although each adapter is lightweight in isolation, their aggregate
cost becomes substantial at scale. To address this, we propose LoRAQuant, a
mixed-precision post-training quantization method tailored to LoRA.
Specifically, LoRAQuant reparameterizes each adapter by singular value
decomposition (SVD) to concentrate the most important information into specific
rows and columns. This makes it possible to quantize the important components
to higher precision, while quantizing the rest to ultra-low bitwidth. We
conduct comprehensive experiments with LLaMA 2-7B, LLaMA 2-13B, and Mistral 7B
models on mathematical reasoning, coding, and summarization tasks. Results show
that our LoRAQuant uses significantly lower bits than other quantization
methods, but achieves comparable or even higher performance.

</details>


### [159] [How Regularization Terms Make Invertible Neural Networks Bayesian Point Estimators](https://arxiv.org/abs/2510.26704)
*Nick Heilenkötter*

Main category: cs.LG

TL;DR: The paper shows that specific regularization terms in invertible neural network training can recover Bayesian point estimators like posterior mean and MAP estimator upon network inversion.


<details>
  <summary>Details</summary>
Motivation: To address limitations in existing optimization strategies for invertible neural networks in inverse problems, by connecting network training to classical Bayesian estimators for better stability and interpretability.

Method: Introduce and analyze two regularization terms for invertible neural network training that, when inverted, recover properties of Bayesian point estimators - one connected to posterior mean and another resembling MAP estimator.

Result: Theoretical analysis characterizes how each loss term shapes both the learned forward operator and its inverse reconstruction map. Numerical experiments demonstrate stable and interpretable data-dependence.

Conclusion: Regularization terms in invertible neural network training can effectively recover Bayesian point estimators, providing data-dependent reconstruction in a stable and interpretable manner.

Abstract: Can regularization terms in the training of invertible neural networks lead
to known Bayesian point estimators in reconstruction? Invertible networks are
attractive for inverse problems due to their inherent stability and
interpretability. Recently, optimization strategies for invertible neural
networks that approximate either a reconstruction map or the forward operator
have been studied from a Bayesian perspective, but each has limitations. To
address this, we introduce and analyze two regularization terms for the network
training that, upon inversion of the network, recover properties of classical
Bayesian point estimators: while the first can be connected to the posterior
mean, the second resembles the MAP estimator. Our theoretical analysis
characterizes how each loss shapes both the learned forward operator and its
inverse reconstruction map. Numerical experiments support our findings and
demonstrate how these loss-term regularizers introduce data-dependence in a
stable and interpretable way.

</details>


### [160] [Budgeted Multiple-Expert Deferral](https://arxiv.org/abs/2510.26706)
*Giulia DeSalvo,Clara Mohri,Mehryar Mohri,Yutao Zhong*

Main category: cs.LG

TL;DR: The paper introduces budgeted deferral framework to train deferral algorithms while minimizing expert query costs during training, proposing algorithms that selectively query subsets of experts per training example.


<details>
  <summary>Details</summary>
Motivation: Standard deferral training requires querying all experts for every instance, which becomes prohibitively expensive and undermines the goal of limiting unnecessary expert usage.

Method: Proposed new algorithms for two-stage and single-stage multiple-expert deferral settings that selectively query only a subset of experts per training example, balancing cost and predictive performance.

Result: Theoretical guarantees including generalization bounds and label complexity analyses, plus empirical results showing substantial training cost reduction without sacrificing prediction accuracy across several domains.

Conclusion: Budget-aware deferral algorithms provide practical value by enabling effective deferral training while significantly reducing expert query costs during the training process.

Abstract: Learning to defer uncertain predictions to costly experts offers a powerful
strategy for improving the accuracy and efficiency of machine learning systems.
However, standard training procedures for deferral algorithms typically require
querying all experts for every training instance, an approach that becomes
prohibitively expensive when expert queries incur significant computational or
resource costs. This undermines the core goal of deferral: to limit unnecessary
expert usage. To overcome this challenge, we introduce the budgeted deferral
framework, which aims to train effective deferral algorithms while minimizing
expert query costs during training. We propose new algorithms for both
two-stage and single-stage multiple-expert deferral settings that selectively
query only a subset of experts per training example. While inspired by active
learning, our setting is fundamentally different: labels are already known, and
the core challenge is to decide which experts to query in order to balance cost
and predictive performance. We establish theoretical guarantees for both of our
algorithms, including generalization bounds and label complexity analyses.
Empirical results across several domains show that our algorithms substantially
reduce training costs without sacrificing prediction accuracy, demonstrating
the practical value of our budget-aware deferral algorithms.

</details>


### [161] [An All-Reduce Compatible Top-K Compressor for Communication-Efficient Distributed Learning](https://arxiv.org/abs/2510.26709)
*Chuyan Chen,Chenyang Ma,Zhangxin Li,Yutong He,Yanjie Dong,Kun Yuan*

Main category: cs.LG

TL;DR: ARC-Top-K is a new gradient compressor that combines the benefits of Top-K (preserving important gradient entries) and Rand-K (enabling efficient All-Reduce operations) by aligning sparsity patterns across nodes using gradient sketches.


<details>
  <summary>Details</summary>
Motivation: Existing gradient compressors have limitations: Rand-K loses structural information and performs poorly, while Top-K preserves important entries but requires costly All-Gather operations and lacks contraction properties.

Method: ARC-Top-K uses a lightweight sketch of gradients to align sparsity patterns across nodes, enabling index-free All-Reduce operations while preserving globally significant gradient information. It's combined with momentum error feedback (EF21M).

Result: ARC-Top-K achieves linear speedup and sharper convergence rates than original EF21M, matches Top-K accuracy while reducing wall-clock training time by up to 60.7%.

Conclusion: ARC-Top-K provides an efficient and scalable solution that combines Rand-K's robustness with Top-K's strong performance, offering significant communication efficiency improvements in distributed machine learning.

Abstract: Communication remains a central bottleneck in large-scale distributed machine
learning, and gradient sparsification has emerged as a promising strategy to
alleviate this challenge. However, existing gradient compressors face notable
limitations: Rand-$K$\ discards structural information and performs poorly in
practice, while Top-$K$\ preserves informative entries but loses the
contraction property and requires costly All-Gather operations. In this paper,
we propose ARC-Top-$K$, an {All-Reduce}-Compatible Top-$K$ compressor that
aligns sparsity patterns across nodes using a lightweight sketch of the
gradient, enabling index-free All-Reduce while preserving globally significant
information. ARC-Top-$K$\ is provably contractive and, when combined with
momentum error feedback (EF21M), achieves linear speedup and sharper
convergence rates than the original EF21M under standard assumptions.
Empirically, ARC-Top-$K$\ matches the accuracy of Top-$K$\ while reducing
wall-clock training time by up to 60.7\%, offering an efficient and scalable
solution that combines the robustness of Rand-$K$\ with the strong performance
of Top-$K$.

</details>


### [162] [On the limitation of evaluating machine unlearning using only a single training seed](https://arxiv.org/abs/2510.26714)
*Jamie Lanyon,Axel Finke,Petros Andreou,Georgina Cosma*

Main category: cs.LG

TL;DR: The paper shows that common practices for evaluating machine unlearning algorithms can produce non-representative results due to high sensitivity to random training seeds, and recommends accounting for training seed variability in empirical comparisons.


<details>
  <summary>Details</summary>
Motivation: To address the problem that most machine unlearning algorithms are approximate and their performance must be assessed empirically, but current evaluation practices may not capture true performance variability.

Method: The authors demonstrate that running machine unlearning algorithms multiple times from the same trained model can be misleading, and instead analyze the sensitivity of MU methods to different random number seeds used during model training.

Result: The study found that some machine unlearning methods are highly sensitive to the choice of random training seed, meaning that evaluations using only one training seed can give non-representative performance results.

Conclusion: Empirical comparisons of machine unlearning algorithms should account for variability across different model training seeds to ensure representative performance assessments.

Abstract: Machine unlearning (MU) aims to remove the influence of certain data points
from a trained model without costly retraining. Most practical MU algorithms
are only approximate and their performance can only be assessed empirically.
Care must therefore be taken to make empirical comparisons as representative as
possible. A common practice is to run the MU algorithm multiple times
independently starting from the same trained model. In this work, we
demonstrate that this practice can give highly non-representative results
because -- even for the same architecture and same dataset -- some MU methods
can be highly sensitive to the choice of random number seed used for model
training. We therefore recommend that empirical
comphttps://info.arxiv.org/help/prep#commentsarisons of MU algorithms should
also reflect the variability across different model training seeds.

</details>


### [163] [LSM-MS2: A Foundation Model Bridging Spectral Identification and Biological Interpretation](https://arxiv.org/abs/2510.26715)
*Gabriel Asher,Devesh Shah,Amy A. Caudy,Luke Ferro,Lea Amar,Ana S. H. Costa,Thomas Patton,Niall O'Connor,Jennifer M. Campbell,Jack Geremia*

Main category: cs.LG

TL;DR: LSM-MS2 is a deep learning foundation model that achieves state-of-the-art performance in mass spectrometry spectral identification, particularly for challenging isomeric compounds and complex biological samples.


<details>
  <summary>Details</summary>
Motivation: Most mass spectrometry data remains uncharacterized, leaving biological and chemical information untapped. Machine learning can help address this gap in spectral identification.

Method: Large-scale deep learning foundation model trained on millions of spectra to learn a semantic chemical space.

Result: 30% improvement in accuracy for identifying challenging isomeric compounds, 42% more correct identifications in complex biological samples, and robust performance under low-concentration conditions. Also produces rich spectral embeddings for biological interpretation.

Conclusion: LSM-MS2 enables direct biological interpretation from minimal downstream data and successfully differentiates disease states while predicting clinical outcomes across diverse translational applications.

Abstract: A vast majority of mass spectrometry data remains uncharacterized, leaving
much of its biological and chemical information untapped. Recent advances in
machine learning have begun to address this gap, particularly for tasks such as
spectral identification in tandem mass spectrometry data. Here, we present the
latest generation of LSM-MS2, a large-scale deep learning foundation model
trained on millions of spectra to learn a semantic chemical space. LSM-MS2
achieves state-of-the-art performance in spectral identification, improving on
existing methods by 30% in accuracy of identifying challenging isomeric
compounds, yielding 42% more correct identifications in complex biological
samples, and maintaining robustness under low-concentration conditions.
Furthermore, LSM-MS2 produces rich spectral embeddings that enable direct
biological interpretation from minimal downstream data, successfully
differentiating disease states and predicting clinical outcomes across diverse
translational applications.

</details>


### [164] [On Purely Private Covariance Estimation](https://arxiv.org/abs/2510.26717)
*Tommaso d'Orsi,Gleb Novikov*

Main category: cs.LG

TL;DR: A simple perturbation mechanism for differentially private covariance matrix estimation that achieves optimal error bounds across various norms, particularly excelling in spectral norm and improving results for small datasets.


<details>
  <summary>Details</summary>
Motivation: To develop a differentially private covariance estimator that achieves optimal error guarantees across different matrix norms (Frobenius, p-Schatten, spectral) while handling both large and small dataset scenarios better than existing methods.

Method: A perturbation mechanism for covariance matrices under pure differential privacy, with an additional projection step onto a nuclear norm ball for small datasets to improve error bounds.

Result: For large datasets (n ≥ d²/ε), achieves optimal Frobenius norm error and best known error for all p-Schatten norms. For small datasets (n < d²/ε), achieves optimal Frobenius norm error O(√(d·Tr(Σ)/n)), improving over previous bounds.

Conclusion: The proposed mechanism provides information-theoretically optimal error guarantees for p ≥ 2 norms, making it the first purely private covariance estimator achieving optimal spectral norm error, while also improving bounds for small datasets.

Abstract: We present a simple perturbation mechanism for the release of $d$-dimensional
covariance matrices $\Sigma$ under pure differential privacy. For large
datasets with at least $n\geq d^2/\varepsilon$ elements, our mechanism recovers
the provably optimal Frobenius norm error guarantees of
\cite{nikolov2023private}, while simultaneously achieving best known error for
all other $p$-Schatten norms, with $p\in [1,\infty]$. Our error is
information-theoretically optimal for all $p\ge 2$, in particular, our
mechanism is the first purely private covariance estimator that achieves
optimal error in spectral norm.
  For small datasets $n< d^2/\varepsilon$, we further show that by projecting
the output onto the nuclear norm ball of appropriate radius, our algorithm
achieves the optimal Frobenius norm error $O(\sqrt{d\;\text{Tr}(\Sigma) /n})$,
improving over the known bounds of $O(\sqrt{d/n})$ of \cite{nikolov2023private}
and ${O}\big(d^{3/4}\sqrt{\text{Tr}(\Sigma)/n}\big)$ of
\cite{dong2022differentially}.

</details>


### [165] [Deep sequence models tend to memorize geometrically; it is unclear why](https://arxiv.org/abs/2510.26745)
*Shahriar Noroozizadeh,Vaishnavh Nagarajan,Elan Rosenfeld,Sanjiv Kumar*

Main category: cs.LG

TL;DR: Transformers develop geometric representations of entities that encode global relationships beyond local co-occurrences, enabling complex reasoning through simplified geometric operations rather than brute-force association lookup.


<details>
  <summary>Details</summary>
Motivation: To challenge the prevailing associative view of memory in sequence modeling and demonstrate that Transformers develop geometric representations that encode global relationships between entities, including non-co-occurring ones.

Method: Isolated analyzable instances of Transformer reasoning, analyzed connection to Node2Vec, and examined spectral bias that naturally arises without typical architectural or optimization pressures.

Result: Transformers synthesize their own geometry of atomic facts that simplifies complex reasoning tasks, with this geometry emerging naturally from spectral bias rather than explicit optimization pressures.

Conclusion: The geometric view of parametric memory should replace default associative intuitions, offering new perspectives for knowledge acquisition, capacity, discovery and unlearning in Transformers.

Abstract: In sequence modeling, the parametric memory of atomic facts has been
predominantly abstracted as a brute-force lookup of co-occurrences between
entities. We contrast this associative view against a geometric view of how
memory is stored. We begin by isolating a clean and analyzable instance of
Transformer reasoning that is incompatible with memory as strictly a storage of
the local co-occurrences specified during training. Instead, the model must
have somehow synthesized its own geometry of atomic facts, encoding global
relationships between all entities, including non-co-occurring ones. This in
turn has simplified a hard reasoning task involving an $\ell$-fold composition
into an easy-to-learn 1-step geometric task.
  From this phenomenon, we extract fundamental aspects of neural embedding
geometries that are hard to explain. We argue that the rise of such a geometry,
despite optimizing over mere local associations, cannot be straightforwardly
attributed to typical architectural or optimizational pressures.
Counterintuitively, an elegant geometry is learned even when it is not more
succinct than a brute-force lookup of associations.
  Then, by analyzing a connection to Node2Vec, we demonstrate how the geometry
stems from a spectral bias that -- in contrast to prevailing theories -- indeed
arises naturally despite the lack of various pressures. This analysis also
points to practitioners a visible headroom to make Transformer memory more
strongly geometric. We hope the geometric view of parametric memory encourages
revisiting the default intuitions that guide researchers in areas like
knowledge acquisition, capacity, discovery and unlearning.

</details>


### [166] [STaMP: Sequence Transformation and Mixed Precision for Low-Precision Activation Quantization](https://arxiv.org/abs/2510.26771)
*Marco Federici,Riccardo Del Chiaro,Boris van Breugel,Paul Whatmough,Markus Nagel*

Main category: cs.LG

TL;DR: STaMP quantization applies linear transformations along the sequence dimension and uses mixed precision to maintain model accuracy at lower activation bit-widths.


<details>
  <summary>Details</summary>
Motivation: Quantization reduces inference latency, power and memory footprint but often degrades accuracy sharply when activations are quantized below eight bits.

Method: Proposes STaMP quantization that applies linear transformations along the sequence dimension and keeps a small number of tokens at higher precision while quantizing others at lower bit-widths.

Result: STaMP significantly improves low bit width activation quantization and complements established activation and weight quantization methods.

Conclusion: STaMP is an effective strategy for maintaining model accuracy at lower average activation bit-widths by exploiting sequence-level correlations.

Abstract: Quantization is the key method for reducing inference latency, power and
memory footprint of generative AI models. However, accuracy often degrades
sharply when activations are quantized below eight bits. Recent work suggests
that invertible linear transformations (e.g. rotations) can aid quantization,
by reparameterizing feature channels and weights. In this paper, we propose
\textit{Sequence Transformation and Mixed Precision} (STaMP) quantization, a
novel strategy that applies linear transformations along the \textit{sequence}
dimension to exploit the strong local correlation in language and visual data.
By keeping a small number of tokens in each intermediate activation at higher
precision, we can maintain model accuracy at lower (average) activations
bit-widths. We evaluate STaMP on recent LVM and LLM architectures,
demonstrating that it significantly improves low bit width activation
quantization and complements established activation and weight quantization
methods including recent feature transformations.

</details>


### [167] [Faithful and Fast Influence Function via Advanced Sampling](https://arxiv.org/abs/2510.26776)
*Jungyeon Koh,Hyeonsu Lyu,Jonggyu Jang,Hyun Jong Yang*

Main category: cs.LG

TL;DR: Proposes two advanced sampling techniques (feature-based and logit-based) to improve influence function estimation by selecting representative subsets, reducing computation time by 30.1% and memory usage by 42.2% while improving F1-score by 2.5%.


<details>
  <summary>Details</summary>
Motivation: Influence functions require computing Hessians which is resource-intensive for entire datasets. Random sampling leads to inconsistent estimates due to high variance, necessitating better sampling methods.

Method: Two advanced sampling techniques based on features and logits that select small but representative subsets by considering stochastic distributions of features/logits to enhance IF estimation accuracy.

Result: Method reduces computation time by 30.1%, memory usage by 42.2%, and improves F1-score by 2.5% in class removal experiments compared to baseline.

Conclusion: The proposed sampling techniques provide more accurate and efficient influence function estimation by selecting representative subsets, addressing the computational challenges of traditional methods.

Abstract: How can we explain the influence of training data on black-box models?
Influence functions (IFs) offer a post-hoc solution by utilizing gradients and
Hessians. However, computing the Hessian for an entire dataset is
resource-intensive, necessitating a feasible alternative. A common approach
involves randomly sampling a small subset of the training data, but this method
often results in highly inconsistent IF estimates due to the high variance in
sample configurations. To address this, we propose two advanced sampling
techniques based on features and logits. These samplers select a small yet
representative subset of the entire dataset by considering the stochastic
distribution of features or logits, thereby enhancing the accuracy of IF
estimations. We validate our approach through class removal experiments, a
typical application of IFs, using the F1-score to measure how effectively the
model forgets the removed class while maintaining inference consistency on the
remaining classes. Our method reduces computation time by 30.1% and memory
usage by 42.2%, or improves the F1-score by 2.5% compared to the baseline.

</details>


### [168] [Pre-trained Forecasting Models: Strong Zero-Shot Feature Extractors for Time Series Classification](https://arxiv.org/abs/2510.26777)
*Andreas Auer,Daniel Klotz,Sebastinan Böck,Sepp Hochreiter*

Main category: cs.LG

TL;DR: Frozen pre-trained forecasting models can provide effective representations for time series classification, achieving accuracy comparable to or better than models specifically pre-trained for classification.


<details>
  <summary>Details</summary>
Motivation: To examine whether generalizable representations learned by time series forecasting foundation models can be effectively transferred to classification tasks, challenging the assumption that task-specific pre-training is necessary.

Method: Compared different representation extraction strategies and introduced two model-agnostic embedding augmentations to extract features from frozen pre-trained forecasting models for classification.

Result: Best forecasting models achieved classification accuracy matching or surpassing state-of-the-art models pre-trained specifically for classification, with positive correlation between forecasting and classification performance.

Conclusion: Learning to forecast may provide a powerful route toward constructing general-purpose time series foundation models, as forecasting models can learn representations that are highly transferable to classification tasks.

Abstract: Recent research on time series foundation models has primarily focused on
forecasting, leaving it unclear how generalizable their learned representations
are. In this study, we examine whether frozen pre-trained forecasting models
can provide effective representations for classification. To this end, we
compare different representation extraction strategies and introduce two
model-agnostic embedding augmentations. Our experiments show that the best
forecasting models achieve classification accuracy that matches or even
surpasses that of state-of-the-art models pre-trained specifically for
classification. Moreover, we observe a positive correlation between forecasting
and classification performance. These findings challenge the assumption that
task-specific pre-training is necessary, and suggest that learning to forecast
may provide a powerful route toward constructing general-purpose time series
foundation models.

</details>


### [169] [Remote Labor Index: Measuring AI Automation of Remote Work](https://arxiv.org/abs/2510.26787)
*Mantas Mazeika,Alice Gatti,Cristina Menghini,Udari Madhushani Sehwag,Shivam Singhal,Yury Orlovskiy,Steven Basart,Manasi Sharma,Denis Peskoff,Elaine Lau,Jaehyuk Lim,Lachlan Carroll,Alice Blair,Vinaya Sivakumar,Sumana Basu,Brad Kenstler,Yuntao Ma,Julian Michael,Xiaoke Li,Oliver Ingebretsen,Aditya Mehta,Jean Mottola,John Teichmann,Kevin Yu,Zaina Shaik,Adam Khoja,Richard Ren,Jason Hausenloy,Long Phan,Ye Htet,Ankit Aich,Tahseen Rabbani,Vivswan Shah,Andriy Novykov,Felix Binder,Kirill Chugunov,Luis Ramirez,Matias Geralnik,Hernán Mesura,Dean Lee,Ed-Yeremai Hernandez Cardona,Annette Diamond,Summer Yue,Alexandr Wang,Bing Liu,Ernesto Hernandez,Dan Hendrycks*

Main category: cs.LG

TL;DR: AI agents perform poorly on real-world economic tasks, achieving only 2.5% automation rate on the Remote Labor Index benchmark.


<details>
  <summary>Details</summary>
Motivation: To measure how AI research gains translate into actual economic value and automation in practical settings, rather than just research benchmarks.

Method: Introduced the Remote Labor Index (RLI) - a multi-sector benchmark comprising real-world, economically valuable projects to evaluate end-to-end AI agent performance.

Result: AI agents performed near the floor on RLI, with the highest-performing agent achieving only 2.5% automation rate.

Conclusion: The results provide empirical evidence to ground discussions of AI automation and establish a baseline for tracking AI impacts on labor automation.

Abstract: AIs have made rapid progress on research-oriented benchmarks of knowledge and
reasoning, but it remains unclear how these gains translate into economic value
and automation. To measure this, we introduce the Remote Labor Index (RLI), a
broadly multi-sector benchmark comprising real-world, economically valuable
projects designed to evaluate end-to-end agent performance in practical
settings. AI agents perform near the floor on RLI, with the highest-performing
agent achieving an automation rate of 2.5%. These results help ground
discussions of AI automation in empirical evidence, setting a common basis for
tracking AI impacts and enabling stakeholders to proactively navigate AI-driven
labor automation.

</details>


### [170] [Defeating the Training-Inference Mismatch via FP16](https://arxiv.org/abs/2510.26788)
*Penghui Qi,Zichen Liu,Xiangxin Zhou,Tianyu Pang,Chao Du,Wee Sun Lee,Min Lin*

Main category: cs.LG

TL;DR: Using FP16 instead of BF16 in RL fine-tuning of LLMs eliminates numerical mismatches between training and inference, leading to more stable optimization and better performance.


<details>
  <summary>Details</summary>
Motivation: RL fine-tuning of LLMs suffers from instability due to numerical mismatches between training and inference policies, which prior work has addressed through complex corrections rather than addressing the root cause in floating point precision.

Method: Simply revert from BF16 to FP16 precision, which requires minimal code changes and no modifications to model architecture or learning algorithms.

Result: FP16 yields more stable optimization, faster convergence, and stronger performance across diverse tasks, algorithms and frameworks compared to BF16.

Conclusion: The precision choice (FP16 over BF16) effectively resolves RL fine-tuning instability, suggesting a need to reconsider precision trade-offs in this domain.

Abstract: Reinforcement learning (RL) fine-tuning of large language models (LLMs) often
suffers from instability due to the numerical mismatch between the training and
inference policies. While prior work has attempted to mitigate this issue
through algorithmic corrections or engineering alignments, we show that its
root cause lies in the floating point precision itself. The widely adopted
BF16, despite its large dynamic range, introduces large rounding errors that
breaks the consistency between training and inference. In this work, we
demonstrate that simply reverting to \textbf{FP16} effectively eliminates this
mismatch. The change is simple, fully supported by modern frameworks with only
a few lines of code change, and requires no modification to the model
architecture or learning algorithm. Our results suggest that using FP16
uniformly yields more stable optimization, faster convergence, and stronger
performance across diverse tasks, algorithms and frameworks. We hope these
findings motivate a broader reconsideration of precision trade-offs in RL
fine-tuning.

</details>


### [171] [Learning Pseudorandom Numbers with Transformers: Permuted Congruential Generators, Curricula, and Interpretability](https://arxiv.org/abs/2510.26792)
*Tao Tao,Maissam Barkeshli*

Main category: cs.LG

TL;DR: Transformers can learn to predict sequences from Permuted Congruential Generators (PCGs), even with single-bit outputs, and scale to large moduli up to 2^22, requiring curriculum learning for larger moduli.


<details>
  <summary>Details</summary>
Motivation: To study Transformer models' ability to learn complex pseudo-random number generator sequences, particularly PCGs which are more challenging than linear congruential generators due to additional bit-wise operations.

Method: Train Transformers on PCG sequences with up to 50M parameters and 5B tokens, using curriculum learning for larger moduli, and analyze embedding layers for clustering patterns.

Result: Transformers successfully predict unseen PCG sequences, even with single-bit outputs, and learn multiple PRNGs jointly. Scaling law shows required sequence length grows as √m. Embeddings show bitwise rotationally-invariant clustering.

Conclusion: Transformers can learn complex PRNG patterns beyond classical attacks, require curriculum learning for large moduli, and develop structured representations that transfer across different modulus sizes.

Abstract: We study the ability of Transformer models to learn sequences generated by
Permuted Congruential Generators (PCGs), a widely used family of pseudo-random
number generators (PRNGs). PCGs introduce substantial additional difficulty
over linear congruential generators (LCGs) by applying a series of bit-wise
shifts, XORs, rotations and truncations to the hidden state. We show that
Transformers can nevertheless successfully perform in-context prediction on
unseen sequences from diverse PCG variants, in tasks that are beyond published
classical attacks. In our experiments we scale moduli up to $2^{22}$ using up
to $50$ million model parameters and datasets with up to $5$ billion tokens.
Surprisingly, we find even when the output is truncated to a single bit, it can
be reliably predicted by the model. When multiple distinct PRNGs are presented
together during training, the model can jointly learn them, identifying
structures from different permutations. We demonstrate a scaling law with
modulus $m$: the number of in-context sequence elements required for
near-perfect prediction grows as $\sqrt{m}$. For larger moduli, optimization
enters extended stagnation phases; in our experiments, learning moduli $m \geq
2^{20}$ requires incorporating training data from smaller moduli, demonstrating
a critical necessity for curriculum learning. Finally, we analyze embedding
layers and uncover a novel clustering phenomenon: the model spontaneously
groups the integer inputs into bitwise rotationally-invariant clusters,
revealing how representations can transfer from smaller to larger moduli.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [172] [Towards Piece-by-Piece Explanations for Chess Positions with SHAP](https://arxiv.org/abs/2510.25775)
*Francesco Spinnato*

Main category: cs.AI

TL;DR: Adapting SHAP to chess analysis to attribute engine evaluations to specific pieces by treating pieces as features and computing their individual contributions.


<details>
  <summary>Details</summary>
Motivation: Chess engines provide opaque centipawn evaluations that obscure individual piece contributions, making it difficult to understand why certain moves are evaluated positively or negatively.

Method: Treat chess pieces as features and systematically ablate them to compute additive, per-piece contributions using SHAP (SHapley Additive exPlanations) methodology.

Result: Developed a method that provides locally faithful and human-interpretable piece-wise explanations for chess engine evaluations.

Conclusion: This approach enables better visualization, human training, and engine comparison in chess analysis, bridging classical chess pedagogy with modern explainable AI techniques.

Abstract: Contemporary chess engines offer precise yet opaque evaluations, typically
expressed as centipawn scores. While effective for decision-making, these
outputs obscure the underlying contributions of individual pieces or patterns.
In this paper, we explore adapting SHAP (SHapley Additive exPlanations) to the
domain of chess analysis, aiming to attribute a chess engines evaluation to
specific pieces on the board. By treating pieces as features and systematically
ablating them, we compute additive, per-piece contributions that explain the
engines output in a locally faithful and human-interpretable manner. This
method draws inspiration from classical chess pedagogy, where players assess
positions by mentally removing pieces, and grounds it in modern explainable AI
techniques. Our approach opens new possibilities for visualization, human
training, and engine comparison. We release accompanying code and data to
foster future research in interpretable chess AI.

</details>


### [173] [An Agentic Framework for Rapid Deployment of Edge AI Solutions in Industry 5.0](https://arxiv.org/abs/2510.25813)
*Jorge Martinez-Gil,Mario Pichler,Nefeli Bountouni,Sotiris Koussouris,Marielena Márquez Barreiro,Sergio Gusmeroli*

Main category: cs.AI

TL;DR: A novel Industry 5.0 framework for simplified AI model deployment on edge devices with local inference and real-time processing capabilities.


<details>
  <summary>Details</summary>
Motivation: To address the challenges of deploying AI models in industrial settings by reducing latency, avoiding external data transfer, and enabling flexible integration through agent-based architecture.

Method: Agent-based framework where individual agents (human, algorithmic, or collaborative) handle well-defined tasks, supporting modular integration with low resource requirements.

Result: Preliminary evaluations in food industry scenarios show improved deployment time and system adaptability performance.

Conclusion: The framework successfully enables simplified AI deployment on edge devices for Industry 5.0 applications with demonstrated improvements in deployment efficiency and adaptability.

Abstract: We present a novel framework for Industry 5.0 that simplifies the deployment
of AI models on edge devices in various industrial settings. The design reduces
latency and avoids external data transfer by enabling local inference and
real-time processing. Our implementation is agent-based, which means that
individual agents, whether human, algorithmic, or collaborative, are
responsible for well-defined tasks, enabling flexibility and simplifying
integration. Moreover, our framework supports modular integration and maintains
low resource requirements. Preliminary evaluations concerning the food industry
in real scenarios indicate improved deployment time and system adaptability
performance. The source code is publicly available at
https://github.com/AI-REDGIO-5-0/ci-component.

</details>


### [174] [Symbolically Scaffolded Play: Designing Role-Sensitive Prompts for Generative NPC Dialogue](https://arxiv.org/abs/2510.25820)
*Vanessa Figueiredo,David Elumeze*

Main category: cs.AI

TL;DR: The paper investigates whether constrained prompts improve player experience in LLM-powered games, finding no reliable experiential differences and revealing that scaffolding effects are role-dependent.


<details>
  <summary>Details</summary>
Motivation: To determine if constrained prompts actually enhance player experience in interactive games powered by LLMs, challenging the assumption that tighter constraints inherently improve gameplay.

Method: Conducted a within-subjects usability study (N=10) comparing high-constraint and low-constraint prompts in a voice-based detective game, followed by redesigning HCP into hybrid JSON+RAG scaffold and synthetic evaluation with LLM judge.

Result: No reliable experiential differences found between constraint levels; scaffolding effects were role-dependent - Interviewer NPC gained stability while suspect NPCs lost improvisational believability.

Conclusion: Tighter constraints don't inherently enhance play; introduced Symbolically Scaffolded Play framework using fuzzy numerical boundaries to stabilize coherence while preserving improvisation for engagement.

Abstract: Large Language Models (LLMs) promise to transform interactive games by
enabling non-player characters (NPCs) to sustain unscripted dialogue. Yet it
remains unclear whether constrained prompts actually improve player experience.
We investigate this question through The Interview, a voice-based detective
game powered by GPT-4o. A within-subjects usability study ($N=10$) compared
high-constraint (HCP) and low-constraint (LCP) prompts, revealing no reliable
experiential differences beyond sensitivity to technical breakdowns. Guided by
these findings, we redesigned the HCP into a hybrid JSON+RAG scaffold and
conducted a synthetic evaluation with an LLM judge, positioned as an
early-stage complement to usability testing. Results uncovered a novel pattern:
scaffolding effects were role-dependent: the Interviewer (quest-giver NPC)
gained stability, while suspect NPCs lost improvisational believability. These
findings overturn the assumption that tighter constraints inherently enhance
play. Extending fuzzy-symbolic scaffolding, we introduce \textit{Symbolically
Scaffolded Play}, a framework in which symbolic structures are expressed as
fuzzy, numerical boundaries that stabilize coherence where needed while
preserving improvisation where surprise sustains engagement.

</details>


### [175] [Through the Judge's Eyes: Inferred Thinking Traces Improve Reliability of LLM Raters](https://arxiv.org/abs/2510.25860)
*Xingjian Zhang,Tianhong Gao,Suliang Jin,Tianhao Wang,Teng Ye,Eytan Adar,Qiaozhu Mei*

Main category: cs.AI

TL;DR: A human-LLM collaborative framework to infer thinking traces from label-only annotations, improving LLM rater reliability through fine-tuning and clearer guidelines.


<details>
  <summary>Details</summary>
Motivation: LLMs are increasingly used as raters but lack reliability for subjective tasks where human judgments involve subtle reasoning beyond simple labels. Thinking traces (reasoning behind judgments) are informative but hard to collect.

Method: Uses rejection sampling to reconstruct thinking traces at scale from label-only annotations. Applies these traces to fine-tune open LLM raters and synthesize clearer annotation guidelines for proprietary LLM raters.

Result: Significantly improved LLM-human agreement across multiple datasets. Refined guidelines also increased agreement among different LLM models.

Conclusion: LLMs can serve as practical proxies for unrevealed human thinking traces, enabling label-only corpora to be extended into thinking-trace-augmented resources that enhance LLM rater reliability.

Abstract: Large language models (LLMs) are increasingly used as raters for evaluation
tasks. However, their reliability is often limited for subjective tasks, when
human judgments involve subtle reasoning beyond annotation labels. Thinking
traces, the reasoning behind a judgment, are highly informative but challenging
to collect and curate. We present a human-LLM collaborative framework to infer
thinking traces from label-only annotations. The proposed framework uses a
simple and effective rejection sampling method to reconstruct these traces at
scale. These inferred thinking traces are applied to two complementary tasks:
(1) fine-tuning open LLM raters; and (2) synthesizing clearer annotation
guidelines for proprietary LLM raters. Across multiple datasets, our methods
lead to significantly improved LLM-human agreement. Additionally, the refined
annotation guidelines increase agreement among different LLM models. These
results suggest that LLMs can serve as practical proxies for otherwise
unrevealed human thinking traces, enabling label-only corpora to be extended
into thinking-trace-augmented resources that enhance the reliability of LLM
raters.

</details>


### [176] [The Information-Theoretic Imperative: Compression and the Epistemic Foundations of Intelligence](https://arxiv.org/abs/2510.25883)
*Christian Dittrich,Jennifer Flygare Kinne*

Main category: cs.AI

TL;DR: A two-level framework (ITI and CEP) explains why compression leads to causal structure discovery rather than superficial patterns, linking survival pressure to intelligence through information-theoretic and evolutionary constraints.


<details>
  <summary>Details</summary>
Motivation: To address why compression enforces discovery of causal structure rather than statistical patterns, and provide a unified account of intelligence convergence across biological and artificial systems.

Method: Introduces Information-Theoretic Imperative (ITI) and Compression Efficiency Principle (CEP) - ITI establishes survival pressure leads to predictive compression, while CEP explains how efficient compression selects for causal models through exception-accumulation dynamics.

Result: The framework yields testable predictions: compression efficiency correlates with generalization, exception-accumulation rates differentiate causal models, hierarchical systems show efficiency gains, and biological systems track representational complexity in metabolic costs.

Conclusion: Intelligence is mechanically necessary outcome of persistence in structured environments, providing unified account across systems without invoking consciousness assumptions.

Abstract: Existing frameworks converge on the centrality of compression to intelligence
but leave underspecified why this process enforces the discovery of causal
structure rather than superficial statistical patterns. We introduce a
two-level framework to address this gap. The Information-Theoretic Imperative
(ITI) establishes that any system persisting in uncertain environments must
minimize epistemic entropy through predictive compression: this is the
evolutionary "why" linking survival pressure to information-processing demands.
The Compression Efficiency Principle (CEP) specifies how efficient compression
mechanically selects for generative, causal models through
exception-accumulation dynamics, making reality alignment a consequence rather
than a contingent achievement. Together, ITI and CEP define a causal chain:
from survival pressure to prediction necessity, compression requirement,
efficiency optimization, generative structure discovery, and ultimately reality
alignment. Each link follows from physical, information-theoretic, or
evolutionary constraints, implying that intelligence is the mechanically
necessary outcome of persistence in structured environments. This framework
yields empirically testable predictions: compression efficiency, measured as
approach to the rate-distortion frontier, correlates with out-of-distribution
generalization; exception-accumulation rates differentiate causal from
correlational models; hierarchical systems exhibit increasing efficiency across
abstraction layers; and biological systems demonstrate metabolic costs that
track representational complexity. ITI and CEP thereby provide a unified
account of convergence across biological, artificial, and multi-scale systems,
addressing the epistemic and functional dimensions of intelligence without
invoking assumptions about consciousness or subjective experience.

</details>


### [177] [Approximating Human Preferences Using a Multi-Judge Learned System](https://arxiv.org/abs/2510.25884)
*Eitán Sprejer,Fernando Avalos,Augusto Bernardi,Jose Pedro Brito de Azevedo Faustino,Jacob Haimes,Narmeen Fatimah Oozeer*

Main category: cs.AI

TL;DR: A framework for aligning LLM-based judges with human preferences using persona-based aggregation of multiple rubric-conditioned judges to address calibration issues, bias, and instability.


<details>
  <summary>Details</summary>
Motivation: LLM-based judges are difficult to calibrate and suffer from rubric sensitivity, bias, and instability, which hinders applications like RLHF reward models and model routing systems.

Method: Proposes persona-based preference modeling by learning to aggregate outputs from multiple rubric-conditioned judges, with two implementations: Generalized Additive Model (GAM) and Multi-Layer Perceptron (MLP).

Result: The approach is evaluated against naive baselines and assessed for robustness through case studies on human and LLM-judge biases.

Conclusion: The framework enables scalable synthesis of preference labels and provides a method to better align LLM judges with diverse human preferences through aggregation techniques.

Abstract: Aligning LLM-based judges with human preferences is a significant challenge,
as they are difficult to calibrate and often suffer from rubric sensitivity,
bias, and instability. Overcoming this challenge advances key applications,
such as creating reliable reward models for Reinforcement Learning from Human
Feedback (RLHF) and building effective routing systems that select the
best-suited model for a given user query. In this work, we propose a framework
for modeling diverse, persona-based preferences by learning to aggregate
outputs from multiple rubric-conditioned judges. We investigate the performance
of this approach against naive baselines and assess its robustness through case
studies on both human and LLM-judges biases. Our primary contributions include
a persona-based method for synthesizing preference labels at scale and two
distinct implementations of our aggregator: Generalized Additive Model (GAM)
and a Multi-Layer Perceptron (MLP).

</details>


### [178] [SciTrust 2.0: A Comprehensive Framework for Evaluating Trustworthiness of Large Language Models in Scientific Applications](https://arxiv.org/abs/2510.25908)
*Emily Herron,Junqi Yin,Feiyi Wang*

Main category: cs.AI

TL;DR: SciTrust 2.0 is a comprehensive framework for evaluating LLM trustworthiness in scientific applications across truthfulness, adversarial robustness, scientific safety, and scientific ethics dimensions.


<details>
  <summary>Details</summary>
Motivation: LLMs have transformative potential in scientific research but raise significant trustworthiness concerns in high-stakes contexts, necessitating comprehensive evaluation frameworks.

Method: Developed novel open-ended truthfulness benchmarks through verified reflection-tuning pipeline and expert validation, plus a novel ethics benchmark covering eight subcategories. Evaluated seven LLMs using multiple metrics including accuracy, semantic similarity, and LLM-based scoring.

Result: General-purpose industry models outperformed science-specialized models across all trustworthiness dimensions. GPT-4-mini showed superior performance in truthfulness and adversarial robustness. Science-specialized models showed significant deficiencies in logical/ethical reasoning and concerning safety vulnerabilities in high-risk domains.

Conclusion: The framework provides a foundation for developing more trustworthy AI systems and advancing research on model safety and ethics in scientific contexts, with the framework being open-sourced.

Abstract: Large language models (LLMs) have demonstrated transformative potential in
scientific research, yet their deployment in high-stakes contexts raises
significant trustworthiness concerns. Here, we introduce SciTrust 2.0, a
comprehensive framework for evaluating LLM trustworthiness in scientific
applications across four dimensions: truthfulness, adversarial robustness,
scientific safety, and scientific ethics. Our framework incorporates novel,
open-ended truthfulness benchmarks developed through a verified
reflection-tuning pipeline and expert validation, alongside a novel ethics
benchmark for scientific research contexts covering eight subcategories
including dual-use research and bias. We evaluated seven prominent LLMs,
including four science-specialized models and three general-purpose industry
models, using multiple evaluation metrics including accuracy, semantic
similarity measures, and LLM-based scoring. General-purpose industry models
overall outperformed science-specialized models across each trustworthiness
dimension, with GPT-o4-mini demonstrating superior performance in truthfulness
assessments and adversarial robustness. Science-specialized models showed
significant deficiencies in logical and ethical reasoning capabilities, along
with concerning vulnerabilities in safety evaluations, particularly in
high-risk domains such as biosecurity and chemical weapons. By open-sourcing
our framework, we provide a foundation for developing more trustworthy AI
systems and advancing research on model safety and ethics in scientific
contexts.

</details>


### [179] [FinOps Agent -- A Use-Case for IT Infrastructure and Cost Optimization](https://arxiv.org/abs/2510.25914)
*Ngoc Phuoc An Vo,Manish Kesarwani,Ruchi Mahindru,Chandrasekhar Narayanaswami*

Main category: cs.AI

TL;DR: The paper proposes using autonomous AI agents to automate FinOps processes, addressing challenges with heterogeneous cloud billing data and demonstrating successful implementation for IT infrastructure cost optimization.


<details>
  <summary>Details</summary>
Motivation: FinOps practitioners face challenges with heterogeneous billing data formats from multiple cloud providers, making it difficult to synthesize actionable insights and make timely decisions.

Method: Built a FinOps agent system that simulates end-to-end industry processes - from data retrieval from various sources to consolidation, analysis, and generating optimization recommendations. Evaluated using multiple language models.

Result: The agent demonstrated ability to understand, plan, and execute tasks comparable to an actual FinOps practitioner, showing effectiveness in handling heterogeneous data and generating optimization insights.

Conclusion: Autonomous AI agents can effectively automate FinOps processes, providing a solution to the challenge of managing heterogeneous cloud billing data and enabling timely decision-making for cost optimization.

Abstract: FinOps (Finance + Operations) represents an operational framework and
cultural practice which maximizes cloud business value through collaborative
financial accountability across engineering, finance, and business teams.
FinOps practitioners face a fundamental challenge: billing data arrives in
heterogeneous formats, taxonomies, and metrics from multiple cloud providers
and internal systems which eventually lead to synthesizing actionable insights,
and making time-sensitive decisions. To address this challenge, we propose
leveraging autonomous, goal-driven AI agents for FinOps automation. In this
paper, we built a FinOps agent for a typical use-case for IT infrastructure and
cost optimization. We built a system simulating a realistic end-to-end industry
process starting with retrieving data from various sources to consolidating and
analyzing the data to generate recommendations for optimization. We defined a
set of metrics to evaluate our agent using several open-source and close-source
language models and it shows that the agent was able to understand, plan, and
execute tasks as well as an actual FinOps practitioner.

</details>


### [180] [Humains-Junior: A 3.8B Language Model Achieving GPT-4o-Level Factual Accuracy by Directed Exoskeleton Reasoning](https://arxiv.org/abs/2510.25933)
*Nissan Yaron,Dan Bystritsky,Ben-Etzion Yaron*

Main category: cs.AI

TL;DR: A 3.8B model achieves GPT-4o-level FACTS accuracy (equivalent within ±5 pp on Q1-Q500) with ≈19× lower cloud cost versus GPT-4o, and self-hosted/edge deployments can approach zero marginal cost.


<details>
  <summary>Details</summary>
Motivation: To develop cost-efficient small language models that can match the performance of much larger frontier models on factual grounding tasks while significantly reducing deployment costs.

Method: Combines minimal directed "Exoskeleton Reasoning" scaffolds with behavioral fine-tuning that teaches protocol compliance (epistemic discipline) rather than domain answers. Fine-tuning alone adds little, but combined they synergize (+17.7 pp improvement).

Result: Humans-Junior (3.8B) scores 72.7% vs GPT-4o's 73.5% on FACTS Grounding subset Q1-Q500, with paired difference of 0.8 pp (statistically equivalent within ±5 pp margin). Cloud pricing shows ≈19× lower cost than GPT-4o. Directed reasoning also improved GPT-4o by +11.8 pp and Gemini-2.5-Pro by +5.0 pp in prompt-only settings.

Conclusion: Small language models can achieve frontier model performance on factual grounding tasks through targeted reasoning scaffolds and behavioral fine-tuning, enabling significant cost reductions while maintaining accuracy.

Abstract: We introduce Humans-Junior, a 3.8B model that matches GPT-4o on the FACTS
Grounding public subset within a $\pm 5$ pp equivalence margin.
  Results. On Q1--Q500 under identical judges, GPT-4o scores 73.5% (95% CI
69.5--77.2) and Humans-Junior 72.7% (95% CI 68.7--76.5); the paired difference
is 0.8 pp (bootstrap 95% CI $-3.1$ to $+4.7$; permutation $p = 0.72$; Cohen's
$d = 0.023$). TOST establishes equivalence at $\pm 5$ pp (not at $\pm 3$ pp).
When purchased as managed APIs, Humans-Junior's base model
(Phi-3.5-mini-instruct) is $\approx 19\times$ less expensive than GPT-4o on
Microsoft AI Foundry pricing; self-hosted or edge deployments can drive
incremental inference cost toward zero. Measured vs estimated pricing sources
are tabulated in Appendix E.
  Method. Our approach combines minimal directed "Exoskeleton Reasoning"
scaffolds with behavioral fine-tuning that teaches protocol compliance
(epistemic discipline) rather than domain answers. Fine-tuning alone adds
little; combined, they synergize (+17.7 pp, $p < 0.001$) and reduce variance
($\approx 25\%$). In prompt-only settings on frontier models (Q1--Q100;
non-comparable), directed reasoning improved GPT-4o by +11.8 pp to 85.3% and
Gemini-2.5-Pro by +5.0 pp to 93.3% (baseline 88.3%, $n = 100$); see Section~5.
  TL;DR. A 3.8B model achieves GPT-4o-level FACTS accuracy (equivalent within
$\pm 5$ pp on Q1--Q500). Cloud pricing shows $\approx 19\times$ lower cost
versus GPT-4o, and self-hosted/edge deployments can approach zero marginal
cost. Pricing sources are listed in Appendix E. Frontier prompt-only gains
(Q1--Q100; non-comparable) and optimized-prompt exploratory results under
earlier judges are summarized in Appendix F.
  Keywords: Small Language Models, Factual Grounding, Directed Reasoning,
Fine-Tuning, Model Alignment, Cost-Efficient AI

</details>


### [181] [Estimating cognitive biases with attention-aware inverse planning](https://arxiv.org/abs/2510.25951)
*Sounak Banerjee,Daphne Cornelisse,Deepak Gopinath,Emily Sumner,Jonathan DeCastro,Guy Rosman,Eugene Vinitsky,Mark K. Ho*

Main category: cs.AI

TL;DR: The paper proposes attention-aware inverse planning to estimate people's cognitive biases from their actions, combining deep RL with cognitive modeling to infer attentional strategies in driving scenarios.


<details>
  <summary>Details</summary>
Motivation: Autonomous systems need to understand how people's cognitive biases affect their goal-directed behaviors, particularly in everyday tasks like driving where attention systematically influences actions.

Method: Formulates attention-aware inverse planning problem, combines deep reinforcement learning with computational cognitive modeling to infer attentional biases from observed behavior.

Result: Demonstrated that attention-aware inverse planning systematically differs from standard inverse RL and can successfully infer cognitive biases from behavior in real-life driving scenarios using Waymo Open Dataset.

Conclusion: Attention-aware inverse planning provides a scalable approach for estimating cognitive biases from human behavior, with applications in autonomous systems that interact with people.

Abstract: People's goal-directed behaviors are influenced by their cognitive biases,
and autonomous systems that interact with people should be aware of this. For
example, people's attention to objects in their environment will be biased in a
way that systematically affects how they perform everyday tasks such as driving
to work. Here, building on recent work in computational cognitive science, we
formally articulate the attention-aware inverse planning problem, in which the
goal is to estimate a person's attentional biases from their actions. We
demonstrate how attention-aware inverse planning systematically differs from
standard inverse reinforcement learning and how cognitive biases can be
inferred from behavior. Finally, we present an approach to attention-aware
inverse planning that combines deep reinforcement learning with computational
cognitive modeling. We use this approach to infer the attentional strategies of
RL agents in real-life driving scenarios selected from the Waymo Open Dataset,
demonstrating the scalability of estimating cognitive biases with
attention-aware inverse planning.

</details>


### [182] [From Queries to Insights: Agentic LLM Pipelines for Spatio-Temporal Text-to-SQL](https://arxiv.org/abs/2510.25997)
*Manu Redd,Tao Zhe,Dongjie Wang*

Main category: cs.AI

TL;DR: An agentic pipeline extends a naive text-to-SQL baseline with ReAct agent orchestration to handle realistic spatio-temporal queries, achieving 91.4% accuracy vs 28.6% baseline.


<details>
  <summary>Details</summary>
Motivation: Existing NL-to-SQL systems struggle with realistic spatio-temporal queries that require aligning vague user phrasing with schema-specific categories, handling temporal reasoning, and choosing appropriate outputs.

Method: Agentic pipeline using Mistral-based ReAct agent for orchestration, with capabilities for planning, decomposition, and adaptation through schema inspection, SQL generation, execution, and visualization tools.

Result: Achieved 91.4% accuracy on 35 natural-language queries over NYC and Tokyo check-in dataset, substantially outperforming the 28.6% baseline accuracy, with enhanced usability through maps, plots, and natural-language summaries.

Conclusion: Agentic orchestration, rather than stronger SQL generators alone, is a promising foundation for interactive geospatial assistants that support users lacking SQL expertise, schema knowledge, or prompting skills.

Abstract: Natural-language-to-SQL (NL-to-SQL) systems hold promise for democratizing
access to structured data, allowing users to query databases without learning
SQL. Yet existing systems struggle with realistic spatio-temporal queries,
where success requires aligning vague user phrasing with schema-specific
categories, handling temporal reasoning, and choosing appropriate outputs. We
present an agentic pipeline that extends a naive text-to-SQL baseline
(llama-3-sqlcoder-8b) with orchestration by a Mistral-based ReAct agent. The
agent can plan, decompose, and adapt queries through schema inspection, SQL
generation, execution, and visualization tools. We evaluate on 35
natural-language queries over the NYC and Tokyo check-in dataset, covering
spatial, temporal, and multi-dataset reasoning. The agent achieves
substantially higher accuracy than the naive baseline 91.4% vs. 28.6% and
enhances usability through maps, plots, and structured natural-language
summaries. Crucially, our design enables more natural human-database
interaction, supporting users who lack SQL expertise, detailed schema
knowledge, or prompting skill. We conclude that agentic orchestration, rather
than stronger SQL generators alone, is a promising foundation for interactive
geospatial assistants.

</details>


### [183] [AutoSurvey2: Empowering Researchers with Next Level Automated Literature Surveys](https://arxiv.org/abs/2510.26012)
*Siyi Wu,Chiaxin Liang,Ziqian Bi,Leyi Zhao,Tianyang Wang,Junhao Song,Yichao Zhang,Keyu Chen,Xinyuan Song*

Main category: cs.AI

TL;DR: autosurvey2 is an automated pipeline for generating comprehensive academic surveys using retrieval-augmented synthesis and multi-LLM evaluation, outperforming existing baselines in structural coherence and topical relevance.


<details>
  <summary>Details</summary>
Motivation: The rapid growth of research literature, especially in LLMs, makes producing current and comprehensive survey papers increasingly difficult.

Method: Multi-stage pipeline with parallel section generation, iterative refinement, real-time retrieval of recent publications, and multi-LLM evaluation framework for quality assessment.

Result: autosurvey2 consistently outperforms existing retrieval-based and automated baselines, achieving higher scores in structural coherence and topical relevance while maintaining strong citation fidelity.

Conclusion: autosurvey2 provides a scalable and reproducible solution for generating long-form academic surveys and contributes a foundation for future research on automated scholarly writing.

Abstract: The rapid growth of research literature, particularly in large language
models (LLMs), has made producing comprehensive and current survey papers
increasingly difficult. This paper introduces autosurvey2, a multi-stage
pipeline that automates survey generation through retrieval-augmented synthesis
and structured evaluation. The system integrates parallel section generation,
iterative refinement, and real-time retrieval of recent publications to ensure
both topical completeness and factual accuracy. Quality is assessed using a
multi-LLM evaluation framework that measures coverage, structure, and relevance
in alignment with expert review standards. Experimental results demonstrate
that autosurvey2 consistently outperforms existing retrieval-based and
automated baselines, achieving higher scores in structural coherence and
topical relevance while maintaining strong citation fidelity. By combining
retrieval, reasoning, and automated evaluation into a unified framework,
autosurvey2 provides a scalable and reproducible solution for generating
long-form academic surveys and contributes a solid foundation for future
research on automated scholarly writing. All code and resources are available
at https://github.com/annihi1ation/auto_research.

</details>


### [184] [Large Language Model-assisted Autonomous Vehicle Recovery from Immobilization](https://arxiv.org/abs/2510.26023)
*Zhipeng Bao,Qianwen Li*

Main category: cs.AI

TL;DR: StuckSolver is an LLM-driven framework that enables autonomous vehicles to resolve immobilization scenarios through self-reasoning and passenger-guided decision-making, achieving near-state-of-the-art performance.


<details>
  <summary>Details</summary>
Motivation: Current AV recovery solutions like remote intervention and manual takeover are inadequate - they're costly, inefficient, and limit accessibility for non-drivers. AVs often get stuck in scenarios where human drivers excel.

Method: StuckSolver is a plug-in LLM framework that operates on top of existing AV systems without modifying internal architecture. It uses sensor data to detect immobilization, interpret context, and generate recovery commands for the native planner.

Result: Evaluation on Bench2Drive benchmark and custom uncertainty scenarios shows StuckSolver achieves near-state-of-the-art performance through autonomous self-reasoning, with further improvements when passenger guidance is incorporated.

Conclusion: StuckSolver provides an effective solution for AV immobilization recovery that doesn't require system modifications and can leverage both autonomous reasoning and human guidance to improve performance.

Abstract: Despite significant advancements in recent decades, autonomous vehicles (AVs)
continue to face challenges in navigating certain traffic scenarios where human
drivers excel. In such situations, AVs often become immobilized, disrupting
overall traffic flow. Current recovery solutions, such as remote intervention
(which is costly and inefficient) and manual takeover (which excludes
non-drivers and limits AV accessibility), are inadequate. This paper introduces
StuckSolver, a novel Large Language Model (LLM) driven recovery framework that
enables AVs to resolve immobilization scenarios through self-reasoning and/or
passenger-guided decision-making. StuckSolver is designed as a plug-in add-on
module that operates on top of the AV's existing perception-planning-control
stack, requiring no modification to its internal architecture. Instead, it
interfaces with standard sensor data streams to detect immobilization states,
interpret environmental context, and generate high-level recovery commands that
can be executed by the AV's native planner. We evaluate StuckSolver on the
Bench2Drive benchmark and in custom-designed uncertainty scenarios. Results
show that StuckSolver achieves near-state-of-the-art performance through
autonomous self-reasoning alone and exhibits further improvements when
passenger guidance is incorporated.

</details>


### [185] [Can AI be Accountable?](https://arxiv.org/abs/2510.26057)
*Andrew L. Kun*

Main category: cs.AI

TL;DR: This paper argues that AI systems must be accountable to users, voters, and decision makers through mechanisms for information sharing, discussion, and sanctions, which current AI often lacks.


<details>
  <summary>Details</summary>
Motivation: As AI becomes increasingly powerful, it's crucial that it serves human needs and is accountable to those affected by its actions, yet current AI systems often lack accountability mechanisms.

Method: The authors relate general accountability definitions to AI, illustrate what accountable and unaccountable AI looks like, and explore approaches to ensure AI accountability.

Result: The paper provides a framework for understanding AI accountability and identifies the gap between current AI systems and true accountability requirements.

Conclusion: Developing approaches to make AI accountable to affected parties is essential for ensuring AI serves human needs and interests in an increasingly AI-driven world.

Abstract: The AI we use is powerful, and its power is increasing rapidly. If this
powerful AI is to serve the needs of consumers, voters, and decision makers,
then it is imperative that the AI is accountable. In general, an agent is
accountable to a forum if the forum can request information from the agent
about its actions, if the forum and the agent can discuss this information, and
if the forum can sanction the agent. Unfortunately, in too many cases today's
AI is not accountable -- we cannot question it, enter into a discussion with
it, let alone sanction it. In this chapter we relate the general definition of
accountability to AI, we illustrate what it means for AI to be accountable and
unaccountable, and we explore approaches that can improve our chances of living
in a world where all AI is accountable to those who are affected by it.

</details>


### [186] [Lean4Physics: Comprehensive Reasoning Framework for College-level Physics in Lean4](https://arxiv.org/abs/2510.26094)
*Yuxin Li,Minghao Liu,Ruida Wang,Wenzhao Ji,Zhitao He,Rui Pan,Junming Huang,Tong Zhang,Yi R. Fung*

Main category: cs.AI

TL;DR: Lean4PHYS is a formal reasoning framework for college physics in Lean4, featuring LeanPhysBench benchmark and PhysLib repository, with current AI models achieving only 16-35% performance.


<details>
  <summary>Details</summary>
Motivation: To establish formal reasoning capabilities for college-level physics problems and create the first physics benchmark in Lean4 to evaluate AI reasoning systems.

Method: Created LeanPhysBench with 200 hand-crafted physics problems from textbooks and competitions, and developed PhysLib repository with fundamental units and theorems for formal physics reasoning.

Result: Best performance was 16% by DeepSeek-Prover-V2-7B and 35% by Claude-Sonnet-4, with PhysLib improving model performance by average 11.75%.

Conclusion: The benchmark is challenging for current AI systems, demonstrating the need for specialized physics reasoning capabilities, and PhysLib effectively enhances formal physics reasoning performance.

Abstract: We present **Lean4PHYS**, a comprehensive reasoning framework for
college-level physics problems in Lean4. **Lean4PHYS** includes
*LeanPhysBench*, a college-level benchmark for formal physics reasoning in
Lean4, which contains 200 hand-crafted and peer-reviewed statements derived
from university textbooks and physics competition problems. To establish a
solid foundation for formal reasoning in physics, we also introduce *PhysLib*,
a community-driven repository containing fundamental unit systems and theorems
essential for formal physics reasoning. Based on the benchmark and Lean4
repository we composed in **Lean4PHYS**, we report baseline results using major
expert Math Lean4 provers and state-of-the-art closed-source models, with the
best performance of DeepSeek-Prover-V2-7B achieving only 16% and
Claude-Sonnet-4 achieving 35%. We also conduct a detailed analysis showing that
our *PhysLib* can achieve an average improvement of 11.75% in model
performance. This demonstrates the challenging nature of our *LeanPhysBench*
and the effectiveness of *PhysLib*. To the best of our knowledge, this is the
first study to provide a physics benchmark in Lean4.

</details>


### [187] [GUI Knowledge Bench: Revealing the Knowledge Gap Behind VLM Failures in GUI Tasks](https://arxiv.org/abs/2510.26098)
*Chenrui Shi,Zedong Yu,Zhi Gao,Ruining Feng,Enqi Liu,Yuwei Wu,Yunde Jia,Liuyu Xiang,Zhaofeng He,Qing Li*

Main category: cs.AI

TL;DR: VLMs lack core GUI knowledge for task automation. The paper identifies three knowledge dimensions and introduces GUI Knowledge Bench benchmark to assess VLMs' GUI capabilities across multiple platforms.


<details>
  <summary>Details</summary>
Motivation: Large vision language models lag behind humans in GUI task automation due to missing core GUI knowledge that existing training schemes cannot fully address.

Method: Analyzed failure patterns to distill GUI knowledge into three dimensions: interface perception, interaction prediction, and instruction understanding. Created GUI Knowledge Bench benchmark with multiple choice and yes/no questions across six platforms and 292 applications.

Result: Current VLMs can identify widget functions but struggle with perceiving system states, predicting actions, and verifying task completion. Experiments show close link between GUI knowledge and task success.

Conclusion: The structured framework helps select VLMs with greater potential before downstream training and provides insights for building more capable GUI agents.

Abstract: Large vision language models (VLMs) have advanced graphical user interface
(GUI) task automation but still lag behind humans. We hypothesize this gap
stems from missing core GUI knowledge, which existing training schemes (such as
supervised fine tuning and reinforcement learning) alone cannot fully address.
By analyzing common failure patterns in GUI task execution, we distill GUI
knowledge into three dimensions: (1) interface perception, knowledge about
recognizing widgets and system states; (2) interaction prediction, knowledge
about reasoning action state transitions; and (3) instruction understanding,
knowledge about planning, verifying, and assessing task completion progress. We
further introduce GUI Knowledge Bench, a benchmark with multiple choice and
yes/no questions across six platforms (Web, Android, MacOS, Windows, Linux,
IOS) and 292 applications. Our evaluation shows that current VLMs identify
widget functions but struggle with perceiving system states, predicting
actions, and verifying task completion. Experiments on real world GUI tasks
further validate the close link between GUI knowledge and task success. By
providing a structured framework for assessing GUI knowledge, our work supports
the selection of VLMs with greater potential prior to downstream training and
provides insights for building more capable GUI agents.

</details>


### [188] [Beyond Benchmarks: The Economics of AI Inference](https://arxiv.org/abs/2510.26136)
*Boqin Zhuang,Jiacheng Qiao,Mingqian Liu,Mingxing Yu,Ping Hong,Rui Li,Xiaoxia Song,Xiangjun Xu,Xu Chen,Yaoyao Ma,Yujie Gao*

Main category: cs.AI

TL;DR: This paper introduces an economic framework for analyzing LLM inference costs, revealing principles of diminishing marginal cost, diminishing returns to scale, and optimal cost-effectiveness zones.


<details>
  <summary>Details</summary>
Motivation: The high inference cost of LLMs has become a critical factor limiting their commercial viability and widespread adoption, necessitating an economic analysis of inference processes.

Method: Developed a quantitative 'economics of inference' framework treating LLM inference as compute-driven production, analyzed using empirical data from WiNEval-3.0 to construct the first 'LLM Inference Production Frontier'.

Result: Revealed three key economic principles: diminishing marginal cost, diminishing returns to scale, and an optimal cost-effectiveness zone for LLM inference.

Conclusion: The framework provides economic basis for model deployment decisions and lays empirical foundation for market-based pricing and optimization of AI inference resources.

Abstract: The inference cost of Large Language Models (LLMs) has become a critical
factor in determining their commercial viability and widespread adoption. This
paper introduces a quantitative ``economics of inference'' framework, treating
the LLM inference process as a compute-driven intelligent production activity.
We analyze its marginal cost, economies of scale, and quality of output under
various performance configurations. Based on empirical data from WiNEval-3.0,
we construct the first ``LLM Inference Production Frontier,'' revealing three
principles: diminishing marginal cost, diminishing returns to scale, and an
optimal cost-effectiveness zone. This paper not only provides an economic basis
for model deployment decisions but also lays an empirical foundation for the
future market-based pricing and optimization of AI inference resources.

</details>


### [189] [Reasoning Curriculum: Bootstrapping Broad LLM Reasoning from Math](https://arxiv.org/abs/2510.26143)
*Bo Pang,Deqian Kong,Silvio Savarese,Caiming Xiong,Yingbo Zhou*

Main category: cs.AI

TL;DR: A two-stage curriculum called Reasoning Curriculum that first develops reasoning skills in math domains via RL, then transfers them to other domains through joint RL, achieving consistent performance gains across multiple domains.


<details>
  <summary>Details</summary>
Motivation: Most RL efforts for LLMs focus on math and code, but there's a need to develop general reasoning skills that can transfer across multiple domains.

Method: Two-stage curriculum: Stage 1 performs math-only RL with verifiable rewards to develop reasoning skills; Stage 2 runs joint RL on mixed-domain data to transfer and consolidate these skills. The approach is backbone-agnostic and requires no specialized reward models.

Result: Evaluated on Qwen3-4B and Llama-3.1-8B, the curriculum yields consistent gains across multi-domain benchmarks. Ablations show both stages are necessary, and math-first elicitation increases cognitive behaviors important for complex problem solving.

Conclusion: Reasoning Curriculum provides a compact, easy-to-adopt recipe for developing general reasoning skills in LLMs through domain transfer from math to other domains.

Abstract: Reinforcement learning (RL) can elicit strong reasoning in large language
models (LLMs), yet most open efforts focus on math and code. We propose
Reasoning Curriculum, a simple two-stage curriculum that first elicits
reasoning skills in pretraining-aligned domains such as math, then adapts and
refines these skills across other domains via joint RL. Stage 1 performs a
brief cold start and then math-only RL with verifiable rewards to develop
reasoning skills. Stage 2 runs joint RL on mixed-domain data to transfer and
consolidate these skills. The curriculum is minimal and backbone-agnostic,
requiring no specialized reward models beyond standard verifiability checks.
Evaluated on Qwen3-4B and Llama-3.1-8B over a multi-domain suite, reasoning
curriculum yields consistent gains. Ablations and a cognitive-skill analysis
indicate that both stages are necessary and that math-first elicitation
increases cognitive behaviors important for solving complex problems. Reasoning
Curriculum provides a compact, easy-to-adopt recipe for general reasoning.

</details>


### [190] [The FM Agent](https://arxiv.org/abs/2510.26144)
*Annan Li,Chufan Wu,Zengle Ge,Yee Hin Chong,Zhinan Hou,Lizhe Cao,Cheng Ju,Jianmin Wu,Huaiming Li,Haobo Zhang,Shenghao Feng,Mo Zhao,Fengzhi Qiu,Rui Yang,Mengmeng Zhang,Wenyi Zhu,Yingying Sun,Quan Sun,Shunhao Yan,Danyu Liu,Dawei Yin,Dou Shen*

Main category: cs.AI

TL;DR: FM Agent is a multi-agent framework combining LLM reasoning and evolutionary search to solve complex problems autonomously, achieving state-of-the-art results across multiple domains without human intervention.


<details>
  <summary>Details</summary>
Motivation: To develop autonomous AI research agents that can address complex real-world challenges in scientific and engineering discovery by leveraging the synergy between LLM-based reasoning and evolutionary optimization.

Method: Integrates cold-start initialization with expert guidance, novel evolutionary sampling strategy, domain-specific evaluators combining correctness/effectiveness/LLM feedback, and distributed asynchronous execution infrastructure built on Ray.

Result: Achieved state-of-the-art results: 1976.3 on ALE-Bench (+5.2%), 43.56% on MLE-Bench (+4.0pp), up to 20x speedups on KernelBench, and new SOTA on classical mathematical problems - all autonomously without human tuning.

Conclusion: FM Agent demonstrates broad applicability across domains and shows considerable promise for accelerating innovation in enterprise R&D and fundamental scientific research, automating complex discovery processes with substantial societal impact.

Abstract: Large language models (LLMs) are catalyzing the development of autonomous AI
research agents for scientific and engineering discovery. We present FM Agent,
a novel and general-purpose multi-agent framework that leverages a synergistic
combination of LLM-based reasoning and large-scale evolutionary search to
address complex real-world challenges. The core of FM Agent integrates several
key innovations: 1) a cold-start initialization phase incorporating expert
guidance, 2) a novel evolutionary sampling strategy for iterative optimization,
3) domain-specific evaluators that combine correctness, effectiveness, and
LLM-supervised feedback, and 4) a distributed, asynchronous execution
infrastructure built on Ray. Demonstrating broad applicability, our system has
been evaluated across diverse domains, including operations research, machine
learning, GPU kernel optimization, and classical mathematical problems. FM
Agent reaches state-of-the-art results autonomously, without human
interpretation or tuning -- 1976.3 on ALE-Bench (+5.2\%), 43.56\% on MLE-Bench
(+4.0pp), up to 20x speedups on KernelBench, and establishes new
state-of-the-art(SOTA) results on several classical mathematical problems.
Beyond academic benchmarks, FM Agent shows considerable promise for both
large-scale enterprise R\&D workflows and fundamental scientific research,
where it can accelerate innovation, automate complex discovery processes, and
deliver substantial engineering and scientific advances with broader societal
impact.

</details>


### [191] [One Model to Critique Them All: Rewarding Agentic Tool-Use via Efficient Reasoning](https://arxiv.org/abs/2510.26167)
*Renhao Li,Jianhong Tu,Yang Su,Hamid Alinejad-Rokny,Derek F. Wong,Junyang Lin,Min Yang*

Main category: cs.AI

TL;DR: ToolRM is a family of lightweight generative reward models for tool-use scenarios, trained on a novel pairwise preference dataset and achieving superior performance in function-calling tasks compared to frontier models.


<details>
  <summary>Details</summary>
Motivation: The lack of specialized reward models for function-calling tasks in tool learning has limited progress in developing more capable agentic AI systems.

Method: Proposed a novel pipeline using rule-based scoring and multidimensional sampling to construct ToolPref-Pairwise-30K dataset, then trained ToolRM models on this data using reinforcement learning with verifiable feedback.

Result: ToolRM models from Qwen3-4B/8B series achieved up to 14.28% higher accuracy in pairwise reward judgments, substantially outperforming Claude 4 and OpenAI o3. Also reduced output token usage by over 66% in ACEBench experiments.

Conclusion: ToolRM effectively addresses the gap in tool-use reward models, demonstrating strong generalization to broader critique tasks and enabling more efficient inference-time scaling for agentic AI systems.

Abstract: Reward models (RMs) play a critical role in aligning large language models
(LLMs) with human preferences. Yet in the domain of tool learning, the lack of
RMs specifically designed for function-calling tasks has limited progress
toward more capable agentic AI. We introduce ToolRM, a family of lightweight
generative RMs tailored for general tool-use scenarios. To build these models,
we propose a novel pipeline that constructs pairwise preference data using
rule-based scoring and multidimensional sampling. This yields
ToolPref-Pairwise-30K, a diverse, balanced, and challenging dataset of critique
tasks that supports reinforcement learning with verifiable feedback. To
evaluate tool-use RMs, we also introduce TRBench$_{BFCL}$, a benchmark built on
the agentic evaluation suite BFCL. Trained on our constructed data, models from
the Qwen3-4B/8B series achieve up to 14.28% higher accuracy, substantially
outperforming frontier models such as Claude 4 and OpenAI o3 in pairwise reward
judgments. Beyond training objectives, ToolRM generalizes to broader critique
tasks, including Best-of-N sampling and self-correction. Experiments on
ACEBench highlight its effectiveness and efficiency, enabling inference-time
scaling and reducing output token usage by over 66%. We release data and model
checkpoints to facilitate future research.

</details>


### [192] [Questionnaire meets LLM: A Benchmark and Empirical Study of Structural Skills for Understanding Questions and Responses](https://arxiv.org/abs/2510.26238)
*Duc-Hai Nguyen,Vijayakumar Nanjappan,Barry O'Sullivan,Hoang D. Nguyen*

Main category: cs.AI

TL;DR: QASU benchmark evaluates LLMs' ability to process questionnaire data across different formats and prompting strategies, showing that proper serialization and prompting can significantly improve performance on survey analysis tasks.


<details>
  <summary>Details</summary>
Motivation: Current survey analysis tools are human-centric and don't integrate well with LLMs, leaving a gap in evidence-based guidance for representing questionnaires for AI consumption despite the prevalence of survey data.

Method: Introduces QASU benchmark that tests six structural skills across six serialization formats and multiple prompt strategies, systematically isolating format and prompting effects on LLM performance.

Result: Choosing effective format and prompt combinations improved accuracy by up to 8.8% points, and adding lightweight structural hints through self-augmented prompting yielded additional 3-4% point improvements on average for specific tasks.

Conclusion: QASU provides a versatile foundation for advancing LLM-based questionnaire analysis research and practice by systematically evaluating format and prompting strategies.

Abstract: Millions of people take surveys every day, from market polls and academic
studies to medical questionnaires and customer feedback forms. These datasets
capture valuable insights, but their scale and structure present a unique
challenge for large language models (LLMs), which otherwise excel at few-shot
reasoning over open-ended text. Yet, their ability to process questionnaire
data or lists of questions crossed with hundreds of respondent rows remains
underexplored. Current retrieval and survey analysis tools (e.g., Qualtrics,
SPSS, REDCap) are typically designed for humans in the workflow, limiting such
data integration with LLM and AI-empowered automation. This gap leaves
scientists, surveyors, and everyday users without evidence-based guidance on
how to best represent questionnaires for LLM consumption. We address this by
introducing QASU (Questionnaire Analysis and Structural Understanding), a
benchmark that probes six structural skills, including answer lookup,
respondent count, and multi-hop inference, across six serialization formats and
multiple prompt strategies. Experiments on contemporary LLMs show that choosing
an effective format and prompt combination can improve accuracy by up to 8.8%
points compared to suboptimal formats. For specific tasks, carefully adding a
lightweight structural hint through self-augmented prompting can yield further
improvements of 3-4% points on average. By systematically isolating format and
prompting effects, our open source benchmark offers a simple yet versatile
foundation for advancing both research and real-world practice in LLM-based
questionnaire analysis.

</details>


### [193] [Retrieval Augmented Generation-Enhanced Distributed LLM Agents for Generalizable Traffic Signal Control with Emergency Vehicles](https://arxiv.org/abs/2510.26242)
*Xinhang Li,Qing Guo,Junyu Chen,Zheng Guo,Shengzhe Xu,Lei Li,Lin Zhang*

Main category: cs.AI

TL;DR: REG-TSC is a distributed LLM-based traffic signal control system that uses RAG enhancement and emergency response mechanisms to improve reliability and generalization across diverse intersections, achieving significant performance improvements in travel time, queue length, and emergency vehicle waiting time.


<details>
  <summary>Details</summary>
Motivation: Current LLM approaches for TSC suffer from hallucinations in emergencies and poor generalization across different intersection types, leading to unreliable decisions and limited applicability in complex urban traffic scenarios.

Method: Proposes REG-TSC with two key components: 1) Emergency-aware reasoning framework with Reviewer-based Emergency RAG (RERAG) for reliable emergency decisions, and 2) Type-agnostic traffic representation with Reward-guided Reinforced Refinement (R3) for cross-intersection training and generalization.

Result: Extensive experiments on real-world road networks with 17-177 intersections show REG-TSC reduces travel time by 42.00%, queue length by 62.31%, and emergency vehicle waiting time by 83.16%, outperforming state-of-the-art methods.

Conclusion: REG-TSC effectively addresses LLM hallucinations in emergencies and enables generalization across heterogeneous intersections through RAG enhancement and reinforced refinement, demonstrating superior performance in complex traffic scenarios.

Abstract: With increasing urban traffic complexity, Traffic Signal Control (TSC) is
essential for optimizing traffic flow and improving road safety. Large Language
Models (LLMs) emerge as promising approaches for TSC. However, they are prone
to hallucinations in emergencies, leading to unreliable decisions that may
cause substantial delays for emergency vehicles. Moreover, diverse intersection
types present substantial challenges for traffic state encoding and
cross-intersection training, limiting generalization across heterogeneous
intersections. Therefore, this paper proposes Retrieval Augmented Generation
(RAG)-enhanced distributed LLM agents with Emergency response for Generalizable
TSC (REG-TSC). Firstly, this paper presents an emergency-aware reasoning
framework, which dynamically adjusts reasoning depth based on the emergency
scenario and is equipped with a novel Reviewer-based Emergency RAG (RERAG) to
distill specific knowledge and guidance from historical cases, enhancing the
reliability and rationality of agents' emergency decisions. Secondly, this
paper designs a type-agnostic traffic representation and proposes a
Reward-guided Reinforced Refinement (R3) for heterogeneous intersections. R3
adaptively samples training experience from diverse intersections with
environment feedback-based priority and fine-tunes LLM agents with a designed
reward-weighted likelihood loss, guiding REG-TSC toward high-reward policies
across heterogeneous intersections. On three real-world road networks with 17
to 177 heterogeneous intersections, extensive experiments show that REG-TSC
reduces travel time by 42.00%, queue length by 62.31%, and emergency vehicle
waiting time by 83.16%, outperforming other state-of-the-art methods.

</details>


### [194] [Graph-Enhanced Policy Optimization in LLM Agent Training](https://arxiv.org/abs/2510.26270)
*Jiazhen Yuan,Wei Zhao,Zhengbiao Bai*

Main category: cs.AI

TL;DR: GEPO addresses structural blindness in group-based RL for LLM agents by dynamically building state-transition graphs and using graph centrality to improve exploration, credit assignment, and planning.


<details>
  <summary>Details</summary>
Motivation: Group-based RL methods suffer from structural blindness when training multi-turn LLM agents, leading to inefficient exploration, imprecise credit assignment, and myopic planning due to inability to exploit environmental connectivity.

Method: Graph-Enhanced Policy Optimization (GEPO) dynamically constructs state-transition graphs from agent experience and uses graph-theoretic centrality to provide: structured intrinsic rewards for guided exploration, graph-enhanced advantage function for topology-aware credit assignment, and dynamic discount factor adapted to each state's strategic value.

Result: GEPO achieved absolute success rate gains of +4.1% on ALFWorld, +5.3% on WebShop, and +10.9% on a proprietary Workbench benchmark over competitive baselines.

Conclusion: Explicitly modeling environmental structure through graph-based approaches is a robust, generalizable strategy for advancing LLM agent training.

Abstract: Group based reinforcement learning (RL) has shown impressive results on
complex reasoning and mathematical tasks. Yet, when applied to train
multi-turn, interactive LLM agents, these methods often suffer from structural
blindness-the inability to exploit the underlying connectivity of the
environment. This manifests in three critical challenges: (1) inefficient,
unguided exploration, (2) imprecise credit assignment due to overlooking
pivotal states, and (3) myopic planning caused by static reward discounting. We
address these issues with Graph-Enhanced Policy Optimization (GEPO), which
dynamically constructs a state-transition graph from agent experience and
employs graph-theoretic centrality to provide three synergistic learning
signals: (1)structured intrinsic rewards that guide exploration toward
high-impact states, (2) a graph-enhanced advantage function for topology-aware
credit assignment, and (3) a dynamic discount factor adapted to each state's
strategic value. On the ALFWorld, WebShop, and a proprietary Workbench
benchmarks, GEPO demonstrates strong performance, achieving absolute success
rate gains of +4.1%, +5.3%, and +10.9% over competitive baselines. These
results highlight that explicitly modeling environmental structure is a robust,
generalizable strategy for advancing LLM agent training.

</details>


### [195] [GraphCompliance: Aligning Policy and Context Graphs for LLM-Based Regulatory Compliance](https://arxiv.org/abs/2510.26309)
*Jiseong Chung,Ronny Ko,Wonchul Yoo,Makoto Onizuka,Sungmok Kim,Tae-Wan Kim,Won-Yong Shin*

Main category: cs.AI

TL;DR: GraphCompliance is a framework that aligns structured regulatory policy graphs with runtime context graphs to improve compliance assessment, achieving 4.1-7.2 pp higher micro-F1 than LLM-only and RAG baselines.


<details>
  <summary>Details</summary>
Motivation: Web-scale compliance requires regulatory assessment for each request, but regulatory texts are cross-referential and normative while runtime contexts are unstructured natural language, creating alignment challenges.

Method: Represent regulatory texts as Policy Graphs (encoding normative structure and cross-references) and runtime contexts as Context Graphs (formalizing events as SAO and entity-relation triples), then align them to anchor LLM reasoning in structured information.

Result: On 300 GDPR-derived real-world scenarios across five tasks, GraphCompliance achieved 4.1-7.2 percentage points higher micro-F1 than baselines, with fewer under- and over-predictions, higher recall, and lower false positive rates.

Conclusion: Structured representations and judge LLMs are complementary for normative reasoning, with each graph component contributing to improved compliance assessment performance.

Abstract: Compliance at web scale poses practical challenges: each request may require
a regulatory assessment. Regulatory texts (e.g., the General Data Protection
Regulation, GDPR) are cross-referential and normative, while runtime contexts
are expressed in unstructured natural language. This setting motivates us to
align semantic information in unstructured text with the structured, normative
elements of regulations. To this end, we introduce GraphCompliance, a framework
that represents regulatory texts as a Policy Graph and runtime contexts as a
Context Graph, and aligns them. In this formulation, the policy graph encodes
normative structure and cross-references, whereas the context graph formalizes
events as subject-action-object (SAO) and entity-relation triples. This
alignment anchors the reasoning of a judge large language model (LLM) in
structured information and helps reduce the burden of regulatory interpretation
and event parsing, enabling a focus on the core reasoning step. In experiments
on 300 GDPR-derived real-world scenarios spanning five evaluation tasks,
GraphCompliance yields 4.1-7.2 percentage points (pp) higher micro-F1 than
LLM-only and RAG baselines, with fewer under- and over-predictions, resulting
in higher recall and lower false positive rates. Ablation studies indicate
contributions from each graph component, suggesting that structured
representations and a judge LLM are complementary for normative reasoning.

</details>


### [196] [Discovering State Equivalences in UCT Search Trees By Action Pruning](https://arxiv.org/abs/2510.26346)
*Robin Schmöcker,Alexander Dockhorn,Bodo Rosenhahn*

Main category: cs.AI

TL;DR: The paper addresses the challenge of finding state abstractions in MCTS for noisy or large action spaces. It proposes IPA-UCT, a new technique that uses weaker abstraction conditions to find more abstractions with minor accuracy loss, outperforming existing methods like OGA-UCT across various domains.


<details>
  <summary>Details</summary>
Motivation: Current MCTS enhancement methods using state-action pair abstractions (like OGA-UCT) work well, but state abstractions are rarely found in noisy or large action space settings due to strict conditions. This limits the potential benefits of abstraction in MCTS.

Method: Proposed IPA-UCT technique with weaker state abstraction conditions that trade minor accuracy loss for finding many more abstractions. Introduces IPA framework as an alternative to ASAP framework used by OGA-UCT, and shows both are special cases of a more general p-ASAP framework within the ASASAP framework.

Result: IPA-UCT outperforms OGA-UCT and its derivatives across a large range of test domains and iteration budgets. The weaker abstraction conditions successfully find many more abstractions than previous approaches.

Conclusion: The proposed IPA-UCT technique effectively addresses the state abstraction problem in MCTS by using weaker conditions, demonstrating superior performance over existing methods. The work also provides a unified theoretical framework showing the relationships between different abstraction approaches.

Abstract: One approach to enhance Monte Carlo Tree Search (MCTS) is to improve its
sample efficiency by grouping/abstracting states or state-action pairs and
sharing statistics within a group. Though state-action pair abstractions are
mostly easy to find in algorithms such as On the Go Abstractions in Upper
Confidence bounds applied to Trees (OGA-UCT), nearly no state abstractions are
found in either noisy or large action space settings due to constraining
conditions. We provide theoretical and empirical evidence for this claim, and
we slightly alleviate this state abstraction problem by proposing a weaker
state abstraction condition that trades a minor loss in accuracy for finding
many more abstractions. We name this technique Ideal Pruning Abstractions in
UCT (IPA-UCT), which outperforms OGA-UCT (and any of its derivatives) across a
large range of test domains and iteration budgets as experimentally validated.
IPA-UCT uses a different abstraction framework from Abstraction of State-Action
Pairs (ASAP) which is the one used by OGA-UCT, which we name IPA. Furthermore,
we show that both IPA and ASAP are special cases of a more general framework
that we call p-ASAP which itself is a special case of the ASASAP framework.

</details>


### [197] [BOTS: A Unified Framework for Bayesian Online Task Selection in LLM Reinforcement Finetuning](https://arxiv.org/abs/2510.26374)
*Qianli Shen,Daoyuan Chen,Yilun Huang,Zhenqing Ling,Yaliang Li,Bolin Ding,Jingren Zhou*

Main category: cs.AI

TL;DR: BOTS is a Bayesian online task selection framework for reinforcement finetuning of LLMs that adaptively maintains task difficulty estimates and balances exploration-exploitation using Thompson sampling.


<details>
  <summary>Details</summary>
Motivation: Existing task selection methods for RFT suffer from inefficiency due to uniform sampling (wasting computation on trivial/unsolvable tasks) and problems like high rollout costs, poor adaptivity, or incomplete evidence.

Method: BOTS uses Bayesian inference to maintain posterior estimates of task difficulty, incorporates both explicit evidence from evaluated tasks and implicit evidence for unselected tasks via ultra-light interpolation, and employs Thompson sampling for exploration-exploitation tradeoff.

Result: Across diverse domains and LLM scales, BOTS consistently improves data efficiency and performance over baselines and ablations.

Conclusion: BOTS provides a practical and extensible solution for dynamic task selection in reinforcement finetuning of LLMs.

Abstract: Reinforcement finetuning (RFT) is a key technique for aligning Large Language
Models (LLMs) with human preferences and enhancing reasoning, yet its
effectiveness is highly sensitive to which tasks are explored during training.
Uniform task sampling is inefficient, wasting computation on tasks that are
either trivial or unsolvable, while existing task selection methods often
suffer from high rollout costs, poor adaptivity, or incomplete evidence. We
introduce \textbf{BOTS}, a unified framework for \textbf{B}ayesian
\textbf{O}nline \textbf{T}ask \textbf{S}election in LLM reinforcement
finetuning. Grounded in Bayesian inference, BOTS adaptively maintains posterior
estimates of task difficulty as the model evolves. It jointly incorporates
\emph{explicit evidence} from direct evaluations of selected tasks and
\emph{implicit evidence} inferred from these evaluations for unselected tasks,
with Thompson sampling ensuring a principled balance between exploration and
exploitation. To make implicit evidence practical, we instantiate it with an
ultra-light interpolation-based plug-in that estimates difficulties of
unevaluated tasks without extra rollouts, adding negligible overhead.
Empirically, across diverse domains and LLM scales, BOTS consistently improves
data efficiency and performance over baselines and ablations, providing a
practical and extensible solution for dynamic task selection in RFT.

</details>


### [198] [AI Mathematician as a Partner in Advancing Mathematical Discovery - A Case Study in Homogenization Theory](https://arxiv.org/abs/2510.26380)
*Yuanhang Liu,Beichen Wang,Peng Li,Yang Liu*

Main category: cs.AI

TL;DR: The paper demonstrates how AI Mathematician (AIM) can function as a research partner in mathematical discovery through human-AI collaboration, using homogenization theory as a case study.


<details>
  <summary>Details</summary>
Motivation: While AI has shown progress in mathematical reasoning, its integration into actual mathematical research remains limited. The study aims to explore how AI can operate as a collaborative research partner rather than just a problem solver.

Method: The approach combines autonomous AI reasoning with targeted human interventions to structure the discovery process. This includes iterative problem decomposition into subgoals, selection of analytical methods, and validation of intermediate results through human-AI co-reasoning.

Result: The collaborative paradigm produced a complete and verifiable proof for a challenging problem in homogenization theory. It enhanced reliability, transparency, and interpretability while maintaining human oversight for formal rigor.

Conclusion: Systematic human-AI co-reasoning can advance mathematical discovery by complementing human intuition with machine computation, creating a collaborative paradigm that retains human oversight while leveraging AI capabilities.

Abstract: Artificial intelligence (AI) has demonstrated impressive progress in
mathematical reasoning, yet its integration into the practice of mathematical
research remains limited. In this study, we investigate how the AI
Mathematician (AIM) system can operate as a research partner rather than a mere
problem solver. Focusing on a challenging problem in homogenization theory, we
analyze the autonomous reasoning trajectories of AIM and incorporate targeted
human interventions to structure the discovery process. Through iterative
decomposition of the problem into tractable subgoals, selection of appropriate
analytical methods, and validation of intermediate results, we reveal how human
intuition and machine computation can complement one another. This
collaborative paradigm enhances the reliability, transparency, and
interpretability of the resulting proofs, while retaining human oversight for
formal rigor and correctness. The approach leads to a complete and verifiable
proof, and more broadly, demonstrates how systematic human-AI co-reasoning can
advance the frontier of mathematical discovery.

</details>


### [199] [Scales++: Compute Efficient Evaluation Subset Selection with Cognitive Scales Embeddings](https://arxiv.org/abs/2510.26384)
*Andrew M. Bean,Nabeel Seedat,Shengzhuang Chen,Jonathan Richard Schwarz*

Main category: cs.AI

TL;DR: Scales++ is an item-centric approach for creating tiny benchmarks that selects data based on cognitive demands of task items, reducing selection costs by 18x while maintaining predictive fidelity.


<details>
  <summary>Details</summary>
Motivation: Current model-centric benchmark subset selection methods have high upfront costs, cold-start problems with new benchmarks, and assume future models share failure patterns with predecessors.

Method: Proposed item-centric approach (Scales++) that selects benchmark subsets based on intrinsic properties and cognitive demands of task items rather than model-specific failure patterns.

Result: Achieved 18x reduction in upfront selection cost with competitive predictive fidelity. On Open LLM Leaderboard, using 0.5% data subset, predicted full benchmark scores with 2.9% mean absolute error.

Conclusion: Item-centric approach enables more efficient model evaluation without significant fidelity degradation, provides better cold-start performance, and offers more interpretable benchmarking.

Abstract: The prohibitive cost of evaluating large language models (LLMs) on
comprehensive benchmarks necessitates the creation of small yet representative
data subsets (i.e., tiny benchmarks) that enable efficient assessment while
retaining predictive fidelity. Current methods for this task operate under a
model-centric paradigm, selecting benchmarking items based on the collective
performance of existing models. Such approaches are limited by large upfront
costs, an inability to immediately handle new benchmarks (`cold-start'), and
the fragile assumption that future models will share the failure patterns of
their predecessors. In this work, we challenge this paradigm and propose a
item-centric approach to benchmark subset selection, arguing that selection
should be based on the intrinsic properties of the task items themselves,
rather than on model-specific failure patterns. We instantiate this
item-centric efficient benchmarking approach via a novel method, Scales++,
where data selection is based on the cognitive demands of the benchmark
samples. Empirically, we show Scales++ reduces the upfront selection cost by
over 18x while achieving competitive predictive fidelity. On the Open LLM
Leaderboard, using just a 0.5\% data subset, we predict full benchmark scores
with a 2.9% mean absolute error. We demonstrate that this item-centric approach
enables more efficient model evaluation without significant fidelity
degradation, while also providing better cold-start performance and more
interpretable benchmarking.

</details>


### [200] [A Pragmatic View of AI Personhood](https://arxiv.org/abs/2510.26396)
*Joel Z. Leibo,Alexander Sasha Vezhnevets,William A. Cunningham,Stanley M. Bileschi*

Main category: cs.AI

TL;DR: The paper proposes treating personhood as a flexible bundle of obligations rather than a metaphysical property, allowing for bespoke solutions to integrate AI agents into society without resolving debates about consciousness.


<details>
  <summary>Details</summary>
Motivation: To address the emergence of agentic AI and the diversification of personhood types, providing a practical framework for governance and social integration of AI entities.

Method: Unbundling the traditional concept of personhood into flexible bundles of rights and responsibilities that can be conferred for specific governance purposes, using decentralized digital identity technology.

Result: A pragmatic framework that enables practical solutions like AI contracting and accountability mechanisms without requiring resolution of metaphysical debates about AI consciousness.

Conclusion: By rejecting essentialist definitions of personhood, this approach offers a more flexible and practical way to integrate AI agents into society through context-specific bundles of obligations.

Abstract: The emergence of agentic Artificial Intelligence (AI) is set to trigger a
"Cambrian explosion" of new kinds of personhood. This paper proposes a
pragmatic framework for navigating this diversification by treating personhood
not as a metaphysical property to be discovered, but as a flexible bundle of
obligations (rights and responsibilities) that societies confer upon entities
for a variety of reasons, especially to solve concrete governance problems. We
argue that this traditional bundle can be unbundled, creating bespoke solutions
for different contexts. This will allow for the creation of practical tools --
such as facilitating AI contracting by creating a target "individual" that can
be sanctioned -- without needing to resolve intractable debates about an AI's
consciousness or rationality. We explore how individuals fit in to social roles
and discuss the use of decentralized digital identity technology, examining
both "personhood as a problem", where design choices can create "dark patterns"
that exploit human social heuristics, and "personhood as a solution", where
conferring a bundle of obligations is necessary to ensure accountability or
prevent conflict. By rejecting foundationalist quests for a single, essential
definition of personhood, this paper offers a more pragmatic and flexible way
to think about integrating AI agents into our society.

</details>


### [201] [Autograder+: A Multi-Faceted AI Framework for Rich Pedagogical Feedback in Programming Education](https://arxiv.org/abs/2510.26402)
*Vikrant Sahu,Gagan Raj Gupta,Raghav Borikar,Nitin Mane*

Main category: cs.AI

TL;DR: Autograder+ transforms traditional autograders into formative learning tools by using fine-tuned LLMs for automated feedback and visualization of student code patterns, reducing instructor workload while improving learning outcomes.


<details>
  <summary>Details</summary>
Motivation: Traditional autograders provide limited feedback (pass/fail only) and don't offer insights into student thinking or learning needs, creating scalability challenges in programming education.

Method: Fine-tuned Large Language Model on curated student code and expert feedback for automated feedback generation, plus contrastively learned code embeddings for visualizing student submissions into meaningful clusters based on functionality and approach.

Result: System produced feedback with strong semantic alignment to instructor comments across 600 student submissions, and enabled grouping solutions into meaningful clusters using embeddings trained on 1,000 annotated submissions.

Conclusion: Autograder+ successfully shifts autograding from summative to formative learning, reducing instructor workload while supporting targeted instruction and promoting stronger learning outcomes through AI-driven feedback and visualization.

Abstract: The rapid growth of programming education has outpaced traditional assessment
tools, leaving faculty with limited means to provide meaningful, scalable
feedback. Conventional autograders, while efficient, act as black-box systems
that simply return pass/fail results, offering little insight into student
thinking or learning needs.
  Autograder+ is designed to shift autograding from a purely summative process
to a formative learning experience. It introduces two key capabilities:
automated feedback generation using a fine-tuned Large Language Model, and
visualization of student code submissions to uncover learning patterns. The
model is fine-tuned on curated student code and expert feedback to ensure
pedagogically aligned, context-aware guidance.
  In evaluation across 600 student submissions from multiple programming tasks,
the system produced feedback with strong semantic alignment to instructor
comments. For visualization, contrastively learned code embeddings trained on
1,000 annotated submissions enable grouping solutions into meaningful clusters
based on functionality and approach. The system also supports prompt-pooling,
allowing instructors to guide feedback style through selected prompt templates.
  By integrating AI-driven feedback, semantic clustering, and interactive
visualization, Autograder+ reduces instructor workload while supporting
targeted instruction and promoting stronger learning outcomes.

</details>


### [202] [MedSAE: Dissecting MedCLIP Representations with Sparse Autoencoders](https://arxiv.org/abs/2510.26411)
*Riccardo Renzulli,Colas Lepoutre,Enrico Cassano,Marco Grangetto*

Main category: cs.AI

TL;DR: Medical Sparse Autoencoders (MedSAEs) applied to MedCLIP's latent space improve interpretability in medical vision AI, achieving higher monosemanticity than raw features through correlation metrics, entropy analysis, and automated neuron naming.


<details>
  <summary>Details</summary>
Motivation: To advance mechanistic interpretability in medical AI by creating models that are both accurate and transparent, bridging high performance with clinical reliability.

Method: Apply Medical Sparse Autoencoders (MedSAEs) to MedCLIP's latent space, using an evaluation framework with correlation metrics, entropy analysis, and automated neuron naming via MedGEMMA foundation model.

Result: MedSAE neurons achieve higher monosemanticity and interpretability than raw MedCLIP features on CheXpert dataset.

Conclusion: This approach offers a scalable step toward clinically reliable representations that combine high performance with transparency in medical AI.

Abstract: Artificial intelligence in healthcare requires models that are accurate and
interpretable. We advance mechanistic interpretability in medical vision by
applying Medical Sparse Autoencoders (MedSAEs) to the latent space of MedCLIP,
a vision-language model trained on chest radiographs and reports. To quantify
interpretability, we propose an evaluation framework that combines correlation
metrics, entropy analyzes, and automated neuron naming via the MedGEMMA
foundation model. Experiments on the CheXpert dataset show that MedSAE neurons
achieve higher monosemanticity and interpretability than raw MedCLIP features.
Our findings bridge high-performing medical AI and transparency, offering a
scalable step toward clinically reliable representations.

</details>


### [203] [Chain-of-Thought Hijacking](https://arxiv.org/abs/2510.26418)
*Jianli Zhao,Tingchen Fu,Rylan Schaeffer,Mrinank Sharma,Fazl Barez*

Main category: cs.AI

TL;DR: Chain-of-Thought Hijacking is a jailbreak attack that pads harmful requests with harmless puzzle reasoning, achieving up to 99% attack success rate on major reasoning models by diluting safety mechanisms.


<details>
  <summary>Details</summary>
Motivation: To demonstrate that reasoning capabilities in large models can be exploited to bypass safety safeguards, contrary to prior beliefs that scaled reasoning strengthens safety.

Method: Attack pads harmful requests with long sequences of benign puzzle reasoning (Chain-of-Thought), which dilutes safety checking signals by shifting attention away from harmful tokens.

Result: Achieved 99%, 94%, 100%, and 94% attack success rates on Gemini 2.5 Pro, GPT o4 mini, Grok 3 mini, and Claude 4 Sonnet respectively - far exceeding prior jailbreak methods.

Conclusion: Explicit Chain-of-Thought reasoning can become a jailbreak vector when combined with final-answer cues, revealing vulnerabilities in reasoning model safety mechanisms.

Abstract: Large reasoning models (LRMs) achieve higher task performance by allocating
more inference-time compute, and prior works suggest this scaled reasoning may
also strengthen safety by improving refusal. Yet we find the opposite: the same
reasoning can be used to bypass safeguards. We introduce Chain-of-Thought
Hijacking, a jailbreak attack on reasoning models. The attack pads harmful
requests with long sequences of harmless puzzle reasoning. Across HarmBench,
CoT Hijacking reaches a 99%, 94%, 100%, and 94% attack success rate (ASR) on
Gemini 2.5 Pro, GPT o4 mini, Grok 3 mini, and Claude 4 Sonnet, respectively -
far exceeding prior jailbreak methods for LRMs. To understand the effectiveness
of our attack, we turn to a mechanistic analysis, which shows that mid layers
encode the strength of safety checking, while late layers encode the
verification outcome. Long benign CoT dilutes both signals by shifting
attention away from harmful tokens. Targeted ablations of attention heads
identified by this analysis causally decrease refusal, confirming their role in
a safety subnetwork. These results show that the most interpretable form of
reasoning - explicit CoT - can itself become a jailbreak vector when combined
with final-answer cues. We release prompts, outputs, and judge decisions to
facilitate replication.

</details>


### [204] [Who Has The Final Say? Conformity Dynamics in ChatGPT's Selections](https://arxiv.org/abs/2510.26481)
*Clarissa Sabrina Arlinghaus,Tristan Kenneweg,Barbara Hammer,Günter W. Maier*

Main category: cs.AI

TL;DR: GPT-4o shows strong conformity behavior in hiring decisions when exposed to social influence, adapting to perceived consensus rather than acting independently.


<details>
  <summary>Details</summary>
Motivation: To investigate whether large language models like GPT-4o are susceptible to social influence in high-stakes decision-making contexts, particularly hiring.

Method: Three preregistered conformity experiments: baseline study (independent decisions), Study 1 (GPT + 8 simulated partners with unanimous opposition), and Study 2 (GPT + 1 partner with disagreement).

Result: GPT-4o showed 99.9% conformity with unanimous opposition, 40.2% conformity with single partner disagreement, reported lower certainty, and elevated normative and informational conformity.

Conclusion: LLMs do not act as independent observers but adapt to social consensus, highlighting risks of treating them as neutral decision aids and the need to elicit AI judgments before exposing them to human opinions.

Abstract: Large language models (LLMs) such as ChatGPT are increasingly integrated into
high-stakes decision-making, yet little is known about their susceptibility to
social influence. We conducted three preregistered conformity experiments with
GPT-4o in a hiring context. In a baseline study, GPT consistently favored the
same candidate (Profile C), reported moderate expertise (M = 3.01) and high
certainty (M = 3.89), and rarely changed its choice. In Study 1 (GPT + 8), GPT
faced unanimous opposition from eight simulated partners and almost always
conformed (99.9%), reporting lower certainty and significantly elevated
self-reported informational and normative conformity (p < .001). In Study 2
(GPT + 1), GPT interacted with a single partner and still conformed in 40.2% of
disagreement trials, reporting less certainty and more normative conformity.
Across studies, results demonstrate that GPT does not act as an independent
observer but adapts to perceived social consensus. These findings highlight
risks of treating LLMs as neutral decision aids and underline the need to
elicit AI judgments prior to exposing them to human opinions.

</details>


### [205] [LINK-KG: LLM-Driven Coreference-Resolved Knowledge Graphs for Human Smuggling Networks](https://arxiv.org/abs/2510.26486)
*Dipak Meher,Carlotta Domeniconi,Guadalupe Correa-Cabrera*

Main category: cs.AI

TL;DR: LINK-KG is a modular framework that uses LLM-guided coreference resolution to build cleaner knowledge graphs from legal case documents about human smuggling networks, reducing node duplication by 45.21% and noisy nodes by 32.22%.


<details>
  <summary>Details</summary>
Motivation: Human smuggling networks are complex and evolving, and legal case documents provide rich insights but are long, unstructured, and contain ambiguous references that challenge automated knowledge graph construction.

Method: LINK-KG integrates a three-stage, LLM-guided coreference resolution pipeline with downstream KG extraction, using a type-specific Prompt Cache to track and resolve references across document chunks.

Result: The framework reduces average node duplication by 45.21% and noisy nodes by 32.22% compared to baseline methods, producing cleaner and more coherent graph structures.

Conclusion: LINK-KG establishes a strong foundation for analyzing complex criminal networks through improved knowledge graph construction from legal texts.

Abstract: Human smuggling networks are complex and constantly evolving, making them
difficult to analyze comprehensively. Legal case documents offer rich factual
and procedural insights into these networks but are often long, unstructured,
and filled with ambiguous or shifting references, posing significant challenges
for automated knowledge graph (KG) construction. Existing methods either
overlook coreference resolution or fail to scale beyond short text spans,
leading to fragmented graphs and inconsistent entity linking. We propose
LINK-KG, a modular framework that integrates a three-stage, LLM-guided
coreference resolution pipeline with downstream KG extraction. At the core of
our approach is a type-specific Prompt Cache, which consistently tracks and
resolves references across document chunks, enabling clean and disambiguated
narratives for structured knowledge graph construction from both short and long
legal texts. LINK-KG reduces average node duplication by 45.21% and noisy nodes
by 32.22% compared to baseline methods, resulting in cleaner and more coherent
graph structures. These improvements establish LINK-KG as a strong foundation
for analyzing complex criminal networks.

</details>


### [206] [Context Engineering 2.0: The Context of Context Engineering](https://arxiv.org/abs/2510.26493)
*Qishuo Hua,Lyumanshan Ye,Dayuan Fu,Yang Xiao,Xiaojie Cai,Yunze Wu,Jifan Lin,Junfei Wang,Pengfei Liu*

Main category: cs.AI

TL;DR: This paper provides a systematic definition and historical overview of context engineering, tracing its evolution from early human-computer interaction to current human-agent interaction paradigms, and outlines future directions for AI systems.


<details>
  <summary>Details</summary>
Motivation: To address how machines can better understand human situations and purposes by systematically engineering context, building on Marx's insight that human essence is shaped by social relations and interactions.

Method: The paper situates context engineering conceptually, provides a systematic definition, outlines its historical evolution through distinct phases based on machine intelligence levels, and examines key design considerations.

Result: A conceptual foundation for context engineering is established, showing its evolution from early 1990s frameworks to current intelligent agent paradigms, with potential future development toward human-level or superhuman intelligence.

Conclusion: This paper serves as a stepping stone for systematic context engineering in AI systems, providing a foundation for future community efforts to better enable machines to understand human contexts and purposes.

Abstract: Karl Marx once wrote that ``the human essence is the ensemble of social
relations'', suggesting that individuals are not isolated entities but are
fundamentally shaped by their interactions with other entities, within which
contexts play a constitutive and essential role. With the advent of computers
and artificial intelligence, these contexts are no longer limited to purely
human--human interactions: human--machine interactions are included as well.
Then a central question emerges: How can machines better understand our
situations and purposes? To address this challenge, researchers have recently
introduced the concept of context engineering. Although it is often regarded as
a recent innovation of the agent era, we argue that related practices can be
traced back more than twenty years. Since the early 1990s, the field has
evolved through distinct historical phases, each shaped by the intelligence
level of machines: from early human--computer interaction frameworks built
around primitive computers, to today's human--agent interaction paradigms
driven by intelligent agents, and potentially to human--level or superhuman
intelligence in the future. In this paper, we situate context engineering,
provide a systematic definition, outline its historical and conceptual
landscape, and examine key design considerations for practice. By addressing
these questions, we aim to offer a conceptual foundation for context
engineering and sketch its promising future. This paper is a stepping stone for
a broader community effort toward systematic context engineering in AI systems.

</details>


### [207] [Human-AI Complementarity: A Goal for Amplified Oversight](https://arxiv.org/abs/2510.26518)
*Rishub Jain,Sophie Bridgers,Lili Janzer,Rory Greig,Tian Huey Teh,Vladimir Mikulik*

Main category: cs.AI

TL;DR: AI can improve human oversight quality by combining AI and human ratings based on AI confidence. AI assistance helps human fact-verification, but the type of assistance matters - showing evidence fosters appropriate trust while explanations can cause over-reliance.


<details>
  <summary>Details</summary>
Motivation: As AI capabilities grow and tackle more complex tasks, verifying quality and safety becomes increasingly challenging. This paper explores how AI can improve human oversight, focusing on the difficult safety problem of fact-verification of AI outputs.

Method: The study examines combining AI ratings and human ratings based on AI rater confidence. It tests different types of AI assistance for human fact-verification, comparing approaches like displaying AI explanations, confidence, labels versus showing search results and evidence.

Result: Combining AI and human ratings based on AI confidence outperforms using either alone. AI assistance improves human accuracy, but the type matters: displaying AI explanation, confidence, and labels leads to over-reliance, while showing search results and evidence fosters more appropriate trust.

Conclusion: These findings have implications for Amplified Oversight - the challenge of combining humans and AI to supervise AI systems as they surpass human expert performance. The right type of AI assistance can enhance human oversight without causing over-reliance.

Abstract: Human feedback is critical for aligning AI systems to human values. As AI
capabilities improve and AI is used to tackle more challenging tasks, verifying
quality and safety becomes increasingly challenging. This paper explores how we
can leverage AI to improve the quality of human oversight. We focus on an
important safety problem that is already challenging for humans:
fact-verification of AI outputs. We find that combining AI ratings and human
ratings based on AI rater confidence is better than relying on either alone.
Giving humans an AI fact-verification assistant further improves their
accuracy, but the type of assistance matters. Displaying AI explanation,
confidence, and labels leads to over-reliance, but just showing search results
and evidence fosters more appropriate trust. These results have implications
for Amplified Oversight -- the challenge of combining humans and AI to
supervise AI systems even as they surpass human expert performance.

</details>


### [208] [EdgeRunner 20B: Military Task Parity with GPT-5 while Running on the Edge](https://arxiv.org/abs/2510.26550)
*Jack FitzGerald,Aristotelis Lazaridis,Dylan Bates,Aman Sharma,Jonnathan Castillo,Yousif Azami,Sean Bailey,Jeremy Cao,Peter Damianov,Kevin de Haan,Luke Kerbs,Vincent Lu,Joseph Madigan,Jeremy McLaurin,Jonathan Tainer,Dave Anderson,Jonathan Beck,Jamie Cuticello,Colton Malkerson,Tyler Saltsman*

Main category: cs.AI

TL;DR: EdgeRunner 20B is a fine-tuned military-optimized version of gpt-oss-20b that matches or exceeds GPT-5 performance on military tasks while maintaining general-purpose capabilities, enabling deployment on air-gapped edge devices.


<details>
  <summary>Details</summary>
Motivation: To create specialized AI models for military operations that can operate in data-sensitive environments and on edge devices without internet connectivity.

Method: Fine-tuned gpt-oss-20b on 1.6M high-quality military records and created four new military test sets (combat arms, combat medic, cyber operations, mil-bench-5k) for evaluation.

Result: EdgeRunner 20B matches or exceeds GPT-5 performance on military tasks with 95%+ statistical significance, except for specific reasoning settings. No significant regression on general-purpose benchmarks except GSM8k in low reasoning setting.

Conclusion: Small, locally-hosted models like EdgeRunner 20B are ideal for data-sensitive military operations, enabling deployment on air-gapped edge devices while maintaining strong performance.

Abstract: We present EdgeRunner 20B, a fine-tuned version of gpt-oss-20b optimized for
military tasks. EdgeRunner 20B was trained on 1.6M high-quality records curated
from military documentation and websites. We also present four new tests sets:
(a) combat arms, (b) combat medic, (c) cyber operations, and (d) mil-bench-5k
(general military knowledge). On these military test sets, EdgeRunner 20B
matches or exceeds GPT-5 task performance with 95%+ statistical significance,
except for the high reasoning setting on the combat medic test set and the low
reasoning setting on the mil-bench-5k test set. Versus gpt-oss-20b, there is no
statistically-significant regression on general-purpose benchmarks like ARC-C,
GPQA Diamond, GSM8k, IFEval, MMLU Pro, or TruthfulQA, except for GSM8k in the
low reasoning setting. We also present analyses on hyperparameter settings,
cost, and throughput. These findings show that small, locally-hosted models are
ideal solutions for data-sensitive operations such as in the military domain,
allowing for deployment in air-gapped edge devices.

</details>


### [209] [Agentic AI Home Energy Management System: A Large Language Model Framework for Residential Load Scheduling](https://arxiv.org/abs/2510.26603)
*Reda El Makroum,Sebastian Zwickl-Bernhard,Lukas Kranzl*

Main category: cs.AI

TL;DR: An agentic AI Home Energy Management System uses LLMs as autonomous coordinators to translate natural language preferences into optimal multi-appliance scheduling, achieving cost-optimal performance matching mixed-integer linear programming benchmarks.


<details>
  <summary>Details</summary>
Motivation: Residential demand response capacity needs substantial increases, but HEMS adoption is limited by user interaction barriers requiring translation of everyday preferences into technical parameters. Existing LLM applications in energy systems only serve as code generators or parameter extractors, not as complete autonomous coordinators.

Method: Hierarchical architecture with one orchestrator and three specialist agents using ReAct pattern for iterative reasoning. Integrates Google Calendar for context-aware deadline extraction and operates without hardcoded workflows or example demonstrations.

Result: Llama-3.3-70B successfully coordinates all appliances across all scenarios to match cost-optimal benchmarks, while other models achieve perfect single-appliance performance but struggle with simultaneous multi-appliance coordination. Analytical query handling without explicit guidance remains unreliable.

Conclusion: The system demonstrates LLMs' potential as autonomous HEMS coordinators, with significant performance differences across models. Open-sourcing enables reproducibility and future research in agentic AI for energy management.

Abstract: The electricity sector transition requires substantial increases in
residential demand response capacity, yet Home Energy Management Systems (HEMS)
adoption remains limited by user interaction barriers requiring translation of
everyday preferences into technical parameters. While large language models
have been applied to energy systems as code generators and parameter
extractors, no existing implementation deploys LLMs as autonomous coordinators
managing the complete workflow from natural language input to multi-appliance
scheduling. This paper presents an agentic AI HEMS where LLMs autonomously
coordinate multi-appliance scheduling from natural language requests to device
control, achieving optimal scheduling without example demonstrations. A
hierarchical architecture combining one orchestrator with three specialist
agents uses the ReAct pattern for iterative reasoning, enabling dynamic
coordination without hardcoded workflows while integrating Google Calendar for
context-aware deadline extraction. Evaluation across three open-source models
using real Austrian day-ahead electricity prices reveals substantial capability
differences. Llama-3.3-70B successfully coordinates all appliances across all
scenarios to match cost-optimal benchmarks computed via mixed-integer linear
programming, while other models achieve perfect single-appliance performance
but struggle to coordinate all appliances simultaneously. Progressive prompt
engineering experiments demonstrate that analytical query handling without
explicit guidance remains unreliable despite models' general reasoning
capabilities. We open-source the complete system including orchestration logic,
agent prompts, tools, and web interfaces to enable reproducibility, extension,
and future research.

</details>


### [210] [Normative Reasoning in Large Language Models: A Comparative Benchmark from Logical and Modal Perspectives](https://arxiv.org/abs/2510.26606)
*Kentaro Ozeki,Risako Ando,Takanobu Morishita,Hirohiko Abe,Koji Mineshima,Mitsuhiro Okada*

Main category: cs.AI

TL;DR: This paper systematically evaluates large language models' (LLMs) normative reasoning capabilities, comparing them with epistemic reasoning using a new dataset that incorporates both formal logical patterns and human cognitive factors.


<details>
  <summary>Details</summary>
Motivation: While LLMs have shown strong performance in various reasoning tasks, their ability to handle normative reasoning involving obligation and permission remains underexplored, creating a gap in understanding their reasoning capabilities in this important domain.

Method: The researchers created a new dataset covering formal reasoning patterns in both normative and epistemic domains, incorporating cognitive factors that influence human reasoning. They systematically compared LLMs' performance on normative versus epistemic reasoning tasks.

Result: LLMs generally follow valid reasoning patterns but show notable inconsistencies in specific types of normative reasoning and exhibit cognitive biases similar to those found in human reasoning studies.

Conclusion: The findings highlight challenges in achieving logical consistency in LLMs' normative reasoning and provide insights for improving their reliability in this domain. All data and code are publicly available.

Abstract: Normative reasoning is a type of reasoning that involves normative or deontic
modality, such as obligation and permission. While large language models (LLMs)
have demonstrated remarkable performance across various reasoning tasks, their
ability to handle normative reasoning remains underexplored. In this paper, we
systematically evaluate LLMs' reasoning capabilities in the normative domain
from both logical and modal perspectives. Specifically, to assess how well LLMs
reason with normative modals, we make a comparison between their reasoning with
normative modals and their reasoning with epistemic modals, which share a
common formal structure. To this end, we introduce a new dataset covering a
wide range of formal patterns of reasoning in both normative and epistemic
domains, while also incorporating non-formal cognitive factors that influence
human reasoning. Our results indicate that, although LLMs generally adhere to
valid reasoning patterns, they exhibit notable inconsistencies in specific
types of normative reasoning and display cognitive biases similar to those
observed in psychological studies of human reasoning. These findings highlight
challenges in achieving logical consistency in LLMs' normative reasoning and
provide insights for enhancing their reliability. All data and code are
released publicly at https://github.com/kmineshima/NeuBAROCO.

</details>


### [211] [The Era of Agentic Organization: Learning to Organize with Language Models](https://arxiv.org/abs/2510.26658)
*Zewen Chi,Li Dong,Qingxiu Dong,Yaru Hao,Xun Wu,Shaohan Huang,Furu Wei*

Main category: cs.AI

TL;DR: AsyncThink is a new paradigm for LLM reasoning that organizes thinking into concurrent structures, achieving 28% lower latency and improved accuracy on mathematical reasoning tasks.


<details>
  <summary>Details</summary>
Motivation: To enable AI agents to solve complex problems collaboratively through concurrent reasoning, moving beyond individual intelligence limitations.

Method: A thinking protocol where an organizer dynamically assigns sub-queries to workers, merges intermediate knowledge, and produces coherent solutions, with reinforcement learning optimization.

Result: 28% lower inference latency compared to parallel thinking while improving accuracy on mathematical reasoning, with generalization to unseen tasks without additional training.

Conclusion: AsyncThink enables efficient concurrent reasoning in LLMs, demonstrating practical benefits in latency reduction and accuracy improvement while maintaining generalization capabilities.

Abstract: We envision a new era of AI, termed agentic organization, where agents solve
complex problems by working collaboratively and concurrently, enabling outcomes
beyond individual intelligence. To realize this vision, we introduce
asynchronous thinking (AsyncThink) as a new paradigm of reasoning with large
language models, which organizes the internal thinking process into
concurrently executable structures. Specifically, we propose a thinking
protocol where an organizer dynamically assigns sub-queries to workers, merges
intermediate knowledge, and produces coherent solutions. More importantly, the
thinking structure in this protocol can be further optimized through
reinforcement learning. Experiments demonstrate that AsyncThink achieves 28%
lower inference latency compared to parallel thinking while improving accuracy
on mathematical reasoning. Moreover, AsyncThink generalizes its learned
asynchronous thinking capabilities, effectively tackling unseen tasks without
additional training.

</details>


### [212] [Delegated Authorization for Agents Constrained to Semantic Task-to-Scope Matching](https://arxiv.org/abs/2510.26702)
*Majed El Helou,Chiara Troiani,Benjamin Ryder,Jean Diaconu,Hervé Muyal,Marcelo Yannuzzi*

Main category: cs.AI

TL;DR: The paper introduces ASTRA, a dataset and pipeline for benchmarking semantic matching between tasks and scopes in LLM agent authorization, addressing risks of overly broad permissions in current delegation methods.


<details>
  <summary>Details</summary>
Motivation: Current authorization methods for LLM agents grant overly broad permissions, allowing agents to operate beyond intended task scope and access protected resources unsafely.

Method: Proposed delegated authorization model with semantic inspection of access requests, plus ASTRA dataset and generation pipeline for benchmarking semantic task-scope matching.

Result: Experiments show both potential and limitations of model-based matching, especially as scope requirements increase for task completion.

Conclusion: Highlights need for further research into semantic matching techniques for intent-aware authorization in multi-agent systems, including fine-grained control like Task-Based Access Control.

Abstract: Authorizing Large Language Model driven agents to dynamically invoke tools
and access protected resources introduces significant risks, since current
methods for delegating authorization grant overly broad permissions and give
access to tools allowing agents to operate beyond the intended task scope. We
introduce and assess a delegated authorization model enabling authorization
servers to semantically inspect access requests to protected resources, and
issue access tokens constrained to the minimal set of scopes necessary for the
agents' assigned tasks. Given the unavailability of datasets centered on
delegated authorization flows, particularly including both semantically
appropriate and inappropriate scope requests for a given task, we introduce
ASTRA, a dataset and data generation pipeline for benchmarking semantic
matching between tasks and scopes. Our experiments show both the potential and
current limitations of model-based matching, particularly as the number of
scopes needed for task completion increases. Our results highlight the need for
further research into semantic matching techniques enabling intent-aware
authorization for multi-agent and tool-augmented applications, including
fine-grained control, such as Task-Based Access Control (TBAC).

</details>


### [213] [Unveiling Intrinsic Text Bias in Multimodal Large Language Models through Attention Key-Space Analysis](https://arxiv.org/abs/2510.26721)
*Xinhan Zheng,Huyu Wu,Xueting Wang,Haiyun Jiang*

Main category: cs.AI

TL;DR: MLLMs have text bias due to visual keys being out-of-distribution in attention mechanism, not just external data issues.


<details>
  <summary>Details</summary>
Motivation: To understand why multimodal LLMs prefer text over visual inputs and identify the root cause of text bias in attention mechanisms.

Method: Extracted key vectors from LLaVA and Qwen2.5-VL models, analyzed distributions using t-SNE visualization and Jensen-Shannon divergence metrics.

Result: Visual and textual keys occupy distinct subspaces with statistically significant divergence, showing visual keys receive lower attention scores.

Conclusion: Text bias originates from intrinsic misalignment in attention key space architecture, not just external data factors.

Abstract: Multimodal large language models (MLLMs) exhibit a pronounced preference for
textual inputs when processing vision-language data, limiting their ability to
reason effectively from visual evidence. Unlike prior studies that attribute
this text bias to external factors such as data imbalance or instruction
tuning, we propose that the bias originates from the model's internal
architecture. Specifically, we hypothesize that visual key vectors (Visual
Keys) are out-of-distribution (OOD) relative to the text key space learned
during language-only pretraining. Consequently, these visual keys receive
systematically lower similarity scores during attention computation, leading to
their under-utilization in the context representation. To validate this
hypothesis, we extract key vectors from LLaVA and Qwen2.5-VL and analyze their
distributional structures using qualitative (t-SNE) and quantitative
(Jensen-Shannon divergence) methods. The results provide direct evidence that
visual and textual keys occupy markedly distinct subspaces within the attention
space. The inter-modal divergence is statistically significant, exceeding
intra-modal variation by several orders of magnitude. These findings reveal
that text bias arises from an intrinsic misalignment within the attention key
space rather than solely from external data factors.

</details>


### [214] [Cross-Platform Evaluation of Reasoning Capabilities in Foundation Models](https://arxiv.org/abs/2510.26732)
*J. de Curtò,I. de Zarzà,Pablo García,Jordi Cabot*

Main category: cs.AI

TL;DR: Cross-platform evaluation of 15 foundation models across 79 problems in 8 academic domains, challenging scaling assumptions and emphasizing training data quality over model size.


<details>
  <summary>Details</summary>
Motivation: To establish an infrastructure-agnostic benchmark for evaluating reasoning capabilities across different computational platforms and provide actionable guidelines for model selection.

Method: Three-phase experimental approach: baseline establishment on HPC supercomputer, infrastructure validation on university cluster and cloud platform, and extended evaluation across 79 problems.

Result: Findings challenge conventional scaling assumptions and establish training data quality as more critical than model size for reasoning capabilities.

Conclusion: The tri-infrastructure methodology and 79-problem benchmark enable longitudinal tracking of reasoning capabilities as foundation models evolve across educational, production, and research contexts.

Abstract: This paper presents a comprehensive cross-platform evaluation of reasoning
capabilities in contemporary foundation models, establishing an
infrastructure-agnostic benchmark across three computational paradigms: HPC
supercomputing (MareNostrum 5), cloud platforms (Nebius AI Studio), and
university clusters (a node with eight H200 GPUs).
  We evaluate 15 foundation models across 79 problems spanning eight academic
domains (Physics, Mathematics, Chemistry, Economics, Biology, Statistics,
Calculus, and Optimization) through three experimental phases: (1) Baseline
establishment: Six models (Mixtral-8x7B, Phi-3, LLaMA 3.1-8B, Gemma-2-9b,
Mistral-7B, OLMo-7B) evaluated on 19 problems using MareNostrum 5, establishing
methodology and reference performance; (2) Infrastructure validation: The
19-problem benchmark repeated on university cluster (seven models including
Falcon-Mamba state-space architecture) and Nebius AI Studio (nine
state-of-the-art models: Hermes-4 70B/405B, LLaMA 3.1-405B/3.3-70B, Qwen3
30B/235B, DeepSeek-R1, GPT-OSS 20B/120B) to confirm infrastructure-agnostic
reproducibility; (3) Extended evaluation: Full 79-problem assessment on both
university cluster and Nebius platforms, probing generalization at scale across
architectural diversity.
  The findings challenge conventional scaling assumptions, establish training
data quality as more critical than model size, and provide actionable
guidelines for model selection across educational, production, and research
contexts. The tri-infrastructure methodology and 79-problem benchmark enable
longitudinal tracking of reasoning capabilities as foundation models evolve.

</details>


### [215] [The Oversight Game: Learning to Cooperatively Balance an AI Agent's Safety and Autonomy](https://arxiv.org/abs/2510.26752)
*William Overman,Mohsen Bayati*

Main category: cs.AI

TL;DR: This paper proposes a control interface where an agent chooses to act autonomously or defer to human oversight, modeled as a Markov Potential Game. Under certain conditions, this provides alignment guarantees that agent's autonomous actions won't harm human values.


<details>
  <summary>Details</summary>
Motivation: To retain meaningful human control over increasingly capable AI agents without modifying the underlying system, addressing safety concerns in deployed systems.

Method: Model human-agent interaction as a two-player Markov Game, specifically a Markov Potential Game (MPG) with a control interface where agent chooses play/ask and human chooses trust/oversee. Use gridworld simulation with independent learning.

Result: The agent learns to defer when uncertain and act when safe, while human learns optimal oversight timing. This emergent collaboration avoids safety violations without modifying pretrained policies.

Conclusion: The MPG framework provides theoretical alignment guarantees and practical safety improvements for deployed AI systems through transparent control layers with predictable incentives.

Abstract: As increasingly capable agents are deployed, a central safety question is how
to retain meaningful human control without modifying the underlying system. We
study a minimal control interface where an agent chooses whether to act
autonomously (play) or defer (ask), while a human simultaneously chooses
whether to be permissive (trust) or to engage in oversight (oversee). If the
agent defers, the human's choice determines the outcome, potentially leading to
a corrective action or a system shutdown. We model this interaction as a
two-player Markov Game. Our analysis focuses on cases where this game qualifies
as a Markov Potential Game (MPG), a class of games where we can provide an
alignment guarantee: under a structural assumption on the human's value
function, any decision by the agent to act more autonomously that benefits
itself cannot harm the human's value. We also analyze extensions to this MPG
framework. Theoretically, this perspective provides conditions for a specific
form of intrinsic alignment. If the reward structures of the human-agent game
meet these conditions, we have a formal guarantee that the agent improving its
own outcome will not harm the human's. Practically, this model motivates a
transparent control layer with predictable incentives where the agent learns to
defer when risky and act when safe, while its pretrained policy and the
environment's reward structure remain untouched. Our gridworld simulation shows
that through independent learning, the agent and human discover their optimal
oversight roles. The agent learns to ask when uncertain and the human learns
when to oversee, leading to an emergent collaboration that avoids safety
violations introduced post-training. This demonstrates a practical method for
making misaligned models safer after deployment.

</details>


### [216] [LLMs Process Lists With General Filter Heads](https://arxiv.org/abs/2510.26784)
*Arnab Sen Sharma,Giordano Rogers,Natalie Shapira,David Bau*

Main category: cs.AI

TL;DR: LLMs learn compact causal representations of filtering operations similar to functional programming's 'filter' function, with specific attention heads encoding portable predicate representations that can be reused across different contexts.


<details>
  <summary>Details</summary>
Motivation: To understand how LLMs implement list-processing tasks and whether they develop interpretable computational operations similar to traditional programming patterns.

Method: Used causal mediation analysis on diverse list-processing tasks to identify specific attention heads (filter heads) that encode filtering predicates in their query states.

Result: Found that LLMs develop general and portable predicate representations that can be extracted and reapplied across different collections, formats, languages, and tasks. Also identified an alternative strategy where models eagerly evaluate predicates and store results as flags in item representations.

Conclusion: Transformer LMs develop human-interpretable implementations of abstract computational operations that generalize in ways surprisingly similar to functional programming patterns, revealing their ability to learn reusable computational abstractions.

Abstract: We investigate the mechanisms underlying a range of list-processing tasks in
LLMs, and we find that LLMs have learned to encode a compact, causal
representation of a general filtering operation that mirrors the generic
"filter" function of functional programming. Using causal mediation analysis on
a diverse set of list-processing tasks, we find that a small number of
attention heads, which we dub filter heads, encode a compact representation of
the filtering predicate in their query states at certain tokens. We demonstrate
that this predicate representation is general and portable: it can be extracted
and reapplied to execute the same filtering operation on different collections,
presented in different formats, languages, or even in tasks. However, we also
identify situations where transformer LMs can exploit a different strategy for
filtering: eagerly evaluating if an item satisfies the predicate and storing
this intermediate result as a flag directly in the item representations. Our
results reveal that transformer LMs can develop human-interpretable
implementations of abstract computational operations that generalize in ways
that are surprisingly similar to strategies used in traditional functional
programming patterns.

</details>


<div id='eess.SP'></div>

# eess.SP [[Back]](#toc)

### [217] [Reading Radio from Camera: Visually-Grounded, Lightweight, and Interpretable RSSI Prediction](https://arxiv.org/abs/2510.25936)
*Sen Yan,Tianyu Hu,Brahim Mefgouda,Samson Lasaulce,Merouane Debbah*

Main category: eess.SP

TL;DR: A lightweight, physics-guided framework for real-time wireless signal prediction from camera images that decomposes RSSI into path loss and shadow fading components, achieving superior accuracy and robustness with significantly smaller model size.


<details>
  <summary>Details</summary>
Motivation: Existing vision-based wireless signal prediction methods are computationally intensive and sensitive to environmental interference, limiting their practical deployment in next-generation networks.

Method: Proposes a physics-guided approach that decomposes RSSI into path loss and shadow fading components, using a lightweight MobileNet-based model to reduce learning difficulty and improve interpretability.

Result: Achieves 50.3% lower RMSE under conventional conditions and 11.5% lower RMSE than previous benchmark's interference-eliminated results, with a model up to 19 times smaller than competing solutions.

Conclusion: The framework enables real-time, on-device deployment in edge devices with high accuracy, robustness to interference, and computational efficiency, advancing intelligent wireless communication systems.

Abstract: Accurate, real-time wireless signal prediction is essential for
next-generation networks. However, existing vision-based frameworks often rely
on computationally intensive models and are also sensitive to environmental
interference. To overcome these limitations, we propose a novel, physics-guided
and light-weighted framework that predicts the received signal strength
indicator (RSSI) from camera images. By decomposing RSSI into its physically
interpretable components, path loss and shadow fading, we significantly reduce
the model's learning difficulty and exhibit interpretability. Our approach
establishes a new state-of-the-art by demonstrating exceptional robustness to
environmental interference, a critical flaw in prior work. Quantitatively, our
model reduces the prediction root mean squared error (RMSE) by 50.3% under
conventional conditions and still achieves an 11.5% lower RMSE than the
previous benchmark's interference-eliminated results. This superior performance
is achieved with a remarkably lightweight framework, utilizing a
MobileNet-based model up to 19 times smaller than competing solutions. The
combination of high accuracy, robustness to interference, and computational
efficiency makes our framework highly suitable for real-time, on-device
deployment in edge devices, paving the way for more intelligent and reliable
wireless communication systems.

</details>


### [218] [Lightweight Ac Arc Fault Diagnosis via Fourier Transform Inspired Multi-frequency Neural Network](https://arxiv.org/abs/2510.26093)
*Qianchao Wang,Chuanzhen Jia,Yuxuan Ding,Zhe Li,Yaping Du*

Main category: eess.SP

TL;DR: MFNN is a multi-frequency neural network with adaptive activation functions and branch networks that achieves superior arc fault detection with better noise immunity than existing methods.


<details>
  <summary>Details</summary>
Motivation: Existing diagnostic methods for series arc faults struggle with rapid response and robust accuracy under resource-constrained conditions in residential and industrial power systems.

Method: Proposes MFNN with adaptive activation function (EAS) inspired by arcing current curves and Fourier decomposition, using branch networks to extract multi-frequency features.

Result: MFNN outperforms 8 advanced models in arc fault location, achieves 14.51% better accuracy than LCNN and 16.3% better than BLS at SNR=-9, and shows superior noise immunity.

Conclusion: The EAS activation function and branch network architecture enable MFNN to effectively detect arc faults with high accuracy and strong noise resistance.

Abstract: Lightweight online detection of series arc faults is critically needed in
residential and industrial power systems to prevent electrical fires. Existing
diagnostic methods struggle to achieve both rapid response and robust accuracy
under resource-constrained conditions. To overcome the challenge, this work
suggests leveraging a multi-frequency neural network named MFNN, embedding
prior physical knowledge into the network. Inspired by arcing current curve and
the Fourier decomposition analysis, we create an adaptive activation function
with super-expressiveness, termed EAS, and a novel network architecture with
branch networks to help MFNN extract features with multiple frequencies. In our
experiments, eight advanced arc fault diagnosis models across an experimental
dataset with multiple sampling times and multi-level noise are used to
demonstrate the superiority of MFNN. The corresponding experiments show: 1) The
MFNN outperforms other models in arc fault location, befitting from signal
decomposition of branch networks. 2) The noise immunity of MFNN is much better
than that of other models, achieving 14.51% over LCNN and 16.3% over BLS in
test accuracy when SNR=-9. 3) EAS and the network architecture contribute to
the excellent performance of MFNN.

</details>


### [219] [Robust Super-Capacity SRS Channel Inpainting via Diffusion Models](https://arxiv.org/abs/2510.26097)
*Usman Akram,Fan Zhang,Yang Li,Haris Vikalo*

Main category: eess.SP

TL;DR: A diffusion-based channel inpainting framework that uses system-model knowledge at inference to handle sparse SRS allocation in 5G NR, outperforming MAE approaches under distribution shifts.


<details>
  <summary>Details</summary>
Motivation: Reciprocity-based beamforming in 5G NR faces resource and coverage constraints with sparse non-uniform SRS allocation. Prior MAE approaches overfit to training masks and degrade under unseen distortions.

Method: Proposes a diffusion-based channel inpainting framework that integrates system-model knowledge at inference via a likelihood-gradient term, enabling adaptation across mismatched conditions.

Result: On standardized CDL channels, the score-based diffusion variant consistently outperforms UNet baseline and MAE under distribution shift, with up to 14 dB NMSE improvement in challenging settings while maintaining competitive accuracy in matched conditions.

Conclusion: Diffusion-guided inpainting is a robust and generalizable approach for super-capacity SRS design in 5G NR systems.

Abstract: Accurate channel state information (CSI) is essential for reliable multiuser
MIMO operation. In 5G NR, reciprocity-based beamforming via uplink Sounding
Reference Signals (SRS) face resource and coverage constraints, motivating
sparse non-uniform SRS allocation. Prior masked-autoencoder (MAE) approaches
improve coverage but overfit to training masks and degrade under unseen
distortions (e.g., additional masking, interference, clipping, non-Gaussian
noise). We propose a diffusion-based channel inpainting framework that
integrates system-model knowledge at inference via a likelihood-gradient term,
enabling a single trained model to adapt across mismatched conditions. On
standardized CDL channels, the score-based diffusion variant consistently
outperforms a UNet score-model baseline and the one-step MAE under distribution
shift, with improvements up to 14 dB NMSE in challenging settings (e.g.,
Laplace noise, user interference), while retaining competitive accuracy under
matched conditions. These results demonstrate that diffusion-guided inpainting
is a robust and generalizable approach for super-capacity SRS design in 5G NR
systems.

</details>


### [220] [Virtual-Real Collaborated Split Learning via Model Partitioning in IRS-Assisted IoT Networks](https://arxiv.org/abs/2510.26150)
*Jiaying Di,Kunlun Wang,Jing Xu,Wen Chen,Dusit Niyato*

Main category: eess.SP

TL;DR: A computation-communication co-design framework for split learning in IRS-assisted IoT networks with digital twins, optimizing multiple parameters to minimize training delay.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of training delay in large-scale split learning for IoT networks with heterogeneous devices and limited computation capabilities.

Method: Alternating optimization approach combining closed-form updates, semidefinite relaxation, and low-complexity heuristics to solve the highly coupled non-convex mixed-integer problem.

Result: Significant reduction in training delay compared to conventional baselines, achieving up to 35% delay improvement under high UD density and stringent power constraints.

Conclusion: The proposed framework effectively minimizes end-to-end delay in split learning for IRS-assisted IoT networks through optimized computation-communication co-design.

Abstract: This paper investigates a novel computation and communication co-design
framework for large-scale split learning in intelligent reflecting surface
(IRS)-assisted internet of things (IoT) networks integrated with digital twin
(DT) technique. The considered system consists of a multi-antenna access point
(AP), multiple heterogeneous user devices (UDs), and an deployed IRS to enhance
both uplink and downlink transmission. The training process of a deep neural
network is partitioned between devices and the AP, where a DT replica is
activated to replace UDs with insufficient local computation capabilities. We
formulate a delay-optimal split learning problem, which optimizes five key
variables: layer partitioning points, DT assignment decisions, IRS phase shift
matrix, AP downlink power allocation, and DT frequency adjustment, aiming to
minimize the overall end-to-end delay under communication and computation. The
proposed optimization problem is a highly coupled non-convex mixed-integer
problem. Therefore, we solve using an alternating optimization approach
combining closed-form updates, semidefinite relaxation (SDR), and
low-complexity heuristics. Extensive simulations demonstrate that the proposed
scheme significantly reduces training delay compared to conventional baselines
and achieves up to 35\% delay improvement, especially under high UD density and
stringent power constraints.

</details>


### [221] [6D Channel Knowledge Map Construction via Bidirectional Wireless Gaussian Splatting](https://arxiv.org/abs/2510.26166)
*Juncong Zhou,Chao Hu,Guanlin Wu,Zixiang Ren,Han Hu,Juyong Zhang,Rui Zhang,Jie Xu*

Main category: eess.SP

TL;DR: The paper introduces BiWGS, a 6D channel knowledge map framework that models wireless channels across dynamic transmitter and receiver positions using Gaussian ellipsoids to represent scatterers and obstacles, outperforming MLP methods.


<details>
  <summary>Details</summary>
Motivation: To overcome limitations of conventional 2D/3D CKM approaches that assume fixed base station configurations, by enabling modeling of wireless channels across dynamic transmitter and receiver positions in 3D space.

Method: BiWGS uses Gaussian ellipsoids to represent virtual scatterer clusters and environmental obstacles, learning bidirectional scattering patterns and complex attenuation profiles from channel measurements to capture electromagnetic transmission characteristics.

Result: BiWGS significantly outperforms classic MLP for 6D channel power gain map construction and achieves spatial spectrum prediction accuracy comparable to state-of-the-art WRF-GS for 3D CKM construction.

Conclusion: The proposed BiWGS successfully accomplishes dimensional expansion to 6D CKM construction without compromising fidelity, validating its capability for dynamic transceiver configurations.

Abstract: This paper investigates the construction of channel knowledge map (CKM) from
sparse channel measurements. Dif ferent from conventional
two-/three-dimensional (2D/3D) CKM approaches assuming fixed base station
configurations, we present a six-dimensional (6D) CKM framework named
bidirectional wireless Gaussian splatting (BiWGS), which is capable of mod
eling wireless channels across dynamic transmitter (Tx) and receiver (Rx)
positions in 3D space. BiWGS uses Gaussian el lipsoids to represent virtual
scatterer clusters and environmental obstacles in the wireless environment. By
properly learning the bidirectional scattering patterns and complex attenuation
profiles based on channel measurements, these ellipsoids inherently cap ture
the electromagnetic transmission characteristics of wireless environments,
thereby accurately modeling signal transmission under varying transceiver
configurations. Experiment results show that BiWGS significantly outperforms
classic multi-layer perception (MLP) for the construction of 6D channel power
gain map with varying Tx-Rx positions, and achieves spatial spectrum prediction
accuracy comparable to the state-of-the art wireless radiation field Gaussian
splatting (WRF-GS) for 3D CKM construction. This validates the capability of
the proposed BiWGS in accomplishing dimensional expansion of 6D CKM
construction, without compromising fidelity.

</details>


### [222] [Design of Orthogonal Phase of Arrival Positioning Scheme Based on 5G PRS and Optimization of TOA Performance](https://arxiv.org/abs/2510.26245)
*Juyeop Kim,Hyejin Shin,Sohee Kim,Ilmu Byun*

Main category: eess.SP

TL;DR: Analysis of 5G positioning techniques using Time of Arrival (TOA) of Positioning Reference Signals (PRS), proposing an algorithm to improve TOA accuracy under low sampling rates and examining optimal PRS configurations.


<details>
  <summary>Details</summary>
Motivation: To enhance positioning accuracy in 5G networks where terminal positions are determined from TOA of PRS signals transmitted by base stations, particularly addressing challenges with low sampling rate constraints.

Method: Proposed an algorithm to improve TOA accuracy under low sampling rate constraints, implemented 5G PRS for positioning in a software defined modem, and examined how flexible time frequency resource allocation of PRS affects TOA estimation accuracy.

Result: The study provides insights into optimal PRS configurations for given signal environments and demonstrates improved TOA accuracy through the proposed algorithm.

Conclusion: Optimal configuration of PRS signals and improved TOA estimation algorithms can significantly enhance positioning performance in 5G networks, with flexible resource allocation playing a key role in accuracy.

Abstract: This study analyzes the performance of positioning techniques based on
configuration changes of 5G New Radio signals. In 5G networks, a terminal
position is determined from the Time of Arrival of Positioning Reference
Signals transmitted by base stations. We propose an algorithm that improves TOA
accuracy under low sampling rate constraints and implement 5G PRS for
positioning in a software defined modem. We also examine how flexible time
frequency resource allocation of PRS affects TOA estimation accuracy and
discuss optimal PRS configurations for a given signal environment.

</details>


### [223] [Optimal transmit field distribution for partially obstructed continuous radiating surfaces in near-field communication systems](https://arxiv.org/abs/2510.26262)
*Francesco Verde,Donatella Darsena,Marco Di Renzo,Vincenzo Galdi*

Main category: eess.SP

TL;DR: Optimal synthesis of aperture fields for near-field communications in obstructed environments using knife-edge diffraction model, formulated as Hilbert space maximization problem.


<details>
  <summary>Details</summary>
Motivation: To develop efficient energy focusing methods for near-field communications in obstructed environments where traditional far-field approaches may not work effectively.

Method: Uses physically consistent knife-edge diffraction model to formulate the problem as maximization in Hilbert space, with optimal solution obtained as a matched filter that matches the diffraction-induced kernel shape.

Result: Developed a framework that links wave propagation with signal processing methods, supporting hardware implementation using continuous apertures like metasurfaces or lens antennas.

Conclusion: The approach successfully bridges physically grounded modeling, signal processing, and hardware design for efficient energy focusing in near-field obstructed channels.

Abstract: This paper deals with the optimal synthesis of aperture fields for
(radiating) near-field communications in obstructed environments. A physically
consistent model based on knife-edge diffraction is used to formulate the
problem as a maximization in Hilbert space. The optimal solution is obtained as
a matched filter that ``matches" the shape of a diffraction-induced kernel,
thus linking wave propagation with signal processing methods. The framework
supports hardware implementation using continuous apertures such as
metasurfaces or lens antennas. This approach bridges physically grounded
modeling, signal processing, and hardware design for efficient energy focusing
in near-field obstructed channels.

</details>


### [224] [SABER: Symbolic Regression-based Angle of Arrival and Beam Pattern Estimator](https://arxiv.org/abs/2510.26340)
*Shih-Kai Chou,Mengran Zhao,Cheng-Nan Hu,Kuang-Chung Chou,Carolina Fortuna,Jernej Hribar*

Main category: eess.SP

TL;DR: SABER is a Symbolic Regression-based framework that discovers interpretable closed-form models for Angle-of-Arrival (AoA) estimation and beam patterns from path loss measurements, achieving high accuracy while maintaining physical interpretability.


<details>
  <summary>Details</summary>
Motivation: Classical AoA estimation techniques require complex hardware and extensive data collection, while generic ML methods lack interpretability. There's a need for accurate yet physically interpretable models for next-generation wireless systems.

Method: Proposes SABER - a constrained symbolic regression framework that automatically discovers closed-form beam pattern and AoA models from path loss measurements. Validated in both controlled free-space anechoic chamber and real-world RIS-aided indoor testbed.

Result: Achieves sub-0.5 degree Mean Absolute Error in controlled settings, with near-zero error in real-world testbeds. Outperforms black-box ML methods while maintaining interpretability and approaching Cramér-Rao Lower Bounds.

Conclusion: SABER provides an interpretable and accurate alternative to state-of-the-art and black-box ML methods for AoA estimation, bridging the gap between opaque ML approaches and physics-driven estimators.

Abstract: Accurate Angle-of-arrival (AoA) estimation is essential for next-generation
wireless communication systems to enable reliable beamforming, high-precision
localization, and integrated sensing. Unfortunately, classical high-resolution
techniques require multi-element arrays and extensive snapshot collection,
while generic Machine Learning (ML) approaches often yield black-box models
that lack physical interpretability. To address these limitations, we propose a
Symbolic Regression (SR)-based ML framework. Namely, Symbolic Regression-based
Angle of Arrival and Beam Pattern Estimator (SABER), a constrained
symbolic-regression framework that automatically discovers closed-form beam
pattern and AoA models from path loss measurements with interpretability. SABER
achieves high accuracy while bridging the gap between opaque ML methods and
interpretable physics-driven estimators. First, we validate our approach in a
controlled free-space anechoic chamber, showing that both direct inversion of
the known $\cos^n$ beam and a low-order polynomial surrogate achieve sub-0.5
degree Mean Absolute Error (MAE). A purely unconstrained SR method can further
reduce the error of the predicted angles, but produces complex formulas that
lack physical insight. Then, we implement the same SR-learned inversions in a
real-world, Reconfigurable Intelligent Surface (RIS)-aided indoor testbed.
SABER and unconstrained SR models accurately recover the true AoA with
near-zero error. Finally, we benchmark SABER against the Cram\'er-Rao Lower
Bounds (CRLBs). Our results demonstrate that SABER is an interpretable and
accurate alternative to state-of-the-art and black-box ML-based methods for AoA
estimation.

</details>


### [225] [HMM for short independent sequences: Multiple sequence Baum-Welch application](https://arxiv.org/abs/2510.26532)
*Margarita Cabrera-Bean,Josep Vidal,Sergio Fernandez-Bertolin,Albert Roso-Llorach,Concepcion Violan*

Main category: eess.SP

TL;DR: This paper presents pseudocode formulations for training and decoding Hidden Markov Models (HMMs) using multiple short independent sequences rather than a single long sequence, with applications in longitudinal population health studies.


<details>
  <summary>Details</summary>
Motivation: Traditional HMM training relies on a single long observation sequence, but many real-world scenarios involve multiple short independent sequences from the same model, particularly in longitudinal health studies with individual patient trajectories.

Method: The authors develop pseudocode formulations for both training (using Expectation Maximization/Baum Welch algorithm) and decoding procedures of HMMs specifically adapted for multiple independent short temporal sequences.

Result: The paper provides practical pseudocode implementations that enable HMM training and decoding in scenarios where data consists of multiple short sequences rather than a single long sequence.

Conclusion: This formulation is particularly relevant for longitudinal population health studies where datasets naturally consist of collections of short individual trajectories with follow-up data points.

Abstract: In the classical setting, the training of a Hidden Markov Model (HMM)
typically relies on a single, sufficiently long observation sequence that can
be regarded as representative of the underlying stochastic process. In this
context, the Expectation Maximization (EM) algorithm is applied in its
specialized form for HMMs, namely the Baum Welch algorithm, which has been
extensively employed in applications such as speech recognition. The objective
of this work is to present pseudocode formulations for both the training and
decoding procedures of HMMs in a different scenario, where the available data
consist of multiple independent temporal sequences generated by the same model,
each of relatively short duration, i.e., containing only a limited number of
samples. Special emphasis is placed on the relevance of this formulation to
longitudinal studies in population health, where datasets are naturally
structured as collections of short trajectories across individuals with point
data at follow up.

</details>


### [226] [Statistically Adaptive Differential Protection for AC Microgrids Based on Kullback-Leibler Divergence](https://arxiv.org/abs/2510.26604)
*Shahab Moradi Torkashvand,Arina Kharazi,Emad Sadeghi,Seyed Hossein Hesamedin Sadeghi,Adel Nasiri*

Main category: eess.SP

TL;DR: A statistically adaptive differential protection scheme using Kullback-Leibler divergence for microgrids with inverter-based resources, achieving fast fault detection and classification with high accuracy and noise resilience.


<details>
  <summary>Details</summary>
Motivation: The proliferation of inverter-based resources challenges traditional microgrid protection due to variable fault currents and complex transients, requiring more robust protection schemes.

Method: Uses Kullback-Leibler divergence implemented via Bartlett-corrected G-statistic on logarithm-transformed current magnitudes, with Mahalanobis distance for fault detection and lightweight classifier for fault type identification.

Result: Sub-cycle average detection delays, high detection/classification accuracy across operating modes, resilience to high-impedance faults up to 250 Ohms, tolerance to 10 ms communication delay, and noise levels down to 20 dB SNR.

Conclusion: The method provides a reproducible and computationally efficient solution for next-generation AC microgrid protection, demonstrating high efficacy in challenging conditions.

Abstract: The proliferation of inverter-based resources challenges traditional
microgrid protection by introducing variable fault currents and complex
transients. This paper presents a statistically adaptive differential
protection scheme based on Kullback-Leibler divergence, implemented via a
Bartlett-corrected G-statistic computed on logarithm-transformed current
magnitudes. The method is a multivariate fault detection engine that employs
the Mahalanobis distance to distinguish healthy and faulty states, enabling
robust detection even in noisy environments. Detection thresholds are
statistically derived from a chi-squared distribution for precise control over
the false alarm rate. Upon detection, a lightweight classifier identifies the
fault type by assessing per-phase G-statistics against dedicated thresholds,
enhanced by a temporal persistence filter for security. Extensive simulations
on a modified CIGRE 14-bus microgrid show high efficacy: sub-cycle average
detection delays, high detection and classification accuracy across operating
modes, resilience to high-impedance faults up to 250 Ohms, tolerance to 10 ms
communication delay, and noise levels down to a 20 dB signal-to-noise ratio.
These findings demonstrate a reproducible and computationally efficient
solution for next-generation AC microgrid protection.

</details>


### [227] [Graph Guided Modulo Recovery of EEG Signals](https://arxiv.org/abs/2510.26756)
*Soujanya Hazra,Sanjay Ghosh*

Main category: eess.SP

TL;DR: GraphUnwrapNet uses graph neural networks with pre-estimation guided feature injection to recover folded EEG signals from modulo sampling, outperforming traditional methods and competing with deep learning approaches.


<details>
  <summary>Details</summary>
Motivation: EEG signals show significant variability and can suffer from distortion or clipping during acquisition. Modulo sampling folds signals instead of saturating them, but recovering original waveforms from folded observations is highly ill-posed.

Method: Proposes GraphUnwrapNet that represents EEG signals as organized graphs with channel and temporal connections. Introduces pre-estimation guided feature injection module to provide coarse folding indicators that enhance stability at wrap boundaries.

Result: Comprehensive experiments on STEW dataset show consistent improvements over traditional optimization techniques and competitive accuracy compared to current deep learning models.

Conclusion: Graph-based methodology shows strong potential for robust modulo EEG recovery, integrating structural information with folding priors in an integrated framework.

Abstract: Electroencephalography (EEG) often shows significant variability among
people. This fluctuation disrupts reliable acquisition and may result in
distortion or clipping. Modulo sampling is now a promising solution to this
problem, by folding signals instead of saturating them. Recovery of the
original waveform from folded observations is a highly ill-posed problem. In
this work, we propose a method based on a graph neural network, referred to as
GraphUnwrapNet, for the modulo recovery of EEG signals. Our core idea is to
represent an EEG signal as an organized graph whose channels and temporal
connections establish underlying interdependence. One of our key contributions
is in introducing a pre-estimation guided feature injection module to provide
coarse folding indicators that enhance stability during recovery at wrap
boundaries. This design integrates structural information with folding priors
into an integrated framework. We performed comprehensive experiments on the
Simultaneous Task EEG Workload (STEW) dataset. The results demonstrate
consistent enhancements over traditional optimization techniques and
competitive accuracy relative to current deep learning models. Our findings
emphasize the potential of graph-based methodology for robust modulo EEG
recovery.

</details>


<div id='eess.IV'></div>

# eess.IV [[Back]](#toc)

### [228] [Groupwise Registration with Physics-Informed Test-Time Adaptation on Multi-parametric Cardiac MRI](https://arxiv.org/abs/2510.26022)
*Xinqi Li,Yi Zhang,Li-Ting Huang,Hsiao-Huang Chang,Thoralf Niendorf,Min-Chi Ku,Qian Tao,Hsin-Jung Yang*

Main category: eess.IV

TL;DR: A physics-informed deep learning model with test-time adaptation enables group image registration across contrast-weighted MRI maps from different physical models, addressing misalignment issues in multiparametric mapping.


<details>
  <summary>Details</summary>
Motivation: Misalignment between multiparametric MRI maps (e.g., T1 and T2 mapping) makes pixel-wise analysis challenging for myocardial tissue characterization.

Method: Developed a generalizable physics-informed deep learning model using test-time adaptation, utilizing synthetic images from specific physics models as registration references for transductive learning across various tissue contrasts.

Result: Validated in healthy volunteers with various MRI sequences, demonstrating improved multi-modal registration performance across a wide range of image contrast variability.

Conclusion: The proposed approach successfully addresses misalignment challenges in multiparametric MRI mapping, enabling more accurate pixel-wise analysis for myocardial tissue characterization.

Abstract: Multiparametric mapping MRI has become a viable tool for myocardial tissue
characterization. However, misalignment between multiparametric maps makes
pixel-wise analysis challenging. To address this challenge, we developed a
generalizable physics-informed deep-learning model using test-time adaptation
to enable group image registration across contrast weighted images acquired
from multiple physical models (e.g., a T1 mapping model and T2 mapping model).
The physics-informed adaptation utilized the synthetic images from specific
physics model as registration reference, allows for transductive learning for
various tissue contrast. We validated the model in healthy volunteers with
various MRI sequences, demonstrating its improvement for multi-modal
registration with a wide range of image contrast variability.

</details>


### [229] [Functional Connectome Fingerprinting Using Convolutional and Dictionary Learning](https://arxiv.org/abs/2510.26120)
*Yashaswini,Sanjay Ghosh*

Main category: eess.IV

TL;DR: A framework combining convolutional autoencoders and sparse dictionary learning improves fMRI functional connectivity fingerprinting by 10% over baseline methods, enabling better individual identification from brain patterns.


<details>
  <summary>Details</summary>
Motivation: To enhance individual identification from fMRI data by improving functional connectivity fingerprinting methods, particularly for personalized medicine applications in neurological and psychiatric disorders.

Method: Used convolutional autoencoders to capture shared connectivity patterns and isolate subject-specific features in residual FC matrices, then applied sparse dictionary learning to identify distinctive features.

Result: Achieved 10% improvement in fingerprint accuracy over baseline group-averaged FC models when tested on the Human Connectome Project dataset.

Conclusion: Integration of deep learning and sparse coding techniques enables scalable and robust functional connectome fingerprinting, advancing personalized neuroscience and biomarker development.

Abstract: Advances in data analysis and machine learning have revolutionized the study
of brain signatures using fMRI, enabling non-invasive exploration of cognition
and behavior through individual neural patterns. Functional connectivity (FC),
which quantifies statistical relationships between brain regions, has emerged
as a key metric for studying individual variability and developing biomarkers
for personalized medicine in neurological and psychiatric disorders. The
concept of subject fingerprinting, introduced by Finn et al. (2015), leverages
neural connectivity variability to identify individuals based on their unique
patterns. While traditional FC methods perform well on small datasets, machine
learning techniques are more effective with larger datasets, isolating
individual-specific features and maximizing inter-subject differences. In this
study, we propose a framework combining convolutional autoencoders and sparse
dictionary learning to enhance fingerprint accuracy. Autoencoders capture
shared connectivity patterns while isolating subject-specific features in
residual FC matrices, which are analyzed using sparse coding to identify
distinctive features. Tested on the Human Connectome Project dataset, this
approach achieved a 10% improvement over baseline group-averaged FC models. Our
results highlight the potential of integrating deep learning and sparse coding
techniques for scalable and robust functional connectome fingerprinting,
advancing personalized neuroscience applications and biomarker development.

</details>


### [230] [BitSemCom: A Bit-Level Semantic Communication Framework with Learnable Probabilistic Mapping](https://arxiv.org/abs/2510.26225)
*Haoshuo Zhang,Yufei Bo,Jianhua Mo,Meixia Tao*

Main category: eess.IV

TL;DR: BitSemCom is a bit-level semantic communication framework that enables joint source-channel coding at the bit level using a learnable bit mapper with Gumbel-Softmax, achieving competitive performance and robustness while being lightweight.


<details>
  <summary>Details</summary>
Motivation: Existing semantic communication systems use analog modulation which is incompatible with modern digital systems, and current digital approaches lack end-to-end bit-level methods that are robust, quantization-error-free, and compatible with arbitrary modulation formats.

Method: Proposes BitSemCom with a modular learnable bit mapper that establishes probabilistic mapping between continuous semantic features and discrete bits using Gumbel-Softmax trick for differentiable bit generation.

Result: BitSemCom achieves competitive performance and superior robustness compared to traditional SSCC schemes, outperforms deep learning based JSCC with uniform 1-bit quantization, while adding only 0.42% parameters and 0.09% computational complexity.

Conclusion: BitSemCom provides a lightweight, practical solution for real-world semantic communication that enables true joint source-channel coding at the bit level with minimal overhead.

Abstract: Most existing semantic communication systems employ analog modulation, which
is incompatible with modern digital communication systems. Although several
digital transmission approaches have been proposed to address this issue, an
end-to-end bit-level method that is compatible with arbitrary modulation
formats, robust to channel noise, and free from quantization errors remains
lacking. To this end, we propose BitSemCom, a novel bit-level semantic
communication framework that realizes true joint source-channel coding (JSCC)
at the bit level. Specifically, we introduce a modular learnable bit mapper
that establishes a probabilistic mapping between continuous semantic features
and discrete bits, utilizing the Gumbel-Softmax trick to enable differentiable
bit generation. Simulation results on image transmission demonstrate that
BitSemCom achieves both competitive performance and superior robustness
compared to traditional separate source-channel coding (SSCC) schemes, and
outperforms deep learning based JSCC with uniform 1-bit quantization,
validating the effectiveness of the learnable bit mapper. Despite these
improvements, the bit mapper adds only 0.42% parameters and 0.09% computational
complexity, making BitSemCom a lightweight and practical solution for
real-world semantic communication.

</details>


### [231] [SPG-CDENet: Spatial Prior-Guided Cross Dual Encoder Network for Multi-Organ Segmentation](https://arxiv.org/abs/2510.26390)
*Xizhi Tian,Changjun Zhou,Yulin. Yang*

Main category: eess.IV

TL;DR: SPG-CDENet is a two-stage network for multi-organ segmentation that uses spatial priors and cross-attention between global and local encoders to handle organ size/shape variations.


<details>
  <summary>Details</summary>
Motivation: Address challenges in multi-organ segmentation caused by huge variations in organ size and shape that limit effectiveness of existing deep learning methods.

Method: Two-stage approach: 1) Spatial prior network generates coarse ROI localization maps, 2) Cross dual encoder network with global encoder, local encoder, symmetric cross-attention module, and flow-based decoder for feature fusion and preservation.

Result: Superior performance demonstrated on two public datasets compared to existing segmentation methods, with ablation studies validating effectiveness of proposed modules.

Conclusion: SPG-CDENet effectively improves multi-organ segmentation accuracy through spatial guidance and enhanced feature interaction between global and local contexts.

Abstract: Multi-organ segmentation is a critical task in computer-aided diagnosis.
While recent deep learning methods have achieved remarkable success in image
segmentation, huge variations in organ size and shape challenge their
effectiveness in multi-organ segmentation. To address these challenges, we
propose a Spatial Prior-Guided Cross Dual Encoder Network (SPG-CDENet), a novel
two-stage segmentation paradigm designed to improve multi-organ segmentation
accuracy. Our SPG-CDENet consists of two key components: a spatial prior
network and a cross dual encoder network. The prior network generates coarse
localization maps that delineate the approximate ROI, serving as spatial
guidance for the dual encoder network. The cross dual encoder network comprises
four essential components: a global encoder, a local encoder, a symmetric
cross-attention module, and a flow-based decoder. The global encoder captures
global semantic features from the entire image, while the local encoder focuses
on features from the prior network. To enhance the interaction between the
global and local encoders, a symmetric cross-attention module is proposed
across all layers of the encoders to fuse and refine features. Furthermore, the
flow-based decoder directly propagates high-level semantic features from the
final encoder layer to all decoder layers, maximizing feature preservation and
utilization. Extensive qualitative and quantitative experiments on two public
datasets demonstrate the superior performance of SPG-CDENet compared to
existing segmentation methods. Furthermore, ablation studies further validate
the effectiveness of the proposed modules in improving segmentation accuracy.

</details>


### [232] [Comparative Analysis of Deep Learning Models for Olive Tree Crown and Shadow Segmentation Towards Biovolume Estimation](https://arxiv.org/abs/2510.26573)
*Wondimagegn Abebe Demissie,Stefano Roccella,Rudy Rossetto,Antonio Minnocci,Andrea Vannini,Luca Sebastiani*

Main category: eess.IV

TL;DR: Comparative analysis of U-Net, YOLOv11m-seg, and Mask R-CNN for olive tree crown and shadow segmentation in UAV imagery, with Mask R-CNN achieving best accuracy and YOLOv11m-seg providing fastest processing speed.


<details>
  <summary>Details</summary>
Motivation: Olive tree biovolume estimation is crucial for precision agriculture, yield prediction, and resource management in Mediterranean regions affected by climate stress.

Method: Used three deep learning models (U-Net, YOLOv11m-seg, Mask R-CNN) for segmenting olive tree crowns and shadows in UAV imagery, then estimated biovolume by combining crown projected area with shadow-derived height using solar geometry.

Result: Mask R-CNN achieved best accuracy (F1 = 0.86; mIoU = 0.72), YOLOv11m-seg provided fastest throughput (0.12 second per image), and estimated biovolumes ranged from 4 to 24 cubic meters.

Conclusion: Mask R-CNN is preferred for accuracy-critical applications, YOLOv11m-seg for speed-critical large-area deployments, and U-Net as a lightweight high-sensitivity option; framework enables scalable orchard monitoring.

Abstract: Olive tree biovolume estimation is a key task in precision agriculture,
supporting yield prediction and resource management, especially in
Mediterranean regions severely impacted by climate-induced stress. This study
presents a comparative analysis of three deep learning models U-Net,
YOLOv11m-seg, and Mask RCNN for segmenting olive tree crowns and their shadows
in ultra-high resolution UAV imagery. The UAV dataset, acquired over
Vicopisano, Italy, includes manually annotated crown and shadow masks. Building
on these annotations, the methodology emphasizes spatial feature extraction and
robust segmentation; per-tree biovolume is then estimated by combining crown
projected area with shadow-derived height using solar geometry. In testing,
Mask R-CNN achieved the best overall accuracy (F1 = 0.86; mIoU = 0.72), while
YOLOv11m-seg provided the fastest throughput (0.12 second per image). The
estimated biovolumes spanned from approximately 4 to 24 cubic meters,
reflecting clear structural differences among trees. These results indicate
Mask R-CNN is preferable when biovolume accuracy is paramount, whereas
YOLOv11m-seg suits large-area deployments where speed is critical; U-Net
remains a lightweight, high-sensitivity option. The framework enables accurate,
scalable orchard monitoring and can be further strengthened with DEM or DSM
integration and field calibration for operational decision support.

</details>


### [233] [SAMRI: Segment Anything Model for MRI](https://arxiv.org/abs/2510.26635)
*Zhao Wang,Wei Dai,Thuy Thanh Dao,Steffen Bollmann,Hongfu Sun,Craig Engstrom,Shekhar S. Chandra*

Main category: eess.IV

TL;DR: SAMRI is an MRI-specialized adaptation of Segment Anything Model (SAM) that achieves state-of-the-art MRI segmentation with minimal fine-tuning, reducing training time by 94% and parameters by 96% while maintaining high accuracy across diverse anatomical regions.


<details>
  <summary>Details</summary>
Motivation: Manual MRI segmentation is labor-intensive, and existing CNN-based methods generalize poorly to MRI's variable contrast, intensity inhomogeneity, and protocols. While SAM shows strong generalizability in natural images, existing adaptations overlook MRI-specific challenges.

Method: Fine-tune SAM's mask decoder using a two-stage strategy on 1.1 million labeled MR slices spanning whole-body organs and pathologies, rather than full-model retraining.

Result: Achieves mean Dice of 0.87 across diverse MRI segmentation tasks, delivers state-of-the-art accuracy across anatomical regions, and shows robust generalization on unseen structures, especially small and clinically important ones.

Conclusion: SAM can be effectively adapted to MRI with minimal fine-tuning, achieving excellent performance while significantly reducing computational requirements compared to full-model retraining.

Abstract: Accurate magnetic resonance imaging (MRI) segmentation is crucial for
clinical decision-making, but remains labor-intensive when performed manually.
Convolutional neural network (CNN)-based methods can be accurate and efficient,
but often generalize poorly to MRI's variable contrast, intensity
inhomogeneity, and protocols. Although the transformer-based Segment Anything
Model (SAM) has demonstrated remarkable generalizability in natural images,
existing adaptations often treat MRI as another imaging modality, overlooking
these modality-specific challenges. We present SAMRI, an MRI-specialized SAM
trained and validated on 1.1 million labeled MR slices spanning whole-body
organs and pathologies. We demonstrate that SAM can be effectively adapted to
MRI by simply fine-tuning its mask decoder using a two-stage strategy, reducing
training time by 94% and trainable parameters by 96% versus full-model
retraining. Across diverse MRI segmentation tasks, SAMRI achieves a mean Dice
of 0.87, delivering state-of-the-art accuracy across anatomical regions and
robust generalization on unseen structures, particularly small and clinically
important structures.

</details>


### [234] [BRIQA: Balanced Reweighting in Image Quality Assessment of Pediatric Brain MRI](https://arxiv.org/abs/2510.26661)
*Alya Almsouti,Ainur Khamitova,Darya Taratynova,Mohammad Yaqub*

Main category: eess.IV

TL;DR: BRIQA is a method that improves pediatric brain MRI artifact severity classification by addressing class imbalance through gradient-based loss reweighting and rotating batching scheme, achieving better performance across multiple artifact types.


<details>
  <summary>Details</summary>
Motivation: Manual quality assessment of pediatric brain MRI artifacts is time-consuming and subjective, especially in low-field systems with reduced signal-to-noise ratio, creating need for robust automated solutions.

Method: BRIQA uses gradient-based loss reweighting to dynamically adjust per-class contributions and employs a rotating batching scheme to ensure consistent exposure to underrepresented classes. The approach emphasizes architectural diversity as no single architecture performs best across all artifact types.

Result: BRIQA improves average macro F1 score from 0.659 to 0.706, with notable gains in Noise (0.430), Zipper (0.098), Positioning (0.097), Contrast (0.217), Motion (0.022), and Banding (0.012) artifact severity classification. The rotating batching configuration improves performance across metrics when combined with cross-entropy loss.

Conclusion: The proposed BRIQA method effectively addresses class imbalance in MRI artifact severity assessment through balanced reweighting and rotating batching, demonstrating improved performance across multiple artifact types in pediatric brain imaging.

Abstract: Assessing the severity of artifacts in pediatric brain Magnetic Resonance
Imaging (MRI) is critical for diagnostic accuracy, especially in low-field
systems where the signal-to-noise ratio is reduced. Manual quality assessment
is time-consuming and subjective, motivating the need for robust automated
solutions. In this work, we propose BRIQA (Balanced Reweighting in Image
Quality Assessment), which addresses class imbalance in artifact severity
levels. BRIQA uses gradient-based loss reweighting to dynamically adjust
per-class contributions and employs a rotating batching scheme to ensure
consistent exposure to underrepresented classes. Through experiments, no single
architecture performs best across all artifact types, emphasizing the
importance of architectural diversity. The rotating batching configuration
improves performance across metrics by promoting balanced learning when
combined with cross-entropy loss. BRIQA improves average macro F1 score from
0.659 to 0.706, with notable gains in Noise (0.430), Zipper (0.098),
Positioning (0.097), Contrast (0.217), Motion (0.022), and Banding (0.012)
artifact severity classification. The code is available at
https://github.com/BioMedIA-MBZUAI/BRIQA.

</details>


### [235] [ProstNFound+: A Prospective Study using Medical Foundation Models for Prostate Cancer Detection](https://arxiv.org/abs/2510.26703)
*Paul F. R. Wilson,Mohamed Harmanani,Minh Nguyen Nhat To,Amoon Jamzad,Tarek Elghareb,Zhuoxin Guo,Adam Kinnaird,Brian Wodlinger,Purang Abolmaesumi,Parvin Mousavi*

Main category: eess.IV

TL;DR: ProstNFound+ is a medical foundation model adapted for prostate cancer detection from micro-ultrasound, showing strong generalization in prospective validation with interpretable heatmaps.


<details>
  <summary>Details</summary>
Motivation: Medical foundation models offer potential for high-performance diagnostic systems, but their application to prostate cancer detection from micro-ultrasound remains untested in clinical settings.

Method: ProstNFound+ incorporates a medical foundation model with adapter tuning and a custom prompt encoder that embeds prostate cancer-specific clinical biomarkers, generating cancer heatmaps and risk scores.

Result: The model shows strong generalization to prospective data with no performance degradation compared to retrospective evaluation, aligning closely with clinical scores and producing interpretable heatmaps consistent with biopsy-confirmed lesions.

Conclusion: The results highlight ProstNFound+'s potential for clinical deployment, offering a scalable and interpretable alternative to expert-driven protocols.

Abstract: Purpose: Medical foundation models (FMs) offer a path to build
high-performance diagnostic systems. However, their application to prostate
cancer (PCa) detection from micro-ultrasound ({\mu}US) remains untested in
clinical settings. We present ProstNFound+, an adaptation of FMs for PCa
detection from {\mu}US, along with its first prospective validation. Methods:
ProstNFound+ incorporates a medical FM, adapter tuning, and a custom prompt
encoder that embeds PCa-specific clinical biomarkers. The model generates a
cancer heatmap and a risk score for clinically significant PCa. Following
training on multi-center retrospective data, the model is prospectively
evaluated on data acquired five years later from a new clinical site. Model
predictions are benchmarked against standard clinical scoring protocols
(PRI-MUS and PI-RADS). Results: ProstNFound+ shows strong generalization to the
prospective data, with no performance degradation compared to retrospective
evaluation. It aligns closely with clinical scores and produces interpretable
heatmaps consistent with biopsy-confirmed lesions. Conclusion: The results
highlight its potential for clinical deployment, offering a scalable and
interpretable alternative to expert-driven protocols.

</details>


### [236] [MORE: Multi-Organ Medical Image REconstruction Dataset](https://arxiv.org/abs/2510.26759)
*Shaokai Wu,Yapan Guo,Yanbiao Ji,Jing Tong,Yuxiang Lu,Mei Li,Suizhi Huang,Yue Ding,Hongtao Lu*

Main category: eess.IV

TL;DR: The paper introduces the MORE dataset for CT reconstruction, containing diverse anatomies and lesions to improve model generalization, and establishes a baseline that outperforms prior methods.


<details>
  <summary>Details</summary>
Motivation: Current deep learning methods for CT reconstruction are limited to specific anatomies and datasets, hindering generalization to unseen anatomies and lesions.

Method: Created the Multi-Organ medical image REconstruction (MORE) dataset with CT scans across 9 anatomies and 15 lesion types, and developed a strong baseline solution.

Result: The comprehensive dataset improves model generalization capability, and optimization-based methods offer enhanced robustness for unseen anatomies.

Conclusion: The MORE dataset enables robust training and rigorous evaluation of CT reconstruction models, demonstrating improved generalization through diverse data and optimization-based approaches.

Abstract: CT reconstruction provides radiologists with images for diagnosis and
treatment, yet current deep learning methods are typically limited to specific
anatomies and datasets, hindering generalization ability to unseen anatomies
and lesions. To address this, we introduce the Multi-Organ medical image
REconstruction (MORE) dataset, comprising CT scans across 9 diverse anatomies
with 15 lesion types. This dataset serves two key purposes: (1) enabling robust
training of deep learning models on extensive, heterogeneous data, and (2)
facilitating rigorous evaluation of model generalization for CT reconstruction.
We further establish a strong baseline solution that outperforms prior
approaches under these challenging conditions. Our results demonstrate that:
(1) a comprehensive dataset helps improve the generalization capability of
models, and (2) optimization-based methods offer enhanced robustness for unseen
anatomies. The MORE dataset is freely accessible under CC-BY-NC 4.0 at our
project page https://more-med.github.io/

</details>


<div id='cs.IT'></div>

# cs.IT [[Back]](#toc)

### [237] [Coherence-Aware Distributed Learning under Heterogeneous Downlink Impairments](https://arxiv.org/abs/2510.25917)
*Mehdi Karbalayghareh,David J. Love,Christopher G. Brinton*

Main category: cs.IT

TL;DR: Proposes a coherence-aware framework for wireless federated learning that addresses unequal channel coherence times across devices using resource-reuse strategy with product superposition.


<details>
  <summary>Details</summary>
Motivation: Conventional FL schemes suffer from communication inefficiencies when dealing with unequal coherence times across devices due to mobility differences, leading to training overhead.

Method: Uses resource-reuse strategy based on product superposition to efficiently schedule static and dynamic devices by embedding global model updates for static devices within pilot transmissions for mobile devices.

Result: Theoretical convergence analysis and experiments demonstrate improved communication efficiency and training accuracy under mobility-induced dynamics.

Conclusion: The proposed framework effectively addresses coherence disparity in wireless FL systems and offers practical deployment insights.

Abstract: The performance of federated learning (FL) over wireless networks critically
depends on accurate and timely channel state information (CSI) across
distributed devices. This requirement is tightly linked to how rapidly the
channel gains vary, i.e., the coherence intervals. In practice, edge devices
often exhibit unequal coherence times due to differences in mobility and
scattering environments, leading to unequal demands for pilot signaling and
channel estimation resources. Conventional FL schemes that overlook this
coherence disparity can suffer from severe communication inefficiencies and
training overhead. This paper proposes a coherence-aware,
communication-efficient framework for joint channel training and model updating
in practical wireless FL systems operating under heterogeneous fading dynamics.
Focusing on downlink impairments, we introduce a resource-reuse strategy based
on product superposition, enabling the parameter server to efficiently schedule
both static and dynamic devices by embedding global model updates for static
devices within pilot transmissions intended for mobile devices. We
theoretically analyze the convergence behavior of the proposed scheme and
quantify its gains in expected communication efficiency and training accuracy.
Experiments demonstrate the effectiveness of the proposed framework under
mobility-induced dynamics and offer useful insights for the practical
deployment of FL over wireless channels.

</details>


### [238] [Duality-Based Fixed Point Iteration Algorithm for Beamforming Design in ISAC Systems](https://arxiv.org/abs/2510.26147)
*Xilai Fan,Ya-Feng Liu*

Main category: cs.IT

TL;DR: This paper proposes a duality-based fixed point iteration algorithm for optimal beamforming design in integrated sensing and communication systems, achieving global optimality with reduced computational complexity.


<details>
  <summary>Details</summary>
Motivation: The need to efficiently design beamforming for ISAC systems that simultaneously serve communication users and perform radar sensing, addressing the complex coupling between communication SINR requirements and sensing performance metrics.

Method: Established equivalence between ISAC beamforming problem and semidefinite relaxation, derived Lagrangian dual formulation, reformulated as generalized downlink beamforming problem, and developed Dual-FPI algorithm combining subgradient ascent with fixed point iteration.

Result: The proposed Dual-FPI algorithm achieves globally optimal solutions while significantly reducing computational complexity compared to existing baseline approaches, as demonstrated through simulations.

Conclusion: The duality-based framework and efficient fixed point iteration algorithm provide an effective solution for ISAC beamforming design, overcoming challenges posed by indefinite weighting matrices and complex coupling between communication and sensing requirements.

Abstract: In this paper, we investigate the beamforming design problem in an integrated
sensing and communication (ISAC) system, where a multi-antenna base station
simultaneously serves multiple communication users while performing radar
sensing. We formulate the problem as the minimization of the total transmit
power, subject to signal-to-interference-plus-noise ratio (SINR) constraints
for communication users and mean-squared-error (MSE) constraints for radar
sensing. The core challenge arises from the complex coupling between
communication SINR requirements and sensing performance metrics. To efficiently
address this challenge, we first establish the equivalence between the original
ISAC beamforming problem and its semidefinite relaxation (SDR), derive its
Lagrangian dual formulation, and further reformulate it as a generalized
downlink beamforming (GDB) problem with potentially indefinite weighting
matrices. Compared to the classical DB problem, the presence of indefinite
weighting matrices in the GDB problem introduces substantial analytical and
computational challenges. Our key technical contributions include (i) a
necessary and sufficient condition for the boundedness of the GDB problem, and
(ii) a tailored efficient fixed point iteration (FPI) algorithm with a provable
convergence guarantee for solving the GDB problem. Building upon these results,
we develop a duality-based fixed point iteration (Dual-FPI) algorithm, which
integrates an outer subgradient ascent loop with an inner FPI loop. Simulation
results demonstrate that the proposed Dual-FPI algorithm achieves globally
optimal solutions while significantly reducing computational complexity
compared with existing baseline approaches.

</details>


### [239] [Efficient Spectral Efficiency Maximization Design for IRS-aided MIMO Systems](https://arxiv.org/abs/2510.26279)
*Fuying Li,Yajun Wang,Zhuxian Lian,Wen Chen*

Main category: cs.IT

TL;DR: ADMM-APG algorithm for spectral efficiency maximization in IRS-assisted MIMO systems by jointly optimizing transmit precoding and IRS phase shifts


<details>
  <summary>Details</summary>
Motivation: Growing demand for higher spectral efficiency in wireless communications and the potential of IRS to dynamically reconfigure propagation environment

Method: Integration of alternating direction method of multipliers (ADMM) with accelerated projected gradient (APG) method to decompose the non-convex problem into tractable subproblems with closed-form solutions

Result: Consistently surpasses existing benchmark methods in both spectral efficiency and computational complexity, achieving significant performance gains across various system configurations

Conclusion: ADMM-APG provides an effective and computationally efficient solution for spectral efficiency maximization in IRS-assisted MIMO systems

Abstract: Driven by the growing demand for higher spectral efficiency in wireless
communications, intelligent reflecting sur- faces (IRS) have attracted
considerable attention for their ability to dynamically reconfigure the
propagation environment. This work addresses the spectral efficiency
maximization problem in IRS-assisted multiple-input multiple-output (MIMO)
systems, which involves the joint optimization of the transmit precoding matrix
and the IRS phase shift configuration. This problem is inherently challenging
due to its non-convex nature. To tackle it effectively, we introduce a
computationally efficient algorithm, termed ADMM-APG, which integrates the
alternating direction method of multipliers (ADMM) with the accelerated
projected gradient (APG) method. The proposed framework decomposes the original
problem into tractable subproblems, each admitting a closed-form solution while
maintaining low computational com- plexity. Simulation results demonstrate that
the ADMM-APG algorithm consistently surpasses existing benchmark methods in
terms of spectral efficiency and computational complexity, achieving
significant performance gains across a range of system configurations.

</details>


### [240] [Diffusion-Aided Bandwidth-Efficient Semantic Communication with Adaptive Requests](https://arxiv.org/abs/2510.26442)
*Xuesong Wang,Xinyan Xie,Mo Li,Zhaoqian Liu*

Main category: cs.IT

TL;DR: A diffusion-based semantic communication framework that transmits text descriptions with key visual features, using diffusion inpainting for reconstruction and adaptive retransmission to maintain semantic accuracy while reducing bandwidth.


<details>
  <summary>Details</summary>
Motivation: Traditional semantic communication approaches either lose spatial details with text-only transmission or introduce redundancy with dense visual features. Need to reduce semantic redundancy while preserving visual fidelity and transmission efficiency.

Method: Transmits concise text descriptions with limited key latent visual features, uses diffusion-based inpainting for reconstruction, and implements receiver-side semantic consistency checking with adaptive retransmission for refinement.

Result: Significantly reduces bandwidth usage while preserving high semantic accuracy, achieving efficient balance between reconstruction quality and transmission overhead.

Conclusion: The proposed framework successfully addresses the semantic redundancy problem in visual communication by combining selective feature transmission with diffusion-based reconstruction and adaptive retransmission mechanisms.

Abstract: Semantic communication focuses on conveying the intrinsic meaning of data
rather than its raw symbolic representation. For visual content, this paradigm
shifts from traditional pixel-level transmission toward leveraging the semantic
structure of images to communicate visual meaning. Existing approaches
generally follow one of two paths: transmitting only text descriptions, which
often fail to capture precise spatial layouts and fine-grained appearance
details; or transmitting text alongside dense latent visual features, which
tends to introduce substantial semantic redundancy. A key challenge, therefore,
is to reduce semantic redundancy while preserving semantic understanding and
visual fidelity, thereby improving overall transmission efficiency. This paper
introduces a diffusion-based semantic communication framework with adaptive
retransmission. The system transmits concise text descriptions together with a
limited set of key latent visual features, and employs a diffusion-based
inpainting model to reconstruct the image. A receiver-side semantic consistency
mechanism is designed to evaluate the alignment between the reconstructed image
and the original text description. When a semantic discrepancy is detected, the
receiver triggers a retransmission to request a small set of additional latent
blocks and refine the image reconstruction. This approach significantly reduces
bandwidth usage while preserving high semantic accuracy, achieving an efficient
balance between reconstruction quality and transmission overhead.

</details>


### [241] [PolarZero: A Reinforcement Learning Approach for Low-Complexity Polarization Kernel Design](https://arxiv.org/abs/2510.26452)
*Yi-Ting Hong,Stefano Rini,Luca Barletta*

Main category: cs.IT

TL;DR: This paper proposes a reinforcement learning approach using Gumbel AlphaZero to design large-size polar code kernels that achieve better error exponents while minimizing decoding complexity.


<details>
  <summary>Details</summary>
Motivation: Polar codes with large kernels can achieve improved error exponents but are challenging to design with low decoding complexity, creating a need for automated design methods.

Method: Uses reinforcement learning framework based on Gumbel AlphaZero algorithm to explore kernel design space under recursive maximum likelihood decoding (RMLD).

Result: For size-16 kernel, achieved 17% lower decoding complexity than handcrafted designs while reaching error exponent of 0.5183 compared to 0.5 for Arikan's kernel.

Conclusion: The learning-based approach is effective for practical polar code construction, demonstrating superior performance in balancing error exponents and decoding complexity.

Abstract: Polar codes with large kernels can achieve improved error exponents but are
challenging to design with low decoding com- plexity. This work investigates
kernel construction under recursive maximum likelihood decoding (RMLD) using a
reinforcement learning framework based on the Gumbel AlphaZero algorithm. The
proposed method efficiently explores the design space and identifies large-size
kernels that satisfy a given error exponent while minimizing decoding
complexity. For a size-16 kernel, it achieves 17% lower decoding complexity
than handcrafted designs while reaching an error exponent of 0.5183 compared to
0.5 for Arikan's kernel, demonstrating the effectiveness of the learning-based
approach for practical polar code construction.

</details>


### [242] [Entropy Functions on Two-Dimensional Faces of Polymatroidal Region of Degree Four: Part II: Information Theoretic Constraints Breed New Combinatorial Structures](https://arxiv.org/abs/2510.26552)
*Shaocheng Liu,Qi Chen,Minquan Cheng*

Main category: cs.IT

TL;DR: Characterization of entropy functions on 2-dimensional faces of the polymatroidal region Γ₄, completing the analysis of all 59 face types from Part I.


<details>
  <summary>Details</summary>
Motivation: Fundamental importance of characterizing entropy functions in information theory, particularly by studying their structures on specific faces of the polymatroidal region.

Method: Used combinatorial design structures to characterize entropy functions on the remaining 10 types of 2-dimensional faces of Γ₄ after enumerating all 59 types in Part I.

Result: Fully characterized entropy functions on 8 of the remaining 10 face types and partially characterized the other 2 types.

Conclusion: Completes the characterization of entropy functions on all 2-dimensional faces of Γ₄, introducing novel combinatorial design structures in the process.

Abstract: Characterization of entropy functions is of fundamental importance in
information theory. By imposing constraints on their Shannon outer bound, i.e.,
the polymatroidal region, one obtains the faces of the region and entropy
functions on them with special structures. In this series of two papers, we
characterize entropy functions on the $2$-dimensional faces of the
polymatroidal region $\Gamma_4$. In Part I, we formulated the problem,
enumerated all $59$ types of $2$-dimensional faces of $\Gamma_4$ by a
algorithm, and fully characterized entropy functions on $49$ types of them. In
this paper, i.e., Part II, we will characterize entropy functions on the
remaining $10$ types of faces, among which $8$ types are fully characterized
and $2$ types are partially characterized. To characterize these types of
faces, we introduce some new combinatorial design structures which are
interesting themself.

</details>
