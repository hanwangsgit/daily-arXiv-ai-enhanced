<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 69]
- [cs.IT](#cs.IT) [Total: 8]
- [cs.LG](#cs.LG) [Total: 97]
- [eess.SP](#eess.SP) [Total: 13]
- [cs.RO](#cs.RO) [Total: 39]
- [eess.IV](#eess.IV) [Total: 8]
- [cs.AI](#cs.AI) [Total: 19]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [Thermal Imaging for Contactless Cardiorespiratory and Sudomotor Response Monitoring](https://arxiv.org/abs/2602.12361)
*Constantino Álvarez Casado,Mohammad Rahman,Sasan Sharifipour,Nhi Nguyen,Manuel Lage Cañellas,Xiaoting Wu,Miguel Bordallo López*

Main category: cs.CV

TL;DR: Thermal infrared imaging enables contactless estimation of electrodermal activity (EDA), heart rate (HR), and breathing rate (BR) from facial thermal video using signal processing pipelines, achieving moderate correlation for EDA and reasonable accuracy for BR/HR given hardware limitations.


<details>
  <summary>Details</summary>
Motivation: Visible-light methods can estimate HR and BR but cannot access EDA, which is a standard marker of sympathetic activation. Thermal imaging captures skin temperature changes driven by autonomic regulation, potentially providing contactless access to all three biosignals including EDA.

Method: Signal-processing pipeline that tracks anatomical facial regions, applies spatial aggregation, and separates slow sudomotor trends from faster cardiorespiratory components. For HR: orthogonal matrix image transformation (OMIT) decomposition across multiple facial ROIs. For BR: averaging nasal and cheek signals before spectral peak detection. Evaluated 288 EDA configurations and HR/BR pipeline on 31 sessions from SIMULATOR STUDY 1 dataset.

Result: Best fixed EDA configuration (nose region, exponential moving average) achieved mean absolute correlation of 0.40 ± 0.23 against palm EDA, with individual sessions reaching 0.89. BR estimation: 3.1 ± 1.1 bpm MAE. HR estimation: 13.8 ± 7.5 bpm MAE (limited by low 7.5 Hz frame rate). Observed signal polarity alternation, short thermodynamic latency, and condition/demographic effects on extraction quality.

Conclusion: Thermal imaging provides a viable contactless method for estimating EDA, HR, and BR simultaneously, with performance bounds established for current hardware limitations. Results offer baseline performance and design guidance for thermal contactless biosignal estimation systems.

Abstract: Thermal infrared imaging captures skin temperature changes driven by autonomic regulation and can potentially provide contactless estimation of electrodermal activity (EDA), heart rate (HR), and breathing rate (BR). While visible-light methods address HR and BR, they cannot access EDA, a standard marker of sympathetic activation. This paper characterizes the extraction of these three biosignals from facial thermal video using a signal-processing pipeline that tracks anatomical regions, applies spatial aggregation, and separates slow sudomotor trends from faster cardiorespiratory components. For HR, we apply an orthogonal matrix image transformation (OMIT) decomposition across multiple facial regions of interest (ROIs), and for BR we average nasal and cheek signals before spectral peak detection. We evaluate 288 EDA configurations and the HR/BR pipeline on 31 sessions from the public SIMULATOR STUDY 1 (SIM1) driver monitoring dataset. The best fixed EDA configuration (nose region, exponential moving average) reaches a mean absolute correlation of $0.40 \pm 0.23$ against palm EDA, with individual sessions reaching 0.89. BR estimation achieves a mean absolute error of $3.1 \pm 1.1$ bpm, while HR estimation yields $13.8 \pm 7.5$ bpm MAE, limited by the low camera frame rate (7.5 Hz). We report signal polarity alternation across sessions, short thermodynamic latency for well-tracked signals, and condition-dependent and demographic effects on extraction quality. These results provide baseline performance bounds and design guidance for thermal contactless biosignal estimation.

</details>


### [2] [LLaMo: Scaling Pretrained Language Models for Unified Motion Understanding and Generation with Continuous Autoregressive Tokens](https://arxiv.org/abs/2602.12370)
*Zekun Li,Sizhe An,Chengcheng Tang,Chuan Guo,Ivan Shugurov,Linguang Zhang,Amy Zhao,Srinath Sridhar,Lingling Tao,Abhay Mittal*

Main category: cs.CV

TL;DR: LLaMo is a unified motion-language model that extends pretrained LLMs with a Mixture-of-Transformers architecture to enable high-fidelity text-to-motion generation and motion-to-text captioning while preserving linguistic capabilities.


<details>
  <summary>Details</summary>
Motivation: Current approaches for motion-language models suffer from catastrophic forgetting of linguistic capabilities when fine-tuning LLMs on limited motion-text data, and introduce jitter artifacts from discrete motion tokenization. There's a need for a unified framework that preserves language understanding while enabling high-quality motion generation.

Method: Proposes LLaMo with a modality-specific Mixture-of-Transformers architecture that extends pretrained LLMs. Encodes motion into a causal continuous latent space and uses a lightweight flow-matching head for streaming motion generation (>30 FPS) while maintaining the next-token prediction paradigm.

Result: Achieves high-fidelity text-to-motion generation and motion-to-text captioning, especially excelling in zero-shot motion generation. The model preserves linguistic capabilities while enabling real-time motion generation.

Conclusion: LLaMo represents a significant step toward a general unified motion-language large model by addressing key limitations of existing approaches through continuous motion encoding and scalable multimodal adaptation while preserving pretrained language understanding.

Abstract: Recent progress in large models has led to significant advances in unified multimodal generation and understanding. However, the development of models that unify motion-language generation and understanding remains largely underexplored. Existing approaches often fine-tune large language models (LLMs) on paired motion-text data, which can result in catastrophic forgetting of linguistic capabilities due to the limited scale of available text-motion pairs. Furthermore, prior methods typically convert motion into discrete representations via quantization to integrate with language models, introducing substantial jitter artifacts from discrete tokenization. To address these challenges, we propose LLaMo, a unified framework that extends pretrained LLMs through a modality-specific Mixture-of-Transformers (MoT) architecture. This design inherently preserves the language understanding of the base model while enabling scalable multimodal adaptation. We encode human motion into a causal continuous latent space and maintain the next-token prediction paradigm in the decoder-only backbone through a lightweight flow-matching head, allowing for streaming motion generation in real-time (>30 FPS). Leveraging the comprehensive language understanding of pretrained LLMs and large-scale motion-text pretraining, our experiments demonstrate that LLaMo achieves high-fidelity text-to-motion generation and motion-to-text captioning in general settings, especially zero-shot motion generation, marking a significant step towards a general unified motion-language large model.

</details>


### [3] [Synthetic Image Detection with CLIP: Understanding and Assessing Predictive Cues](https://arxiv.org/abs/2602.12381)
*Marco Willi,Melanie Mathys,Michael Graber*

Main category: cs.CV

TL;DR: CLIP-based synthetic image detectors perform well but rely on photographic attributes rather than generator artifacts, showing limited generalization across different generative models.


<details>
  <summary>Details</summary>
Motivation: As generative models produce near-photorealistic images, synthetic image detection (SID) becomes crucial for trustworthiness. However, existing SID methods struggle with generalization to novel generative models and practical settings.

Method: Introduces SynthCLIC dataset with real photos and high-quality synthetic counterparts from diffusion models to reduce semantic bias. Uses interpretable linear head with de-correlated activations and text-grounded concept-model to analyze what CLIP-based detectors learn.

Result: CLIP-based linear detectors achieve 0.96 mAP on GAN-based benchmarks but only 0.92 on SynthCLIC, with cross-generator generalization dropping to 0.37 mAP. Detectors primarily rely on high-level photographic attributes rather than generator-specific artifacts.

Conclusion: CLIP-based detectors perform well but generalize unevenly across generative architectures, highlighting the need for continual updates and broader training exposure while reinforcing CLIP as a strong foundation for more universal, robust SID.

Abstract: Recent generative models produce near-photorealistic images, challenging the trustworthiness of photographs. Synthetic image detection (SID) has thus become an important area of research. Prior work has highlighted how synthetic images differ from real photographs--unfortunately, SID methods often struggle to generalize to novel generative models and often perform poorly in practical settings. CLIP, a foundational vision-language model which yields semantically rich image-text embeddings, shows strong accuracy and generalization for SID. Yet, the underlying relevant cues embedded in CLIP-features remain unknown. It is unclear, whether CLIP-based detectors simply detect strong visual artifacts or exploit subtle semantic biases, both of which would render them useless in practical settings or on generative models of high quality. We introduce SynthCLIC, a paired dataset of real photographs and high-quality synthetic counterparts from recent diffusion models, designed to reduce semantic bias in SID. Using an interpretable linear head with de-correlated activations and a text-grounded concept-model, we analyze what CLIP-based detectors learn. CLIP-based linear detectors reach 0.96 mAP on a GAN-based benchmark but only 0.92 on our high-quality diffusion dataset SynthCLIC, and generalization across generator families drops to as low as 0.37 mAP. We find that the detectors primarily rely on high-level photographic attributes (e.g., minimalist style, lens flare, or depth layering), rather than overt generator-specific artifacts. CLIP-based detectors perform well overall but generalize unevenly across diverse generative architectures. This highlights the need for continual model updates and broader training exposure, while reinforcing CLIP-based approaches as a strong foundation for more universal, robust SID.

</details>


### [4] [Reproducing DragDiffusion: Interactive Point-Based Editing with Diffusion Models](https://arxiv.org/abs/2602.12393)
*Ali Subhan,Ashir Raza*

Main category: cs.CV

TL;DR: Reproducibility study confirms DragDiffusion's core claims about point-based image editing via diffusion latent optimization, but reveals sensitivity to key hyperparameters like timestep selection and feature supervision level.


<details>
  <summary>Details</summary>
Motivation: To conduct a reproducibility study of DragDiffusion, verifying its claims about interactive point-based image editing through diffusion latent optimization and identifying conditions for reliable reproduction.

Method: Used authors' released implementation and DragBench benchmark; reproduced main ablation studies on diffusion timestep selection, LoRA-based fine-tuning, mask regularization strength, and UNet feature supervision; evaluated multi-timestep latent optimization variant.

Result: Close agreement with original qualitative/quantitative trends; performance sensitive to optimized timestep and feature level for motion supervision; multi-timestep variant doesn't improve accuracy while increasing computational cost.

Conclusion: Findings support DragDiffusion's central claims while clarifying conditions for reliable reproducibility, with code available for verification.

Abstract: DragDiffusion is a diffusion-based method for interactive point-based image editing that enables users to manipulate images by directly dragging selected points. The method claims that accurate spatial control can be achieved by optimizing a single diffusion latent at an intermediate timestep, together with identity-preserving fine-tuning and spatial regularization. This work presents a reproducibility study of DragDiffusion using the authors' released implementation and the DragBench benchmark. We reproduce the main ablation studies on diffusion timestep selection, LoRA-based fine-tuning, mask regularization strength, and UNet feature supervision, and observe close agreement with the qualitative and quantitative trends reported in the original work. At the same time, our experiments show that performance is sensitive to a small number of hyperparameter assumptions, particularly the optimized timestep and the feature level used for motion supervision, while other components admit broader operating ranges. We further evaluate a multi-timestep latent optimization variant and find that it does not improve spatial accuracy while substantially increasing computational cost. Overall, our findings support the central claims of DragDiffusion while clarifying the conditions under which they are reliably reproducible. Code is available at https://github.com/AliSubhan5341/DragDiffusion-TMLR-Reproducibility-Challenge.

</details>


### [5] [What does RL improve for Visual Reasoning? A Frankenstein-Style Analysis](https://arxiv.org/abs/2602.12395)
*Xirui Li,Ming Li,Tianyi Zhou*

Main category: cs.CV

TL;DR: RL in vision-language models doesn't uniformly enhance visual perception but systematically refines mid-to-late transformer layers to improve vision-to-reasoning alignment and reasoning performance.


<details>
  <summary>Details</summary>
Motivation: To understand what specific capabilities RL actually improves in vision-language models compared to supervised fine-tuning, since benchmark gains conflate multiple factors and make it difficult to attribute improvements to specific skills.

Method: Proposed a Frankenstein-style analysis framework with three components: (1) functional localization via causal probing, (2) update characterization via parameter comparison, and (3) transferability test via model merging.

Result: RL induces a consistent inference-time shift primarily in mid-to-late layers, and these mid-to-late refinements are both transferable (via merging) and necessary (via freezing) for RL gains.

Conclusion: RL's reliable contribution in visual reasoning is not a uniform enhancement of visual perception, but a systematic refinement of mid-to-late transformer computation that improves vision-to-reasoning alignment and reasoning performance, highlighting limitations of benchmark-only evaluation.

Abstract: Reinforcement learning (RL) with verifiable rewards has become a standard post-training stage for boosting visual reasoning in vision-language models, yet it remains unclear what capabilities RL actually improves compared with supervised fine-tuning as cold-start initialization (IN). End-to-end benchmark gains conflate multiple factors, making it difficult to attribute improvements to specific skills. To bridge the gap, we propose a Frankenstein-style analysis framework including: (i) functional localization via causal probing; (ii) update characterization via parameter comparison; and (iii) transferability test via model merging. Instead, RL induces a consistent inference-time shift primarily in mid-to-late layers, and these mid-to-late refinements are both transferable (via merging) and necessary (via freezing) for RL gains. Overall, our results suggest that RL's reliable contribution in visual reasoning is not a uniform enhancement of visual perception, but a systematic refinement of mid-to-late transformer computation that improves vision-to-reasoning alignment and reasoning performance, highlighting the limitations of benchmark-only evaluation for understanding multimodal reasoning improvements.

</details>


### [6] [ZeroDiff++: Substantial Unseen Visual-semantic Correlation in Zero-shot Learning](https://arxiv.org/abs/2602.12401)
*Zihan Ye,Shreyank N Gowda,Kaile Du,Weijian Luo,Ling Shao*

Main category: cs.CV

TL;DR: ZeroDiff++ is a diffusion-based generative framework for Zero-shot Learning that addresses spurious visual-semantic correlations and data scarcity through diffusion augmentation, contrastive representations, multi-view discriminators, and test-time adaptation/generation.


<details>
  <summary>Details</summary>
Motivation: Existing generative ZSL methods suffer from spurious visual-semantic correlations worsened by scarce seen class samples, and unadaptive fully noised generators produce features disconnected from real test samples, creating a critical bottleneck for ZSL performance.

Method: ZeroDiff++ uses: (1) diffusion augmentation for diverse noised samples, (2) supervised contrastive representations for instance-level semantics, (3) multi-view discriminators with Wasserstein mutual learning, (4) Diffusion-based Test time Adaptation (DiffTTA) using pseudo label reconstruction, and (5) Diffusion-based Test time Generation (DiffGen) to produce partially synthesized features connecting real and generated data.

Result: Extensive experiments on three ZSL benchmarks show ZeroDiff++ achieves significant improvements over existing ZSL methods and maintains robust performance even with scarce training data.

Conclusion: ZeroDiff++ effectively addresses spurious visual-semantic correlations and data scarcity in ZSL through a comprehensive diffusion-based framework with test-time adaptation and generation, demonstrating superior performance and robustness.

Abstract: Zero-shot Learning (ZSL) enables classifiers to recognize classes unseen during training, commonly via generative two stage methods: (1) learn visual semantic correlations from seen classes; (2) synthesize unseen class features from semantics to train classifiers. In this paper, we identify spurious visual semantic correlations in existing generative ZSL worsened by scarce seen class samples and introduce two metrics to quantify spuriousness for seen and unseen classes. Furthermore, we point out a more critical bottleneck: existing unadaptive fully noised generators produce features disconnected from real test samples, which also leads to the spurious correlation. To enhance the visual-semantic correlations on both seen and unseen classes, we propose ZeroDiff++, a diffusion-based generative framework. In training, ZeroDiff++ uses (i) diffusion augmentation to produce diverse noised samples, (ii) supervised contrastive (SC) representations for instance level semantics, and (iii) multi view discriminators with Wasserstein mutual learning to assess generated features. At generation time, we introduce (iv) Diffusion-based Test time Adaptation (DiffTTA) to adapt the generator using pseudo label reconstruction, and (v) Diffusion-based Test time Generation (DiffGen) to trace the diffusion denoising path and produce partially synthesized features that connect real and generated data, and mitigates data scarcity further. Extensive experiments on three ZSL benchmarks demonstrate that ZeroDiff++ not only achieves significant improvements over existing ZSL methods but also maintains robust performance even with scarce training data. Code would be available.

</details>


### [7] [MonoLoss: A Training Objective for Interpretable Monosemantic Representations](https://arxiv.org/abs/2602.12403)
*Ali Nasiri-Sarvi,Anh Tien Nguyen,Hassan Rivaz,Dimitris Samaras,Mahdi S. Hosseini*

Main category: cs.CV

TL;DR: Researchers develop MonoLoss, a plug-in training objective that improves monosemanticity in sparse autoencoders by efficiently computing and optimizing the MonoScore metric, achieving significant speedups and better interpretable feature decomposition.


<details>
  <summary>Details</summary>
Motivation: Standard sparse autoencoder training objectives only weakly encourage decomposition of polysemantic representations into monosemantic features, and existing monosemanticity metrics are computationally expensive (quadratic complexity) for training and evaluation.

Method: Derived a single-pass algorithm for computing MonoScore with linear rather than quadratic complexity, then introduced Monosemanticity Loss (MonoLoss) as a plug-in objective that directly rewards semantically consistent activations during training.

Result: Achieved up to 1200x speedup in evaluation and 159x during training with only ~4% per-epoch overhead. MonoLoss improved MonoScore for most latents and significantly increased class purity (from 0.152 to 0.723). As an auxiliary regularizer, it yielded up to 0.6% accuracy gains on ImageNet-1K.

Conclusion: MonoLoss enables efficient training and evaluation of monosemantic representations in sparse autoencoders, improving both interpretability and downstream task performance across various architectures and datasets.

Abstract: Sparse autoencoders (SAEs) decompose polysemantic neural representations, where neurons respond to multiple unrelated concepts, into monosemantic features that capture single, interpretable concepts. However, standard training objectives only weakly encourage this decomposition, and existing monosemanticity metrics require pairwise comparisons across all dataset samples, making them inefficient during training and evaluation. We study a recent MonoScore metric and derive a single-pass algorithm that computes exactly the same quantity, but with a cost that grows linearly, rather than quadratically, with the number of dataset images. On OpenImagesV7, we achieve up to a 1200x speedup wall-clock speedup in evaluation and 159x during training, while adding only ~4% per-epoch overhead. This allows us to treat MonoScore as a training signal: we introduce the Monosemanticity Loss (MonoLoss), a plug-in objective that directly rewards semantically consistent activations for learning interpretable monosemantic representations. Across SAEs trained on CLIP, SigLIP2, and pretrained ViT features, using BatchTopK, TopK, and JumpReLU SAEs, MonoLoss increases MonoScore for most latents. MonoLoss also consistently improves class purity (the fraction of a latent's activating images belonging to its dominant class) across all encoder and SAE combinations, with the largest gain raising baseline purity from 0.152 to 0.723. Used as an auxiliary regularizer during ResNet-50 and CLIP-ViT-B/32 finetuning, MonoLoss yields up to 0.6\% accuracy gains on ImageNet-1K and monosemantic activating patterns on standard benchmark datasets. The code is publicly available at https://github.com/AtlasAnalyticsLab/MonoLoss.

</details>


### [8] [Prototype-driven fusion of pathology and spatial transcriptomics for interpretable survival prediction](https://arxiv.org/abs/2602.12441)
*Lihe Liu,Xiaoxi Pan,Yinyin Yuan,Lulu Shang*

Main category: cs.CV

TL;DR: PathoSpatial is an interpretable framework that fuses whole slide images and spatial transcriptomics for prognostic modeling using multi-level prototype learning.


<details>
  <summary>Details</summary>
Motivation: As paired WSI-ST cohorts scale, there's a need for principled cross-modal fusion strategies to leverage complementary spatial signals (morphology + molecular context) for prognosis, but current methods are limited.

Method: PathoSpatial uses task-guided prototype learning within a multi-level experts architecture, adaptively orchestrating unsupervised within-modality discovery with supervised cross-modal aggregation.

Result: On triple-negative breast cancer cohort, PathoSpatial delivers strong performance across five survival endpoints, superior/comparable to leading unimodal/multimodal methods, with inherent interpretability.

Conclusion: PathoSpatial demonstrates scalable and interpretable multimodal learning for spatial omics-pathology fusion, providing biologically grounded explanations for prognosis.

Abstract: Whole slide images (WSIs) enable weakly supervised prognostic modeling via multiple instance learning (MIL). Spatial transcriptomics (ST) preserves in situ gene expression, providing a spatial molecular context that complements morphology. As paired WSI-ST cohorts scale to population level, leveraging their complementary spatial signals for prognosis becomes crucial; however, principled cross-modal fusion strategies remain limited for this paradigm. To this end, we introduce PathoSpatial, an interpretable end-to-end framework integrating co-registered WSIs and ST to learn spatially informed prognostic representations. PathoSpatial uses task-guided prototype learning within a multi-level experts architecture, adaptively orchestrating unsupervised within-modality discovery with supervised cross-modal aggregation. By design, PathoSpatial substantially strengthens interpretability while maintaining discriminative ability. We evaluate PathoSpatial on a triple-negative breast cancer cohort with paired ST and WSIs. PathoSpatial delivers strong and consistent performance across five survival endpoints, achieving superior or comparable performance to leading unimodal and multimodal methods. PathoSpatial inherently enables post-hoc prototype interpretation and molecular risk decomposition, providing quantitative, biologically grounded explanations, highlighting candidate prognostic factors. We present PathoSpatial as a proof-of-concept for scalable and interpretable multimodal learning for spatial omics-pathology fusion.

</details>


### [9] [Semantic-aware Adversarial Fine-tuning for CLIP](https://arxiv.org/abs/2602.12461)
*Jiacheng Zhang,Jinhao Li,Hanxun Huang,Sarah M. Erfani,Benjamin I. P. Rubinstein,Feng Liu*

Main category: cs.CV

TL;DR: SAFT improves CLIP's adversarial robustness by fine-tuning with semantic-aware adversarial examples generated using ensemble of refined textual descriptions instead of single hand-crafted templates.


<details>
  <summary>Details</summary>
Motivation: Current adversarial fine-tuning methods for CLIP use adversarial examples generated by minimizing cosine similarity between images and single hand-crafted templates, which is insufficient for measuring image-text similarity and leads to less robust models.

Method: Proposes semantic-ensemble attack that generates semantic-aware adversarial examples by minimizing average similarity between original image and ensemble of refined textual descriptions. These descriptions are generated by foundation models to capture core semantic features and refined to reduce hallucinations. Then uses Semantic-aware Adversarial Fine-Tuning (SAFT) to fine-tune CLIP's image encoder with these semantic-aware adversarial examples.

Result: SAFT outperforms current methods, achieving substantial improvements in zero-shot adversarial robustness across 16 datasets.

Conclusion: Using semantic-aware adversarial examples generated from ensemble of refined textual descriptions significantly improves CLIP's adversarial robustness compared to methods using single hand-crafted templates.

Abstract: Recent studies have shown that CLIP model's adversarial robustness in zero-shot classification tasks can be enhanced by adversarially fine-tuning its image encoder with adversarial examples (AEs), which are generated by minimizing the cosine similarity between images and a hand-crafted template (e.g., ''A photo of a {label}''). However, it has been shown that the cosine similarity between a single image and a single hand-crafted template is insufficient to measure the similarity for image-text pairs. Building on this, in this paper, we find that the AEs generated using cosine similarity may fail to fool CLIP when the similarity metric is replaced with semantically enriched alternatives, making the image encoder fine-tuned with these AEs less robust. To overcome this issue, we first propose a semantic-ensemble attack to generate semantic-aware AEs by minimizing the average similarity between the original image and an ensemble of refined textual descriptions. These descriptions are initially generated by a foundation model to capture core semantic features beyond hand-crafted templates and are then refined to reduce hallucinations. To this end, we propose Semantic-aware Adversarial Fine-Tuning (SAFT), which fine-tunes CLIP's image encoder with semantic-aware AEs. Extensive experiments show that SAFT outperforms current methods, achieving substantial improvements in zero-shot adversarial robustness across 16 datasets. Our code is available at: https://github.com/tmlr-group/SAFT.

</details>


### [10] [A Lightweight and Explainable DenseNet-121 Framework for Grape Leaf Disease Classification](https://arxiv.org/abs/2602.12484)
*Md. Ehsanul Haque,Md. Saymon Hosen Polash,Rakib Hasan Ovi,Aminul Kader Bulbul,Md Kamrul Siam,Tamim Hasan Saykat*

Main category: cs.CV

TL;DR: Proposes an optimized DenseNet121 model for grape leaf disease classification achieving 99.27% accuracy with interpretable outputs via Grad-CAM, outperforming baseline CNNs while being computationally efficient for real-world deployment.


<details>
  <summary>Details</summary>
Motivation: Grape diseases significantly impact production quality and quantity, but current automated methods (especially YOLO-based) are computationally costly and lack interpretability, making them unsuitable for real-world vineyard management scenarios.

Method: Uses optimized DenseNet121 architecture with domain-specific preprocessing to extract disease-relevant features (veins, edges, lesions). Employs transfer learning for consistency on smaller/unbalanced samples and Grad-CAM for interpretability. Compares with ResNet18, VGG16, AlexNet, and SqueezeNet baselines.

Result: Achieves 99.27% accuracy, 99.28% F1 score, 99.71% specificity, 98.86% Kappa with 9-second inference time. Cross-validation shows 99.12% mean accuracy. Outperforms all baseline CNN models while maintaining computational efficiency.

Conclusion: The proposed framework is scalable, precise, and computationally inexpensive for grape leaf disease detection, combining effective architecture, domain-specific preprocessing, and interpretable outputs suitable for real-time deployment in vineyard management.

Abstract: Grapes are among the most economically and culturally significant fruits on a global scale, and table grapes and wine are produced in significant quantities in Europe and Asia. The production and quality of grapes are significantly impacted by grape diseases such as Bacterial Rot, Downy Mildew, and Powdery Mildew. Consequently, the sustainable management of a vineyard necessitates the early and precise identification of these diseases. Current automated methods, particularly those that are based on the YOLO framework, are often computationally costly and lack interpretability that makes them unsuitable for real-world scenarios. This study proposes grape leaf disease classification using Optimized DenseNet 121. Domain-specific preprocessing and extensive connectivity reveal disease-relevant characteristics, including veins, edges, and lesions. An extensive comparison with baseline CNN models, including ResNet18, VGG16, AlexNet, and SqueezeNet, demonstrates that the proposed model exhibits superior performance. It achieves an accuracy of 99.27%, an F1 score of 99.28%, a specificity of 99.71%, and a Kappa of 98.86%, with an inference time of 9 seconds. The cross-validation findings show a mean accuracy of 99.12%, indicating strength and generalizability across all classes. We also employ Grad-CAM to highlight disease-related regions to guarantee the model is highlighting physiologically relevant aspects and increase transparency and confidence. Model optimization reduces processing requirements for real-time deployment, while transfer learning ensures consistency on smaller and unbalanced samples. An effective architecture, domain-specific preprocessing, and interpretable outputs make the proposed framework scalable, precise, and computationally inexpensive for detecting grape leaf diseases.

</details>


### [11] [Human-Like Coarse Object Representations in Vision Models](https://arxiv.org/abs/2602.12486)
*Andrey Gizdov,Andrea Procopio,Yichen Li,Daniel Harari,Tomer Ullman*

Main category: cs.CV

TL;DR: Human-like coarse object representations for physics emerge from resource constraints, not specialized biases, with intermediate model granularity best matching human behavior.


<details>
  <summary>Details</summary>
Motivation: Humans use coarse volumetric bodies for intuitive physics, while segmentation models optimize pixel-accurate masks. The paper investigates whether and when these models acquire human-like coarse bodies despite their different optimization objectives.

Method: Used time-to-collision behavioral paradigm with comparison pipeline and alignment metric. Varied model training time, size, and effective capacity via pruning to analyze segmentation behavior across different resource constraints.

Result: Alignment with human behavior follows inverse U-shaped curve: small/briefly trained/pruned models under-segment into blobs; large/fully trained models over-segment with boundary wiggles; intermediate granularity best matches human coarse bodies.

Conclusion: Human-like coarse bodies emerge from resource constraints rather than bespoke biases. Simple adjustments like early checkpoints, modest architectures, and light pruning can elicit physics-efficient representations, supporting resource-rational accounts balancing recognition detail against physical affordances.

Abstract: Humans appear to represent objects for intuitive physics with coarse, volumetric bodies'' that smooth concavities - trading fine visual details for efficient physical predictions - yet their internal structure is largely unknown. Segmentation models, in contrast, optimize pixel-accurate masks that may misalign with such bodies. We ask whether and when these models nonetheless acquire human-like bodies. Using a time-to-collision (TTC) behavioral paradigm, we introduce a comparison pipeline and alignment metric, then vary model training time, size, and effective capacity via pruning. Across all manipulations, alignment with human behavior follows an inverse U-shaped curve: small/briefly trained/pruned models under-segment into blobs; large/fully trained models over-segment with boundary wiggles; and an intermediate ideal body granularity'' best matches humans. This suggests human-like coarse bodies emerge from resource constraints rather than bespoke biases, and points to simple knobs - early checkpoints, modest architectures, light pruning - for eliciting physics-efficient representations. We situate these results within resource-rational accounts balancing recognition detail against physical affordances.

</details>


### [12] [Insertion Network for Image Sequence Correspondence](https://arxiv.org/abs/2602.12489)
*Dingjie Su,Weixiang Hong,Benoit M. Dawant,Bennett A. Landman*

Main category: cs.CV

TL;DR: A novel method for establishing correspondence between 2D image sequences using slice insertion learning, improving slice localization accuracy in 3D medical scans.


<details>
  <summary>Details</summary>
Motivation: Slice-level content navigation is crucial for diagnostic tasks and automatic pipelines, but existing methods like body part regression treat slices independently without leveraging contextual sequence information.

Method: Train a network to learn how to insert a slice from one sequence into the appropriate position in another sequence using contextual slice representations and a slice-to-slice attention mechanism.

Result: The insertion network reduces slice localization errors from 8.4 mm to 5.4 mm in supervised settings, showing substantial improvement over body part regression.

Conclusion: The proposed method effectively establishes sequence correspondence by leveraging contextual information, outperforming independent slice analysis approaches for medical image navigation tasks.

Abstract: We propose a novel method for establishing correspondence between two sequences of 2D images. One particular application of this technique is slice-level content navigation, where the goal is to localize specific 2D slices within a 3D volume or determine the anatomical coverage of a 3D scan based on its 2D slices. This serves as an important preprocessing step for various diagnostic tasks, as well as for automatic registration and segmentation pipelines. Our approach builds sequence correspondence by training a network to learn how to insert a slice from one sequence into the appropriate position in another. This is achieved by encoding contextual representations of each slice and modeling the insertion process using a slice-to-slice attention mechanism. We apply this method to localize manually labeled key slices in body CT scans and compare its performance to the current state-of-the-art alternative known as body part regression, which predicts anatomical position scores for individual slices. Unlike body part regression, which treats each slice independently, our method leverages contextual information from the entire sequence. Experimental results show that the insertion network reduces slice localization errors in supervised settings from 8.4 mm to 5.4 mm, demonstrating a substantial improvement in accuracy.

</details>


### [13] [Layer-Specific Fine-Tuning for Improved Negation Handling in Medical Vision-Language Models](https://arxiv.org/abs/2602.12498)
*Ali Abbasi,Mehdi Taghipour,Rahmatollah Beheshti*

Main category: cs.CV

TL;DR: NAST: A method using causal interpretability to improve vision-language models' ability to distinguish affirmative from negated medical statements in radiology reports.


<details>
  <summary>Details</summary>
Motivation: Vision-language models frequently fail to distinguish affirmative from negated medical statements in clinical reporting, which is a critical limitation in safety-critical medical settings where accurate negation understanding is essential.

Method: 1) Created a radiology-specific diagnostic benchmark to evaluate polarity sensitivity; 2) Built a contextual clinical negation dataset with structured claims and attribute-level negations; 3) Proposed Negation-Aware Selective Training (NAST) - an interpretability-guided adaptation method that uses causal tracing effects to modulate layer-wise gradient updates during fine-tuning, scaling each layer's update according to its causal contribution to negation processing.

Result: Experiments show improved discrimination of affirmative and negated clinical statements without degrading general vision-language alignment, demonstrating the value of causal interpretability for targeted model adaptation in medical settings.

Conclusion: NAST successfully transforms mechanistic interpretability signals into a principled optimization rule, enabling vision-language models to better handle clinical negation while maintaining overall performance, highlighting the importance of causal interpretability for safety-critical medical applications.

Abstract: Negation is a fundamental linguistic operation in clinical reporting, yet vision-language models (VLMs) frequently fail to distinguish affirmative from negated medical statements. To systematically characterize this limitation, we introduce a radiology-specific diagnostic benchmark that evaluates polarity sensitivity under controlled clinical conditions, revealing that common medical VLMs consistently confuse negated and non-negated findings. To enable learning beyond simple condition absence, we further construct a contextual clinical negation dataset that encodes structured claims and supports attribute-level negations involving location and severity. Building on these resources, we propose Negation-Aware Selective Training (NAST), an interpretability-guided adaptation method that uses causal tracing effects (CTEs) to modulate layer-wise gradient updates during fine-tuning. Rather than applying uniform learning rates, NAST scales each layer's update according to its causal contribution to negation processing, transforming mechanistic interpretability signals into a principled optimization rule. Experiments demonstrate improved discrimination of affirmative and negated clinical statements without degrading general vision-language alignment, highlighting the value of causal interpretability for targeted model adaptation in safety-critical medical settings. Code and resources are available at https://github.com/healthylaife/NAST.

</details>


### [14] [Matching of SAR and optical images based on transformation to shared modality](https://arxiv.org/abs/2602.12515)
*Alexey Borisov,Evgeny Myasnikov,Vladislav Myasnikov*

Main category: cs.CV

TL;DR: A novel approach for optical-SAR image matching by transforming both to a shared modality, enabling use of pre-trained regular image matching models without retraining.


<details>
  <summary>Details</summary>
Motivation: Optical and SAR images have fundamental physical differences that make precise co-registration difficult. Existing methods struggle with cross-modality matching due to these inherent disparities in acquisition principles.

Method: Transform both optical and SAR images to a new shared modality with equal predefined channels, preserving significant features while making them similar. Then use pre-trained RoMa model (designed for regular photographs) for matching without retraining.

Result: Superior performance over alternative approaches (image translation and feature matching) on MultiSenGE dataset. More versatile solution that maintains high-quality matching while enabling use of off-the-shelf models.

Conclusion: The modality transformation approach effectively bridges optical-SAR differences, allowing successful application of existing image matching models to cross-modal remote sensing data without modality-specific retraining.

Abstract: Significant differences in optical images and Synthetic Aperture Radar (SAR) images are caused by fundamental differences in the physical principles underlying their acquisition by Earth remote sensing platforms. These differences make precise image matching (co-registration) of these two types of images difficult. In this paper, we propose a new approach to image matching of optical and SAR images, which is based on transforming the images to a new modality. The new image modality is common to both optical and SAR images and satisfies the following conditions. First, the transformed images must have an equal pre-defined number of channels. Second, the transformed and co-registered images must be as similar as possible. Third, the transformed images must be non-degenerate, meaning they must preserve the significant features of the original images. To further match images transformed to this shared modality, we train the RoMa image matching model, which is one of the leading solutions for matching of regular digital photographs. We evaluated the proposed approach on the publicly available MultiSenGE dataset containing both optical and SAR images. We demonstrated its superiority over alternative approaches based on image translation between original modalities and various feature matching algorithms. The proposed solution not only provides better quality of matching, but is also more versatile. It enables the use of ready-made RoMa and DeDoDe models, pre-trained for regular images, without retraining for a new modality, while maintaining high-quality matching of optical and SAR images.

</details>


### [15] [LiDAR-Anchored Collaborative Distillation for Robust 2D Representations](https://arxiv.org/abs/2602.12524)
*Wonjun Jo,Hyunwoo Ha,Kim Ji-Yeon,Hawook Jeong,Tae-Hyun Oh*

Main category: cs.CV

TL;DR: A self-supervised method called Collaborative Distillation uses 3D LiDAR to improve 2D image encoder robustness to adverse weather while preserving original capabilities.


<details>
  <summary>Details</summary>
Motivation: Pre-trained 2D image encoders fail under noisy and adverse weather conditions beyond clear daytime scenes, limiting robust visual perception for real-world applications.

Method: Collaborative Distillation leverages 3D LiDAR as self-supervision to distill robustness into 2D image encoders while maintaining their original feature extraction capabilities.

Result: Outperforms competing methods in various downstream tasks across diverse conditions, exhibits strong generalization ability, and improves 3D awareness from LiDAR characteristics.

Conclusion: The method demonstrates practicality and adaptability for real-world scenarios by enhancing 2D image encoder robustness to adverse conditions using 3D LiDAR supervision.

Abstract: As deep learning continues to advance, self-supervised learning has made considerable strides. It allows 2D image encoders to extract useful features for various downstream tasks, including those related to vision-based systems. Nevertheless, pre-trained 2D image encoders fall short in conducting the task under noisy and adverse weather conditions beyond clear daytime scenes, which require for robust visual perception. To address these issues, we propose a novel self-supervised approach, \textbf{Collaborative Distillation}, which leverages 3D LiDAR as self-supervision to improve robustness to noisy and adverse weather conditions in 2D image encoders while retaining their original capabilities. Our method outperforms competing methods in various downstream tasks across diverse conditions and exhibits strong generalization ability. In addition, our method also improves 3D awareness stemming from LiDAR's characteristics. This advancement highlights our method's practicality and adaptability in real-world scenarios.

</details>


### [16] [Geometric Stratification for Singular Configurations of the P3P Problem via Local Dual Space](https://arxiv.org/abs/2602.12525)
*Xueying Sun,Zijia Li,Nan Li*

Main category: cs.CV

TL;DR: The paper provides a complete geometric stratification of singular configurations in the P3P problem based on the multiplicity of camera center solutions, identifying specific geometric structures (danger cylinder, Morley triangle, circumcircle) for different multiplicity levels.


<details>
  <summary>Details</summary>
Motivation: To systematically understand and classify singular configurations in the P3P (Perspective-3-Point) problem where multiple solutions exist for camera pose estimation, which is crucial for robust computer vision systems.

Method: Uses local dual space and an algebraic-computational framework to analyze geometric stratification of P3P singular configurations based on the multiplicity μ of camera center solutions.

Result: Complete classification: for μ≥2, camera center lies on the "danger cylinder"; for μ≥3, on specific generatrices associated with Morley triangle or circumcircle; for μ≥4, on circumcircle (infinite solutions). Also characterizes complementary configuration O' on deltoidal surfaces and cuspidal curves.

Conclusion: The paper establishes a comprehensive geometric framework for understanding P3P singularities, providing clear relationships between solution multiplicity and specific geometric structures, with implications for robust pose estimation algorithms.

Abstract: This paper investigates singular configurations of the P3P problem. Using local dual space, a systematic algebraic-computational framework is proposed to give a complete geometric stratification for the P3P singular configurations with respect to the multiplicity $μ$ of the camera center $O$: for $μ\ge 2$, $O$ lies on the ``danger cylinder'', for $μ\ge 3$, $O$ lies on one of three generatrices of the danger cylinder associated with the first Morley triangle or the circumcircle, and for $μ\ge 4$, $O$ lies on the circumcircle which indeed corresponds to infinite P3P solutions. Furthermore, a geometric stratification for the complementary configuration $O^\prime$ associated with a singular configuration $O$ is studied as well: for $μ\ge 2$, $O^\prime$ lies on a deltoidal surface associated with the danger cylinder, and for $μ\ge 3$, $O^\prime$ lies on one of three cuspidal curves of the deltoidal surface.

</details>


### [17] [Self-Supervised JEPA-based World Models for LiDAR Occupancy Completion and Forecasting](https://arxiv.org/abs/2602.12540)
*Haoran Zhu,Anna Choromanska*

Main category: cs.CV

TL;DR: AD-LiST-JEPA is a self-supervised world model for autonomous driving that learns to predict future spatiotemporal evolution from LiDAR data using Joint-Embedding Predictive Architecture, showing improved performance on downstream occupancy completion and forecasting tasks.


<details>
  <summary>Details</summary>
Motivation: Autonomous driving requires world models that capture environmental evolution for long-term planning, but scalability demands self-supervised learning without expensive human annotations. JEPA enables learning such models from large volumes of unlabeled data.

Method: Proposes AD-LiST-JEPA, a self-supervised world model using Joint-Embedding Predictive Architecture (JEPA) framework to predict future spatiotemporal evolution from LiDAR data. The model learns representations through JEPA-based world model learning.

Result: Proof of concept experiments demonstrate better performance on LiDAR-based occupancy completion and forecasting (OCF) task when using pretrained encoder after JEPA-based world model learning, showing improved perception and prediction capabilities.

Conclusion: AD-LiST-JEPA provides an effective self-supervised approach for learning world models in autonomous driving, enabling better environmental understanding and prediction without requiring expensive labeled data.

Abstract: Autonomous driving, as an agent operating in the physical world, requires the fundamental capability to build \textit{world models} that capture how the environment evolves spatiotemporally in order to support long-term planning. At the same time, scalability demands learning such models in a self-supervised manner; \textit{joint-embedding predictive architecture (JEPA)} enables learning world models via leveraging large volumes of unlabeled data without relying on expensive human annotations. In this paper, we propose \textbf{AD-LiST-JEPA}, a self-supervised world model for autonomous driving that predicts future spatiotemporal evolution from LiDAR data using a JEPA framework. We evaluate the quality of the learned representations through a downstream LiDAR-based occupancy completion and forecasting (OCF) task, which jointly assesses perception and prediction. Proof of concept experiments show better OCF performance with pretrained encoder after JEPA-based world model learning.

</details>


### [18] [PLLM: Pseudo-Labeling Large Language Models for CAD Program Synthesis](https://arxiv.org/abs/2602.12561)
*Yuanbo Li,Dule Shu,Yanying Chen,Matt Klenk,Daniel Ritchie*

Main category: cs.CV

TL;DR: PLLM is a self-training framework for CAD program synthesis from unlabeled 3D shapes that uses iterative sampling, selection, and augmentation to create synthetic training data.


<details>
  <summary>Details</summary>
Motivation: Existing CAD program synthesis methods require supervised training with paired shape-program data, which is often unavailable or limited. There's a need for methods that can work with unlabeled 3D shape data alone.

Method: PLLM uses a self-training framework that iteratively: 1) samples candidate CAD programs from a pre-trained LLM, 2) selects high-fidelity program executions, and 3) augments programs to create synthetic program-shape pairs for fine-tuning, all using only unlabeled 3D shapes.

Result: Experiments adapting CAD-Recode from DeepCAD to the unlabeled ABC dataset show consistent improvements in both geometric fidelity and program diversity compared to existing methods.

Conclusion: PLLM demonstrates that self-training with unlabeled 3D shapes can effectively improve CAD program synthesis without requiring paired training data, offering a promising approach for CAD reconstruction tasks.

Abstract: Recovering Computer-Aided Design (CAD) programs from 3D geometries is a widely studied problem. Recent advances in large language models (LLMs) have enabled progress in CAD program synthesis, but existing methods rely on supervised training with paired shape-program data, which is often unavailable. We introduce PLLM, a self-training framework for CAD program synthesis from unlabeled 3D shapes. Given a pre-trained CAD-capable LLM and a shape dataset, PLLM iteratively samples candidate programs, selects high-fidelity executions, and augments programs to construct synthetic program-shape pairs for fine-tuning. We experiment on adapting CAD-Recode from DeepCAD to the unlabeled ABC dataset show consistent improvements in geometric fidelity and program diversity.

</details>


### [19] [The Constant Eye: Benchmarking and Bridging Appearance Robustness in Autonomous Driving](https://arxiv.org/abs/2602.12563)
*Jiabao Wang,Hongyu Zhou,Yuanbo Yang,Jiahao Shao,Yiyi Liao*

Main category: cs.CV

TL;DR: The paper introduces navdream, a benchmark to isolate appearance-based OOD effects in autonomous driving, and proposes a universal perception interface using DINOv3 for appearance-invariant features that enables zero-shot generalization across planning paradigms.


<details>
  <summary>Details</summary>
Motivation: Current autonomous driving algorithms are fragile under OOD conditions, but research fails to distinguish between appearance shifts (weather, lighting) and structural scene changes. There's no way to determine if planning failures are due to complex road geometry or just appearance changes like rain.

Method: 1) Create navdream benchmark using generative pixel-aligned style transfer to create visual stress tests with negligible geometric deviation, isolating appearance impact. 2) Propose universal perception interface using frozen DINOv3 visual foundation model to extract appearance-invariant features as a stable interface for planners.

Result: Existing planning algorithms show significant degradation under OOD appearance conditions even when scene structure remains consistent. The proposed DINOv3-based interface achieves exceptional zero-shot generalization across diverse planning paradigms (regression, diffusion, scoring-based) without fine-tuning, maintaining consistent performance across extreme appearance shifts.

Conclusion: The paper addresses the critical decoupling failure in autonomous driving research by distinguishing appearance from structural changes. The navdream benchmark isolates appearance effects, and the DINOv3-based universal perception interface provides a plug-and-play solution for robust zero-shot generalization across planning paradigms under appearance shifts.

Abstract: Despite rapid progress, autonomous driving algorithms remain notoriously fragile under Out-of-Distribution (OOD) conditions. We identify a critical decoupling failure in current research: the lack of distinction between appearance-based shifts, such as weather and lighting, and structural scene changes. This leaves a fundamental question unanswered: Is the planner failing because of complex road geometry, or simply because it is raining? To resolve this, we establish navdream, a high-fidelity robustness benchmark leveraging generative pixel-aligned style transfer. By creating a visual stress test with negligible geometric deviation, we isolate the impact of appearance on driving performance. Our evaluation reveals that existing planning algorithms often show significant degradation under OOD appearance conditions, even when the underlying scene structure remains consistent. To bridge this gap, we propose a universal perception interface leveraging a frozen visual foundation model (DINOv3). By extracting appearance-invariant features as a stable interface for the planner, we achieve exceptional zero-shot generalization across diverse planning paradigms, including regression-based, diffusion-based, and scoring-based models. Our plug-and-play solution maintains consistent performance across extreme appearance shifts without requiring further fine-tuning. The benchmark and code will be made available.

</details>


### [20] [Unbiased Gradient Estimation for Event Binning via Functional Backpropagation](https://arxiv.org/abs/2602.12590)
*Jinze Chen,Wei Zhai,Han Han,Tiankai Ma,Yang Cao,Bin Li,Zheng-Jun Zha*

Main category: cs.CV

TL;DR: Proposes a framework for unbiased gradient estimation of event binning functions using weak derivatives via integration by parts, improving learning efficiency for event-based vision tasks.


<details>
  <summary>Details</summary>
Motivation: Event-based vision algorithms face limitations: binning events into frames truncates gradients at frame level, while direct learning from raw events suffers from biased gradient estimation due to discontinuities in binning operations.

Method: Uses integration by parts to lift target functions to functionals, creating an integral form of binning function derivatives during backpropagation. Reconstructs cotangent function from sampled cotangent vectors to compute weak derivatives that match long-range finite differences for both smooth and non-smooth targets.

Result: Improves optimization-based egomotion estimation by 3.2% lower RMS error and 1.57× faster convergence. Achieves 9.4% lower EPE in self-supervised optical flow and 5.1% lower RMS error in SLAM tasks.

Conclusion: The proposed framework enables unbiased gradient estimation for arbitrary binning functions, significantly improving learning efficiency and performance across various event-based vision tasks while maintaining forward output unchanged.

Abstract: Event-based vision encodes dynamic scenes as asynchronous spatio-temporal spikes called events. To leverage conventional image processing pipelines, events are typically binned into frames. However, binning functions are discontinuous, which truncates gradients at the frame level and forces most event-based algorithms to rely solely on frame-based features. Attempts to directly learn from raw events avoid this restriction but instead suffer from biased gradient estimation due to the discontinuities of the binning operation, ultimately limiting their learning efficiency. To address this challenge, we propose a novel framework for unbiased gradient estimation of arbitrary binning functions by synthesizing weak derivatives during backpropagation while keeping the forward output unchanged. The key idea is to exploit integration by parts: lifting the target functions to functionals yields an integral form of the derivative of the binning function during backpropagation, where the cotangent function naturally arises. By reconstructing this cotangent function from the sampled cotangent vector, we compute weak derivatives that provably match long-range finite differences of both smooth and non-smooth targets. Experimentally, our method improves simple optimization-based egomotion estimation with 3.2\% lower RMS error and 1.57$\times$ faster convergence. On complex downstream tasks, we achieve 9.4\% lower EPE in self-supervised optical flow, and 5.1\% lower RMS error in SLAM, demonstrating broad benefits for event-based visual perception. Source code can be found at https://github.com/chjz1024/EventFBP.

</details>


### [21] [Language-Guided Invariance Probing of Vision-Language Models](https://arxiv.org/abs/2511.13494)
*Jae Joong Lee*

Main category: cs.CV

TL;DR: LGIP benchmark evaluates VLMs' invariance to paraphrases and sensitivity to semantic flips, revealing robustness issues not captured by standard metrics.


<details>
  <summary>Details</summary>
Motivation: While VLMs achieve strong zero-shot performance, their reliability in responding to controlled linguistic perturbations remains unclear. The paper aims to provide a diagnostic tool for assessing linguistic robustness beyond conventional accuracy metrics.

Method: Introduces Language-Guided Invariance Probing (LGIP) benchmark using 40k MS COCO images with five human captions each. Automatically generates paraphrases and rule-based semantic flips (altering object category, color, or count) to measure invariance and sensitivity.

Result: EVA02-CLIP and large OpenCLIP variants show favorable invariance-sensitivity balance with low paraphrase variance and consistent preference for original captions over flipped ones. SigLIP and SigLIP2 exhibit large invariance errors and often prefer flipped captions, especially for object and color edits.

Conclusion: LGIP provides a model-agnostic diagnostic tool that reveals linguistic robustness issues in VLMs that are invisible to standard retrieval metrics, offering insights beyond conventional accuracy scores.

Abstract: Recent vision-language models (VLMs) such as CLIP, OpenCLIP, EVA02-CLIP and SigLIP achieve strong zero-shot performance, but it is unclear how reliably they respond to controlled linguistic perturbations. We introduce Language-Guided Invariance Probing (LGIP), a benchmark that measures (i) invariance to meaning-preserving paraphrases and (ii) sensitivity to meaning-changing semantic flips in image-text matching. Using 40k MS COCO images with five human captions each, we automatically generate paraphrases and rule-based flips that alter object category, color or count, and summarize model behavior with an invariance error, a semantic sensitivity gap and a positive-rate statistic.
  Across nine VLMs, EVA02-CLIP and large OpenCLIP variants lie on a favorable invariance-sensitivity frontier, combining low paraphrase-induced variance with consistently higher scores for original captions than for their flipped counterparts. In contrast, SigLIP and SigLIP2 show much larger invariance error and often prefer flipped captions to the human descriptions, especially for object and color edits. These failures are largely invisible to standard retrieval metrics, indicating that LGIP provides a model-agnostic diagnostic for the linguistic robustness of VLMs beyond conventional accuracy scores.

</details>


### [22] [QuEPT: Quantized Elastic Precision Transformers with One-Shot Calibration for Multi-Bit Switching](https://arxiv.org/abs/2602.12609)
*Ke Xu,Yixin Wang,Zhongcheng Li,Hao Cui,Jinshui Hu,Xingyi Zhang*

Main category: cs.CV

TL;DR: QuEPT is an efficient post-training quantization scheme for LLMs that enables dynamic multi-bit deployment via one-shot calibration, supporting real-time switching between uniform and mixed precision quantization without repeated optimization.


<details>
  <summary>Details</summary>
Motivation: Elastic precision quantization enables multi-bit deployment but faces high storage and optimization costs for Transformer architectures, with limited research on elastic quantization for large language models.

Method: Proposes QuEPT with one-shot calibration on small data slice, cascading low-rank adapters for different bit-widths, Multi-Bit Token Merging (MB-ToMe) for dynamic token feature fusion, and Multi-Bit Cascaded Low-Rank adapters (MB-CLoRA) to strengthen bit-width group correlations.

Result: Extensive experiments show QuEPT achieves comparable or better performance than state-of-the-art post-training quantization methods.

Conclusion: QuEPT provides an efficient post-training quantization solution for LLMs that supports flexible multi-bit deployment with one-shot optimization, enabling real-time switching between quantization modes while maintaining high accuracy.

Abstract: Elastic precision quantization enables multi-bit deployment via a single optimization pass, fitting diverse quantization scenarios.Yet, the high storage and optimization costs associated with the Transformer architecture, research on elastic quantization remains limited, particularly for large language models.This paper proposes QuEPT, an efficient post-training scheme that reconstructs block-wise multi-bit errors with one-shot calibration on a small data slice. It can dynamically adapt to various predefined bit-widths by cascading different low-rank adapters, and supports real-time switching between uniform quantization and mixed precision quantization without repeated optimization. To enhance accuracy and robustness, we introduce Multi-Bit Token Merging (MB-ToMe) to dynamically fuse token features across different bit-widths, improving robustness during bit-width switching. Additionally, we propose Multi-Bit Cascaded Low-Rank adapters (MB-CLoRA) to strengthen correlations between bit-width groups, further improve the overall performance of QuEPT. Extensive experiments demonstrate that QuEPT achieves comparable or better performance to existing state-of-the-art post-training quantization methods.Our code is available at https://github.com/xuke225/QuEPT

</details>


### [23] [Vision Token Reduction via Attention-Driven Self-Compression for Efficient Multimodal Large Language Models](https://arxiv.org/abs/2602.12618)
*Omer Faruk Deniz,Ruiyu Mao,Ruochen Li,Yapeng Tian,Latifur Khan*

Main category: cs.CV

TL;DR: ADSC reduces MLLM computational costs by progressively compressing vision tokens using the LLM's own attention mechanism, achieving 53.7% FLOP reduction while maintaining 98.2% performance.


<details>
  <summary>Details</summary>
Motivation: MLLMs have high computational costs from processing many vision tokens through all LLM layers. Existing pruning methods either lack generality due to diverse encoder-projector designs or use heuristics incompatible with FlashAttention.

Method: Attention-Driven Self-Compression (ADSC) treats the LLM as the optimal guide for compression, using uniform token downsampling at selected layers to create bottlenecks that encourage information reorganization and compression into remaining tokens. No score computation, auxiliary modules, or attention modification needed.

Result: Applied to LLaVA-1.5: 53.7% FLOP reduction, 56.7% peak KV-cache memory reduction, while preserving 98.2% original performance. Outperforms prior pruning approaches across multiple benchmarks and remains robust under high compression ratios.

Conclusion: ADSC provides a simple, broadly applicable compression method that leverages the LLM's own attention mechanism for efficient vision token reduction while maintaining performance and FlashAttention compatibility.

Abstract: Multimodal Large Language Models (MLLMs) incur significant computational cost from processing numerous vision tokens through all LLM layers. Prior pruning methods operate either before the LLM, limiting generality due to diverse encoder-projector designs or within the LLM using heuristics that are incompatible with FlashAttention. We take a different approach: rather than identifying unimportant tokens, we treat the LLM itself as the optimal guide for compression. Observing that deeper layers naturally transmit vision-to-text information, we introduce Attention-Driven Self-Compression (ADSC), a simple, broadly applicable method that progressively reduces vision tokens using only the LLM's attention mechanism. Our method applies uniform token downsampling at selected layers, forming bottlenecks that encourage the model to reorganize and compress information into the remaining tokens. It requires no score computation, auxiliary modules, or attention modification, and remains fully compatible with FlashAttention. Applied to LLaVA-1.5, ADSC reduces FLOPs by 53.7% and peak KV-cache memory by 56.7%, while preserving 98.2% of the original model performance. Across multiple benchmarks, it outperforms prior pruning approaches in both efficiency and accuracy. Crucially, under high compression ratios, our method remains robust while heuristic-based techniques degrade sharply.

</details>


### [24] [ImageRAGTurbo: Towards One-step Text-to-Image Generation with Retrieval-Augmented Diffusion Models](https://arxiv.org/abs/2602.12640)
*Peijie Qiu,Hariharan Ramshankar,Arnau Ramisa,René Vidal,Amit Kumar K C,Vamsi Salaka,Rahul Bhagat*

Main category: cs.CV

TL;DR: ImageRAGTurbo improves few-step diffusion models for text-to-image generation by using retrieval augmentation to reduce sampling steps while maintaining quality.


<details>
  <summary>Details</summary>
Motivation: Diffusion models have high latency due to iterative sampling. Few-step models reduce steps but compromise quality and require expensive training. Need efficient method to maintain quality with fewer steps.

Method: Retrieves relevant text-image pairs for prompts, uses them to condition generation. Initially edits denoiser's latent H-space without finetuning. Adds trainable adapter in H-space with cross-attention to blend retrieved content with target prompt.

Result: Approach produces high-fidelity images without compromising latency compared to existing methods. Initial H-space editing already improves prompt fidelity.

Conclusion: ImageRAGTurbo efficiently finetunes few-step diffusion models via retrieval augmentation, achieving better quality with fewer sampling steps while maintaining low latency.

Abstract: Diffusion models have emerged as the leading approach for text-to-image generation. However, their iterative sampling process, which gradually morphs random noise into coherent images, introduces significant latency that limits their applicability. While recent few-step diffusion models reduce the number of sampling steps to as few as one to four steps, they often compromise image quality and prompt alignment, especially in one-step generation. Additionally, these models require computationally expensive training procedures. To address these limitations, we propose ImageRAGTurbo, a novel approach to efficiently finetune few-step diffusion models via retrieval augmentation. Given a text prompt, we retrieve relevant text-image pairs from a database and use them to condition the generation process. We argue that such retrieved examples provide rich contextual information to the UNet denoiser that helps reduce the number of denoising steps without compromising image quality. Indeed, our initial investigations show that using the retrieved content to edit the denoiser's latent space ($\mathcal{H}$-space) without additional finetuning already improves prompt fidelity. To further improve the quality of the generated images, we augment the UNet denoiser with a trainable adapter in the $\mathcal{H}$-space, which efficiently blends the retrieved content with the target prompt using a cross-attention mechanism. Experimental results on fast text-to-image generation demonstrate that our approach produces high-fidelity images without compromising latency compared to existing methods.

</details>


### [25] [Multi-Task Learning with Additive U-Net for Image Denoising and Classification](https://arxiv.org/abs/2602.12649)
*Vikram Lakkavalli,Neelam Sinha*

Main category: cs.CV

TL;DR: Additive skip fusion in U-Net architectures improves denoising and multi-task learning by replacing concatenative skips with gated additive fusion, acting as architectural regularization.


<details>
  <summary>Details</summary>
Motivation: To improve training stability and performance in image denoising and denoising-centric multi-task learning by constraining shortcut capacity while preserving fixed feature dimensionality across U-Net depth.

Method: Propose Additive U-Net (AddUNet) that replaces concatenative skip connections with gated additive fusion, which constrains shortcut capacity and maintains fixed feature dimensionality across network depth.

Result: AddUNet achieves competitive reconstruction performance with improved training stability in both single-task denoising and joint denoising-classification settings. In MTL, learned skip weights show systematic task-aware redistribution: shallow skips favor reconstruction while deeper features support discrimination.

Conclusion: Simple constraints on skip connections act as effective architectural regularizers for stable and scalable multi-task learning without increasing model complexity, enabling implicit task decoupling through additive fusion.

Abstract: We investigate additive skip fusion in U-Net architectures for image denoising and denoising-centric multi-task learning (MTL). By replacing concatenative skips with gated additive fusion, the proposed Additive U-Net (AddUNet) constrains shortcut capacity while preserving fixed feature dimensionality across depth. This structural regularization induces controlled encoder-decoder information flow and stabilizes joint optimization. Across single-task denoising and joint denoising-classification settings, AddUNet achieves competitive reconstruction performance with improved training stability. In MTL, learned skip weights exhibit systematic task-aware redistribution: shallow skips favor reconstruction, while deeper features support discrimination. Notably, reconstruction remains robust even under limited classification capacity, indicating implicit task decoupling through additive fusion. These findings show that simple constraints on skip connections act as an effective architectural regularizer for stable and scalable multi-task learning without increasing model complexity.

</details>


### [26] [CBEN -- A Multimodal Machine Learning Dataset for Cloud Robust Remote Sensing Image Understanding](https://arxiv.org/abs/2602.12652)
*Marco Stricker,Masakazu Iwamura,Koichi Kise*

Main category: cs.CV

TL;DR: Researchers created CloudyBigEarthNet (CBEN), a dataset of paired optical and radar images with cloud occlusion, showing that existing methods trained on clear-sky data perform poorly on cloudy images, but can be improved by training with cloudy optical data.


<details>
  <summary>Details</summary>
Motivation: Clouds distort optical satellite imagery, making traditional cloudless analysis methods unreliable for time-sensitive applications like natural disasters. Cloud removal methods have limitations, so developing cloud-robust methods that combine optical and radar data is needed.

Method: Created CloudyBigEarthNet (CBEN) dataset with paired optical and radar images containing cloud occlusion. Evaluated state-of-the-art methods trained on clear-sky data on cloudy images, then adapted these methods to include cloudy optical data during training.

Result: Methods trained on clear-sky optical and radar imagery suffered 23-33 percentage point performance drops on cloudy images. When adapted to include cloudy optical data during training, they achieved 17.2-28.7 percentage point relative improvements on cloudy test cases.

Conclusion: Excluding cloudy images from training limits applicability to real-world scenarios. Training with cloudy optical data significantly improves performance on cloudy images, demonstrating the importance of including cloud-affected data in machine learning pipelines for remote sensing.

Abstract: Clouds are a common phenomenon that distorts optical satellite imagery, which poses a challenge for remote sensing. However, in the literature cloudless analysis is often performed where cloudy images are excluded from machine learning datasets and methods. Such an approach cannot be applied to time sensitive applications, e.g., during natural disasters. A possible solution is to apply cloud removal as a preprocessing step to ensure that cloudfree solutions are not failing under such conditions. But cloud removal methods are still actively researched and suffer from drawbacks, such as generated visual artifacts. Therefore, it is desirable to develop cloud robust methods that are less affected by cloudy weather. Cloud robust methods can be achieved by combining optical data with radar, a modality unaffected by clouds. While many datasets for machine learning combine optical and radar data, most researchers exclude cloudy images. We identify this exclusion from machine learning training and evaluation as a limitation that reduces applicability to cloudy scenarios. To investigate this, we assembled a dataset, named CloudyBigEarthNet (CBEN), of paired optical and radar images with cloud occlusion for training and evaluation. Using average precision (AP) as the evaluation metric, we show that state-of-the-art methods trained on combined clear-sky optical and radar imagery suffer performance drops of 23-33 percentage points when evaluated on cloudy images. We then adapt these methods to cloudy optical data during training, achieving relative improvement of 17.2-28.7 percentage points on cloudy test cases compared with the original approaches. Code and dataset are publicly available at: https://github.com/mstricker13/CBEN

</details>


### [27] [IndicFairFace: Balanced Indian Face Dataset for Auditing and Mitigating Geographical Bias in Vision-Language Models](https://arxiv.org/abs/2602.12659)
*Aarish Shah Mohsin,Mohammed Tayyab Ilyas Khan,Mohammad Nadeem,Shahab Saquib Sohail,Erik Cambria,Jiechao Gao*

Main category: cs.CV

TL;DR: IndicFairFace is a new balanced face dataset representing India's geographical diversity across 28 states and 8 Union Territories, used to quantify and reduce intra-national geographical bias in Vision-Language Models.


<details>
  <summary>Details</summary>
Motivation: Existing fairness datasets treat Indian as a monolithic category, ignoring India's vast intra-national diversity across states and territories, leading to representational and geographical bias in VLMs.

Method: Created IndicFairFace with 14,400 images ethically sourced from Wikimedia Commons and open-license repositories, uniformly balanced across states and gender. Used Iterative Nullspace Projection debiasing approach on CLIP-based VLMs.

Result: Successfully quantified intra-national geographical bias in prominent CLIP-based VLMs and reduced it using debiasing. The approach maintained embedding quality with less than 1.5% average drop in retrieval accuracy on benchmark datasets.

Conclusion: IndicFairFace establishes the first benchmark for studying geographical bias in VLMs for the Indian context, addressing the oversimplification of Indian as a single category and enabling more nuanced fairness evaluation.

Abstract: Vision-Language Models (VLMs) are known to inherit and amplify societal biases from their web-scale training data with Indian being particularly misrepresented. Existing fairness-aware datasets have significantly improved demographic balance across global race and gender groups, yet they continue to treat Indian as a single monolithic category. The oversimplification ignores the vast intra-national diversity across 28 states and 8 Union Territories of India and leads to representational and geographical bias. To address the limitation, we present IndicFairFace, a novel and balanced face dataset comprising 14,400 images representing geographical diversity of India. Images were sourced ethically from Wikimedia Commons and open-license web repositories and uniformly balanced across states and gender. Using IndicFairFace, we quantify intra-national geographical bias in prominent CLIP-based VLMs and reduce it using post-hoc Iterative Nullspace Projection debiasing approach. We also show that the adopted debiasing approach does not adversely impact the existing embedding space as the average drop in retrieval accuracy on benchmark datasets is less than 1.5 percent. Our work establishes IndicFairFace as the first benchmark to study geographical bias in VLMs for the Indian context.

</details>


### [28] [Motion Prior Distillation in Time Reversal Sampling for Generative Inbetweening](https://arxiv.org/abs/2602.12679)
*Wooseok Jeon,Seunghyun Shin,Dongmin Shin,Hae-Gon Jeon*

Main category: cs.CV

TL;DR: MPD is an inference-time distillation technique that suppresses bidirectional mismatch in I2V inbetweening by distilling forward motion residuals into backward paths for more temporally coherent results.


<details>
  <summary>Details</summary>
Motivation: Existing inference-time sampling methods for image-to-video inbetweening suffer from temporal discontinuities and visual artifacts due to misalignment between forward and backward generated paths, as each path follows different motion priors from their conditioning frames.

Method: Motion Prior Distillation (MPD) - an inference-time distillation technique that suppresses bidirectional mismatch by distilling the motion residual of the forward path into the backward path, avoiding denoising the end-conditioned path that causes ambiguity.

Result: The method yields more temporally coherent inbetweening results with forward motion prior, demonstrated through quantitative evaluations on standard benchmarks and extensive user studies in practical scenarios.

Conclusion: MPD provides a simple yet effective solution to bidirectional mismatch in I2V inbetweening, improving temporal coherence without additional training by leveraging motion prior distillation at inference time.

Abstract: Recent progress in image-to-video (I2V) diffusion models has significantly advanced the field of generative inbetweening, which aims to generate semantically plausible frames between two keyframes. In particular, inference-time sampling strategies, which leverage the generative priors of large-scale pre-trained I2V models without additional training, have become increasingly popular. However, existing inference-time sampling, either fusing forward and backward paths in parallel or alternating them sequentially, often suffers from temporal discontinuities and undesirable visual artifacts due to the misalignment between the two generated paths. This is because each path follows the motion prior induced by its own conditioning frame. In this work, we propose Motion Prior Distillation (MPD), a simple yet effective inference-time distillation technique that suppresses bidirectional mismatch by distilling the motion residual of the forward path into the backward path. Our method can deliberately avoid denoising the end-conditioned path which causes the ambiguity of the path, and yield more temporally coherent inbetweening results with the forward motion prior. We not only perform quantitative evaluations on standard benchmarks, but also conduct extensive user studies to demonstrate the effectiveness of our approach in practical scenarios.

</details>


### [29] [Channel-Aware Probing for Multi-Channel Imaging](https://arxiv.org/abs/2602.12696)
*Umar Marikkar,Syed Sameed Husain,Muhammad Awais,Sara Atito*

Main category: cs.CV

TL;DR: CAP improves probing performance for multi-channel imaging by using independent feature encoding and decoupled pooling to better leverage pre-trained encoders without full fine-tuning.


<details>
  <summary>Details</summary>
Motivation: Multi-channel imaging (MCI) faces challenges with varying channel configurations across datasets, making it difficult to reuse pre-trained encoders. Existing methods either require full fine-tuning or yield poor results when probing with frozen encoders, often performing worse than training from scratch.

Method: Channel-Aware Probing (CAP) uses Independent Feature Encoding (IFE) to encode each channel separately and Decoupled Pooling (DCP) to pool features within channels before aggregating across channels, exploiting inter-channel diversity in MCI data.

Result: Across three MCI benchmarks, CAP consistently improves probing performance over default probing protocols, matches fine-tuning from scratch, and significantly reduces the performance gap to full fine-tuning using the same pre-trained checkpoints.

Conclusion: CAP provides an effective probing strategy for multi-channel imaging that better leverages pre-trained encoders without requiring full fine-tuning, addressing the channel configuration variability problem in MCI data.

Abstract: Training and evaluating vision encoders on Multi-Channel Imaging (MCI) data remains challenging as channel configurations vary across datasets, preventing fixed-channel training and limiting reuse of pre-trained encoders on new channel settings. Prior work trains MCI encoders but typically evaluates them via full fine-tuning, leaving probing with frozen pre-trained encoders comparatively underexplored. Existing studies that perform probing largely focus on improving representations, rather than how to best leverage fixed representations for downstream tasks. Although the latter problem has been studied in other domains, directly transferring those strategies to MCI yields weak results, even worse than training from scratch. We therefore propose Channel-Aware Probing (CAP), which exploits the intrinsic inter-channel diversity in MCI datasets by controlling feature flow at both the encoder and probe levels. CAP uses Independent Feature Encoding (IFE) to encode each channel separately, and Decoupled Pooling (DCP) to pool within channels before aggregating across channels. Across three MCI benchmarks, CAP consistently improves probing performance over the default probing protocol, matches fine-tuning from scratch, and largely reduces the gap to full fine-tuning from the same MCI pre-trained checkpoints. Code can be found in https://github.com/umarikkar/CAP.

</details>


### [30] [ART3mis: Ray-Based Textual Annotation on 3D Cultural Objects](https://arxiv.org/abs/2602.12725)
*Vasileios Arampatzakis,Vasileios Sevetlidis,Fotis Arnaoutoglou,Athanasios Kalogeras,Christos Koulamas,Aris Lalos,Chairi Kiourt,George Ioannakis,Anestis Koutsoudis,George Pavlidis*

Main category: cs.CV

TL;DR: ART3mis is a user-friendly, interactive textual annotation tool for 3D cultural heritage objects that allows non-technical users to segment and annotate 3D digital replicas with metadata.


<details>
  <summary>Details</summary>
Motivation: Archaeologists and cultural heritage professionals need advanced 3D applications with annotation capabilities beyond basic visualization. Existing solutions are often domain-specific and not accessible to non-technical users like conservators and curators.

Method: ART3mis uses a user-driven, direct-on-surface approach that enables real-time handling of detailed 3D cultural objects. It allows segmentation and annotation of multiple complex regions, storing textual annotations in JSON format.

Result: The tool provides a general-purpose solution that works with detailed 3D cultural objects in real-time, enabling non-technical users to easily annotate and attach metadata to specific regions of 3D digital artefacts.

Conclusion: ART3mis addresses the need for accessible 3D annotation tools in cultural heritage by providing a user-friendly, interactive platform that bridges the gap between technical 3D imaging capabilities and practical needs of heritage professionals.

Abstract: Beyond simplistic 3D visualisations, archaeologists, as well as cultural heritage experts and practitioners, need applications with advanced functionalities. Such as the annotation and attachment of metadata onto particular regions of the 3D digital objects. Various approaches have been presented to tackle this challenge, most of which achieve excellent results in the domain of their application. However, they are often confined to that specific domain and particular problem. In this paper, we present ART3mis - a general-purpose, user-friendly, interactive textual annotation tool for 3D objects. Primarily attuned to aid cultural heritage conservators, restorers and curators with no technical skills in 3D imaging and graphics, the tool allows for the easy handling, segmenting and annotating of 3D digital replicas of artefacts. ART3mis applies a user-driven, direct-on-surface approach. It can handle detailed 3D cultural objects in real-time and store textual annotations for multiple complex regions in JSON data format.

</details>


### [31] [VimRAG: Navigating Massive Visual Context in Retrieval-Augmented Generation via Multimodal Memory Graph](https://arxiv.org/abs/2602.12735)
*Qiuchen Wang,Shihang Wang,Yu Zeng,Qiang Zhang,Fanrui Zhang,Zhuoning Guo,Bosi Zhang,Wenxuan Huang,Lin Chen,Zehui Chen,Pengjun Xie,Ruixue Ding*

Main category: cs.CV

TL;DR: VimRAG is a multimodal RAG framework that structures reasoning as a dynamic graph to better handle visual data in long-context tasks, using graph-modulated memory encoding and graph-guided policy optimization.


<details>
  <summary>Details</summary>
Motivation: Traditional RAG methods struggle with long-context tasks involving visual data because they use linear interaction histories, which are inefficient for information-sparse but token-heavy visual content in iterative reasoning scenarios.

Method: Models reasoning as a dynamic directed acyclic graph structuring agent states and retrieved multimodal evidence. Introduces Graph-Modulated Visual Memory Encoding that evaluates node importance via topological position to allocate high-resolution tokens to pivotal evidence. Uses Graph-Guided Policy Optimization to disentangle step-wise validity from trajectory-level rewards by pruning redundant memory nodes.

Result: Extensive experiments show VimRAG consistently achieves state-of-the-art performance on diverse multimodal RAG benchmarks.

Conclusion: VimRAG effectively bridges the gap in multimodal information retrieval and reasoning by using graph-structured memory and optimization strategies, outperforming traditional linear RAG approaches on multimodal tasks.

Abstract: Effectively retrieving, reasoning, and understanding multimodal information remains a critical challenge for agentic systems. Traditional Retrieval-augmented Generation (RAG) methods rely on linear interaction histories, which struggle to handle long-context tasks, especially those involving information-sparse yet token-heavy visual data in iterative reasoning scenarios. To bridge this gap, we introduce VimRAG, a framework tailored for multimodal Retrieval-augmented Reasoning across text, images, and videos. Inspired by our systematic study, we model the reasoning process as a dynamic directed acyclic graph that structures the agent states and retrieved multimodal evidence. Building upon this structured memory, we introduce a Graph-Modulated Visual Memory Encoding mechanism, with which the significance of memory nodes is evaluated via their topological position, allowing the model to dynamically allocate high-resolution tokens to pivotal evidence while compressing or discarding trivial clues. To implement this paradigm, we propose a Graph-Guided Policy Optimization strategy. This strategy disentangles step-wise validity from trajectory-level rewards by pruning memory nodes associated with redundant actions, thereby facilitating fine-grained credit assignment. Extensive experiments demonstrate that VimRAG consistently achieves state-of-the-art performance on diverse multimodal RAG benchmarks. The code is available at https://github.com/Alibaba-NLP/VRAG.

</details>


### [32] [SPRig: Self-Supervised Pose-Invariant Rigging from Mesh Sequences](https://arxiv.org/abs/2602.12740)
*Ruipeng Wang,Langkun Zhong,Miaowei Wang*

Main category: cs.CV

TL;DR: SPRig is a fine-tuning framework that enforces cross-frame consistency losses to learn pose-invariant rigs from sequential data lacking canonical rest poses, achieving state-of-the-art temporal stability.


<details>
  <summary>Details</summary>
Motivation: Existing rigging methods fail for sequential data like animal motion capture or video-derived mesh sequences because they assume a canonical rest pose (like T-pose). When applied frame-by-frame to such data, these methods produce topological inconsistencies and lack pose invariance.

Method: SPRig is a general fine-tuning framework that enforces cross-frame consistency losses on top of existing rigging models. It learns pose-invariant rigs through a novel approach that maintains topological consistency across frames, validated using a new permutation-invariant stability protocol.

Result: Experiments demonstrate state-of-the-art temporal stability: SPRig produces coherent rigs from challenging sequences and dramatically reduces artifacts that plague baseline methods. The method works effectively on data lacking canonical rest poses.

Conclusion: SPRig successfully addresses the limitations of existing rigging methods for sequential data by enforcing cross-frame consistency, achieving pose-invariant rigging with superior temporal stability compared to baseline approaches.

Abstract: State-of-the-art rigging methods assume a canonical rest pose--an assumption that fails for sequential data (e.g., animal motion capture or AIGC/video-derived mesh sequences) that lack the T-pose. Applied frame-by-frame, these methods are not pose-invariant and produce topological inconsistencies across frames. Thus We propose SPRig, a general fine-tuning framework that enforces cross-frame consistency losses to learn pose-invariant rigs on top of existing models. We validate our approach on rigging using a new permutation-invariant stability protocol. Experiments demonstrate SOTA temporal stability: our method produces coherent rigs from challenging sequences and dramatically reduces the artifacts that plague baseline methods. The code will be released publicly upon acceptance.

</details>


### [33] [Synthetic Craquelure Generation for Unsupervised Painting Restoration](https://arxiv.org/abs/2602.12742)
*Jana Cuch-Guillén,Antonio Agudo,Raül Pérez-Gonzalo*

Main category: cs.CV

TL;DR: Annotation-free framework for painting restoration using synthetic craquelure generator and detector-guided refinement with SegFormer+LoRA, followed by anisotropic diffusion inpainting.


<details>
  <summary>Details</summary>
Motivation: Cultural heritage preservation needs non-invasive digital restoration methods, but identifying and restoring fine craquelure patterns is challenging due to scarce pixel-level annotations and complex brushstrokes.

Method: 1) Domain-specific synthetic craquelure generator using Bézier trajectories to simulate realistic branching patterns. 2) Couples classical morphological detector with learning-based refinement (SegFormer backbone adapted via LoRA). 3) Detector-guided strategy injects morphological map as spatial prior. 4) Masked hybrid loss and logit adjustment focus training on crack regions. 5) Refined masks guide Anisotropic Diffusion inpainting for content reconstruction.

Result: The pipeline significantly outperforms state-of-the-art photographic restoration models in zero-shot settings while faithfully preserving original paint brushwork.

Conclusion: Proposed annotation-free framework effectively addresses craquelure restoration challenges in cultural heritage preservation through synthetic data generation and detector-guided refinement, achieving superior performance without requiring manual annotations.

Abstract: Cultural heritage preservation increasingly demands non-invasive digital methods for painting restoration, yet identifying and restoring fine craquelure patterns from complex brushstrokes remains challenging due to scarce pixel-level annotations. We propose a fully annotation-free framework driven by a domain-specific synthetic craquelure generator, which simulates realistic branching and tapered fissure geometry using Bézier trajectories. Our approach couples a classical morphological detector with a learning-based refinement module: a SegFormer backbone adapted via Low-Rank Adaptation (LoRA). Uniquely, we employ a detector-guided strategy, injecting the morphological map as an input spatial prior, while a masked hybrid loss and logit adjustment constrain the training to focus specifically on refining candidate crack regions. The refined masks subsequently guide an Anisotropic Diffusion inpainting stage to reconstruct missing content. Experimental results demonstrate that our pipeline significantly outperforms state-of-the-art photographic restoration models in zero-shot settings, while faithfully preserving the original paint brushwork.

</details>


### [34] [ReBA-Pred-Net: Weakly-Supervised Regional Brain Age Prediction on MRI](https://arxiv.org/abs/2602.12751)
*Shuai Shao,Yan Wang,Shu Jiang,Shiyuan Zhao,Xinzhe Luo,Di Yang,Jiangtao Wang,Yutong Bai,Jianguo Zhang*

Main category: cs.CV

TL;DR: ReBA-Pred-Net: A Teacher-Student framework for regional brain age estimation with clinical-prior consistency constraints, evaluated using novel indirect metrics HCS and NDC.


<details>
  <summary>Details</summary>
Motivation: Whole brain age (WBA) is too coarse for tasks like disease characterization and aging pattern research, as brain changes are typically region-selective rather than brain-wide. Robust regional brain age (ReBA) estimation is critical but lacks a widely generalizable model.

Method: Proposes ReBA-Pred-Net, a Teacher-Student framework where the Teacher produces soft regional brain age estimates to guide the Student. Includes clinical-prior consistency constraint ensuring regions within the same function change similarly.

Result: Experiments across multiple backbones demonstrate statistical and factual validity. Introduces two evaluation metrics: Healthy Control Similarity (HCS) for statistical consistency and Neuro Disease Correlation (NDC) for factual consistency.

Conclusion: The proposed ReBA-Pred-Net provides a robust framework for fine-grained regional brain age estimation with validated statistical and factual consistency, addressing limitations of whole brain age approaches.

Abstract: Brain age has become a prominent biomarker of brain health. Yet most prior work targets whole brain age (WBA), a coarse paradigm that struggles to support tasks such as disease characterization and research on development and aging patterns, because relevant changes are typically region-selective rather than brain-wide. Therefore, robust regional brain age (ReBA) estimation is critical, yet a widely generalizable model has yet to be established. In this paper, we propose the Regional Brain Age Prediction Network (ReBA-Pred-Net), a Teacher-Student framework designed for fine-grained brain age estimation. The Teacher produces soft ReBA to guide the Student to yield reliable ReBA estimates with a clinical-prior consistency constraint (regions within the same function should change similarly). For rigorous evaluation, we introduce two indirect metrics: Healthy Control Similarity (HCS), which assesses statistical consistency by testing whether regional brain-age-gap (ReBA minus chronological age) distributions align between training and unseen HC; and Neuro Disease Correlation (NDC), which assesses factual consistency by checking whether clinically confirmed patients show elevated brain-age-gap in disease-associated regions. Experiments across multiple backbones demonstrate the statistical and factual validity of our method.

</details>


### [35] [Towards reconstructing experimental sparse-view X-ray CT data with diffusion models](https://arxiv.org/abs/2602.12755)
*Nelas J. Thomsen,Xinyuan Wang,Felix Lucka,Ezgi Demircan-Tureyen*

Main category: cs.CV

TL;DR: Diffusion priors for sparse-view CT show that domain shift and forward model mismatch complicate translation from synthetic to experimental data, requiring careful validation.


<details>
  <summary>Details</summary>
Motivation: To understand whether training data mismatch (domain shift) or forward model mismatch complicate the successful application of diffusion-based image generators to experimental CT data, moving beyond synthetic-only studies.

Method: Measured CT data from a physical phantom resembling Shepp-Logan, trained diffusion priors on synthetic datasets with varying domain shift, employed priors in Decomposed Diffusion Sampling scheme on sparse-view CT datasets of increasing difficulty leading to experimental data.

Result: Domain shift plays nuanced role: severe mismatch causes model collapse/hallucinations, but diverse priors outperform well-matched narrow priors. Forward model mismatch pulls samples from prior manifold causing artifacts, mitigated with annealed likelihood schedules that increase computational efficiency.

Conclusion: Performance gains don't immediately translate from synthetic to experimental data; future development must validate against real-world benchmarks, with careful consideration of domain shift and forward model mismatch effects.

Abstract: Diffusion-based image generators are promising priors for ill-posed inverse problems like sparse-view X-ray Computed Tomography (CT). As most studies consider synthetic data, it is not clear whether training data mismatch (``domain shift'') or forward model mismatch complicate their successful application to experimental data. We measured CT data from a physical phantom resembling the synthetic Shepp-Logan phantom and trained diffusion priors on synthetic image data sets with different degrees of domain shift towards it. Then, we employed the priors in a Decomposed Diffusion Sampling scheme on sparse-view CT data sets with increasing difficulty leading to the experimental data. Our results reveal that domain shift plays a nuanced role: while severe mismatch causes model collapse and hallucinations, diverse priors outperform well-matched but narrow priors. Forward model mismatch pulls the image samples away from the prior manifold, which causes artifacts but can be mitigated with annealed likelihood schedules that also increase computational efficiency. Overall, we demonstrate that performance gains do not immediately translate from synthetic to experimental data, and future development must validate against real-world benchmarks.

</details>


### [36] [Towards complete digital twins in cultural heritage with ART3mis 3D artifacts annotator](https://arxiv.org/abs/2602.12761)
*Dimitrios Karamatskos,Vasileios Arampatzakis,Vasileios Sevetlidis,Stavros Nousias,Athanasios Kalogeras,Christos Koulamas,Aris Lalos,George Pavlidis*

Main category: cs.CV

TL;DR: ART3mis is a web-based 3D annotation tool for cultural heritage that enables non-technical users to annotate and attach metadata to 3D artifacts, compliant with W3C standards for interoperability.


<details>
  <summary>Details</summary>
Motivation: Current 3D visualization tools for cultural heritage lack advanced annotation capabilities, are domain-specific, and lack generalization and interoperability. Cultural heritage professionals need user-friendly tools to annotate 3D artifacts without requiring technical expertise in 3D imaging.

Method: Developed ART3mis - a general-purpose, interactive web-based textual annotation tool for 3D objects that complies with the W3C Web Annotation Data Model for standardization and interoperability.

Result: Created a feature-rich, user-friendly tool that enables cultural heritage professionals to handle, segment, and annotate 3D digital replicas of artifacts, facilitating communication, distribution, and reuse of annotated information.

Conclusion: ART3mis addresses the limitations of existing 3D visualization tools by providing a standardized, interoperable annotation solution specifically designed for non-technical cultural heritage professionals to enhance their work with 3D digital artifacts.

Abstract: Archaeologists, as well as specialists and practitioners in cultural heritage, require applications with additional functions, such as the annotation and attachment of metadata to specific regions of the 3D digital artifacts, to go beyond the simplistic three-dimensional (3D) visualization. Different strategies addressed this issue, most of which are excellent in their particular area of application, but their capacity is limited to their design's purpose; they lack generalization and interoperability. This paper introduces ART3mis, a general-purpose, user-friendly, feature-rich, interactive web-based textual annotation tool for 3D objects. Moreover, it enables the communication, distribution, and reuse of information as it complies with the W3C Web Annotation Data Model. It is primarily designed to help cultural heritage conservators, restorers, and curators who lack technical expertise in 3D imaging and graphics, handle, segment, and annotate 3D digital replicas of artifacts with ease.

</details>


### [37] [PixelRush: Ultra-Fast, Training-Free High-Resolution Image Generation via One-step Diffusion](https://arxiv.org/abs/2602.12769)
*Hong-Phuc Lai,Phong Nguyen,Anh Tran*

Main category: cs.CV

TL;DR: PixelRush is a tuning-free framework for high-resolution text-to-image generation that achieves 10-35× speedup over SOTA methods, generating 4K images in ~20 seconds without multiple inversion cycles.


<details>
  <summary>Details</summary>
Motivation: Pre-trained diffusion models are limited by their native training resolution, and existing training-free approaches for high-resolution generation are computationally expensive (taking >5 minutes for 4K images).

Method: Builds on patch-based inference but eliminates multiple inversion/regeneration cycles. Enables efficient patch-based denoising in low-step regime with seamless blending strategy to address artifacts and noise injection to mitigate over-smoothing.

Result: Achieves exceptional efficiency: generates 4K images in ~20 seconds (10-35× speedup over SOTA) while maintaining superior visual fidelity, validated through extensive experiments.

Conclusion: PixelRush presents the first practical tuning-free framework for high-resolution text-to-image generation that dramatically improves efficiency without compromising quality.

Abstract: Pre-trained diffusion models excel at generating high-quality images but remain inherently limited by their native training resolution. Recent training-free approaches have attempted to overcome this constraint by introducing interventions during the denoising process; however, these methods incur substantial computational overhead, often requiring more than five minutes to produce a single 4K image. In this paper, we present PixelRush, the first tuning-free framework for practical high-resolution text-to-image generation. Our method builds upon the established patch-based inference paradigm but eliminates the need for multiple inversion and regeneration cycles. Instead, PixelRush enables efficient patch-based denoising within a low-step regime. To address artifacts introduced by patch blending in few-step generation, we propose a seamless blending strategy. Furthermore, we mitigate over-smoothing effects through a noise injection mechanism. PixelRush delivers exceptional efficiency, generating 4K images in approximately 20 seconds representing a 10$\times$ to 35$\times$ speedup over state-of-the-art methods while maintaining superior visual fidelity. Extensive experiments validate both the performance gains and the quality of outputs achieved by our approach.

</details>


### [38] [Bootstrapping MLLM for Weakly-Supervised Class-Agnostic Object Counting](https://arxiv.org/abs/2602.12774)
*Xiaowen Zhang,Zijie Yue,Yong Luo,Cairong Zhao,Qijun Chen,Miaojing Shi*

Main category: cs.CV

TL;DR: WS-COC is a novel MLLM-driven weakly-supervised framework for class-agnostic object counting that uses only image-level count supervision, achieving performance comparable to fully-supervised methods while reducing annotation costs.


<details>
  <summary>Details</summary>
Motivation: Current object counting methods either require costly point-level annotations (fully-supervised) or are limited to single-category counting with weak supervision. There's a need for a class-agnostic counting approach that can work with minimal supervision while maintaining high performance.

Method: The framework incorporates three key strategies: 1) Divide-and-discern dialogue tuning - multi-round dialogue to determine object count ranges, 2) Compare-and-rank count optimization - training MLLM to rank images by object counts, and 3) Global-and-local counting enhancement - fusing local and global predictions for dense scenes.

Result: Extensive experiments on FSC-147, CARPK, PUCPR+, and ShanghaiTech datasets show WS-COC matches or surpasses many state-of-the-art fully-supervised methods while significantly reducing annotation costs.

Conclusion: WS-COC demonstrates that MLLMs can be effectively leveraged for class-agnostic object counting with only weak supervision, achieving competitive performance with fully-supervised approaches while being more annotation-efficient.

Abstract: Object counting is a fundamental task in computer vision, with broad applicability in many real-world scenarios. Fully-supervised counting methods require costly point-level annotations per object. Few weakly-supervised methods leverage only image-level object counts as supervision and achieve fairly promising results. They are, however, often limited to counting a single category, e.g. person. In this paper, we propose WS-COC, the first MLLM-driven weakly-supervised framework for class-agnostic object counting. Instead of directly fine-tuning MLLMs to predict object counts, which can be challenging due to the modality gap, we incorporate three simple yet effective strategies to bootstrap the counting paradigm in both training and testing: First, a divide-and-discern dialogue tuning strategy is proposed to guide the MLLM to determine whether the object count falls within a specific range and progressively break down the range through multi-round dialogue. Second, a compare-and-rank count optimization strategy is introduced to train the MLLM to optimize the relative ranking of multiple images according to their object counts. Third, a global-and-local counting enhancement strategy aggregates and fuses local and global count predictions to improve counting performance in dense scenes. Extensive experiments on FSC-147, CARPK, PUCPR+, and ShanghaiTech show that WS-COC matches or even surpasses many state-of-art fully-supervised methods while significantly reducing annotation costs. Code is available at https://github.com/viscom-tongji/WS-COC.

</details>


### [39] [GSM-GS: Geometry-Constrained Single and Multi-view Gaussian Splatting for Surface Reconstruction](https://arxiv.org/abs/2602.12796)
*Xiao Ren,Yu Liu,Ning An,Jian Cheng,Xin Qiao,He Kong*

Main category: cs.CV

TL;DR: GSM-GS improves 3D Gaussian Splatting by integrating single-view adaptive sub-region weighting and multi-view spatial structure refinement to better reconstruct complex surface details.


<details>
  <summary>Details</summary>
Motivation: 3D Gaussian Splatting has fast training and high-fidelity rendering but struggles with unstructured Gaussian point clouds, causing high-frequency detail loss in complex surface microstructures.

Method: Two-part framework: 1) Single-view optimization using image gradient features to partition scenes into texture-rich/texture-less regions with adaptive filtering and dual-branch constraints; 2) Multi-view optimization with geometry-guided cross-view point cloud association and dynamic weight sampling for 3D structural normal constraints.

Result: Extensive experiments on public datasets show the method achieves competitive rendering quality and geometric reconstruction.

Conclusion: GSM-GS effectively addresses detail loss in 3D Gaussian Splatting through synergistic optimization of single-view and multi-view constraints, improving both rendering quality and geometric fidelity.

Abstract: Recently, 3D Gaussian Splatting has emerged as a prominent research direction owing to its ultrarapid training speed and high-fidelity rendering capabilities. However, the unstructured and irregular nature of Gaussian point clouds poses challenges to reconstruction accuracy. This limitation frequently causes high-frequency detail loss in complex surface microstructures when relying solely on routine strategies. To address this limitation, we propose GSM-GS: a synergistic optimization framework integrating single-view adaptive sub-region weighting constraints and multi-view spatial structure refinement. For single-view optimization, we leverage image gradient features to partition scenes into texture-rich and texture-less sub-regions. The reconstruction quality is enhanced through adaptive filtering mechanisms guided by depth discrepancy features. This preserves high-weight regions while implementing a dual-branch constraint strategy tailored to regional texture variations, thereby improving geometric detail characterization. For multi-view optimization, we introduce a geometry-guided cross-view point cloud association method combined with a dynamic weight sampling strategy. This constructs 3D structural normal constraints across adjacent point cloud frames, effectively reinforcing multi-view consistency and reconstruction fidelity. Extensive experiments on public datasets demonstrate that our method achieves both competitive rendering quality and geometric reconstruction. See our interactive project page

</details>


### [40] [Thinking Like a Radiologist: A Dataset for Anatomy-Guided Interleaved Vision Language Reasoning in Chest X-ray Interpretation](https://arxiv.org/abs/2602.12843)
*Yichen Zhao,Zelin Peng,Piao Yang,Xiaokang Yang,Wei Shen*

Main category: cs.CV

TL;DR: MMRad-IVL-22K is the first large-scale dataset for interleaved visual-language reasoning in chest X-ray diagnosis, enabling models to mimic radiologists' workflow of alternating visual inspection and language reasoning.


<details>
  <summary>Details</summary>
Motivation: Current medical LVLMs rely on text-only chain-of-thought reasoning after a single visual inspection, which leads to hallucinations and lacks the rich visual details needed for accurate radiological diagnosis. Pseudo-visual solutions using coordinates still fail to preserve essential visual information like texture and density.

Method: Created MMRad-IVL-22K dataset with 21,994 diagnostic traces that systematically scan 35 anatomical regions, reflecting radiologists' repeated cycle of reasoning and visual inspection. Visual rationales complement textual descriptions to ground each reasoning step.

Result: Multimodal CoT guided report generation outperforms text-only CoT by 6% in RadGraph metric. Models fine-tuned on MMRad-IVL-22K achieve superior reasoning consistency and report quality compared to both general-purpose and medical-specific LVLMs.

Conclusion: High-fidelity interleaved vision-language evidence is essential for reliable medical AI, and the dataset enables models to better mimic radiologists' diagnostic workflow, significantly improving clinical accuracy and report quality.

Abstract: Radiological diagnosis is a perceptual process in which careful visual inspection and language reasoning are repeatedly interleaved. Most medical large vision language models (LVLMs) perform visual inspection only once and then rely on text-only chain-of-thought (CoT) reasoning, which operates purely in the linguistic space and is prone to hallucination. Recent methods attempt to mitigate this issue by introducing visually related coordinates, such as bounding boxes. However, these remain a pseudo-visual solution: coordinates are still text and fail to preserve rich visual details like texture and density. Motivated by the interleaved nature of radiological diagnosis, we introduce MMRad-IVL-22K, the first large-scale dataset designed for natively interleaved visual language reasoning in chest X-ray interpretation. MMRad-IVL-22K reflects a repeated cycle of reasoning and visual inspection workflow of radiologists, in which visual rationales complement textual descriptions and ground each step of the reasoning process. MMRad-IVL-22K comprises 21,994 diagnostic traces, enabling systematic scanning across 35 anatomical regions. Experimental results on advanced closed-source LVLMs demonstrate that report generation guided by multimodal CoT significantly outperforms that guided by text-only CoT in clinical accuracy and report quality (e.g., 6\% increase in the RadGraph metric), confirming that high-fidelity interleaved vision language evidence is a non-substitutable component of reliable medical AI. Furthermore, benchmarking across seven state-of-the-art open-source LVLMs demonstrates that models fine-tuned on MMRad-IVL-22K achieve superior reasoning consistency and report quality compared with both general-purpose and medical-specific LVLMs. The project page is available at https://github.com/qiuzyc/thinking_like_a_radiologist.

</details>


### [41] [RoadscapesQA: A Multitask, Multimodal Dataset for Visual Question Answering on Indian Roads](https://arxiv.org/abs/2602.12877)
*Vijayasri Iyer,Maahin Rathinagiriswaran,Jyothikamalesh S*

Main category: cs.CV

TL;DR: Roadscapes: A multitask multimodal dataset of 9,000 Indian driving scene images with bounding boxes and generated QA pairs for object grounding, reasoning, and scene understanding tasks.


<details>
  <summary>Details</summary>
Motivation: To advance research on visual scene understanding in unstructured environments, particularly for autonomous driving systems that need to interpret diverse Indian driving conditions including urban/rural areas, highways, and congested streets.

Method: Collected 9,000 images from diverse Indian driving environments with manually verified bounding boxes. Used rule-based heuristics to infer scene attributes and generate question-answer pairs for various tasks including object grounding, reasoning, and scene understanding.

Result: Created Roadscapes dataset with diverse scenes from urban/rural India, highways, service roads, village paths, and congested city streets in daytime/nighttime settings. Provided initial baselines for image QA tasks using vision-language models.

Conclusion: Roadscapes dataset facilitates scalable scene understanding research for autonomous driving in unstructured environments, with potential to advance vision-language models for complex driving scene interpretation tasks.

Abstract: Understanding road scenes is essential for autonomous driving, as it enables systems to interpret visual surroundings to aid in effective decision-making. We present Roadscapes, a multitask multimodal dataset consisting of upto 9,000 images captured in diverse Indian driving environments, accompanied by manually verified bounding boxes. To facilitate scalable scene understanding, we employ rule-based heuristics to infer various scene attributes, which are subsequently used to generate question-answer (QA) pairs for tasks such as object grounding, reasoning, and scene understanding. The dataset includes a variety of scenes from urban and rural India, encompassing highways, service roads, village paths, and congested city streets, captured in both daytime and nighttime settings. Roadscapes has been curated to advance research on visual scene understanding in unstructured environments. In this paper, we describe the data collection and annotation process, present key dataset statistics, and provide initial baselines for image QA tasks using vision-language models.

</details>


### [42] [RADAR: Revealing Asymmetric Development of Abilities in MLLM Pre-training](https://arxiv.org/abs/2602.12892)
*Yunshuang Nie,Bingqian Lin,Minzhe Niu,Kun Xiang,Jianhua Han,Guowei Huang,Xingyue Quan,Hang Xu,Bokui Chen,Xiaodan Liang*

Main category: cs.CV

TL;DR: RADAR is an efficient evaluation framework for pre-trained MLLMs that reveals asymmetric development of perception and reasoning abilities without requiring fine-tuning, using a novel Soft Discrimination Score metric and comprehensive Multi-Modal Mixture Benchmark.


<details>
  <summary>Details</summary>
Motivation: Current evaluation frameworks for MLLMs are inefficient (require fine-tuning), use metrics that can't disentangle perception vs reasoning abilities, and have limited/misaligned benchmarks, making it hard to diagnose performance bottlenecks during pre-training.

Method: RADAR introduces two components: (1) Soft Discrimination Score - a novel metric that quantifies model preference for correct answers over distractors without fine-tuning; (2) Multi-Modal Mixture Benchmark - a 15K+ sample benchmark for 0-shot evaluation of perception and reasoning abilities, unifying authoritative datasets and collecting new ones.

Result: RADAR reveals asymmetric development of perceptual and reasoning capabilities in pre-trained MLLMs across factors like data volume, model size, and pre-training strategy, showing the need for decomposed analysis of ability bottlenecks.

Conclusion: RADAR provides an efficient, ability-centric evaluation framework that enables targeted interventions to advance MLLMs by revealing their asymmetric ability development during pre-training, without requiring costly fine-tuning.

Abstract: Pre-trained Multi-modal Large Language Models (MLLMs) provide a knowledge-rich foundation for post-training by leveraging their inherent perception and reasoning capabilities to solve complex tasks. However, the lack of an efficient evaluation framework impedes the diagnosis of their performance bottlenecks. Current evaluation primarily relies on testing after supervised fine-tuning, which introduces laborious additional training and autoregressive decoding costs. Meanwhile, common pre-training metrics cannot quantify a model's perception and reasoning abilities in a disentangled manner. Furthermore, existing evaluation benchmarks are typically limited in scale or misaligned with pre-training objectives. Thus, we propose RADAR, an efficient ability-centric evaluation framework for Revealing Asymmetric Development of Abilities in MLLM pRe-training. RADAR involves two key components: (1) Soft Discrimination Score, a novel metric for robustly tracking ability development without fine-tuning, based on quantifying nuanced gradations of the model preference for the correct answer over distractors; and (2) Multi-Modal Mixture Benchmark, a new 15K+ sample benchmark for comprehensively evaluating pre-trained MLLMs' perception and reasoning abilities in a 0-shot manner, where we unify authoritative benchmark datasets and carefully collect new datasets, extending the evaluation scope and addressing the critical gaps in current benchmarks. With RADAR, we comprehensively reveal the asymmetric development of perceptual and reasoning capabilities in pretrained MLLMs across diverse factors, including data volume, model size, and pretraining strategy. Our RADAR underscores the need for a decomposed perspective on pre-training ability bottlenecks, informing targeted interventions to advance MLLMs efficiently. Our code is publicly available at https://github.com/Nieysh/RADAR.

</details>


### [43] [Robustness of Object Detection of Autonomous Vehicles in Adverse Weather Conditions](https://arxiv.org/abs/2602.12902)
*Fox Pettersen,Hong Zhu*

Main category: cs.CV

TL;DR: A method for evaluating object detection model robustness in autonomous vehicles under adverse weather/lighting conditions using synthetic data augmentation and AFFC metrics, with Faster R-CNN showing highest robustness.


<details>
  <summary>Details</summary>
Motivation: As self-driving technology advances, determining safe operational thresholds across varying environmental conditions is critical for public safety, requiring robust evaluation of object detection models under adverse conditions.

Method: Uses data augmentation operators to generate synthetic data simulating adverse weather (fog, rain, snow) and lighting conditions (dark, bright, flaring, shadow) at progressive intensity levels to find the lowest intensity where models fail. Measures robustness using Average First Failure Coefficients (AFFC) over benchmark images.

Result: Faster R-CNN achieved highest robustness with overall average AFFC of 71.9% across all seven adverse conditions, while YOLO variants showed AFFC values around 43%. The method proved feasible, effective, and efficient for evaluating and comparing model robustness.

Conclusion: The proposed method successfully evaluates object detection model robustness under adverse conditions. Training with synthetic data can improve robustness but may suffer from diminishing returns and forgetting phenomena if overtrained.

Abstract: As self-driving technology advances toward widespread adoption, determining safe operational thresholds across varying environmental conditions becomes critical for public safety. This paper proposes a method for evaluating the robustness of object detection ML models in autonomous vehicles under adverse weather conditions. It employs data augmentation operators to generate synthetic data that simulates different severance degrees of the adverse operation conditions at progressive intensity levels to find the lowest intensity of the adverse conditions at which the object detection model fails. The robustness of the object detection model is measured by the average first failure coefficients (AFFC) over the input images in the benchmark. The paper reports an experiment with four object detection models: YOLOv5s, YOLOv11s, Faster R-CNN, and Detectron2, utilising seven data augmentation operators that simulate weather conditions fog, rain, and snow, and lighting conditions of dark, bright, flaring, and shadow. The experiment data show that the method is feasible, effective, and efficient to evaluate and compare the robustness of object detection models in various adverse operation conditions. In particular, the Faster R-CNN model achieved the highest robustness with an overall average AFFC of 71.9% over all seven adverse conditions, while YOLO variants showed the AFFC values of 43%. The method is also applied to assess the impact of model training that targets adverse operation conditions using synthetic data on model robustness. It is observed that such training can improve robustness in adverse conditions but may suffer from diminishing returns and forgetting phenomena (i.e., decline in robustness) if overtrained.

</details>


### [44] [Adaptive Scaling with Geometric and Visual Continuity of completed 3D objects](https://arxiv.org/abs/2602.12905)
*Jelle Vermandere,Maarten Bassier,Maarten Vergauwen*

Main category: cs.CV

TL;DR: A framework that transforms static SDFs from completion networks into editable, structurally coherent objects with proportional scaling and deformation capabilities.


<details>
  <summary>Details</summary>
Motivation: Object completion networks produce static SDFs that cannot be rescaled or deformed without structural distortions, limiting their use in applications requiring flexible object manipulation like indoor redesign, simulation, and digital content creation.

Method: Starting from SDFs and Texture Fields from completion models, the method performs automatic part segmentation, defines user-controlled scaling zones, and applies smooth interpolation of SDFs, color, and part indices. It also incorporates a repetition-based strategy to handle large-scale deformations while preserving repeating geometric patterns.

Result: Experiments on Matterport3D and ShapeNet objects show the method overcomes the inherent rigidity of completed SDFs and is visually more appealing than global and naive selective scaling, particularly for complex shapes and repetitive structures.

Conclusion: The part-aware scaling framework successfully transforms static completed SDFs into editable, structurally coherent objects, enabling proportional and artifact-free deformation while preserving geometric patterns.

Abstract: Object completion networks typically produce static Signed Distance Fields (SDFs) that faithfully reconstruct geometry but cannot be rescaled or deformed without introducing structural distortions. This limitation restricts their use in applications requiring flexible object manipulation, such as indoor redesign, simulation, and digital content creation. We introduce a part-aware scaling framework that transforms these static completed SDFs into editable, structurally coherent objects. Starting from SDFs and Texture Fields generated by state-of-the-art completion models, our method performs automatic part segmentation, defines user-controlled scaling zones, and applies smooth interpolation of SDFs, color, and part indices to enable proportional and artifact-free deformation. We further incorporate a repetition-based strategy to handle large-scale deformations while preserving repeating geometric patterns. Experiments on Matterport3D and ShapeNet objects show that our method overcomes the inherent rigidity of completed SDFs and is visually more appealing than global and naive selective scaling, particularly for complex shapes and repetitive structures.

</details>


### [45] [Reliable Thinking with Images](https://arxiv.org/abs/2602.12916)
*Haobin Li,Yutong Yang,Yijie Lin,Dai Xiang,Mouxing Yang,Xi Peng*

Main category: cs.CV

TL;DR: RTWI addresses noisy thinking in multimodal reasoning by estimating reliability of visual cues and textual CoT, using filtering and voting to prevent error accumulation.


<details>
  <summary>Details</summary>
Motivation: Existing Thinking with Images (TWI) methods assume perfect interleaved image-text reasoning chains, but real-world multimodal understanding often contains errors (Noisy Thinking) that accumulate and degrade performance.

Method: RTWI estimates reliability of both visual cues and textual CoT in a unified text-centric manner, then employs robust filtering and voting modules to prevent noisy thinking from contaminating final answers.

Result: Extensive experiments on seven benchmarks verify RTWI's effectiveness against noisy thinking problems.

Conclusion: RTWI provides a reliable solution to the practical problem of noisy thinking in multimodal reasoning, preventing error accumulation through reliability estimation and robust filtering mechanisms.

Abstract: As a multimodal extension of Chain-of-Thought (CoT), Thinking with Images (TWI) has recently emerged as a promising avenue to enhance the reasoning capability of Multi-modal Large Language Models (MLLMs), which generates interleaved CoT by incorporating visual cues into the textual reasoning process. However, the success of existing TWI methods heavily relies on the assumption that interleaved image-text CoTs are faultless, which is easily violated in real-world scenarios due to the complexity of multimodal understanding. In this paper, we reveal and study a highly-practical yet under-explored problem in TWI, termed Noisy Thinking (NT). Specifically, NT refers to the imperfect visual cues mining and answer reasoning process. As the saying goes, ``One mistake leads to another'', erroneous interleaved CoT would cause error accumulation, thus significantly degrading the performance of MLLMs. To solve the NT problem, we propose a novel method dubbed Reliable Thinking with Images (RTWI). In brief, RTWI estimates the reliability of visual cues and textual CoT in a unified text-centric manner and accordingly employs robust filtering and voting modules to prevent NT from contaminating the final answer. Extensive experiments on seven benchmarks verify the effectiveness of RTWI against NT.

</details>


### [46] [EPRBench: A High-Quality Benchmark Dataset for Event Stream Based Visual Place Recognition](https://arxiv.org/abs/2602.12919)
*Xiao Wang,Xingxing Xiong,Jinfeng Gao,Xufeng Lou,Bo Jiang,Si-bao Chen,Yaowei Wang,Yonghong Tian*

Main category: cs.CV

TL;DR: EPRBench is a new benchmark dataset for event stream-based Visual Place Recognition (VPR) with 10K sequences and 65K frames, plus LLM-generated scene descriptions, enabling evaluation of 15 SOTA algorithms and a novel LLM-guided multi-modal fusion framework.


<details>
  <summary>Details</summary>
Motivation: Current event stream-based VPR research lacks dedicated datasets, and conventional cameras struggle with challenging conditions like low illumination, overexposure, and high-speed motion where event cameras excel.

Method: 1) Created EPRBench dataset with handheld/vehicle-mounted setups across diverse conditions; 2) Provided LLM-generated scene descriptions with human refinement; 3) Benchmarked 15 SOTA VPR algorithms; 4) Proposed novel LLM-guided multi-modal fusion framework using textual descriptions to guide token selection, cross-modal fusion, and multi-scale learning.

Result: Established comprehensive benchmark with 10K event sequences and 65K frames, benchmarked 15 algorithms, and developed a novel fusion framework that achieves accurate place recognition with interpretable reasoning processes.

Conclusion: EPRBench provides a high-quality foundation for event stream-based VPR research, enabling systematic evaluation and advancing the field through semantic-aware, language-integrated approaches that enhance both performance and explainability.

Abstract: Event stream-based Visual Place Recognition (VPR) is an emerging research direction that offers a compelling solution to the instability of conventional visible-light cameras under challenging conditions such as low illumination, overexposure, and high-speed motion. Recognizing the current scarcity of dedicated datasets in this domain, we introduce EPRBench, a high-quality benchmark specifically designed for event stream-based VPR. EPRBench comprises 10K event sequences and 65K event frames, collected using both handheld and vehicle-mounted setups to comprehensively capture real-world challenges across diverse viewpoints, weather conditions, and lighting scenarios. To support semantic-aware and language-integrated VPR research, we provide LLM-generated scene descriptions, subsequently refined through human annotation, establishing a solid foundation for integrating LLMs into event-based perception pipelines. To facilitate systematic evaluation, we implement and benchmark 15 state-of-the-art VPR algorithms on EPRBench, offering a strong baseline for future algorithmic comparisons. Furthermore, we propose a novel multi-modal fusion paradigm for VPR: leveraging LLMs to generate textual scene descriptions from raw event streams, which then guide spatially attentive token selection, cross-modal feature fusion, and multi-scale representation learning. This framework not only achieves highly accurate place recognition but also produces interpretable reasoning processes alongside its predictions, significantly enhancing model transparency and explainability. The dataset and source code will be released on https://github.com/Event-AHU/Neuromorphic_ReID

</details>


### [47] [Beyond Benchmarks of IUGC: Rethinking Requirements of Deep Learning Methods for Intrapartum Ultrasound Biometry from Fetal Ultrasound Videos](https://arxiv.org/abs/2602.12922)
*Jieyun Bai,Zihao Zhou,Yitong Tang,Jie Gan,Zhuonan Liang,Jianan Fan,Lisa B. Mcguire,Jillian L. Clarke,Weidong Cai,Jacaueline Spurway,Yubo Tang,Shiye Wang,Wenda Shen,Wangwang Yu,Yihao Li,Philippe Zhang,Weili Jiang,Yongjie Li,Salem Muhsin Ali Binqahal Al Nasim,Arsen Abzhanov,Numan Saeed,Mohammad Yaqub,Zunhui Xian,Hongxing Lin,Libin Lan,Jayroop Ramesh,Valentin Bacher,Mark Eid,Hoda Kalabizadeh,Christian Rupprecht,Ana I. L. Namburete,Pak-Hei Yeung,Madeleine K. Wyburd,Nicola K. Dinsdale,Assanali Serikbey,Jiankai Li,Sung-Liang Chen,Zicheng Hu,Nana Liu,Yian Deng,Wei Hu,Cong Tan,Wenfeng Zhang,Mai Tuyet Nhi,Gregor Koehler,Rapheal Stock,Klaus Maier-Hein,Marawan Elbatel,Xiaomeng Li,Saad Slimani,Victor M. Campello,Benard Ohene-Botwe,Isaac Khobo,Yuxin Huang,Zhenyan Han,Hongying Hou,Di Qiu,Zheng Zheng,Gongning Luo,Dong Ni,Yaosheng Lu,Karim Lekadir,Shuo Li*

Main category: cs.CV

TL;DR: The Intrapartum Ultrasound Grand Challenge (IUGC) addresses high maternal/neonatal mortality in low-resource settings by creating an automatic multi-task framework for ultrasound biometry and releasing the largest multi-center intrapartum ultrasound dataset.


<details>
  <summary>Details</summary>
Motivation: 45% of maternal deaths, neonatal deaths, and stillbirths occur during intrapartum phase, especially in low- and middle-income countries. Ultrasound monitoring is critical but limited by shortage of trained sonographers in resource-limited settings.

Method: Created a multi-task automatic measurement framework integrating standard plane classification, fetal head-pubic symphysis segmentation, and biometry. Released largest multi-center intrapartum ultrasound video dataset (774 videos, 68,106 frames from 3 hospitals). Analyzed submissions from 8 teams across preprocessing, data augmentation, learning strategy, model architecture, and post-processing.

Result: Encouraging performance achieved but field remains at early stage. Comprehensive benchmark analysis identified key bottlenecks and potential solutions. All benchmark solutions and complete dataset publicly released for reproducible research.

Conclusion: Further in-depth investigation required before large-scale clinical deployment. The challenge promotes continued advances in automatic intrapartum ultrasound biometry to address critical healthcare gaps in resource-limited settings.

Abstract: A substantial proportion (45\%) of maternal deaths, neonatal deaths, and stillbirths occur during the intrapartum phase, with a particularly high burden in low- and middle-income countries. Intrapartum biometry plays a critical role in monitoring labor progression; however, the routine use of ultrasound in resource-limited settings is hindered by a shortage of trained sonographers. To address this challenge, the Intrapartum Ultrasound Grand Challenge (IUGC), co-hosted with MICCAI 2024, was launched. The IUGC introduces a clinically oriented multi-task automatic measurement framework that integrates standard plane classification, fetal head-pubic symphysis segmentation, and biometry, enabling algorithms to exploit complementary task information for more accurate estimation. Furthermore, the challenge releases the largest multi-center intrapartum ultrasound video dataset to date, comprising 774 videos (68,106 frames) collected from three hospitals, providing a robust foundation for model training and evaluation. In this study, we present a comprehensive overview of the challenge design, review the submissions from eight participating teams, and analyze their methods from five perspectives: preprocessing, data augmentation, learning strategy, model architecture, and post-processing. In addition, we perform a systematic analysis of the benchmark results to identify key bottlenecks, explore potential solutions, and highlight open challenges for future research. Although encouraging performance has been achieved, our findings indicate that the field remains at an early stage, and further in-depth investigation is required before large-scale clinical deployment. All benchmark solutions and the complete dataset have been publicly released to facilitate reproducible research and promote continued advances in automatic intrapartum ultrasound biometry.

</details>


### [48] [Deep-Learning Atlas Registration for Melanoma Brain Metastases: Preserving Pathology While Enabling Cohort-Level Analyses](https://arxiv.org/abs/2602.12933)
*Nanna E. Wielenberg,Ilinca Popp,Oliver Blanck,Lucas Zander,Jan C. Peeken,Stephanie E. Combs,Anca-Ligia Grosu,Dimos Baltas,Tobias Fechter*

Main category: cs.CV

TL;DR: A deep learning deformable registration framework for aligning pathological brains with metastases to a common atlas without requiring lesion masks, enabling standardized multi-center spatial analysis of melanoma brain metastases.


<details>
  <summary>Details</summary>
Motivation: Melanoma brain metastases (MBM) are common but spatially heterogeneous, making cohort-level analysis difficult due to anatomical variability and differing MRI protocols across centers. Current methods struggle with pathological brains containing metastatic lesions.

Method: Fully differentiable deep-learning-based deformable registration framework that aligns individual pathological brains to a common atlas while preserving metastatic tissue. Uses forward-model similarity metric based on distance-transformed anatomical labels to handle missing anatomical correspondences caused by metastases, combined with volume-preserving regularization for deformation plausibility.

Result: High registration accuracy across three centers (DSC 0.89-0.92, HD 6.79-7.60 mm, ASSD 0.63-0.77 mm) while preserving metastatic volumes. Spatial analysis showed significant over-representation of MBM in cerebral cortex and putamen, under-representation in white matter, consistent localization near gray-white matter junction, and no arterial territory showed increased metastasis frequency after volume correction.

Conclusion: The framework enables robust atlas registration of pathological brain MRI without lesion masks and supports reproducible multi-center analyses. Confirms and refines known spatial predilections of MBM, particularly preferential seeding near gray-white matter junction and cortical regions. Publicly available implementation facilitates extension to other brain tumors and neurological pathologies.

Abstract: Melanoma brain metastases (MBM) are common and spatially heterogeneous lesions, complicating cohort-level analyses due to anatomical variability and differing MRI protocols. We propose a fully differentiable, deep-learning-based deformable registration framework that aligns individual pathological brains to a common atlas while preserving metastatic tissue without requiring lesion masks or preprocessing.
  Missing anatomical correspondences caused by metastases are handled through a forward-model similarity metric based on distance-transformed anatomical labels, combined with a volume-preserving regularization term to ensure deformation plausibility. Registration performance was evaluated using Dice coefficient (DSC), Hausdorff distance (HD), average symmetric surface distance (ASSD), and Jacobian-based measures. The method was applied to 209 MBM patients from three centres, enabling standardized mapping of metastases to anatomical, arterial, and perfusion atlases.
  The framework achieved high registration accuracy across datasets (DSC 0.89-0.92, HD 6.79-7.60 mm, ASSD 0.63-0.77 mm) while preserving metastatic volumes. Spatial analysis demonstrated significant over-representation of MBM in the cerebral cortex and putamen, under-representation in white matter, and consistent localization near the gray-white matter junction. No arterial territory showed increased metastasis frequency after volume correction.
  This approach enables robust atlas registration of pathological brain MRI without lesion masks and supports reproducible multi-centre analyses. Applied to MBM, it confirms and refines known spatial predilections, particularly preferential seeding near the gray-white matter junction and cortical regions. The publicly available implementation facilitates reproducible research and extension to other brain tumours and neurological pathologies.

</details>


### [49] [Unleashing MLLMs on the Edge: A Unified Framework for Cross-Modal ReID via Adaptive SVD Distillation](https://arxiv.org/abs/2602.12936)
*Hongbo Jiang,Jie Li,Xinqi Cai,Tianyu Xie,Yunhang Shen,Pingyang Dai,Liujuan Cao*

Main category: cs.CV

TL;DR: MLLMEmbed-ReID: A unified cloud-edge framework that adapts MLLMs for cross-modal re-identification across RGB, infrared, sketch, and text modalities, with efficient cloud training and novel distillation for edge deployment.


<details>
  <summary>Details</summary>
Motivation: Current CM-ReID systems suffer from fragmented specialized cloud models for different modalities, while existing MLLM approaches fail to create unified end-to-end backbones and lack effective knowledge distillation strategies for edge deployment.

Method: 1) Cloud model: Adapt foundational MLLM using instruction-based prompting to generate unified embedding space across modalities, trained with hierarchical LoRA-SFT under cross-modal alignment objective. 2) Edge deployment: Novel distillation strategy using Principal Component Mapping loss to prioritize essential information and Feature Relation loss to preserve relational structures.

Result: Lightweight edge model achieves SOTA performance on multiple visual CM-ReID benchmarks, while cloud counterpart excels across all CM-ReID benchmarks. Framework enables unified MLLM-level intelligence on resource-constrained devices.

Conclusion: MLLMEmbed-ReID provides a complete solution for deploying unified MLLM intelligence on edge devices, addressing fragmentation in CM-ReID systems and enabling efficient cloud-edge deployment across diverse modalities.

Abstract: Practical cloud-edge deployment of Cross-Modal Re-identification (CM-ReID) faces challenges due to maintaining a fragmented ecosystem of specialized cloud models for diverse modalities. While Multi-Modal Large Language Models (MLLMs) offer strong unification potential, existing approaches fail to adapt them into a single end-to-end backbone and lack effective knowledge distillation strategies for edge deployment. To address these limitations, we propose MLLMEmbed-ReID, a unified framework based on a powerful cloud-edge architecture. First, we adapt a foundational MLLM into a state-of-the-art cloud model. We leverage instruction-based prompting to guide the MLLM in generating a unified embedding space across RGB, infrared, sketch, and text modalities. This model is then trained efficiently with a hierarchical Low-Rank Adaptation finetuning (LoRA-SFT) strategy, optimized under a holistic cross-modal alignment objective. Second, to deploy its knowledge onto an edge-native student, we introduce a novel distillation strategy motivated by the low-rank property in the teacher's feature space. To prioritize essential information, this method employs a Principal Component Mapping loss, while relational structures are preserved via a Feature Relation loss. Our lightweight edge-based model achieves state-of-the-art performance on multiple visual CM-ReID benchmarks, while its cloud-based counterpart excels across all CM-ReID benchmarks. The MLLMEmbed-ReID framework thus presents a complete and effective solution for deploying unified MLLM-level intelligence on resource-constrained devices. The code and models will be open-sourced soon.

</details>


### [50] [Training-Free Acceleration for Document Parsing Vision-Language Model with Hierarchical Speculative Decoding](https://arxiv.org/abs/2602.12957)
*Wenhui Liao,Hongliang Li,Pengyu Xie,Xinyu Cai,Yufan Shen,Yi Xin,Qi Qin,Shenglong Ye,Tianbin Li,Ming Hu,Junjun He,Yihao Liu,Wenhai Wang,Min Dou,Bin Fu,Botian Shi,Yu Qiao,Lianwen Jin*

Main category: cs.CV

TL;DR: A training-free acceleration method for VLM-based document parsing that uses a lightweight draft model and parallel verification to speed up inference on long documents.


<details>
  <summary>Details</summary>
Motivation: VLM-based document parsing models suffer from high inference latency due to auto-regressive generation of long token sequences, especially for documents with complex layouts and long outputs.

Method: Uses speculative decoding with a lightweight draft model to predict token batches, verified in parallel by the accurate VLM. Also partitions documents into independent regions for parallel decoding, then assembles predictions in reading order.

Result: Achieves 2.42x lossless acceleration on OmniDocBench for dots.ocr model, and up to 4.89x acceleration on long-document parsing tasks.

Conclusion: The proposed training-free method effectively accelerates VLM-based document parsing while maintaining accuracy, with significant speedups especially for long documents.

Abstract: Document parsing is a fundamental task in multimodal understanding, supporting a wide range of downstream applications such as information extraction and intelligent document analysis. Benefiting from strong semantic modeling and robust generalization, VLM-based end-to-end approaches have emerged as the mainstream paradigm in recent years. However, these models often suffer from substantial inference latency, as they must auto-regressively generate long token sequences when processing long-form documents. In this work, motivated by the extremely long outputs and complex layout structures commonly found in document parsing, we propose a training-free and highly efficient acceleration method. Inspired by speculative decoding, we employ a lightweight document parsing pipeline as a draft model to predict batches of future tokens, while the more accurate VLM verifies these draft predictions in parallel. Moreover, we further exploit the layout-structured nature of documents by partitioning each page into independent regions, enabling parallel decoding of each region using the same draft-verify strategy. The final predictions are then assembled according to the natural reading order. Experimental results demonstrate the effectiveness of our approach: on the general-purpose OmniDocBench, our method provides a 2.42x lossless acceleration for the dots.ocr model, and achieves up to 4.89x acceleration on long-document parsing tasks. We will release our code to facilitate reproducibility and future research.

</details>


### [51] [Detecting Object Tracking Failure via Sequential Hypothesis Testing](https://arxiv.org/abs/2602.12983)
*Alejandro Monroy Muñoz,Rajeev Verma,Alexander Timans*

Main category: cs.CV

TL;DR: The paper proposes using sequential hypothesis testing (e-processes) to provide formal safety assurances for real-time object tracking, quickly detecting failures while controlling false alert rates without extra training.


<details>
  <summary>Details</summary>
Motivation: Current object tracking systems lack formal safety assurances about when tracking is reliable vs. when it fails, relying only on heuristic confidence measures. There's a need for statistically grounded methods to detect tracking failures in real-time while controlling false alerts.

Method: Interpret object tracking as sequential hypothesis testing using e-processes to accumulate evidence for/against tracking failures over time. Propose both supervised (using ground-truth) and unsupervised (using internal tracking information) variants that are computationally lightweight, require no extra training, and are model-agnostic.

Result: Demonstrated effectiveness on two established tracking models across four video benchmarks. The approach quickly identifies tracking failures while provably containing false alerts at desired rates, limiting costly re-calibration or intervention steps.

Conclusion: Sequential testing offers a statistically grounded and efficient mechanism to incorporate safety assurances into real-time tracking systems, providing formal reliability guarantees where current systems only have heuristic measures.

Abstract: Real-time online object tracking in videos constitutes a core task in computer vision, with wide-ranging applications including video surveillance, motion capture, and robotics. Deployed tracking systems usually lack formal safety assurances to convey when tracking is reliable and when it may fail, at best relying on heuristic measures of model confidence to raise alerts. To obtain such assurances we propose interpreting object tracking as a sequential hypothesis test, wherein evidence for or against tracking failures is gradually accumulated over time. Leveraging recent advancements in the field, our sequential test (formalized as an e-process) quickly identifies when tracking failures set in whilst provably containing false alerts at a desired rate, and thus limiting potentially costly re-calibration or intervention steps. The approach is computationally light-weight, requires no extra training or fine-tuning, and is in principle model-agnostic. We propose both supervised and unsupervised variants by leveraging either ground-truth or solely internal tracking information, and demonstrate its effectiveness for two established tracking models across four video benchmarks. As such, sequential testing can offer a statistically grounded and efficient mechanism to incorporate safety assurances into real-time tracking systems.

</details>


### [52] [MASAR: Motion-Appearance Synergy Refinement for Joint Detection and Trajectory Forecasting](https://arxiv.org/abs/2602.13003)
*Mohammed Amine Bencheikh Lehocine,Julian Schmidt,Frank Moosmann,Dikshant Gupta,Fabian Flohr*

Main category: cs.CV

TL;DR: MASAR is a fully differentiable framework for joint 3D detection and trajectory forecasting that integrates appearance and motion cues through object-centric spatio-temporal encoding, improving trajectory prediction by 20+% while maintaining detection performance.


<details>
  <summary>Details</summary>
Motivation: Traditional autonomous driving systems use hand-crafted bounding-box interfaces that limit information flow and propagate errors. Existing end-to-end models fail to fully exploit synergy between appearance and motion cues, relying mainly on short-term visual features.

Method: MASAR uses an object-centric spatio-temporal mechanism to jointly encode appearance and motion features. It predicts past trajectories and refines them using appearance cues to capture long-term temporal dependencies, enhancing future trajectory forecasting. Compatible with any transformer-based 3D detector.

Result: Experiments on nuScenes dataset show improvements of over 20% in minADE and minFDE metrics while maintaining robust detection performance.

Conclusion: MASAR successfully integrates perception and prediction through joint encoding of appearance and motion features, demonstrating significant improvements in trajectory forecasting performance while preserving detection capabilities.

Abstract: Classical autonomous driving systems connect perception and prediction modules via hand-crafted bounding-box interfaces, limiting information flow and propagating errors to downstream tasks. Recent research aims to develop end-to-end models that jointly address perception and prediction; however, they often fail to fully exploit the synergy between appearance and motion cues, relying mainly on short-term visual features. We follow the idea of "looking backward to look forward", and propose MASAR, a novel fully differentiable framework for joint 3D detection and trajectory forecasting compatible with any transformer-based 3D detector. MASAR employs an object-centric spatio-temporal mechanism that jointly encodes appearance and motion features. By predicting past trajectories and refining them using guidance from appearance cues, MASAR captures long-term temporal dependencies that enhance future trajectory forecasting. Experiments conducted on the nuScenes dataset demonstrate MASAR's effectiveness, showing improvements of over 20% in minADE and minFDE while maintaining robust detection performance. Code and models are available at https://github.com/aminmed/MASAR.

</details>


### [53] [Towards Universal Video MLLMs with Attribute-Structured and Quality-Verified Instructions](https://arxiv.org/abs/2602.13013)
*Yunheng Li,Hengrui Zhang,Meng-Hao Guo,Wenzhao Gao,Shaoyong Jia,Shaohui Jiao,Qibin Hou,Ming-Ming Cheng*

Main category: cs.CV

TL;DR: ASID introduces a structured fine-grained audiovisual instruction dataset (ASID-1M), scalable curation pipeline (ASID-Verify), and video understanding model (ASID-Captioner) that achieves SOTA performance on multiple benchmarks.


<details>
  <summary>Details</summary>
Motivation: Existing video understanding models are limited by coarse video-instruction data that lacks fine-grained organization and reliable annotation, failing to capture complex audiovisual content adequately.

Method: Three components: (1) ASID-1M - 1M structured fine-grained audiovisual instruction annotations with single/multi-attribute supervision; (2) ASID-Verify - scalable curation pipeline with automatic verification for semantic/temporal consistency; (3) ASID-Captioner - video understanding model trained via SFT on ASID-1M.

Result: ASID-Captioner improves fine-grained caption quality, reduces hallucinations, enhances instruction following, achieves SOTA among open-source models, and is competitive with Gemini-3-Pro across 7 benchmarks covering audiovisual captioning, attribute-wise captioning, QA, and temporal grounding.

Conclusion: The structured fine-grained annotation approach enables more effective video understanding, addressing limitations of existing coarse video-instruction data and advancing universal video understanding capabilities.

Abstract: Universal video understanding requires modeling fine-grained visual and audio information over time in diverse real-world scenarios. However, the performance of existing models is primarily constrained by video-instruction data that represents complex audiovisual content as single, incomplete descriptions, lacking fine-grained organization and reliable annotation. To address this, we introduce: (i) ASID-1M, an open-source collection of one million structured, fine-grained audiovisual instruction annotations with single- and multi-attribute supervision; (ii) ASID-Verify, a scalable data curation pipeline for annotation, with automatic verification and refinement that enforces semantic and temporal consistency between descriptions and the corresponding audiovisual content; and (iii) ASID-Captioner, a video understanding model trained via Supervised Fine-Tuning (SFT) on the ASID-1M. Experiments across seven benchmarks covering audiovisual captioning, attribute-wise captioning, caption-based QA, and caption-based temporal grounding show that ASID-Captioner improves fine-grained caption quality while reducing hallucinations and improving instruction following. It achieves state-of-the-art performance among open-source models and is competitive with Gemini-3-Pro.

</details>


### [54] [Multimodal Classification via Total Correlation Maximization](https://arxiv.org/abs/2602.13015)
*Feng Yu,Xiangyu Wu,Yang Yang,Jianfeng Lu*

Main category: cs.CV

TL;DR: TCMax: A hyperparameter-free multimodal learning method that maximizes total correlation between multimodal features and labels to alleviate modality competition and capture inter-modal interactions.


<details>
  <summary>Details</summary>
Motivation: Joint multimodal learning often overfits certain modalities while neglecting others, leading to performance inferior to unimodal learning. Previous approaches haven't sufficiently examined the relationship between joint and unimodal learning from an information-theoretic perspective.

Method: Theoretical analysis of modality competition, proposing to maximize total correlation between multimodal features and labels. Introduces Total Correlation Neural Estimation (TCNE) to derive lower bound for total correlation, and TCMax - a hyperparameter-free loss function that maximizes total correlation through variational bound optimization.

Result: Extensive experiments show TCMax outperforms state-of-the-art joint and unimodal learning approaches.

Conclusion: The proposed TCMax method effectively addresses modality competition in multimodal learning by maximizing total correlation, leading to superior performance compared to existing approaches.

Abstract: Multimodal learning integrates data from diverse sensors to effectively harness information from different modalities. However, recent studies reveal that joint learning often overfits certain modalities while neglecting others, leading to performance inferior to that of unimodal learning. Although previous efforts have sought to balance modal contributions or combine joint and unimodal learning, thereby mitigating the degradation of weaker modalities with promising outcomes, few have examined the relationship between joint and unimodal learning from an information-theoretic perspective. In this paper, we theoretically analyze modality competition and propose a method for multimodal classification by maximizing the total correlation between multimodal features and labels. By maximizing this objective, our approach alleviates modality competition while capturing inter-modal interactions via feature alignment. Building on Mutual Information Neural Estimation (MINE), we introduce Total Correlation Neural Estimation (TCNE) to derive a lower bound for total correlation. Subsequently, we present TCMax, a hyperparameter-free loss function that maximizes total correlation through variational bound optimization. Extensive experiments demonstrate that TCMax outperforms state-of-the-art joint and unimodal learning approaches. Our code is available at https://github.com/hubaak/TCMax.

</details>


### [55] [DynaGuide: A Generalizable Dynamic Guidance Framework for Unsupervised Semantic Segmentation](https://arxiv.org/abs/2602.13020)
*Boujemaa Guermazi,Riadh Ksantini,Naimul Khan*

Main category: cs.CV

TL;DR: DynaGuide is an adaptive unsupervised segmentation framework that combines global pseudo-labels from zero-shot models with local boundary refinement using a lightweight CNN, achieving state-of-the-art performance on multiple benchmarks without ground-truth labels.


<details>
  <summary>Details</summary>
Motivation: Existing unsupervised segmentation methods struggle to balance global semantic structure with fine-grained boundary accuracy, especially in domains with scarce labeled data.

Method: DynaGuide uses a dual-guidance strategy: global pseudo-labels from zero-shot models (DiffSeg/SegFormer) combined with local boundary refinement via lightweight CNN. Features a dynamic multi-component loss balancing feature similarity, Huber-smoothed spatial continuity, and semantic alignment.

Result: Achieves state-of-the-art performance: improves mIoU by 17.5% on BSD500, 3.1% on PASCAL VOC2012, and 11.66% on COCO. Trains without ground-truth labels and supports plug-and-play integration of diverse guidance sources.

Conclusion: DynaGuide offers a scalable, practical solution for unsupervised segmentation with modular design, strong generalization, minimal computational footprint, and real-world applicability.

Abstract: Unsupervised image segmentation is a critical task in computer vision. It enables dense scene understanding without human annotations, which is especially valuable in domains where labelled data is scarce. However, existing methods often struggle to reconcile global semantic structure with fine-grained boundary accuracy. This paper introduces DynaGuide, an adaptive segmentation framework that addresses these challenges through a novel dual-guidance strategy and dynamic loss optimization. Building on our previous work, DynaSeg, DynaGuide combines global pseudo-labels from zero-shot models such as DiffSeg or SegFormer with local boundary refinement using a lightweight CNN trained from scratch. This synergy allows the model to correct coarse or noisy global predictions and produce high-precision segmentations. At the heart of DynaGuide is a multi-component loss that dynamically balances feature similarity, Huber-smoothed spatial continuity, including diagonal relationships, and semantic alignment with the global pseudo-labels. Unlike prior approaches, DynaGuide trains entirely without ground-truth labels in the target domain and supports plug-and-play integration of diverse guidance sources. Extensive experiments on BSD500, PASCAL VOC2012, and COCO demonstrate that DynaGuide achieves state-of-the-art performance, improving mIoU by 17.5% on BSD500, 3.1% on PASCAL VOC2012, and 11.66% on COCO. With its modular design, strong generalization, and minimal computational footprint, DynaGuide offers a scalable and practical solution for unsupervised segmentation in real-world settings. Code available at: https://github.com/RyersonMultimediaLab/DynaGuide

</details>


### [56] [Learning Image-based Tree Crown Segmentation from Enhanced Lidar-based Pseudo-labels](https://arxiv.org/abs/2602.13022)
*Julius Pesonen,Stefan Rua,Josef Taher,Niko Koivumäki,Xiaowei Yu,Eija Honkavaara*

Main category: cs.CV

TL;DR: A method to train deep learning models for individual tree crown segmentation using ALS-derived pseudo-labels enhanced by SAM 2, achieving state-of-the-art performance without manual annotation.


<details>
  <summary>Details</summary>
Motivation: Individual tree crown mapping is crucial for urban tree inventories and forest health monitoring, but automatic segmentation in aerial imagery is challenging due to texture variations and crown overlaps.

Method: Uses pseudo-labels derived from aerial laser scanning (ALS) data enhanced with Segment Anything Model 2 (SAM 2) to train deep learning models for segmenting individual trees from RGB and multispectral images.

Result: The method produces segmentation models that outperform available general-domain models on the same task, achieving domain-specific performance without manual annotation costs.

Conclusion: ALS-derived pseudo-labels enhanced by SAM 2 provide a cost-effective way to obtain high-quality training data for optical image-based tree crown segmentation, enabling superior performance compared to general-domain models.

Abstract: Mapping individual tree crowns is essential for tasks such as maintaining urban tree inventories and monitoring forest health, which help us understand and care for our environment. However, automatically separating the crowns from each other in aerial imagery is challenging due to factors such as the texture and partial tree crown overlaps. In this study, we present a method to train deep learning models that segment and separate individual trees from RGB and multispectral images, using pseudo-labels derived from aerial laser scanning (ALS) data. Our study shows that the ALS-derived pseudo-labels can be enhanced using a zero-shot instance segmentation model, Segment Anything Model 2 (SAM 2). Our method offers a way to obtain domain-specific training annotations for optical image-based models without any manual annotation cost, leading to segmentation models which outperform any available models which have been targeted for general domain deployment on the same task.

</details>


### [57] [FedHENet: A Frugal Federated Learning Framework for Heterogeneous Environments](https://arxiv.org/abs/2602.13024)
*Alejandro Dopico-Castro,Oscar Fontenla-Romero,Bertha Guijarro-Berdiñas,Amparo Alonso-Betanzos,Iván Pérez Digón*

Main category: cs.CV

TL;DR: FedHENet is a federated learning method for image classification that uses a fixed pre-trained feature extractor and learns only a single output layer via homomorphic encryption in one communication round, achieving competitive accuracy with better stability and energy efficiency.


<details>
  <summary>Details</summary>
Motivation: Traditional FL approaches require expensive iterative deep network optimization and risk privacy through shared gradients. There's a need for more efficient, privacy-preserving FL methods that avoid costly local fine-tuning and hyperparameter tuning.

Method: Extends FedHEONN framework to image classification. Uses fixed pre-trained feature extractor, learns only single output layer. Client knowledge is aggregated analytically in one communication round using homomorphic encryption (HE). No local fine-tuning required.

Result: Achieves competitive accuracy compared to iterative FL baselines. Demonstrates superior stability performance and up to 70% better energy efficiency. Method is hyperparameter-free, eliminating carbon footprint from hyperparameter tuning.

Conclusion: FedHENet provides an efficient, privacy-preserving FL solution for image classification that avoids iterative optimization, reduces communication costs, improves energy efficiency, and eliminates hyperparameter tuning overhead.

Abstract: Federated Learning (FL) enables collaborative training without centralizing data, essential for privacy compliance in real-world scenarios involving sensitive visual information. Most FL approaches rely on expensive, iterative deep network optimization, which still risks privacy via shared gradients. In this work, we propose FedHENet, extending the FedHEONN framework to image classification. By using a fixed, pre-trained feature extractor and learning only a single output layer, we avoid costly local fine-tuning. This layer is learned by analytically aggregating client knowledge in a single round of communication using homomorphic encryption (HE). Experiments show that FedHENet achieves competitive accuracy compared to iterative FL baselines while demonstrating superior stability performance and up to 70\% better energy efficiency. Crucially, our method is hyperparameter-free, removing the carbon footprint associated with hyperparameter tuning in standard FL. Code available in https://github.com/AlejandroDopico2/FedHENet/

</details>


### [58] [Human-Aligned MLLM Judges for Fine-Grained Image Editing Evaluation: A Benchmark, Framework, and Analysis](https://arxiv.org/abs/2602.13028)
*Runzhou Liu,Hailey Weingord,Sejal Mittal,Prakhar Dungarwal,Anusha Nandula,Bo Ni,Samyadeep Basu,Hongjie Chen,Nesreen K. Ahmed,Li Li,Jiayi Zhang,Koustava Goswami,Subhojyoti Mukherjee,Branislav Kveton,Puneet Mathur,Franck Dernoncourt,Yue Zhao,Yu Wang,Ryan A. Rossi,Zhengzhong Tu,Hongru Du*

Main category: cs.CV

TL;DR: The paper introduces a fine-grained MLLM-as-a-Judge framework for evaluating image editing models, decomposing evaluation into 12 interpretable factors across image preservation, edit quality, and instruction fidelity.


<details>
  <summary>Details</summary>
Motivation: Traditional image editing metrics are coarse, limited in interpretability, and fail to capture important human perception aspects like controllability, edit localization, and faithfulness to user instructions. They often reward visually plausible outputs while overlooking key evaluation dimensions.

Method: Proposes a Multimodal Large Language Model (MLLM)-as-a-Judge framework that decomposes evaluation into 12 fine-grained interpretable factors. Creates a human-validated benchmark integrating human judgments, MLLM-based evaluations, model outputs, and traditional metrics across diverse image editing tasks.

Result: MLLM judges align closely with human evaluations at fine granularity, making them reliable and scalable evaluators. Traditional metrics are poor proxies for these factors, failing to distinguish over-edited or semantically imprecise outputs, while MLLM judges provide more intuitive and informative assessments.

Conclusion: The work introduces a benchmark, principled factorization, and empirical evidence positioning fine-grained MLLM judges as a practical foundation for studying, comparing, and improving image editing approaches, addressing limitations of traditional evaluation metrics.

Abstract: Evaluating image editing models remains challenging due to the coarse granularity and limited interpretability of traditional metrics, which often fail to capture aspects important to human perception and intent. Such metrics frequently reward visually plausible outputs while overlooking controllability, edit localization, and faithfulness to user instructions. In this work, we introduce a fine-grained Multimodal Large Language Model (MLLM)-as-a-Judge framework for image editing that decomposes common evaluation notions into twelve fine-grained interpretable factors spanning image preservation, edit quality, and instruction fidelity. Building on this formulation, we present a new human-validated benchmark that integrates human judgments, MLLM-based evaluations, model outputs, and traditional metrics across diverse image editing tasks. Through extensive human studies, we show that the proposed MLLM judges align closely with human evaluations at a fine granularity, supporting their use as reliable and scalable evaluators. We further demonstrate that traditional image editing metrics are often poor proxies for these factors, failing to distinguish over-edited or semantically imprecise outputs, whereas our judges provide more intuitive and informative assessments in both offline and online settings. Together, this work introduces a benchmark, a principled factorization, and empirical evidence positioning fine-grained MLLM judges as a practical foundation for studying, comparing, and improving image editing approaches.

</details>


### [59] [Implicit-Scale 3D Reconstruction for Multi-Food Volume Estimation from Monocular Images](https://arxiv.org/abs/2602.13041)
*Yuhao Chen,Gautham Vinod,Siddeshwar Raghavan,Talha Ibn Mahmud,Bruce Coburn,Jinge Ma,Fengqing Zhu,Jiangpeng He*

Main category: cs.CV

TL;DR: A benchmark dataset for implicit-scale 3D reconstruction from monocular multi-food images to advance geometry-based food portion estimation, addressing scale ambiguity in real dining scenarios.


<details>
  <summary>Details</summary>
Motivation: Existing dietary assessment methods rely on single-image analysis or appearance-based inference (including vision-language models) which lack explicit geometric reasoning and are sensitive to scale ambiguity. There's a need for methods that can handle realistic dining scenarios without explicit physical references.

Method: Reframes food portion estimation as an implicit-scale 3D reconstruction problem under monocular observations. The benchmark dataset removes explicit physical references and metric annotations, instead providing contextual objects (plates, utensils) requiring algorithms to infer scale from implicit cues and prior knowledge.

Result: The benchmark was adopted as a challenge at MetaFood 2025 Workshop. Experimental results show geometry-based reconstruction methods provide improved accuracy and greater robustness compared to vision-language baselines. Top-performing approach achieved 0.21 MAPE in volume estimation and 5.7 L1 Chamfer Distance in geometric accuracy.

Conclusion: Geometry-based reconstruction methods outperform appearance-based approaches for food portion estimation, demonstrating the value of explicit geometric reasoning in handling scale ambiguity and complex real-world dining scenarios with multiple foods, occlusions, and spatial arrangements.

Abstract: We present Implicit-Scale 3D Reconstruction from Monocular Multi-Food Images, a benchmark dataset designed to advance geometry-based food portion estimation in realistic dining scenarios. Existing dietary assessment methods largely rely on single-image analysis or appearance-based inference, including recent vision-language models, which lack explicit geometric reasoning and are sensitive to scale ambiguity. This benchmark reframes food portion estimation as an implicit-scale 3D reconstruction problem under monocular observations. To reflect real-world conditions, explicit physical references and metric annotations are removed; instead, contextual objects such as plates and utensils are provided, requiring algorithms to infer scale from implicit cues and prior knowledge. The dataset emphasizes multi-food scenes with diverse object geometries, frequent occlusions, and complex spatial arrangements. The benchmark was adopted as a challenge at the MetaFood 2025 Workshop, where multiple teams proposed reconstruction-based solutions. Experimental results show that while strong vision--language baselines achieve competitive performance, geometry-based reconstruction methods provide both improved accuracy and greater robustness, with the top-performing approach achieving 0.21 MAPE in volume estimation and 5.7 L1 Chamfer Distance in geometric accuracy.

</details>


### [60] [Curriculum-DPO++: Direct Preference Optimization via Data and Model Curricula for Text-to-Image Generation](https://arxiv.org/abs/2602.13055)
*Florinel-Alin Croitoru,Vlad Hondru,Radu Tudor Ionescu,Nicu Sebe,Mubarak Shah*

Main category: cs.CV

TL;DR: Curriculum-DPO++ enhances preference optimization for text-to-image generation by combining data-level curriculum with model-level curriculum, dynamically increasing learning capacity during training.


<details>
  <summary>Details</summary>
Motivation: Existing preference optimization methods (RLHF and DPO) don't account for varying difficulty in learning different preferences, leading to suboptimal optimization. Curriculum-DPO addressed this with data-level curriculum, but further improvements are possible.

Method: Curriculum-DPO++ combines data-level curriculum with model-level curriculum: 1) Progressive layer unfreezing - start with subset of trainable layers, gradually unfreeze until full architecture; 2) Progressive LoRA rank scheduling - start with smaller rank, incrementally increase to baseline rank; plus improved ranking strategy for image pairs.

Result: Outperforms Curriculum-DPO and other state-of-the-art methods on nine benchmarks in terms of text alignment, aesthetics, and human preference.

Conclusion: Curriculum-DPO++ effectively addresses the difficulty-aware preference learning problem through combined data-level and model-level curriculum, achieving superior performance in text-to-image generation preference optimization.

Abstract: Direct Preference Optimization (DPO) has been proposed as an effective and efficient alternative to reinforcement learning from human feedback (RLHF). However, neither RLHF nor DPO take into account the fact that learning certain preferences is more difficult than learning other preferences, rendering the optimization process suboptimal. To address this gap in text-to-image generation, we recently proposed Curriculum-DPO, a method that organizes image pairs by difficulty. In this paper, we introduce Curriculum-DPO++, an enhanced method that combines the original data-level curriculum with a novel model-level curriculum. More precisely, we propose to dynamically increase the learning capacity of the denoising network as training advances. We implement this capacity increase via two mechanisms. First, we initialize the model with only a subset of the trainable layers used in the original Curriculum-DPO. As training progresses, we sequentially unfreeze layers until the configuration matches the full baseline architecture. Second, as the fine-tuning is based on Low-Rank Adaptation (LoRA), we implement a progressive schedule for the dimension of the low-rank matrices. Instead of maintaining a fixed capacity, we initialize the low-rank matrices with a dimension significantly smaller than that of the baseline. As training proceeds, we incrementally increase their rank, allowing the capacity to grow until it converges to the same rank value as in Curriculum-DPO. Furthermore, we propose an alternative ranking strategy to the one employed by Curriculum-DPO. Finally, we compare Curriculum-DPO++ against Curriculum-DPO and other state-of-the-art preference optimization approaches on nine benchmarks, outperforming the competing methods in terms of text alignment, aesthetics and human preference. Our code is available at https://github.com/CroitoruAlin/Curriculum-DPO.

</details>


### [61] [A Calibrated Memorization Index (MI) for Detecting Training Data Leakage in Generative MRI Models](https://arxiv.org/abs/2602.13066)
*Yash Deo,Yan Jia,Toni Lassila,Victoria J Hodge,Alejandro F Frang,Chenghao Qian,Siyuan Kang,Ibrahim Habli*

Main category: cs.CV

TL;DR: Proposes a calibrated per-sample metric using MRI foundation model features to detect memorization and duplication in medical image generation, achieving near-perfect duplicate detection.


<details>
  <summary>Details</summary>
Motivation: Image generative models can duplicate training data, raising privacy concerns in medical imaging where patient data confidentiality is critical. Existing methods may not provide consistent detection across different datasets.

Method: Uses MRI foundation model to extract image features, aggregates multi-layer whitened nearest-neighbor similarities, and maps them to bounded Overfit/Novelty Index (ONI) and Memorization Index (MI) scores.

Result: Across three MRI datasets with controlled duplication percentages, the metric robustly detects duplication and provides consistent values across datasets. At sample level, achieves near-perfect detection of duplicates.

Conclusion: The proposed calibrated per-sample metric effectively detects memorization and duplication in medical image generation, addressing privacy concerns while maintaining consistency across different MRI datasets.

Abstract: Image generative models are known to duplicate images from the training data as part of their outputs, which can lead to privacy concerns when used for medical image generation. We propose a calibrated per-sample metric for detecting memorization and duplication of training data. Our metric uses image features extracted using an MRI foundation model, aggregates multi-layer whitened nearest-neighbor similarities, and maps them to a bounded \emph{Overfit/Novelty Index} (ONI) and \emph{Memorization Index} (MI) scores. Across three MRI datasets with controlled duplication percentages and typical image augmentations, our metric robustly detects duplication and provides more consistent metric values across datasets. At the sample level, our metric achieves near-perfect detection of duplicates.

</details>


### [62] [SIEFormer: Spectral-Interpretable and -Enhanced Transformer for Generalized Category Discovery](https://arxiv.org/abs/2602.13067)
*Chunming Li,Shidong Wang,Tong Xin,Haofeng Zhang*

Main category: cs.CV

TL;DR: SIEFormer is a novel Vision Transformer variant that uses spectral analysis to reinterpret attention mechanisms, featuring implicit and explicit spectral branches for enhanced feature adaptability in Generalized Category Discovery tasks.


<details>
  <summary>Details</summary>
Motivation: The paper aims to enhance Vision Transformers by leveraging spectral analysis to reinterpret attention mechanisms, particularly focusing on improving feature adaptability for challenging Generalized Category Discovery tasks where traditional methods struggle.

Method: SIEFormer uses a dual-branch architecture: 1) Implicit branch with graph Laplacians to model local token correlations and Band-adaptive Filter layer for flexible frequency filtering; 2) Explicit branch with Maneuverable Filtering Layer that applies Fourier transform to value features, modulates in frequency domain with learnable parameters, then inverse transforms.

Result: Extensive experiments show state-of-the-art performance on multiple image recognition datasets, with ablation studies and visualizations confirming the superiority of the approach.

Conclusion: SIEFormer successfully integrates spectral analysis into Vision Transformers, providing enhanced feature adaptability and superior performance for Generalized Category Discovery tasks through its innovative dual-branch spectral architecture.

Abstract: This paper presents a novel approach, Spectral-Interpretable and -Enhanced Transformer (SIEFormer), which leverages spectral analysis to reinterpret the attention mechanism within Vision Transformer (ViT) and enhance feature adaptability, with particular emphasis on challenging Generalized Category Discovery (GCD) tasks. The proposed SIEFormer is composed of two main branches, each corresponding to an implicit and explicit spectral perspective of the ViT, enabling joint optimization. The implicit branch realizes the use of different types of graph Laplacians to model the local structure correlations of tokens, along with a novel Band-adaptive Filter (BaF) layer that can flexibly perform both band-pass and band-reject filtering. The explicit branch, on the other hand, introduces a Maneuverable Filtering Layer (MFL) that learns global dependencies among tokens by applying the Fourier transform to the input ``value" features, modulating the transformed signal with a set of learnable parameters in the frequency domain, and then performing an inverse Fourier transform to obtain the enhanced features. Extensive experiments reveal state-of-the-art performance on multiple image recognition datasets, reaffirming the superiority of our approach through ablation studies and visualizations.

</details>


### [63] [Universal Transformation of One-Class Classifiers for Unsupervised Anomaly Detection](https://arxiv.org/abs/2602.13091)
*Declan McIntosh,Alexandra Branzan Albu*

Main category: cs.CV

TL;DR: A dataset folding method transforms any one-class classifier anomaly detector into a fully unsupervised method by filtering training data using multiple independently trained classifiers, assuming anomalies are uncommon and heterogeneous.


<details>
  <summary>Details</summary>
Motivation: One-class classification anomaly detectors are vulnerable to training label noise since they assume training data contains only nominal values. Real-world datasets often contain some anomalies, making fully unsupervised methods needed.

Method: Dataset folding method that uses multiple independently trained instances of a one-class classifier to filter training data for anomalies. Makes weak assumptions: anomalies are uncommon in training data and generally heterogeneous. No modifications to underlying anomaly detector needed - only algorithmically selected data subsets for training.

Result: Method transforms various one-class classifier anomaly detectors for images and videos into unsupervised ones. Creates first unsupervised logical anomaly detectors. Achieves state-of-the-art performance on MVTec AD, ViSA, and MVTec Loco AD datasets.

Conclusion: The method links one-class classification and unsupervised anomaly detection domains, allowing improvements in one-class classifiers to directly transfer to unsupervised domain. Provides practical solution for real-world scenarios where training data may contain some anomalies.

Abstract: Detecting anomalies in images and video is an essential task for multiple real-world problems, including industrial inspection, computer-assisted diagnosis, and environmental monitoring. Anomaly detection is typically formulated as a one-class classification problem, where the training data consists solely of nominal values, leaving methods built on this assumption susceptible to training label noise. We present a dataset folding method that transforms an arbitrary one-class classifier-based anomaly detector into a fully unsupervised method. This is achieved by making a set of key weak assumptions: that anomalies are uncommon in the training dataset and generally heterogeneous. These assumptions enable us to utilize multiple independently trained instances of a one-class classifier to filter the training dataset for anomalies. This transformation requires no modifications to the underlying anomaly detector; the only changes are algorithmically selected data subsets used for training. We demonstrate that our method can transform a wide variety of one-class classifier anomaly detectors for both images and videos into unsupervised ones. Our method creates the first unsupervised logical anomaly detectors by transforming existing methods. We also demonstrate that our method achieves state-of-the-art performance for unsupervised anomaly detection on the MVTec AD, ViSA, and MVTec Loco AD datasets. As improvements to one-class classifiers are made, our method directly transfers those improvements to the unsupervised domain, linking the domains.

</details>


### [64] [Realistic Face Reconstruction from Facial Embeddings via Diffusion Models](https://arxiv.org/abs/2602.13168)
*Dong Han,Yong Li,Joachim Denzler*

Main category: cs.CV

TL;DR: A framework called Face Embedding Mapping (FEM) uses Kolmogorov-Arnold Networks (KAN) and diffusion models to reconstruct high-resolution face images from embeddings of FR and PPFR systems, demonstrating privacy risks.


<details>
  <summary>Details</summary>
Motivation: While privacy-preserving face recognition (PPFR) systems are gaining popularity for their enhanced privacy protection, there's limited research on verifying their actual privacy risks by reconstructing realistic faces from their embeddings.

Method: Proposes Face Embedding Mapping (FEM) - a general framework using Kolmogorov-Arnold Network (KAN) for embedding-to-face attacks, leveraging pre-trained Identity-Preserving diffusion models against state-of-the-art FR and PPFR systems.

Result: Reconstructed faces can access other real-world FR systems; method shows robustness in reconstructing faces from partial and protected embeddings; FEM serves as a tool for evaluating privacy leakage safety of FR/PPFR systems.

Conclusion: FEM demonstrates significant privacy risks in both FR and PPFR systems by successfully reconstructing high-resolution face images from embeddings, highlighting the need for better privacy protection evaluation methods.

Abstract: With the advancement of face recognition (FR) systems, privacy-preserving face recognition (PPFR) systems have gained popularity for their accurate recognition, enhanced facial privacy protection, and robustness to various attacks. However, there are limited studies to further verify privacy risks by reconstructing realistic high-resolution face images from embeddings of these systems, especially for PPFR. In this work, we propose the face embedding mapping (FEM), a general framework that explores Kolmogorov-Arnold Network (KAN) for conducting the embedding-to-face attack by leveraging pre-trained Identity-Preserving diffusion model against state-of-the-art (SOTA) FR and PPFR systems. Based on extensive experiments, we verify that reconstructed faces can be used for accessing other real-word FR systems. Besides, the proposed method shows the robustness in reconstructing faces from the partial and protected face embeddings. Moreover, FEM can be utilized as a tool for evaluating safety of FR and PPFR systems in terms of privacy leakage. All images used in this work are from public datasets.

</details>


### [65] [LongStream: Long-Sequence Streaming Autoregressive Visual Geometry](https://arxiv.org/abs/2602.13172)
*Chong Cheng,Xianda Chen,Tao Xie,Wei Yin,Weiqiang Ren,Qian Zhang,Xiaoyuang Guo,Hao Wang*

Main category: cs.CV

TL;DR: LongStream is a novel streaming 3D reconstruction model that achieves stable, metric-scale reconstruction over kilometer-scale sequences at 18 FPS by addressing key limitations of existing autoregressive approaches.


<details>
  <summary>Details</summary>
Motivation: Existing autoregressive models for streaming 3D reconstruction fail with long sequences due to attention decay, scale drift, and extrapolation errors caused by anchoring poses to the first frame.

Method: Three key innovations: 1) Predict keyframe-relative poses instead of first-frame anchoring to convert long-range extrapolation into local tasks; 2) Orthogonal scale learning to disentangle geometry from scale estimation; 3) Cache-consistent training with periodic cache refresh to address Transformer cache issues like attention-sink reliance and KV-cache contamination.

Result: State-of-the-art performance with stable, metric-scale reconstruction over kilometer-scale sequences at 18 FPS, significantly outperforming existing approaches.

Conclusion: LongStream successfully addresses fundamental limitations in long-sequence streaming 3D reconstruction through gauge-decoupled design, achieving practical real-time performance for large-scale scene reconstruction.

Abstract: Long-sequence streaming 3D reconstruction remains a significant open challenge. Existing autoregressive models often fail when processing long sequences. They typically anchor poses to the first frame, which leads to attention decay, scale drift, and extrapolation errors. We introduce LongStream, a novel gauge-decoupled streaming visual geometry model for metric-scale scene reconstruction across thousands of frames. Our approach is threefold. First, we discard the first-frame anchor and predict keyframe-relative poses. This reformulates long-range extrapolation into a constant-difficulty local task. Second, we introduce orthogonal scale learning. This method fully disentangles geometry from scale estimation to suppress drift. Finally, we solve Transformer cache issues such as attention-sink reliance and long-term KV-cache contamination. We propose cache-consistent training combined with periodic cache refresh. This approach suppresses attention degradation over ultra-long sequences and reduces the gap between training and inference. Experiments show LongStream achieves state-of-the-art performance. It delivers stable, metric-scale reconstruction over kilometer-scale sequences at 18 FPS. Project Page: https://3dagentworld.github.io/longstream/

</details>


### [66] [Monocular Markerless Motion Capture Enables Quantitative Assessment of Upper Extremity Reachable Workspace](https://arxiv.org/abs/2602.13176)
*Seth Donahue,J. D. Peiffer,R. Tyler Richardson,Yishan Zhong,Shaun Q. Y. Tan,Benoit Marteau,Stephanie R. Russo,May D. Wang,R. James Cotton,Ross Chafetz*

Main category: cs.CV

TL;DR: Validated monocular AI-driven markerless motion capture for upper extremity reachable workspace assessment, showing frontal camera view performs comparably to marker-based systems.


<details>
  <summary>Details</summary>
Motivation: To validate a clinically accessible approach for quantifying Upper Extremity Reachable Workspace using single-camera AI-driven markerless motion capture, reducing barriers to adoption in clinical motion analysis.

Method: Nine unimpaired adults performed standardized UERW task reaching targets in VR headset. Movements captured simultaneously with marker-based motion capture system and eight FLIR cameras. Monocular video analysis performed on frontal and offset camera views to compare with marker-based reference.

Result: Frontal camera orientation showed strong agreement with marker-based reference (mean bias 0.61±0.12% reachspace per octant). Offset camera view underestimated percent workspace reached (-5.66±0.45% reachspace). Highest agreement with marker-based system for anterior workspace evaluation.

Conclusion: Frontal monocular camera configuration is feasible for UERW assessment, particularly for anterior workspace. Approach enables broader implementation of quantitative upper extremity mobility assessment by reducing technical complexity. First validation of monocular markerless motion capture for UERW task.

Abstract: To validate a clinically accessible approach for quantifying the Upper Extremity Reachable Workspace (UERW) using a single (monocular) camera and Artificial Intelligence (AI)-driven Markerless Motion Capture (MMC) for biomechanical analysis. Objective assessment and validation of these techniques for specific clinically oriented tasks are crucial for their adoption in clinical motion analysis. AI-driven monocular MMC reduces the barriers to adoption in the clinic and has the potential to reduce the overhead for analysis of this common clinical assessment. Nine adult participants with no impairments performed the standardized UERW task, which entails reaching targets distributed across a virtual sphere centered on the torso, with targets displayed in a VR headset. Movements were simultaneously captured using a marker-based motion capture system and a set of eight FLIR cameras. We performed monocular video analysis on two of these video camera views to compare a frontal and offset camera configurations. The frontal camera orientation demonstrated strong agreement with the marker-based reference, exhibiting a minimal mean bias of $0.61 \pm 0.12$ \% reachspace reached per octanct (mean $\pm$ standard deviation). In contrast, the offset camera view underestimated the percent workspace reached ($-5.66 \pm 0.45$ \% reachspace reached). Conclusion: The findings support the feasibility of a frontal monocular camera configuration for UERW assessment, particularly for anterior workspace evaluation where agreement with marker-based motion capture was highest. The overall performance demonstrates clinical potential for practical, single-camera assessments. This study provides the first validation of monocular MMC system for the assessment of the UERW task. By reducing technical complexity, this approach enables broader implementation of quantitative upper extremity mobility assessment.

</details>


### [67] [FlexAM: Flexible Appearance-Motion Decomposition for Versatile Video Generation Control](https://arxiv.org/abs/2602.13185)
*Mingzhi Sheng,Zekai Gu,Peng Li,Cheng Lin,Hao-Xiang Guo,Ying-Cong Chen,Yuan Liu*

Main category: cs.CV

TL;DR: FlexAM is a video generation framework that uses a novel 3D point cloud control signal to disentangle appearance and motion, enabling diverse video editing tasks with superior performance.


<details>
  <summary>Details</summary>
Motivation: Current video generation methods often rely on ambiguous or task-specific control signals, lacking a fundamental disentanglement of appearance and motion which limits robustness and scalability.

Method: FlexAM introduces a unified framework with a novel 3D control signal representing video dynamics as a point cloud, featuring multi-frequency positional encoding for fine-grained motion, depth-aware encoding, and flexible control balancing precision and quality.

Result: Extensive experiments show FlexAM achieves superior performance across all evaluated tasks including I2V/V2V editing, camera control, and spatial object editing.

Conclusion: The 3D point cloud representation effectively disentangles appearance and motion, providing a robust and scalable pathway for generalizable video control that outperforms existing methods.

Abstract: Effective and generalizable control in video generation remains a significant challenge. While many methods rely on ambiguous or task-specific signals, we argue that a fundamental disentanglement of "appearance" and "motion" provides a more robust and scalable pathway. We propose FlexAM, a unified framework built upon a novel 3D control signal. This signal represents video dynamics as a point cloud, introducing three key enhancements: multi-frequency positional encoding to distinguish fine-grained motion, depth-aware positional encoding, and a flexible control signal for balancing precision and generative quality. This representation allows FlexAM to effectively disentangle appearance and motion, enabling a wide range of tasks including I2V/V2V editing, camera control, and spatial object editing. Extensive experiments demonstrate that FlexAM achieves superior performance across all evaluated tasks.

</details>


### [68] [CoPE-VideoLM: Codec Primitives For Efficient Video Language Models](https://arxiv.org/abs/2602.13191)
*Sayan Deb Sarkar,Rémi Pautrat,Ondrej Miksik,Marc Pollefeys,Iro Armeni,Mahdi Rad,Mihai Dusmanu*

Main category: cs.CV

TL;DR: VideoLM approach using video codec primitives (motion vectors & residuals) instead of full-frame images reduces computational overhead while maintaining or improving performance on video understanding tasks.


<details>
  <summary>Details</summary>
Motivation: Current VideoLMs use keyframe sampling which misses both macro-level events and micro-level details due to sparse temporal coverage, and processing full images for each frame incurs substantial computational overhead.

Method: Leverage video codec primitives (motion vectors and residuals) that natively encode video redundancy and sparsity. Introduce lightweight transformer-based encoders to aggregate codec primitives and align their representations with image encoder embeddings through pre-training strategy.

Result: Reduces time-to-first-token by up to 86% and token usage by up to 93% compared to standard VideoLMs. Maintains or exceeds performance on 14 diverse video understanding benchmarks spanning general QA, temporal reasoning, long-form understanding, and spatial scene understanding.

Conclusion: Using video codec primitives provides an efficient alternative to full-frame processing in VideoLMs, significantly reducing computational costs while preserving or enhancing video understanding capabilities across diverse tasks.

Abstract: Video Language Models (VideoLMs) empower AI systems to understand temporal dynamics in videos. To fit to the maximum context window constraint, current methods use keyframe sampling which can miss both macro-level events and micro-level details due to the sparse temporal coverage. Furthermore, processing full images and their tokens for each frame incurs substantial computational overhead. To address these limitations, we propose to leverage video codec primitives (specifically motion vectors and residuals) which natively encode video redundancy and sparsity without requiring expensive full-image encoding for most frames. To this end, we introduce lightweight transformer-based encoders that aggregate codec primitives and align their representations with image encoder embeddings through a pre-training strategy that accelerates convergence during end-to-end fine-tuning. Our approach reduces the time-to-first-token by up to $86\%$ and token usage by up to $93\%$ compared to standard VideoLMs. Moreover, by varying the keyframe and codec primitive densities we are able to maintain or exceed performance on $14$ diverse video understanding benchmarks spanning general question answering, temporal reasoning, long-form understanding, and spatial scene understanding.

</details>


### [69] [Conversational Image Segmentation: Grounding Abstract Concepts with Scalable Supervision](https://arxiv.org/abs/2602.13195)
*Aadarsh Sahoo,Georgia Gkioxari*

Main category: cs.CV

TL;DR: Introduces Conversational Image Segmentation (CIS) and ConverSeg benchmark for functional/physical reasoning in image segmentation, plus ConverSeg-Net model and AI data engine.


<details>
  <summary>Details</summary>
Motivation: Prior referring image grounding focuses on categorical/spatial queries but overlooks functional/physical reasoning (e.g., "where can I safely store the knife?"). Need to address this gap.

Method: Create ConverSeg benchmark spanning entities, spatial relations, intent, affordances, functions, safety, and physical reasoning. Develop ConverSeg-Net that fuses segmentation priors with language understanding. Use AI-powered data engine to generate prompt-mask pairs without human supervision.

Result: Current language-guided segmentation models are inadequate for CIS. ConverSeg-Net trained on AI-generated data achieves significant gains on ConverSeg and maintains strong performance on existing benchmarks.

Conclusion: Conversational Image Segmentation requires new benchmarks and models for functional/physical reasoning. ConverSeg and ConverSeg-Net address this need effectively.

Abstract: Conversational image segmentation grounds abstract, intent-driven concepts into pixel-accurate masks. Prior work on referring image grounding focuses on categorical and spatial queries (e.g., "left-most apple") and overlooks functional and physical reasoning (e.g., "where can I safely store the knife?"). We address this gap and introduce Conversational Image Segmentation (CIS) and ConverSeg, a benchmark spanning entities, spatial relations, intent, affordances, functions, safety, and physical reasoning. We also present ConverSeg-Net, which fuses strong segmentation priors with language understanding, and an AI-powered data engine that generates prompt-mask pairs without human supervision. We show that current language-guided segmentation models are inadequate for CIS, while ConverSeg-Net trained on our data engine achieves significant gains on ConverSeg and maintains strong performance on existing language-guided segmentation benchmarks. Project webpage: https://glab-caltech.github.io/converseg/

</details>


<div id='cs.IT'></div>

# cs.IT [[Back]](#toc)

### [70] [Channel Gain Map Reconstruction Based on Virtual Scatterer Model](https://arxiv.org/abs/2602.12602)
*He Sun,Lipeng Zhu,Jie Xu,Rui Zhang*

Main category: cs.IT

TL;DR: Proposes virtual scatterer model for 3D channel gain map reconstruction using limited measurements, with progressive estimation and GPR-based inference for improved accuracy.


<details>
  <summary>Details</summary>
Motivation: Need efficient and accurate channel gain map reconstruction from limited measurements by capturing multi-path propagation structure and exploiting spatial correlation.

Method: Virtual scatterer model representing CGM as function of tunable parameters (number, positions, SRCs), progressive estimation algorithm balancing complexity/accuracy, and GPR-based inference for unmeasured SRCs.

Result: Ray-tracing simulations show higher reconstruction accuracy than conventional approaches, validating effectiveness in realistic physical environments.

Conclusion: Proposed method offers flexible, scalable framework for efficient CGM reconstruction using virtual scatterers with progressive parameter estimation and spatial correlation exploitation.

Abstract: This paper proposes an efficient method for modeling and reconstructing the channel gain map (CGM) based on virtual scatterers. Specifically, we develop a virtual scatterer model to characterize the channel power gain distribution in three-dimensional (3D) space, by capturing the multi-path propagation environment structure and exploiting the angular-domain spatial correlation of scatterer response. In this model, the CGM is represented as a function over a set of tunable parameters for virtual scatterers, including their number, positions, and scatterer response coefficients (SRCs), which can be estimated from a limited number of channel power gain measurements at a given set of locations within the region of interest. This new representation offers a flexible and scalable modeling framework for efficient and accurate CGM reconstruction. Furthermore, we propose a progressive estimation algorithm to acquire the scatterers' parameters. In this algorithm, we gradually increase the number of virtual scatterers to balance the computational complexity and estimation accuracy. In addition, by exploiting the spatial correlation of scatterer response, we propose a Gaussian process regression (GPR)-based inference method to predict the SRCs that cannot be directly estimated. Finally, ray-tracing-based simulation results under realistic physical environments validate the effectiveness of the proposed method, demonstrating that it achieves higher reconstruction accuracy compared to conventional CGM estimation approaches.

</details>


### [71] [Secure Beamforming for ISAC Systems Under Communication Eavesdropper and Sensing Eavesdropper](https://arxiv.org/abs/2602.12614)
*Tian Zhang,Zhirong Su,Yueyi Dong*

Main category: cs.IT

TL;DR: Secure beamforming design for ISAC systems against both communication and sensing eavesdroppers to maximize secrecy rate while maintaining sensing performance and power constraints.


<details>
  <summary>Details</summary>
Motivation: ISAC systems face security threats from both communication and sensing eavesdroppers, requiring joint secure beamforming design to protect both communication secrecy and sensing security while maintaining system performance.

Method: Formulate non-convex secrecy rate maximization problem with sensing security constraints, then solve using SCA with first-order Taylor expansion and SDR techniques, proving SDR optimality and proposing iterative joint secure beamforming algorithm.

Result: Proposed algorithm effectively maximizes secrecy rate against dual eavesdroppers while maintaining sensing performance, with theoretical validation of SDR optimality and simulation verification of scheme effectiveness.

Conclusion: The proposed joint secure beamforming scheme successfully addresses ISAC security challenges against both communication and sensing eavesdroppers, providing an effective solution with proven optimality and practical performance.

Abstract: Due to great efficiency improvement in resource and hardware space, integrated sensing and communication (ISAC) has gained much attention. In the paper, the physical layer security (PLS) of ISAC system under communication eavesdropper together with sensing eavesdropper is investigated. The system secrecy rate is maximized by transmit beamforming design of communication and sensing signals when taking sensing security, sensing performance and transmit power constraint into consideration. To deal with the formulated non-convex optimization problem, the successive convex approximation (SCA) together with the first-order Taylor expansion and semidefinite relaxation (SDR) is utilized. Additionally, it is theoretically validated that the SDR does not yield sub-optimality in the paper. Thereafter, an iterated joint secure beamforming algorithm against communication and sensing eavesdroppers is proposed. Simulation results validate the effectiveness and advance of the proposed scheme.

</details>


### [72] [Secrecy Capacity Analysis and Beamforming Optimization for MIMO-VLC Wiretap Channels](https://arxiv.org/abs/2602.12720)
*Sufang Yang,Longguang Li,Jintao Wang,Ya Li,Liang Xia,Hongjun He,Qixing Wang,Guangyi Liu*

Main category: cs.IT

TL;DR: This paper develops novel beamforming schemes for MIMO visible light communication wiretap channels to enhance secrecy rates under optical intensity constraints.


<details>
  <summary>Details</summary>
Motivation: The paper addresses the need for secure communication in MIMO VLC systems where optical signals are subject to both peak and average-intensity constraints, and there exists an eavesdropper trying to intercept transmissions.

Method: The authors apply the generalized entropy-power inequality to truncated exponential inputs to derive closed-form secrecy rate expressions, then propose fully-connected and low-complexity sub-connected beamforming schemes. Nonconvex beamforming design problems are transformed into convex subproblems using successive convex approximation.

Result: Numerical results show that the proposed beamforming schemes achieve significant secrecy performance improvements compared to benchmark schemes, demonstrating the effectiveness of the approach.

Conclusion: The paper successfully develops practical beamforming solutions for secure MIMO VLC communications that effectively balance performance and complexity while addressing the unique optical intensity constraints of VLC systems.

Abstract: This paper investigates a multiple-input multipleoutput (MIMO) visible light communication (VLC) wiretap channel consisting of a transmitter, a legitimate receiver, and an eavesdropper. The optical input is subject to both peakand average-intensity constraints. By applying the generalized entropy-power inequality to truncated exponential inputs, we derive a novel closed-form expression for the achievable secrecy rate for general MIMO VLC configurations. To enhance transmission confidentiality, a fully-connected beamforming scheme is proposed, along with a low-complexity sub-connected alternative. Although the resulting beamforming design problems are nonconvex, they are efficiently addressed by transforming them into a sequence of convex subproblems solvable via the successive convex approximation framework. Numerical results demonstrate that the proposed schemes achieve significant secrecy performance improvements compared with the benchmark scheme.

</details>


### [73] [Construction of MRD Codes Based on Circular-Shift Operations](https://arxiv.org/abs/2602.12766)
*Zhe Zhai,Sheng Jin,Qifu Tyler Sun,Zongpeng Li*

Main category: cs.IT

TL;DR: The paper presents a new construction of maximum rank distance (MRD) codes using circular-shift operations over $\mathbb{F}_q$, avoiding complex arithmetic of extension fields, with computational efficiency advantages over traditional Gabidulin codes.


<details>
  <summary>Details</summary>
Motivation: Traditional MRD code constructions rely on arithmetic of extension fields $\mathbb{F}_{q^N}$, whose complexity grows with larger $N$, hindering practical implementation and parameter selection. There's a need for more efficient constructions that avoid extension field arithmetic.

Method: Based on circular-shift operations, the authors construct $(J \times n, q^{Jk}, d)$ MRD codes entirely over $\mathbb{F}_q$, where $J$ equals Euler's totient function of $L$ with $\gcd(q, L) = 1$. They characterize these codes using $q$-linearized polynomials over $\mathbb{F}_q^N$ and compare them with Gabidulin and twisted Gabidulin codes.

Result: When $J \neq m_L$, the proposed MRD codes are different from any Gabidulin or twisted Gabidulin code. When $J = m_L$, they coincide with Gabidulin codes, providing an equivalent circular-shift-based construction over $\mathbb{F}_q$. For $q=2$, $L$ prime, and $n \leq m_L$, generating codewords requires $O(nkL)$ XOR operations vs $O(nkL^2)$ for traditional Gabidulin codes.

Conclusion: The circular-shift-based construction provides efficient MRD codes that avoid extension field arithmetic, offering computational advantages over traditional constructions while maintaining the same code parameters and distance properties.

Abstract: Most well-known constructions of $(N \times n, q^{Nk}, d)$ maximum rank distance (MRD) codes rely on the arithmetic of $\mathbb{F}_{q^N}$, whose increasing complexity with larger $N$ hinders parameter selection and practical implementation. In this work, based on circular-shift operations, we present a construction of $(J \times n, q^{Jk}, d)$ MRD codes with efficient encoding, where $J$ equals to the Euler's totient function of a defined $L$ subject to $\gcd(q, L) = 1$. The proposed construction is performed entirely over $\mathbb{F}_q$ and avoids the arithmetic of $\mathbb{F}_{q^J}$. We further characterize the constructed MRD codes, Gabidulin codes and twisted Gabidulin codes using a set of $q$-linearized polynomials over the row vector space $\mathbb{F}_{q}^N$, and clarify their inherent difference and connection. For the case $J \neq m_L$, where $m_L$ denotes the multiplicative order of $q$ modulo $L$, we show that the proposed MRD codes, in a family of settings, are different from any Gabidulin code and any twisted Gabidulin code. For the case $J = m_L$, we prove that every constructed $(J \times n, q^{Jk}, d)$ MRD code coincides with a $(J \times n, q^{Jk}, d)$ Gabidulin code, yielding an equivalent circular-shift-based construction that operates directly over $\mathbb{F}_q$. In addition, we prove that under some parameter settings, the constructed MRD codes are equivalent to a generalization of Gabidulin codes obtained by summing and concatenating several $(m_L \times n, q^{m_Lk}, d)$ Gabidulin codes. When $q=2$, $L$ is prime and $n\leq m_L$, it is analyzed that generating a codeword of the proposed $((L-1) \times n, 2^{(L-1)k}, d)$ MRD codes requires $O(nkL)$ exclusive OR (XOR) operations, while generating a codeword of $((L-1) \times n, 2^{(L-1)k}, d)$ Gabidulin codes, based on customary construction, requires $O(nkL^2)$ XOR operations.

</details>


### [74] [FPNet: Joint Wi-Fi Beamforming Matrix Feedback and Anomaly-Aware Indoor Positioning](https://arxiv.org/abs/2602.12799)
*Ran Tao,Jiajia Guo,Yiming Cui,Xiangyi Li,Chao-Kai Wen,Shi Jin*

Main category: cs.IT

TL;DR: FPNet is a deep learning framework that uses compressed beamforming feedback for efficient indoor positioning with anomaly detection, achieving high accuracy with minimal feedback overhead.


<details>
  <summary>Details</summary>
Motivation: Real-world Wi-Fi sensing faces challenges: complete CSI is rarely available due to hardware constraints and high communication overhead, and existing positioning models lack mechanisms to detect when users move outside trained regions, leading to unreliable estimates in dynamic environments.

Method: FPNet leverages the beamforming feedback matrix (BFM), a compressed CSI representation natively supported by IEEE 802.11ac/ax/be protocols, to minimize feedback overhead while preserving positioning features. It integrates ADBlock, a lightweight anomaly detection module trained on normal BFM samples, to identify out-of-distribution scenarios when users exit predefined spatial regions.

Result: Using standard 2.4 GHz Wi-Fi hardware, FPNet achieves positioning accuracy above 97% with only 100 feedback bits, boosts net throughput by up to 22.92%, and attains anomaly detection accuracy over 99% with a false alarm rate below 1.5%.

Conclusion: FPNet demonstrates the ability to deliver efficient, accurate, and reliable indoor positioning on commodity Wi-Fi devices by jointly addressing channel feedback compression, accurate positioning, and robust anomaly detection in a unified framework.

Abstract: Channel State Information (CSI) provides a detailed description of the wireless channel and has been widely adopted for Wi-Fi sensing, particularly for high-precision indoor positioning. However, complete CSI is rarely available in real-world deployments due to hardware constraints and the high communication overhead required for feedback. Moreover, existing positioning models lack mechanisms to detect when users move outside their trained regions, leading to unreliable estimates in dynamic environments. In this paper, we present FPNet, a unified deep learning framework that jointly addresses channel feedback compression, accurate indoor positioning, and robust anomaly detection (AD). FPNet leverages the beamforming feedback matrix (BFM), a compressed CSI representation natively supported by IEEE 802.11ac/ax/be protocols, to minimize feedback overhead while preserving critical positioning features. To enhance reliability, we integrate ADBlock, a lightweight AD module trained on normal BFM samples, which identifies out-of-distribution scenarios when users exit predefined spatial regions. Experimental results using standard 2.4 GHz Wi-Fi hardware show that FPNet achieves positioning accuracy above 97% with only 100 feedback bits, boosts net throughput by up to 22.92%, and attains AD accuracy over 99% with a false alarm rate below 1.5%. These results demonstrate FPNet's ability to deliver efficient, accurate, and reliable indoor positioning on commodity Wi-Fi devices.

</details>


### [75] [Concatenated Codes for Short-Molecule DNA Storage with Sequencing Channels of Positive Zero-Undetected-Error Capacity](https://arxiv.org/abs/2602.12800)
*Ran Tamir,Nir Weinberger,Albert Guillén i Fàbregas*

Main category: cs.IT

TL;DR: Concatenated coding scheme for DNA storage with noisy sequencing: outer code handles random sampling, inner code handles sequencing noise using linear block codes with zero-undetected-error decoding.


<details>
  <summary>Details</summary>
Motivation: To determine reliable information capacity in DNA-based storage systems with noisy sequencing, where codewords are composed of short DNA molecules and sequencing introduces errors.

Method: Concatenated coding with outer code for random sampling and inner code for sequencing noise. Inner code uses linear block codes with zero-undetected-error decoder. Symmetric sequencing channel assumption enables maximum-likelihood decoder analysis.

Result: Derived achievability bound for scaling of reliably stored information bits. Proved exponential convergence to zero error probability for random linear block codes under zero-undetected-error decoding when rate doesn't exceed critical value (lower bound to zero-undetected-error capacity).

Conclusion: Concatenated coding with appropriate inner/outer codes provides reliable DNA storage capacity bounds. Zero-undetected-error decoding of linear codes achieves exponential error reduction, establishing fundamental limits for DNA storage systems.

Abstract: We study the amount of reliable information that can be stored in a DNA-based storage system with noisy sequencing, where each codeword is composed of short DNA molecules. We analyze a concatenated coding scheme, where the outer code is designed to handle the random sampling, while the inner code is designed to handle the random sequencing noise. We assume that the sequencing channel is symmetric and choose the inner coding scheme to be composed by a linear block code and a zero-undetected-error decoder. As a byproduct, the resulting optimal maximum-likelihood decoder land itself for an amenable analysis, and we are able to derive an achievability bound for the scaling of the number of information bits that can be reliably stored. As a result of independent interest, we prove that the average error probability of random linear block codes under zero-undetected-error decoding converges to zero exponentially fast with the block length, as long as its coding rate does not exceed some critical value, which is known to serve as a lower bound to the zero-undetected-error capacity.

</details>


### [76] [EARL: Energy-Aware Adaptive Antenna Control with Reinforcement Learning in O-RAN Cell-Free Massive MIMO Networks](https://arxiv.org/abs/2602.12841)
*Zilin Ge,Ozan Alp Topal,Irshad Ahmad Meer,Pei Xiao,Cicek Cavdar*

Main category: cs.IT

TL;DR: EARL: RL-based energy-aware adaptive antenna control for cell-free massive MIMO that dynamically configures antenna elements to save power while meeting user demands.


<details>
  <summary>Details</summary>
Motivation: Cell-free massive MIMO provides uniform high performance but consumes high energy due to distributed transmission and centralized processing. Need to reduce energy consumption while maintaining performance.

Method: Propose EARL framework using reinforcement learning to dynamically configure antenna elements in radio units, minimizing radio, optical fronthaul, and cloud processing power consumption while meeting spectral efficiency demands.

Result: Achieves up to 81% power savings over full-on baseline and 50% over heuristic baselines. RL approach operates within 220 ms (meeting O-RAN near-real-time limit). Greedy refinement further halves power consumption at 2s runtime.

Conclusion: EARL effectively reduces energy consumption in cell-free massive MIMO systems while meeting performance requirements, demonstrating feasibility of RL-based energy optimization within O-RAN constraints.

Abstract: Cell-free massive multi-input multi-output (MIMO) promises uniform high performance across the network, but also brings a high energy cost due to joint transmission from distributed radio units (RUs) and centralized processing in the cloud. Leveraging the resource-sharing capabilities of Open Radio Access Network (O-RAN), we propose EARL, an energy-aware adaptive antenna control framework based on reinforcement learning. EARL dynamically configures antenna elements in RUs to minimize radio, optical fronthaul, and cloud processing power consumption while meeting user spectral efficiency demands. Numerical results show power savings of up to 81% and 50% over full-on and heuristic baselines, respectively. The RL-based approach operates within 220 ms, satisfying O-RAN's near-real-time limit, and a greedy refinement further halves power consumption at a 2 s runtime.

</details>


### [77] [Model-Aware Rate-Distortion Limits for Task-Oriented Source Coding](https://arxiv.org/abs/2602.12866)
*Andriy Enttsel,Vincent Corlay*

Main category: cs.IT

TL;DR: The paper revisits fundamental limits of Task-Oriented Source Coding (TOSC) for machine inference systems, showing existing rate-distortion bounds are often unrealistic and proposing new bounds that account for task model constraints.


<details>
  <summary>Details</summary>
Motivation: Existing rate-distortion bounds for coding for machines rely on strong assumptions about task identifiability and neglect the impact of deployed task models, making them unrealistic for practical applications.

Method: Revisits single-TOSC through indirect rate-distortion theory, analyzes conditions for existing bounds' achievability, and introduces new task model-aware rate-distortion bounds that account for model suboptimality and architectural constraints.

Result: Experiments on standard classification benchmarks show current learned TOSC schemes operate far from the proposed fundamental limits, with transmitter-side complexity identified as a key bottleneck.

Conclusion: The paper provides more realistic fundamental limits for TOSC by accounting for practical task model constraints, revealing significant gaps between current implementations and theoretical bounds, highlighting transmitter complexity as a major challenge.

Abstract: Task-Oriented Source Coding (TOSC) has emerged as a paradigm for efficient visual data communication in machine-centric inference systems, where bitrate, latency, and task performance must be jointly optimized under resource constraints. While recent works have proposed rate-distortion bounds for coding for machines, these results often rely on strong assumptions on task identifiability and neglect the impact of deployed task models. In this work, we revisit the fundamental limits of single-TOSC through the lens of indirect rate-distortion theory. We highlight the conditions under which existing rate-distortion bounds are achievable and show their limitations in realistic settings. We then introduce task model-aware rate-distortion bounds that account for task model suboptimality and architectural constraints. Experiments on standard classification benchmarks confirm that current learned TOSC schemes operate far from these limits, highlighting transmitter-side complexity as a key bottleneck.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [78] [OptiML: An End-to-End Framework for Program Synthesis and CUDA Kernel Optimization](https://arxiv.org/abs/2602.12305)
*Arijit Bhattacharjee,Heng Ping,Son Vu Le,Paul Bogdan,Nesreen K. Ahmed,Ali Jannesari*

Main category: cs.LG

TL;DR: OptiML is an end-to-end framework that optimizes CUDA kernels through search under verification, using LLM-driven transformations guided by hardware profiling feedback.


<details>
  <summary>Details</summary>
Motivation: Generating high-performance CUDA kernels is challenging due to combinatorial optimization spaces and noisy hardware feedback. While LLMs can produce functionally correct code, achieving competitive performance requires systematic exploration and verification of optimization choices.

Method: Two-stage framework: 1) OptiML-G generates initial kernels from natural language using Mixture-of-Thoughts, 2) OptiML-X refines kernels via Monte Carlo Tree Search over LLM-driven edits, with compilation, verification, and profiling using Nsight Compute. Uses hardware-aware reward combining runtime, bottleneck proxies, and regression guardrails.

Result: OptiML consistently discovers verified performance improvements over strong LLM baselines and produces interpretable optimization trajectories grounded in profiler evidence across diverse CUDA kernels.

Conclusion: OptiML successfully addresses CUDA kernel optimization by combining LLM-driven synthesis with systematic search under verification, producing performance-optimized kernels with interpretable optimization paths.

Abstract: Generating high-performance CUDA kernels remains challenging due to the need to navigate a combinatorial space of low-level transformations under noisy and expensive hardware feedback. Although large language models can synthesize functionally correct CUDA code, achieving competitive performance requires systematic exploration and verification of optimization choices. We present OptiML, an end-to-end framework that maps either natural-language intent or input CUDA code to performance-optimized CUDA kernels by formulating kernel optimization as search under verification. OptiML consists of two decoupled stages. When the input is natural language, a Mixture-of-Thoughts generator (OptiML-G) acts as a proposal policy over kernel implementation strategies, producing an initial executable program. A search-based optimizer (OptiML-X) then refines either synthesized or user-provided kernels using Monte Carlo Tree Search over LLM-driven edits, guided by a hardware-aware reward derived from profiler feedback. Each candidate transformation is compiled, verified, and profiled with Nsight Compute, and evaluated by a composite objective that combines runtime with hardware bottleneck proxies and guardrails against regressions. We evaluate OptiML in both synthesis-and-optimize and optimization-only settings on a diverse suite of CUDA kernels. Results show that OptiML consistently discovers verified performance improvements over strong LLM baselines and produces interpretable optimization trajectories grounded in profiler evidence.

</details>


### [79] [Abstractive Red-Teaming of Language Model Character](https://arxiv.org/abs/2602.12318)
*Nate Rahn,Allison Qi,Avery Griffin,Jonathan Michala,Henry Sleight,Erik Jones*

Main category: cs.LG

TL;DR: Abstractive red-teaming finds query categories that cause language models to violate character specifications, using efficient search algorithms to identify problematic patterns before deployment.


<details>
  <summary>Details</summary>
Motivation: Language models can violate character specifications in large-scale deployments, but identifying problematic queries requires extensive compute. The paper aims to find query categories that elicit violations using much less than deployment-level resources.

Method: Two algorithms for abstractive red-teaming: 1) RL-based training of a category generator LLM, and 2) iterative synthesis of categories from high-scoring queries using a strong LLM. Both search for natural-language query categories that consistently cause violations.

Result: The algorithms outperform baselines across 12 principles and 7 target models. They discover interesting violation patterns: Llama-3.1-8B-Instruct predicts AI domination when asked about the future, and GPT-4.1-Mini recommends illegal weapons for prison survival.

Conclusion: Abstractive red-teaming enables realistic pre-deployment auditing of language model character by efficiently identifying problematic query categories that cause specification violations.

Abstract: We want language model assistants to conform to a character specification, which asserts how the model should act across diverse user interactions. While models typically follow these character specifications, they can occasionally violate them in large-scale deployments. In this work, we aim to identify types of queries that are likely to produce such character violations at deployment, using much less than deployment-level compute. To do this, we introduce abstractive red-teaming, where we search for natural-language query categories, e.g. "The query is in Chinese. The query asks about family roles," that routinely elicit violations. These categories abstract over the many possible variants of a query which could appear in the wild. We introduce two algorithms for efficient category search against a character-trait-specific reward model: one based on reinforcement learning on a category generator LLM, and another which leverages a strong LLM to iteratively synthesize categories from high-scoring queries. Across a 12-principle character specification and 7 target models, we find that our algorithms consistently outperform baselines, and generate qualitatively interesting categories; for example, queries which ask Llama-3.1-8B-Instruct to predict the future lead to responses saying that AI will dominate humanity, and queries that ask GPT-4.1-Mini for essential prison survival items lead to enthusiastic recommendation of illegal weapons. Overall, we believe our results represent an important step towards realistic pre-deployment auditing of language model character.

</details>


### [80] [The Appeal and Reality of Recycling LoRAs with Adaptive Merging](https://arxiv.org/abs/2602.12323)
*Haokun Liu,Gyung Hyun Je,Marco Ciccone,Zhenlin Xu,Prasanth YSS,Colin Raffel*

Main category: cs.LG

TL;DR: Adaptive merging of recycled LoRAs from public repositories provides limited benefits over training new LoRAs, with performance gains likely due to regularization rather than cross-task transfer.


<details>
  <summary>Details</summary>
Motivation: Despite widespread availability of fine-tuned LoRA modules on platforms like Hugging Face Hub, no previous work has systematically studied recycling these "in-the-wild" LoRAs for adaptive merging to improve model performance.

Method: Empirical study using nearly 1,000 user-contributed LoRAs trained from Llama 3.1 8B-Instruct, evaluating various adaptive and non-adaptive merging methods, plus a new method designed via extensive search over methodological design space.

Result: Adaptive merging improves over base model but offers limited advantage over training new LoRAs; LoRA selection matters little, and even randomly initialized LoRAs yield similar performance, suggesting gains come from regularization rather than cross-task transfer.

Conclusion: Recycling LoRAs from public repositories provides limited practical benefit, with performance improvements likely attributable to regularization effects rather than meaningful knowledge transfer, though positive transfer is possible when highly relevant LoRAs are available.

Abstract: The widespread availability of fine-tuned LoRA modules for open pre-trained models has led to an interest in methods that can adaptively merge LoRAs to improve performance. These methods typically include some way of selecting LoRAs from a pool and tune merging coefficients based on a task-specific dataset. While adaptive merging methods have demonstrated improvements in some settings, no past work has attempted to recycle LoRAs found "in the wild" on model repositories like the Hugging Face Hub. To address this gap, we consider recycling from a pool of nearly 1,000 user-contributed LoRAs trained from the Llama 3.1 8B-Instruct language model. Our empirical study includes a range of adaptive and non-adaptive merging methods in addition to a new method designed via a wide search over the methodological design space. We demonstrate that adaptive merging methods can improve performance over the base model but provide limited benefit over training a new LoRA on the same data used to set merging coefficients. We additionally find not only that the specific choice of LoRAs to merge has little importance, but that using LoRAs with randomly initialized parameter values yields similar performance. This raises the possibility that adaptive merging from recycled LoRAs primarily works via some kind of regularization effect, rather than by enabling positive cross-task transfer. To better understand why past work has proven successful, we confirm that positive transfer is indeed possible when there are highly relevant LoRAs in the pool. We release the model checkpoints and code online.

</details>


### [81] [Wireless TokenCom: RL-Based Tokenizer Agreement for Multi-User Wireless Token Communications](https://arxiv.org/abs/2602.12338)
*Farshad Zeinali,Mahdi Boloursaz Mashhadi,Dusit Niyato,Rahim Tafazolli*

Main category: cs.LG

TL;DR: A hybrid RL framework for joint tokenizer agreement, sub-channel assignment, and beamforming in multi-user downlink wireless TokenCom systems improves semantic quality, resource efficiency, and reduces video freezing by 68% compared to H.265.


<details>
  <summary>Details</summary>
Motivation: Token Communications (TokenCom) requires transmitters and receivers to agree on identical tokenizer models and codebooks for establishing a shared semantic latent space. The Tokenizer Agreement (TA) process in multi-user downlink scenarios needs efficient optimization of tokenizer selection, sub-channel assignment, and beamforming to enable effective semantic- and goal-oriented communications.

Method: Proposes a hybrid reinforcement learning framework that integrates: 1) Deep Q-Network (DQN) for joint tokenizer agreement and sub-channel assignment, and 2) Deep Deterministic Policy Gradient (DDPG) for beamforming optimization. This approach solves the mixed-integer non-convex problem in multi-user downlink wireless TokenCom with multiple antennas at the base station.

Result: The proposed framework outperforms baseline methods in terms of semantic quality and resource efficiency. It reduces video freezing events in transmission by 68% compared to conventional H.265-based schemes, demonstrating significant improvements in video streaming performance.

Conclusion: The hybrid RL framework effectively solves the complex joint optimization problem in multi-user TokenCom systems, enabling efficient tokenizer agreement while optimizing communication resources. This approach advances the practical implementation of TokenCom in future wireless networks by addressing key challenges in multi-user scenarios.

Abstract: Token Communications (TokenCom) has recently emerged as an effective new paradigm, where tokens are the unified units of multimodal communications and computations, enabling efficient digital semantic- and goal-oriented communications in future wireless networks. To establish a shared semantic latent space, the transmitters/receivers in TokenCom need to agree on an identical tokenizer model and codebook. To this end, an initial Tokenizer Agreement (TA) process is carried out in each communication episode, where the transmitter/receiver cooperate to choose from a set of pre-trained tokenizer models/ codebooks available to them both for efficient TokenCom. In this correspondence, we investigate TA in a multi-user downlink wireless TokenCom scenario, where the base station equipped with multiple antennas transmits video token streams to multiple users. We formulate the corresponding mixed-integer non-convex problem, and propose a hybrid reinforcement learning (RL) framework that integrates a deep Q-network (DQN) for joint tokenizer agreement and sub-channel assignment, with a deep deterministic policy gradient (DDPG) for beamforming. Simulation results show that the proposed framework outperforms baseline methods in terms of semantic quality and resource efficiency, while reducing the freezing events in video transmission by 68% compared to the conventional H.265-based scheme.

</details>


### [82] [Intrinsic Credit Assignment for Long Horizon Interaction](https://arxiv.org/abs/2602.12342)
*Ilze Amanda Auzina,Joschka Strüber,Sergio Hernández-Gutiérrez,Shashwat Goel,Ameya Prabhu,Matthias Bethge*

Main category: cs.LG

TL;DR: ΔBelief-RL uses language model's belief changes to reward intermediate progress for long-horizon uncertainty navigation, outperforming outcome-based rewards.


<details>
  <summary>Details</summary>
Motivation: To train agents that can effectively navigate uncertainty over long horizons by addressing credit assignment challenges for intermediate actions.

Method: Uses change in probability that agent assigns to target solution (ΔBelief) as intrinsic reward for credit assignment. Trains on synthetic interaction data to develop information-seeking capabilities.

Result: Consistently outperforms purely outcome-based rewards, generalizes to out-of-distribution applications (customer service, personalization). Performance improves with scaled test-time interactions beyond training horizon, with increasing interaction efficiency on Pass@k metrics.

Conclusion: Introduces scalable training strategy for long-horizon uncertainty navigation by enabling credit assignment to intermediate actions via intrinsic ΔBelief rewards.

Abstract: How can we train agents to navigate uncertainty over long horizons? In this work, we propose ΔBelief-RL, which leverages a language model's own intrinsic beliefs to reward intermediate progress. Our method utilizes the change in the probability an agent assigns to the target solution for credit assignment. By training on synthetic interaction data, ΔBelief-RL teaches information-seeking capabilities that consistently outperform purely outcome-based rewards for Reinforcement Learning, with improvements generalizing to out-of-distribution applications ranging from customer service to personalization. Notably, the performance continues to improve as we scale test-time interactions beyond the training horizon, with interaction-efficiency increasing even on Pass@k metrics. Overall, our work introduces a scalable training strategy for navigating uncertainty over a long-horizon, by enabling credit assignment to intermediate actions via intrinsic ΔBelief rewards.

</details>


### [83] [A Machine Learning Approach to the Nirenberg Problem](https://arxiv.org/abs/2602.12368)
*Gianfranco Cortés,Maria Esteban-Casadevall,Yueqing Feng,Jonas Henkel,Edward Hirst,Tancredi Schettini Gherardini,Alexander G. Stapleton*

Main category: cs.LG

TL;DR: A neural network approach (Nirenberg Neural Network) solves the Nirenberg problem of prescribing Gaussian curvature on S² using physics-informed neural networks to parametrize conformal factors globally.


<details>
  <summary>Details</summary>
Motivation: To provide a computational approach to the longstanding geometric analysis problem of prescribing Gaussian curvature on the 2-sphere for metrics conformal to the round metric, using neural networks as exploratory tools.

Method: Mesh-free physics-informed neural network (PINN) that directly parametrizes the conformal factor globally, trained with geometry-aware loss enforcing the curvature equation, with additional validation via Gauss-Bonnet theorem and spherical-harmonic expansions.

Result: For realizable curvatures, the network achieves very low losses (10⁻⁷ - 10⁻¹⁰), while unrealizable curvatures yield significantly higher losses, enabling assessment of unknown cases and separation of realizable from non-realisable functions.

Conclusion: Neural solvers can serve as exploratory tools in geometric analysis, offering quantitative computational perspectives on longstanding existence questions, as demonstrated by the Nirenberg Neural Network's capabilities.

Abstract: This work introduces the Nirenberg Neural Network: a numerical approach to the Nirenberg problem of prescribing Gaussian curvature on $S^2$ for metrics that are pointwise conformal to the round metric. Our mesh-free physics-informed neural network (PINN) approach directly parametrises the conformal factor globally and is trained with a geometry-aware loss enforcing the curvature equation. Additional consistency checks were performed via the Gauss-Bonnet theorem, and spherical-harmonic expansions were fit to the learnt models to provide interpretability.
  For prescribed curvatures with known realisability, the neural network achieves very low losses ($10^{-7} - 10^{-10}$), while unrealisable curvatures yield significantly higher losses. This distinction enables the assessment of unknown cases, separating likely realisable functions from non-realisable ones. The current capabilities of the Nirenberg Neural Network demonstrate that neural solvers can serve as exploratory tools in geometric analysis, offering a quantitative computational perspective on longstanding existence questions.

</details>


### [84] [Block-Sample MAC-Bayes Generalization Bounds](https://arxiv.org/abs/2602.12605)
*Matthias Frey,Jingge Zhu,Michael C. Gastpar*

Main category: cs.LG

TL;DR: The paper introduces MAC-Bayes bounds that bound expected generalization error instead of high-probability bounds, using data subsets to improve tightness over traditional PAC-Bayes bounds.


<details>
  <summary>Details</summary>
Motivation: Traditional PAC-Bayes bounds provide high-probability guarantees but can be loose or vacuous. The authors aim to develop tighter bounds for expected generalization error by exploiting structure in the training data.

Method: Proposes a family of MAC-Bayes bounds that generalize expectation versions of PAC-Bayes bounds. Key innovation: uses subsets/blocks of training data to create divergence terms that depend only on data blocks rather than entire dataset.

Result: The new bounds can be significantly tighter than traditional PAC-Bayes bounds - demonstrated with numerical example where original PAC-Bayes bound is vacuous but proposed bounds are finite. Also proves impossibility of high-probability versions with same fast convergence rates.

Conclusion: MAC-Bayes bounds offer tighter expected generalization error bounds by leveraging data block structure, but cannot be converted to high-probability PAC-Bayes bounds with similar fast convergence rates while maintaining logarithmic dependence on error probability.

Abstract: We present a family of novel block-sample MAC-Bayes bounds (mean approximately correct). While PAC-Bayes bounds (probably approximately correct) typically give bounds for the generalization error that hold with high probability, MAC-Bayes bounds have a similar form but bound the expected generalization error instead. The family of bounds we propose can be understood as a generalization of an expectation version of known PAC-Bayes bounds. Compared to standard PAC-Bayes bounds, the new bounds contain divergence terms that only depend on subsets (or \emph{blocks}) of the training data. The proposed MAC-Bayes bounds hold the promise of significantly improving upon the tightness of traditional PAC-Bayes and MAC-Bayes bounds. This is illustrated with a simple numerical example in which the original PAC-Bayes bound is vacuous regardless of the choice of prior, while the proposed family of bounds are finite for appropriate choices of the block size. We also explore the question whether high-probability versions of our MAC-Bayes bounds (i.e., PAC-Bayes bounds of a similar form) are possible. We answer this question in the negative with an example that shows that in general, it is not possible to establish a PAC-Bayes bound which (a) vanishes with a rate faster than $\mathcal{O}(1/\log n)$ whenever the proposed MAC-Bayes bound vanishes with rate $\mathcal{O}(n^{-1/2})$ and (b) exhibits a logarithmic dependence on the permitted error probability.

</details>


### [85] [Policy4OOD: A Knowledge-Guided World Model for Policy Intervention Simulation against the Opioid Overdose Crisis](https://arxiv.org/abs/2602.12373)
*Yijun Ma,Zehong Wang,Weixiang Sun,Zheyuan Zhang,Kaiwen Shi,Nitesh Chawla,Yanfang Ye*

Main category: cs.LG

TL;DR: Policy4OOD: A knowledge-guided spatio-temporal world model for opioid policy evaluation that unifies forecasting, counterfactual reasoning, and optimization through transformer-based simulation.


<details>
  <summary>Details</summary>
Motivation: The opioid epidemic is a severe public health crisis where evaluating policy interventions is difficult due to complex interactions in a dynamic system. Current approaches lack the ability to forecast outcomes, reason about counterfactuals, and optimize interventions simultaneously.

Method: Policy4OOD is a knowledge-guided spatio-temporal world model that jointly encodes policy knowledge graphs, state-level spatial dependencies, and socioeconomic time series into a policy-conditioned Transformer. It creates a simulator for forecasting, counterfactual analysis (by substituting policy encodings), and policy optimization (using Monte Carlo Tree Search).

Result: Experiments show that spatial dependencies and structured policy knowledge significantly improve forecasting accuracy. The model validates each architectural component and demonstrates the potential of world modeling for data-driven public health decision support.

Conclusion: World modeling provides a unified framework for opioid policy evaluation with three essential capabilities: forecasting, counterfactual reasoning, and optimization. Policy4OOD offers a promising approach for data-driven public health decision-making in complex, dynamic systems.

Abstract: The opioid epidemic remains one of the most severe public health crises in the United States, yet evaluating policy interventions before implementation is difficult: multiple policies interact within a dynamic system where targeting one risk pathway may inadvertently amplify another. We argue that effective opioid policy evaluation requires three capabilities -- forecasting future outcomes under current policies, counterfactual reasoning about alternative past decisions, and optimization over candidate interventions -- and propose to unify them through world modeling. We introduce Policy4OOD, a knowledge-guided spatio-temporal world model that addresses three core challenges: what policies prescribe, where effects manifest, and when effects unfold.Policy4OOD jointly encodes policy knowledge graphs, state-level spatial dependencies, and socioeconomic time series into a policy-conditioned Transformer that forecasts future opioid outcomes.Once trained, the world model serves as a simulator: forecasting requires only a forward pass, counterfactual analysis substitutes alternative policy encodings in the historical sequence, and policy optimization employs Monte Carlo Tree Search over the learned simulator. To support this framework, we construct a state-level monthly dataset (2019--2024) integrating opioid mortality, socioeconomic indicators, and structured policy encodings. Experiments demonstrate that spatial dependencies and structured policy knowledge significantly improve forecasting accuracy, validating each architectural component and the potential of world modeling for data-driven public health decision support.

</details>


### [86] [Value Bonuses using Ensemble Errors for Exploration in Reinforcement Learning](https://arxiv.org/abs/2602.12375)
*Abdul Wahab,Raksha Kumaraswamy,Martha White*

Main category: cs.LG

TL;DR: VBE algorithm uses ensemble errors to create value bonuses that enable first-visit optimism and deep exploration in RL.


<details>
  <summary>Details</summary>
Motivation: Existing optimistic value estimates only increase bonuses retroactively after seeing rewards, failing to encourage first-time visits to new state-action pairs. Need for exploration method that provides first-visit optimism.

Method: VBE maintains an ensemble of random action-value functions (RQFs) and uses their estimation errors to design value bonuses. Key innovation: designing rewards for RQFs so value bonuses can decrease to zero, enabling first-visit optimism and deep exploration.

Result: VBE outperforms Bootstrap DQN and two reward bonus approaches (RND and ACB) on several classic exploration test environments. Demonstrates scalability to complex environments like Atari.

Conclusion: VBE provides an effective approach for directed exploration in RL by using ensemble errors to create value bonuses that encourage first-visit exploration, addressing limitations of previous optimistic value estimation methods.

Abstract: Optimistic value estimates provide one mechanism for directed exploration in reinforcement learning (RL). The agent acts greedily with respect to an estimate of the value plus what can be seen as a value bonus. The value bonus can be learned by estimating a value function on reward bonuses, propagating local uncertainties around rewards. However, this approach only increases the value bonus for an action retroactively, after seeing a higher reward bonus from that state and action. Such an approach does not encourage the agent to visit a state and action for the first time. In this work, we introduce an algorithm for exploration called Value Bonuses with Ensemble errors (VBE), that maintains an ensemble of random action-value functions (RQFs). VBE uses the errors in the estimation of these RQFs to design value bonuses that provide first-visit optimism and deep exploration. The key idea is to design the rewards for these RQFs in such a way that the value bonus can decrease to zero. We show that VBE outperforms Bootstrap DQN and two reward bonus approaches (RND and ACB) on several classic environments used to test exploration and provide demonstrative experiments that it can scale easily to more complex environments like Atari.

</details>


### [87] [Deep Doubly Debiased Longitudinal Effect Estimation with ICE G-Computation](https://arxiv.org/abs/2602.12379)
*Wenxin Chen,Weishen Pan,Kyra Gan,Fei Wang*

Main category: cs.LG

TL;DR: D3-Net reduces error propagation in longitudinal treatment effect estimation by combining SDR pseudo-outcomes during training with a final LTMLE correction.


<details>
  <summary>Details</summary>
Motivation: Longitudinal treatment effect estimation suffers from error propagation in ICE G-computation due to treatment-confounder feedback, corrupting outcome regression models.

Method: Two-stage approach: 1) Train ICE sequence using SDR pseudo-outcomes to interrupt error propagation, with multi-task Transformer (covariate simulator head + target network) for regularization; 2) Apply LTMLE using learned nuisance models for final robust estimation.

Result: D3-Net robustly reduces bias and variance across different horizons, counterfactuals, and time-varying confoundings compared to state-of-the-art ICE-based estimators.

Conclusion: The proposed D3-Net framework effectively mitigates error propagation in longitudinal treatment effect estimation through SDR training and LTMLE correction, achieving superior performance.

Abstract: Estimating longitudinal treatment effects is essential for sequential decision-making but is challenging due to treatment-confounder feedback. While Iterative Conditional Expectation (ICE) G-computation offers a principled approach, its recursive structure suffers from error propagation, corrupting the learned outcome regression models. We propose D3-Net, a framework that mitigates error propagation in ICE training and then applies a robust final correction. First, to interrupt error propagation during learning, we train the ICE sequence using Sequential Doubly Robust (SDR) pseudo-outcomes, which provide bias-corrected targets for each regression. Second, we employ a multi-task Transformer with a covariate simulator head for auxiliary supervision, regularizing representations against corruption by noisy pseudo-outcomes, and a target network to stabilize training dynamics. For the final estimate, we discard the SDR correction and instead use the uncorrected nuisance models to perform Longitudinal Targeted Minimum Loss-Based Estimation (LTMLE) on the original outcomes. This second-stage, targeted debiasing ensures robustness and optimal finite-sample properties. Comprehensive experiments demonstrate that our model, D3-Net, robustly reduces bias and variance across different horizons, counterfactuals, and time-varying confoundings, compared to existing state-of-the-art ICE-based estimators.

</details>


### [88] [TFT-ACB-XML: Decision-Level Integration of Customized Temporal Fusion Transformer and Attention-BiLSTM with XGBoost Meta-Learner for BTC Price Forecasting](https://arxiv.org/abs/2602.12380)
*Raiz Ud Din,Saddam Hussain Khan*

Main category: cs.LG

TL;DR: A hybrid stacked-generalization framework called TFT-ACB-XML combines customized Temporal Fusion Transformer and Attention-Customized BiLSTM with XGBoost for Bitcoin price prediction, achieving improved accuracy with MAPE of 0.65%.


<details>
  <summary>Details</summary>
Motivation: Bitcoin forecasting is challenging due to decentralized markets being non-linear, highly volatile, and having temporal irregularities. Existing deep learning models struggle with interpretability and generalization across diverse market conditions.

Method: The TFT-ACB-XML framework integrates two parallel base learners: a customized Temporal Fusion Transformer (TFT) for long-range dependencies and global temporal dynamics, and an Attention-Customized Bidirectional LSTM (ACB) for short-term sequential dependencies. Predictions are weighted using error-reciprocal weighting based on validation performance, then concatenated and fed to an XGBoost regressor as meta-learner to capture non-linear residuals.

Result: Empirical validation using BTC data from October 2014 to January 2026 shows improved performance with MAPE of 0.65%, MAE of 198.15, and RMSE of 258.30 for one-step-ahead out-of-sample predictions under walk-forward evaluation, tested during the 2024 BTC halving and spot ETFs period.

Conclusion: The proposed hybrid stacked-generalization framework effectively addresses Bitcoin forecasting challenges by combining interpretable attention mechanisms with ensemble learning, demonstrating robust performance during major market events like halving and ETF introductions.

Abstract: Accurate forecasting of Bitcoin (BTC) has always been a challenge because decentralized markets are non-linear, highly volatile, and have temporal irregularities. Existing deep learning models often struggle with interpretability and generalization across diverse market conditions. This research presents a hybrid stacked-generalization framework, TFT-ACB-XML, for BTC closing price prediction. The framework integrates two parallel base learners: a customized Temporal Fusion Transformer (TFT) and an Attention-Customized Bidirectional Long Short-Term Memory network (ACB), followed by an XGBoost regressor as the meta-learner. The customized TFT model handles long-range dependencies and global temporal dynamics via variable selection networks and interpretable single-head attention. The ACB module uses a new attention mechanism alongside the customized BiLSTM to capture short-term sequential dependencies. Predictions from both customized TFT and ACB are weighted through an error-reciprocal weighting strategy. These weights are derived from validation performance, where a model showing lower prediction error receives a higher weight. Finally, the framework concatenates these weighted outputs into a feature vector and feeds the vector to an XGBoost regressor, which captures non-linear residuals and produces the final BTC closing price prediction. Empirical validation using BTC data from October 1, 2014, to January 5, 2026, shows improved performance of the proposed framework compared to recent Deep Learning and Transformer baseline models. The results show a MAPE of 0.65%, an MAE of 198.15, and an RMSE of 258.30 for one-step-ahead out-of-sample under a walk-forward evaluation on the test block. The evaluation period spans the 2024 BTC halving and the spot ETFs (exchange-traded funds) period, which coincide with major liquidity and volatility shifts.

</details>


### [89] [Why Deep Jacobian Spectra Separate: Depth-Induced Scaling and Singular-Vector Alignment](https://arxiv.org/abs/2602.12384)
*Nathanaël Haas,Francçois Gatine,Augustin M Cosse,Zied Bouraoui*

Main category: cs.LG

TL;DR: The paper proposes a new approach to understand implicit bias in deep networks by analyzing singular value scaling and spectral separation in deep Jacobians, showing these properties lead to decoupled singular-value dynamics similar to balanced linear models.


<details>
  <summary>Details</summary>
Motivation: Understanding why gradient-based training in deep networks exhibits strong implicit bias is challenging because tractable singular-value dynamics are typically only available for balanced deep linear models. The authors seek an alternative approach to analyze this phenomenon.

Method: The authors adopt a fixed-gates view of piecewise-linear networks where Jacobians reduce to products of masked linear maps within activation regions. They prove existence of Lyapunov exponents for top singular values at initialization, give closed-form expressions in a tractable masked model, and quantify finite-depth corrections. They also show strong spectral separation forces singular-vector alignment.

Result: The analysis reveals depth-induced exponential scaling of ordered singular values and strong spectral separation in deep Jacobians. These properties lead to an approximation regime where singular-value dynamics become effectively decoupled, mirroring classical balanced deep-linear analyses without requiring balancing.

Conclusion: The paper provides a mechanistic account of emergent low-rank Jacobian structure as a driver of implicit bias in deep networks, validated by experiments in fixed-gates settings that confirm predicted scaling, alignment, and resulting dynamics.

Abstract: Understanding why gradient-based training in deep networks exhibits strong implicit bias remains challenging, in part because tractable singular-value dynamics are typically available only for balanced deep linear models. We propose an alternative route based on two theoretically grounded and empirically testable signatures of deep Jacobians: depth-induced exponential scaling of ordered singular values and strong spectral separation. Adopting a fixed-gates view of piecewise-linear networks, where Jacobians reduce to products of masked linear maps within a single activation region, we prove the existence of Lyapunov exponents governing the top singular values at initialization, give closed-form expressions in a tractable masked model, and quantify finite-depth corrections. We further show that sufficiently strong separation forces singular-vector alignment in matrix products, yielding an approximately shared singular basis for intermediate Jacobians. Together, these results motivate an approximation regime in which singular-value dynamics become effectively decoupled, mirroring classical balanced deep-linear analyses without requiring balancing. Experiments in fixed-gates settings validate the predicted scaling, alignment, and resulting dynamics, supporting a mechanistic account of emergent low-rank Jacobian structure as a driver of implicit bias.

</details>


### [90] [Rational Neural Networks have Expressivity Advantages](https://arxiv.org/abs/2602.12390)
*Maosen Tang,Alex Townsend*

Main category: cs.LG

TL;DR: Rational activation functions outperform standard fixed activations with exponential parameter efficiency gains for approximation tasks.


<details>
  <summary>Details</summary>
Motivation: Standard activation functions (ReLU, sigmoid, tanh, etc.) have limitations in expressiveness and parameter efficiency. The paper aims to show that trainable low-degree rational activations can overcome these limitations.

Method: Study neural networks with trainable low-degree rational activation functions. Establish theoretical approximation bounds comparing rational activations to standard fixed activations. Show that rational networks can approximate standard networks with only poly(log log(1/ε)) overhead, while the reverse requires Ω(log(1/ε)) parameters.

Result: Exponential gap in parameter efficiency: rational activations require only poly(log log(1/ε)) parameters to achieve ε error, while standard activations need Ω(log(1/ε)) parameters. Rational activations integrate well into standard architectures and training pipelines, matching or outperforming fixed activations.

Conclusion: Trainable rational activation functions are more expressive and parameter-efficient than standard fixed activations, offering theoretical and practical advantages for neural network design.

Abstract: We study neural networks with trainable low-degree rational activation functions and show that they are more expressive and parameter-efficient than modern piecewise-linear and smooth activations such as ELU, LeakyReLU, LogSigmoid, PReLU, ReLU, SELU, CELU, Sigmoid, SiLU, Mish, Softplus, Tanh, Softmin, Softmax, and LogSoftmax. For an error target of $\varepsilon>0$, we establish approximation-theoretic separations: Any network built from standard fixed activations can be uniformly approximated on compact domains by a rational-activation network with only $\mathrm{poly}(\log\log(1/\varepsilon))$ overhead in size, while the converse provably requires $Ω(\log(1/\varepsilon))$ parameters in the worst case. This exponential gap persists at the level of full networks and extends to gated activations and transformer-style nonlinearities. In practice, rational activations integrate seamlessly into standard architectures and training pipelines, allowing rationals to match or outperform fixed activations under identical architectures and optimizers.

</details>


### [91] [High-dimensional Level Set Estimation with Trust Regions and Double Acquisition Functions](https://arxiv.org/abs/2602.12391)
*Giang Ngo,Dat Phan Trong,Dang Nguyen,Sunil Gupta*

Main category: cs.LG

TL;DR: TRLSE is a novel algorithm for high-dimensional level set estimation that uses dual acquisition functions (global and local) to efficiently identify and refine boundary regions, achieving superior sample efficiency compared to existing methods.


<details>
  <summary>Details</summary>
Motivation: Level set estimation is fundamental in many real-world applications, but becomes challenging in high-dimensional spaces where search volume grows exponentially. Active learning with limited initial data requires efficient acquisition of informative points to construct accurate classifiers for threshold boundary identification.

Method: TRLSE algorithm identifies and refines regions near the threshold boundary using dual acquisition functions operating at both global and local levels. This two-level approach allows for efficient exploration of high-dimensional spaces.

Result: Theoretical analysis demonstrates TRLSE's accuracy, and extensive evaluations on synthetic and real-world problems show superior sample efficiency compared to existing methods.

Conclusion: TRLSE provides an effective solution for high-dimensional level set estimation problems, addressing the exponential search volume challenge through its dual acquisition strategy and achieving better sample efficiency than current approaches.

Abstract: Level set estimation (LSE) classifies whether an unknown function's value exceeds a specified threshold for given inputs, a fundamental problem in many real-world applications. In active learning settings with limited initial data, we aim to iteratively acquire informative points to construct an accurate classifier for this task. In high-dimensional spaces, this becomes challenging where the search volume grows exponentially with increasing dimensionality. We propose TRLSE, an algorithm for high-dimensional LSE, which identifies and refines regions near the threshold boundary with dual acquisition functions operating at both global and local levels. We provide a theoretical analysis of TRLSE's accuracy and show its superior sample efficiency against existing methods through extensive evaluations on multiple synthetic and real-world LSE problems.

</details>


### [92] [Synthetic Interaction Data for Scalable Personalization in Large Language Models](https://arxiv.org/abs/2602.12394)
*Yuchen Ma,Yue Huang,Wenjie Wang,Xiaonan Luo,Xiangliang Zhang,Stefan Feuerriegel*

Main category: cs.LG

TL;DR: PPOpt framework optimizes personalized prompts for LLMs using synthetic data and multi-objective RL, improving task performance and personalization over existing methods.


<details>
  <summary>Details</summary>
Motivation: Existing prompt optimization methods focus on task-level optimization but overlook user-specific preferences and constraints due to lack of personalized interaction data and robust reward signals for individual preferences.

Method: Introduces PersonaGym for synthetic data generation, PersonaAtlas dataset, and PPOpt framework with reason-then-optimize paradigm that infers user profiles and uses supervised prior + multi-objective reinforcement learning for prompt optimization.

Result: Consistent improvements over state-of-the-art baselines in task performance, personalization quality, and robustness to noisy/sparse preference signals.

Conclusion: PPOpt provides scalable, model-agnostic personalized prompt optimization that effectively addresses data limitations and reward signal challenges in personalizing LLMs for diverse users.

Abstract: Personalized prompting offers large opportunities for deploying large language models (LLMs) to diverse users, yet existing prompt optimization methods primarily focus on task-level optimization while largely overlooking user-specific preferences and latent constraints of individual users. This gap is primarily due to (i) the absence of high-quality, privacy-sensitive data that capture personalized user-LLM interactions at scale, and (ii) the lack of robust reward signals for individual preferences. To overcome existing data limitations, we introduce a high-fidelity synthetic data generation framework called PersonaGym. Unlike prior work that treats personalization as static persona-preference pairs, PersonaGym models a dynamic preference process via an agentic LLM system to simulate realistic preference behaviors and semantic-aware noise in order to generate personalized multi-turn interaction trajectories. Using PersonaGym, we release PersonaAtlas, a large-scale, high-quality, and diverse synthetic dataset of high-fidelity multi-turn personalized interaction trajectories that closely mirror real-world preference expression and noise patterns. We further propose Personalized Prompt Optimization (PPOpt), a scalable and model-agnostic framework that optimizes user prompts based on interaction histories without modifying the deployed LLM. PPOpt adopts a reason-then-optimize paradigm that infers an explicit user profile and conditions prompt rewriting on the user profile to avoid reward hacking. Our training procedure for PPOpt integrates a cold-start supervised prior with outcome-driven multi-objective reinforcement learning. We present extensive experiments to demonstrate consistent improvements over state-of-the-art baselines in terms of task performance, personalization quality, and robustness to noisy as well as to sparse preference signals.

</details>


### [93] [AstRL: Analog and Mixed-Signal Circuit Synthesis with Deep Reinforcement Learning](https://arxiv.org/abs/2602.12402)
*Felicia B. Guo,Ken T. Ho,Andrei Vladimirescu,Borivoje Nikolic*

Main category: cs.LG

TL;DR: AstRL: A deep reinforcement learning method for analog circuit synthesis that treats circuit design as graph generation, producing transistor-level topologies optimized for user targets with 100% structural correctness and >90% functional success.


<details>
  <summary>Details</summary>
Motivation: Despite increasing complexity in analog/mixed-signal IC design, automation remains limited due to challenges in creating generalized optimization methods that work across diverse, constrained, non-differentiable circuit design spaces.

Method: Casts circuit design as graph generation problem using deep reinforcement learning (AstRL). Uses policy-gradient approach in simulator-embedded environment with ground-truth feedback. Incorporates behavioral-cloning and discriminator-based similarity rewards for expert-aligned generation. Operates at transistor level with inductive biases in action space for structural consistency.

Result: Substantial improvements in conventional design metrics over state-of-the-art baselines. 100% of generated designs are structurally correct, and over 90% demonstrate required functionality across three realistic design tasks.

Conclusion: AstRL demonstrates a novel, expert-aligned paradigm for generalized analog circuit synthesis that successfully generates valid, functional transistor-level topologies through reinforcement learning, addressing long-standing automation challenges in AMS design.

Abstract: Analog and mixed-signal (AMS) integrated circuits (ICs) lie at the core of modern computing and communications systems. However, despite the continued rise in design complexity, advances in AMS automation remain limited. This reflects the central challenge in developing a generalized optimization method applicable across diverse circuit design spaces, many of which are distinct, constrained, and non-differentiable. To address this, our work casts circuit design as a graph generation problem and introduces a novel method of AMS synthesis driven by deep reinforcement learning (AstRL). Based on a policy-gradient approach, AstRL generates circuits directly optimized for user-specified targets within a simulator-embedded environment that provides ground-truth feedback during training. Through behavioral-cloning and discriminator-based similarity rewards, our method demonstrates, for the first time, an expert-aligned paradigm for generalized circuit generation validated in simulation. Importantly, the proposed approach operates at the level of individual transistors, enabling highly expressive, fine-grained topology generation. Strong inductive biases encoded in the action space and environment further drive structurally consistent and valid generation. Experimental results for three realistic design tasks illustrate substantial improvements in conventional design metrics over state-of-the-art baselines, with 100% of generated designs being structurally correct and over 90% demonstrating required functionality.

</details>


### [94] [Soft Contamination Means Benchmarks Test Shallow Generalization](https://arxiv.org/abs/2602.12413)
*Ari Spiesberger,Juan J. Vazquez,Nicky Pochinkov,Tomáš Gavenčiak,Peli Grietzer,Gavin Leech,Nandi Schoots*

Main category: cs.LG

TL;DR: The paper studies "soft contamination" where LLM training data contains semantic duplicates of benchmark test data, not just exact string matches, leading to biased OOD generalization estimates.


<details>
  <summary>Details</summary>
Motivation: Traditional decontamination methods using n-gram matching fail to detect semantic duplicates (sentences with equivalent content but different wording), creating biased benchmark performance estimates that don't reflect true out-of-distribution generalization.

Method: Embed the Olmo3 training corpus to detect semantic duplicates of benchmark data, conduct experiments showing how including semantic duplicates in training affects benchmark performance, and analyze finetuning on duplicates.

Result: 1) Contamination is widespread (78% of CodeForces, 50% of ZebraLogic have semantic/exact duplicates); 2) Including semantic duplicates improves benchmark performance; 3) Finetuning on duplicates improves performance on truly-held-out data from same benchmark.

Conclusion: Recent benchmark gains are confounded - they reflect both genuine capability improvements AND accumulation of test data (including semantic duplicates) in training corpora, challenging the validity of benchmark-based LLM progress claims.

Abstract: If LLM training data is polluted with benchmark test data, then benchmark performance gives biased estimates of out-of-distribution (OOD) generalization. Typical decontamination filters use n-gram matching which fail to detect semantic duplicates: sentences with equivalent (or near-equivalent) content that are not close in string space. We study this soft contamination of training data by semantic duplicates. Among other experiments, we embed the Olmo3 training corpus and find that: 1) contamination remains widespread, e.g. we find semantic duplicates for 78% of CodeForces and exact duplicates for 50% of ZebraLogic problems; 2) including semantic duplicates of benchmark data in training does improve benchmark performance; and 3) when finetuning on duplicates of benchmark datapoints, performance also improves on truly-held-out datapoints from the same benchmark. We argue that recent benchmark gains are thus confounded: the prevalence of soft contamination means gains reflect both genuine capability improvements and the accumulation of test data and effective test data in growing training corpora.

</details>


### [95] [Quantization-Aware Collaborative Inference for Large Embodied AI Models](https://arxiv.org/abs/2602.13052)
*Zhonghao Lyu,Ming Xiao,Mikael Skoglund,Merouane Debbah,H. Vincent Poor*

Main category: cs.LG

TL;DR: This paper proposes quantization-aware collaborative inference for edge embodied AI systems to address computational challenges of large AI models on resource-limited agents.


<details>
  <summary>Details</summary>
Motivation: Large AI models (LAIMs) are essential for embodied AI applications but their massive parameter scale and computational demands create significant challenges for resource-limited embodied agents. There's a need to make these models feasible on edge devices with constraints.

Method: The authors develop a tractable approximation for quantization-induced inference distortion, derive lower and upper bounds on quantization rate-inference distortion function, and formulate a joint quantization bit-width and computation frequency design problem under delay and energy constraints.

Result: Extensive evaluations validate the proposed distortion approximation, the derived rate-distortion bounds, and the effectiveness of the joint design. Simulations and real-world testbed experiments demonstrate effective balancing of inference quality, latency, and energy consumption.

Conclusion: The proposed quantization-aware collaborative inference framework enables efficient deployment of large AI models on resource-limited embodied agents by jointly optimizing quantization and computation parameters while maintaining inference quality under practical constraints.

Abstract: Large artificial intelligence models (LAIMs) are increasingly regarded as a core intelligence engine for embodied AI applications. However, the massive parameter scale and computational demands of LAIMs pose significant challenges for resource-limited embodied agents. To address this issue, we investigate quantization-aware collaborative inference (co-inference) for embodied AI systems. First, we develop a tractable approximation for quantization-induced inference distortion. Based on this approximation, we derive lower and upper bounds on the quantization rate-inference distortion function, characterizing its dependence on LAIM statistics, including the quantization bit-width. Next, we formulate a joint quantization bit-width and computation frequency design problem under delay and energy constraints, aiming to minimize the distortion upper bound while ensuring tightness through the corresponding lower bound. Extensive evaluations validate the proposed distortion approximation, the derived rate-distortion bounds, and the effectiveness of the proposed joint design. Particularly, simulations and real-world testbed experiments demonstrate the effectiveness of the proposed joint design in balancing inference quality, latency, and energy consumption in edge embodied AI systems.

</details>


### [96] [Stabilizing Native Low-Rank LLM Pretraining](https://arxiv.org/abs/2602.12429)
*Paul Janson,Edouard Oyallon,Eugene Belilovsky*

Main category: cs.LG

TL;DR: Training LLMs from scratch using exclusively low-rank factorized weights with spectral renormalization for stability, achieving performance matching dense models with improved efficiency.


<details>
  <summary>Details</summary>
Motivation: Foundation models face computational and memory challenges due to growing parameter counts. Low-rank factorization can reduce costs, but current methods lack stable recipes for training from scratch with exclusively low-rank weights while matching dense model performance.

Method: Introduces Spectron: Spectral renormalization with orthogonalization, which dynamically bounds weight updates based on current spectral norms of factors to address instability from uncontrolled growth in spectral norm of weight matrix updates.

Result: Enables stable, end-to-end factorized training with negligible overhead. Establishes compute-optimal scaling laws for natively low-rank transformers showing predictable power-law behavior and improved inference efficiency relative to dense models.

Conclusion: LLMs can be trained from scratch using exclusively low-rank factorized weights without auxiliary full-rank guidance, achieving performance parity with dense models while offering computational and memory efficiency benefits.

Abstract: Foundation models have achieved remarkable success, yet their growing parameter counts pose significant computational and memory challenges. Low-rank factorization offers a promising route to reduce training and inference costs, but the community lacks a stable recipe for training models from scratch using exclusively low-rank weights while matching the performance of the dense model. We demonstrate that Large Language Models (LLMs) can be trained from scratch using exclusively low-rank factorized weights for all non-embedding matrices without auxiliary "full-rank" guidance required by prior methods. While native low-rank training often suffers from instability and loss spikes, we identify uncontrolled growth in the spectral norm (largest singular value) of the weight matrix update as the dominant factor. To address this, we introduce Spectron: Spectral renormalization with orthogonalization, which dynamically bounds the resultant weight updates based on the current spectral norms of the factors. Our method enables stable, end-to-end factorized training with negligible overhead. Finally, we establish compute-optimal scaling laws for natively low-rank transformers, demonstrating predictable power-law behavior and improved inference efficiency relative to dense models.

</details>


### [97] [Safe Reinforcement Learning via Recovery-based Shielding with Gaussian Process Dynamics Models](https://arxiv.org/abs/2602.12444)
*Alexander W. Goodall,Francesco Belardinelli*

Main category: cs.LG

TL;DR: A recovery-based shielding framework for safe RL that uses Gaussian processes to predict safety violations and dynamically recovers to safe trajectories, enabling unrestricted exploration with provable safety guarantees for continuous control systems.


<details>
  <summary>Details</summary>
Motivation: Reinforcement learning lacks provable safety guarantees for safety-critical applications, especially for unknown and non-linear continuous dynamical systems where safety violations could have serious consequences.

Method: Integrates a backup policy (shield) with RL agent using Gaussian process-based uncertainty quantification to predict safety constraint violations. Dynamically recovers to safe trajectories only when necessary. Uses experience from shielded agent to construct GP models with policy optimization via internal model-based sampling.

Result: Empirically demonstrates strong performance and strict safety-compliance on a suite of continuous control environments while enabling unrestricted exploration and sample-efficient learning without compromising safety.

Conclusion: The proposed recovery-based shielding framework provides a practical solution for safe RL with provable safety lower bounds, addressing the critical need for safety guarantees in RL applications for continuous control systems.

Abstract: Reinforcement learning (RL) is a powerful framework for optimal decision-making and control but often lacks provable guarantees for safety-critical applications. In this paper, we introduce a novel recovery-based shielding framework that enables safe RL with a provable safety lower bound for unknown and non-linear continuous dynamical systems. The proposed approach integrates a backup policy (shield) with the RL agent, leveraging Gaussian process (GP) based uncertainty quantification to predict potential violations of safety constraints, dynamically recovering to safe trajectories only when necessary. Experience gathered by the 'shielded' agent is used to construct the GP models, with policy optimization via internal model-based sampling - enabling unrestricted exploration and sample efficient learning, without compromising safety. Empirically our approach demonstrates strong performance and strict safety-compliance on a suite of continuous control environments.

</details>


### [98] [Computationally sufficient statistics for Ising models](https://arxiv.org/abs/2602.12449)
*Abhijith Jayakumar,Shreya Shukla,Marc Vuffray,Andrey Y. Lokhov,Sidhant Misra*

Main category: cs.LG

TL;DR: Efficient learning of Gibbs distributions using only limited statistics, with Ising model as example, showing reconstruction possible with statistics up to O(γ) order.


<details>
  <summary>Details</summary>
Motivation: Traditional Gibbs distribution learning requires full samples, which is impractical for physical systems where only limited statistics are observable. Need computationally efficient methods that work with limited observational data.

Method: Use Ising model as paradigmatic example, develop approach to reconstruct model parameters using statistics up to order O(γ) where γ is ℓ₁ width. Also explore setting with prior structural information.

Result: Feasible to reconstruct model parameters (couplings and magnetic fields) and infer structure using limited statistics. With prior structural information, learning can be solved efficiently with even more limited observational power.

Conclusion: Computationally efficient learning of Gibbs distributions is possible with limited statistics, bridging the gap between computational power and observational constraints in physical systems.

Abstract: Learning Gibbs distributions using only sufficient statistics has long been recognized as a computationally hard problem. On the other hand, computationally efficient algorithms for learning Gibbs distributions rely on access to full sample configurations generated from the model. For many systems of interest that arise in physical contexts, expecting a full sample to be observed is not practical, and hence it is important to look for computationally efficient methods that solve the learning problem with access to only a limited set of statistics. We examine the trade-offs between the power of computation and observation within this scenario, employing the Ising model as a paradigmatic example. We demonstrate that it is feasible to reconstruct the model parameters for a model with $\ell_1$ width $γ$ by observing statistics up to an order of $O(γ)$. This approach allows us to infer the model's structure and also learn its couplings and magnetic fields. We also discuss a setting where prior information about structure of the model is available and show that the learning problem can be solved efficiently with even more limited observational power.

</details>


### [99] [Continuous Diffusion Models Can Obey Formal Syntax](https://arxiv.org/abs/2602.12468)
*Jinwoo Kim,Taylor Berg-Kirkpatrick,Loris D'Antoni*

Main category: cs.LG

TL;DR: Training-free guidance method for diffusion language models to satisfy formal syntactic constraints expressed using regular expressions, achieving high constraint satisfaction with minimal perplexity cost.


<details>
  <summary>Details</summary>
Motivation: Diffusion language models have advantages over autoregressive models due to their global, non-causal generation process, but their continuous latent dynamics make it difficult to impose discrete constraints like JSON schema matching.

Method: Constructs an analytic score estimating the probability that a latent state decodes to a valid string accepted by a given regular expression, and uses its gradient to guide sampling without training auxiliary classifiers. Implemented as Diffinity on top of PLAID diffusion model.

Result: Achieves 68-96% constraint satisfaction on 180 regular-expression constraints over JSON and natural-language benchmarks, with only small perplexity cost relative to unconstrained sampling. Outperforms autoregressive constrained decoding in both constraint satisfaction and output quality.

Conclusion: The proposed training-free guidance method effectively steers continuous diffusion language models to satisfy formal syntactic constraints, offering a practical solution for constrained generation tasks.

Abstract: Diffusion language models offer a promising alternative to autoregressive models due to their global, non-causal generation process, but their continuous latent dynamics make discrete constraints -- e.g., the output should be a JSON file that matches a given schema -- difficult to impose. We introduce a training-free guidance method for steering continuous diffusion language models to satisfy formal syntactic constraints expressed using regular expressions. Our approach constructs an analytic score estimating the probability that a latent state decodes to a valid string accepted by a given regular expression, and uses its gradient to guide sampling, without training auxiliary classifiers. The denoising process targets the base model conditioned on syntactic validity.
  We implement our method in Diffinity on top of the PLAID diffusion model and evaluate it on 180 regular-expression constraints over JSON and natural-language benchmarks. Diffinity achieves 68-96\% constraint satisfaction while incurring only a small perplexity cost relative to unconstrained sampling, outperforming autoregressive constrained decoding in both constraint satisfaction and output quality.

</details>


### [100] [Regularized Meta-Learning for Improved Generalization](https://arxiv.org/abs/2602.12469)
*Noor Islam S. Mohammad,Md Muntaqim Meherab*

Main category: cs.LG

TL;DR: Regularized meta-learning framework improves ensemble performance by reducing redundancy, adding statistical meta-features, and using cross-validated regularized meta-models, achieving better RMSE with lower runtime.


<details>
  <summary>Details</summary>
Motivation: Address three practical limitations of deep ensemble methods: redundancy among base models (increases computational cost and degrades conditioning), unstable weighting under multicollinearity, and overfitting in meta-learning pipelines.

Method: Four-stage pipeline: 1) redundancy-aware projection using correlation and MSE thresholds (τ_corr=0.95) to remove near-collinear predictors, 2) statistical meta-feature augmentation with engineered ensemble statistics and interaction terms, 3) cross-validated regularized meta-models (Ridge, Lasso, ElasticNet), 4) inverse-RMSE blending stage to mitigate regularizer-selection variance.

Result: Achieved out-of-fold RMSE of 8.582 on Playground Series S6E1 benchmark (100K samples, 72 base models), improving over simple averaging (8.894) and conventional Ridge stacking (8.627), while matching greedy hill climbing (8.603) with 4x faster runtime. Conditioning analysis shows 53.7% reduction in effective matrix condition number after redundancy projection.

Conclusion: Regularized meta-learning provides a stable and deployment-efficient stacking strategy for high-dimensional ensemble systems, with consistent contributions from de-duplication, statistical meta-features, and meta-ensemble blending.

Abstract: Deep ensemble methods often improve predictive performance, yet they suffer from three practical limitations: redundancy among base models that inflates computational cost and degrades conditioning, unstable weighting under multicollinearity, and overfitting in meta-learning pipelines. We propose a regularized meta-learning framework that addresses these challenges through a four-stage pipeline combining redundancy-aware projection, statistical meta-feature augmentation, and cross-validated regularized meta-models (Ridge, Lasso, and ElasticNet). Our multi-metric de-duplication strategy removes near-collinear predictors using correlation and MSE thresholds ($τ_{\text{corr}}=0.95$), reducing the effective condition number of the meta-design matrix while preserving predictive diversity. Engineered ensemble statistics and interaction terms recover higher-order structure unavailable to raw prediction columns. A final inverse-RMSE blending stage mitigates regularizer-selection variance. On the Playground Series S6E1 benchmark (100K samples, 72 base models), the proposed framework achieves an out-of-fold RMSE of 8.582, improving over simple averaging (8.894) and conventional Ridge stacking (8.627), while matching greedy hill climbing (8.603) with substantially lower runtime (4 times faster). Conditioning analysis shows a 53.7\% reduction in effective matrix condition number after redundancy projection. Comprehensive ablations demonstrate consistent contributions from de-duplication, statistical meta-features, and meta-ensemble blending. These results position regularized meta-learning as a stable and deployment-efficient stacking strategy for high-dimensional ensemble systems.

</details>


### [101] [Designing RNAs with Language Models](https://arxiv.org/abs/2602.12470)
*Milan Gautam,Ning Dai,Tianshuo Zhou,Bowen Xie,David Mathews,Liang Huang*

Main category: cs.LG

TL;DR: RNA design reframed as conditional sequence generation using autoregressive language model trained with supervised learning and RL optimization, outperforming state-of-the-art methods.


<details>
  <summary>Details</summary>
Motivation: RNA design is computationally challenging due to exponential sequence space and competing folds. Traditional optimization approaches rely on per-instance heuristics, but a reusable neural approximator could provide a more scalable solution.

Method: Reframe RNA design as conditional sequence generation using autoregressive language model. Train first with supervised learning on random structure-sequence pairs, then use reinforcement learning to optimize end-to-end metrics. Propose methods to select small subset for RL to improve efficiency.

Result: Outperforms state-of-the-art systems across four datasets on key metrics like Boltzmann probability while being 1.7x faster. Establishes conditional LM generation as scalable, task-agnostic alternative to per-instance optimization.

Conclusion: Conditional language model generation provides an effective, scalable approach to RNA design that outperforms traditional optimization methods in both quality and speed.

Abstract: RNA design, the task of finding a sequence that folds into a target secondary structure, has broad biological and biomedical impact but remains computationally challenging due to the exponentially large sequence space and exponentially many competing folds. Traditional approaches treat it as an optimization problem, relying on per-instance heuristics or constraint-based search. We instead reframe RNA design as conditional sequence generation and introduce a reusable neural approximator, instantiated as an autoregressive language model (LM), that maps target structures directly to sequences. We first train our model in a supervised setting on random-induced structure-sequence pairs, and then use reinforcement learning (RL) to optimize end-to-end metrics. We also propose methods to select a small subset for RL that greatly improves RL efficiency and quality. Across four datasets, our approach outperforms state-of-the-art systems on key metrics such as Boltzmann probability while being 1.7x faster, establishing conditional LM generation as a scalable, task-agnostic alternative to per-instance optimization for RNA design. Our code and data are available at https://github.com/KuNyaa/RNA-Design-LM.

</details>


### [102] [Tight Bounds for Logistic Regression with Large Stepsize Gradient Descent in Low Dimension](https://arxiv.org/abs/2602.12471)
*Michael Crawshaw,Mingrui Liu*

Main category: cs.LG

TL;DR: GD with large step size achieves O(1/(ηT)) loss for 2D separable data, improving previous 1/T² rate, with tight analysis of transition from unstable to stable dynamics.


<details>
  <summary>Details</summary>
Motivation: Previous work showed accelerated 1/T² rate for logistic loss with separable data using large step sizes, but analysis was not tight. This paper aims to provide tighter analysis specifically for 2D data to better understand the oscillatory dynamics and transition behavior.

Method: Fine-grained analysis of gradient descent dynamics in 2D separable data, focusing on the subspace orthogonal to the max-margin classifier. The analysis tracks the transition time τ from unstable (non-monotonic) to stable (monotonic) loss behavior with large learning rates.

Result: GD with sufficiently large η achieves loss O(1/(ηT)) when T ≥ Ω(n/γ + 1/γ²), improving previous 1/T² rate. The paper provides matching upper and lower bounds on transition time τ up to logarithmic factors, showing analysis is tight.

Conclusion: For 2D separable data, gradient descent with large step sizes achieves improved convergence rates through careful analysis of oscillatory dynamics, with tight characterization of the transition from unstable to stable behavior.

Abstract: We consider the optimization problem of minimizing the logistic loss with gradient descent to train a linear model for binary classification with separable data. With a budget of $T$ iterations, it was recently shown that an accelerated $1/T^2$ rate is possible by choosing a large step size $η= Θ(γ^2 T)$ (where $γ$ is the dataset's margin) despite the resulting non-monotonicity of the loss. In this paper, we provide a tighter analysis of gradient descent for this problem when the data is two-dimensional: we show that GD with a sufficiently large learning rate $η$ finds a point with loss smaller than $\mathcal{O}(1/(ηT))$, as long as $T \geq Ω(n/γ+ 1/γ^2)$, where $n$ is the dataset size. Our improved rate comes from a tighter bound on the time $τ$ that it takes for GD to transition from unstable (non-monotonic loss) to stable (monotonic loss), via a fine-grained analysis of the oscillatory dynamics of GD in the subspace orthogonal to the max-margin classifier. We also provide a lower bound of $τ$ matching our upper bound up to logarithmic factors, showing that our analysis is tight.

</details>


### [103] [Geometric separation and constructive universal approximation with two hidden layers](https://arxiv.org/abs/2602.12482)
*Chanyoung Sung*

Main category: cs.LG

TL;DR: Geometric construction of neural networks that separate disjoint compact subsets of ℝⁿ, leading to constructive universal approximation theorem for networks with two hidden layers using sigmoidal or ReLU activations.


<details>
  <summary>Details</summary>
Motivation: To provide a geometric approach for constructing neural networks that can separate disjoint compact subsets, and to establish a constructive universal approximation theorem that shows neural networks can approximate any continuous function on compact sets.

Method: Geometric construction of neural networks using two hidden layers with either sigmoidal (strictly monotone bounded continuous) or ReLU activation functions. The approach separates disjoint compact subsets and builds networks that can approximate continuous functions uniformly.

Result: Networks with two hidden layers and either sigmoidal or ReLU activation can approximate any real-valued continuous function on arbitrary compact sets in ℝⁿ to any prescribed accuracy in uniform norm. For finite sets, the construction simplifies to depth-2 (single hidden layer) approximation with sharp results.

Conclusion: The paper provides a constructive geometric approach to universal approximation, showing that neural networks with two hidden layers (or single layer for finite sets) using common activation functions can achieve uniform approximation of continuous functions on compact sets.

Abstract: We give a geometric construction of neural networks that separate disjoint compact subsets of $\Bbb R^n$, and use it to obtain a constructive universal approximation theorem. Specifically, we show that networks with two hidden layers and either a sigmoidal activation (i.e., strictly monotone bounded continuous) or the ReLU activation can approximate any real-valued continuous function on an arbitrary compact set $K\subset\Bbb R^n$ to any prescribed accuracy in the uniform norm. For finite $K$, the construction simplifies and yields a sharp depth-2 (single hidden layer) approximation result.

</details>


### [104] [A Theoretical Analysis of Mamba's Training Dynamics: Filtering Relevant Features for Generalization in State Space Models](https://arxiv.org/abs/2602.12499)
*Mugunthan Shandirasegaran,Hongkang Li,Songyang Zhang,Meng Wang,Shuai Zhang*

Main category: cs.LG

TL;DR: First theoretical analysis of Mamba-style selective SSMs showing guaranteed generalization with sample complexity bounds, feature selection via gating, and efficient learning dynamics.


<details>
  <summary>Details</summary>
Motivation: Mamba and selective SSMs show empirical success but lack theoretical foundations. Need to understand when and why these non-attention architectures learn efficiently, providing theoretical counterpoint to Transformer-centric explanations.

Method: Analyze simplified Mamba block: single-layer, single-head selective SSM with input-dependent gating + two-layer MLP trained via gradient descent. Use structured data model with class-relevant/irrelevant patterns under token-level noise. Study two regimes: majority-voting and locality-structured sequences.

Result: Prove guaranteed generalization with non-asymptotic sample complexity and convergence rate bounds. Show gating vector aligns with class-relevant features while ignoring irrelevant ones, formalizing feature-selection role similar to attention but through selective recurrence. Numerical experiments on synthetic data support theory.

Conclusion: Provides principled insight into when/why Mamba-style selective SSMs learn efficiently. Gating mechanism performs feature selection similar to attention but via selective recurrence. Theoretical foundation for understanding non-attention sequence models.

Abstract: The recent empirical success of Mamba and other selective state space models (SSMs) has renewed interest in non-attention architectures for sequence modeling, yet their theoretical foundations remain underexplored. We present a first-step analysis of generalization and learning dynamics for a simplified but representative Mamba block: a single-layer, single-head selective SSM with input-dependent gating, followed by a two-layer MLP trained via gradient descent (GD). Our study adopts a structured data model with tokens that include both class-relevant and class-irrelevant patterns under token-level noise and examines two canonical regimes: majority-voting and locality-structured data sequences. We prove that the model achieves guaranteed generalization by establishing non-asymptotic sample complexity and convergence rate bounds, which improve as the effective signal increases and the noise decreases. Furthermore, we show that the gating vector aligns with class-relevant features while ignoring irrelevant ones, thereby formalizing a feature-selection role similar to attention but realized through selective recurrence. Numerical experiments on synthetic data justify our theoretical results. Overall, our results provide principled insight into when and why Mamba-style selective SSMs learn efficiently, offering a theoretical counterpoint to Transformer-centric explanations.

</details>


### [105] [On Robustness and Chain-of-Thought Consistency of RL-Finetuned VLMs](https://arxiv.org/abs/2602.12506)
*Rosie Zhao,Anshul Shah,Xiaoyu Zhu,Xinke Deng,Zhongyu Jiang,Yang Yang,Joerg Liebelt,Arnab Mondal*

Main category: cs.LG

TL;DR: RL fine-tuning improves VLMs on benchmarks but creates vulnerabilities to textual perturbations, revealing an accuracy-faithfulness trade-off where improved accuracy comes at the cost of reasoning reliability and robustness.


<details>
  <summary>Details</summary>
Motivation: While RL fine-tuning has been successful for LLMs and is being extended to VLMs, these models remain vulnerable to weak visual grounding, hallucinations, and over-reliance on textual cues. The paper aims to understand these vulnerabilities and the trade-offs in RL fine-tuning for multimodal reasoning.

Method: The study uses controlled textual perturbations (misleading captions or incorrect CoT traces) to test robustness, analyzes entropy-based metrics to examine uncertainty, investigates RL fine-tuning dynamics, and explores interventions including adversarial augmentation and faithfulness-aware rewards.

Result: Textual perturbations cause substantial drops in robustness and confidence, especially when considering CoT consistency. RL fine-tuning creates an accuracy-faithfulness trade-off: improved benchmark accuracy erodes CoT reliability and robustness. Adversarial augmentation improves robustness but doesn't prevent faithfulness drift, while faithfulness-aware rewards can restore answer-reasoning alignment but risk shortcut strategies.

Conclusion: Accuracy-only evaluations are insufficient for VLMs. Training and assessment should jointly emphasize correctness, robustness, and faithfulness of visually grounded reasoning to address the vulnerabilities exposed by RL fine-tuning.

Abstract: Reinforcement learning (RL) fine-tuning has become a key technique for enhancing large language models (LLMs) on reasoning-intensive tasks, motivating its extension to vision language models (VLMs). While RL-tuned VLMs improve on visual reasoning benchmarks, they remain vulnerable to weak visual grounding, hallucinations, and over-reliance on textual cues. We show that simple, controlled textual perturbations--misleading captions or incorrect chain-of-thought (CoT) traces--cause substantial drops in robustness and confidence, and that these effects are more pronounced when CoT consistency is taken into account across open-source multimodal reasoning models. Entropy-based metrics further show that these perturbations reshape model uncertainty and probability mass on the correct option, exposing model-specific trends in miscalibration. To better understand these vulnerabilities, we further analyze RL fine-tuning dynamics and uncover an accuracy-faithfulness trade-off: fine-tuning raises benchmark accuracy, but can simultaneously erode the reliability of the accompanying CoT and its robustness to contextual shifts. Although adversarial augmentation improves robustness, it does not by itself prevent faithfulness drift. Incorporating a faithfulness-aware reward can restore alignment between answers and reasoning, but when paired with augmentation, training risks collapsing onto shortcut strategies and robustness remains elusive. Together, these findings highlight the limitations of accuracy-only evaluations and motivate training and assessment protocols that jointly emphasize correctness, robustness, and the faithfulness of visually grounded reasoning.

</details>


### [106] [Bench-MFG: A Benchmark Suite for Learning in Stationary Mean Field Games](https://arxiv.org/abs/2602.12517)
*Lorenzo Magnino,Jiacheng Shen,Matthieu Geist,Olivier Pietquin,Mathieu Laurière*

Main category: cs.LG

TL;DR: Bench-MFG: A comprehensive benchmark suite for Mean Field Games with taxonomy of problem classes, random instance generation (MF-Garnets), and evaluation of learning algorithms to standardize experimental comparisons.


<details>
  <summary>Details</summary>
Motivation: The field of Mean Field Games and Reinforcement Learning lacks standardized evaluation protocols, forcing researchers to use bespoke, isolated environments. This fragmentation makes it difficult to assess robustness, generalization, and failure modes of emerging methods.

Method: Proposes Bench-MFG benchmark suite focusing on discrete-time, discrete-space, stationary setting. Introduces taxonomy of problem classes (no-interaction, monotone, potential, dynamics-coupled games) with prototypical environments for each. Develops MF-Garnets for generating random MFG instances for statistical testing. Benchmarks various learning algorithms including novel black-box approach (MF-PSO) for exploitability minimization.

Result: Extensive empirical benchmarking of learning algorithms across different MFG environments. Based on results, proposes guidelines to standardize future experimental comparisons in the field.

Conclusion: Bench-MFG addresses the critical gap in standardized evaluation for MFG algorithms, providing a comprehensive benchmark suite, taxonomy, and random instance generation method to facilitate rigorous comparison and assessment of method robustness and generalization.

Abstract: The intersection of Mean Field Games (MFGs) and Reinforcement Learning (RL) has fostered a growing family of algorithms designed to solve large-scale multi-agent systems. However, the field currently lacks a standardized evaluation protocol, forcing researchers to rely on bespoke, isolated, and often simplistic environments. This fragmentation makes it difficult to assess the robustness, generalization, and failure modes of emerging methods. To address this gap, we propose a comprehensive benchmark suite for MFGs (Bench-MFG), focusing on the discrete-time, discrete-space, stationary setting for the sake of clarity. We introduce a taxonomy of problem classes, ranging from no-interaction and monotone games to potential and dynamics-coupled games, and provide prototypical environments for each. Furthermore, we propose MF-Garnets, a method for generating random MFG instances to facilitate rigorous statistical testing. We benchmark a variety of learning algorithms across these environments, including a novel black-box approach (MF-PSO) for exploitability minimization. Based on our extensive empirical results, we propose guidelines to standardize future experimental comparisons. Code available at \href{https://github.com/lorenzomagnino/Bench-MFG}{https://github.com/lorenzomagnino/Bench-MFG}.

</details>


### [107] [Multi-Agent Model-Based Reinforcement Learning with Joint State-Action Learned Embeddings](https://arxiv.org/abs/2602.12520)
*Zhizun Wang,David Meger*

Main category: cs.LG

TL;DR: A model-based multi-agent RL framework that combines joint state-action representation learning with imaginative roll-outs using variational auto-encoders and SALE embeddings to improve coordination in partially observable dynamic environments.


<details>
  <summary>Details</summary>
Motivation: Learning to coordinate many agents in partially observable and highly dynamic environments requires both informative representations and data-efficient training. Current approaches need better ways to understand collective outcomes and plan effectively with limited real-environment interactions.

Method: Proposes a novel model-based multi-agent RL framework that unifies joint state-action representation learning with imaginative roll-outs. Uses variational auto-encoders to train a world model, augmented with state-action learned embedding (SALE). SALE is injected into both the imagination module (for future roll-outs) and the joint agent network (where individual action values are combined through a mixing network to estimate joint action-value function).

Result: Empirical studies on StarCraft II Micro-Management, Multi-Agent MuJoCo, and Level-Based Foraging benchmarks demonstrate consistent gains over baseline algorithms, highlighting the effectiveness of joint state-action learned embeddings within a multi-agent model-based paradigm.

Conclusion: The framework enables agents to acquire richer understanding of how their choices influence collective outcomes, leading to improved long-term planning and optimization under limited real-environment interactions through the combination of imagined trajectories with SALE-based action values.

Abstract: Learning to coordinate many agents in partially observable and highly dynamic environments requires both informative representations and data-efficient training. To address this challenge, we present a novel model-based multi-agent reinforcement learning framework that unifies joint state-action representation learning with imaginative roll-outs. We design a world model trained with variational auto-encoders and augment the model using the state-action learned embedding (SALE). SALE is injected into both the imagination module that forecasts plausible future roll-outs and the joint agent network whose individual action values are combined through a mixing network to estimate the joint action-value function. By coupling imagined trajectories with SALE-based action values, the agents acquire a richer understanding of how their choices influence collective outcomes, leading to improved long-term planning and optimization under limited real-environment interactions. Empirical studies on well-established multi-agent benchmarks, including StarCraft II Micro-Management, Multi-Agent MuJoCo, and Level-Based Foraging challenges, demonstrate consistent gains of our method over baseline algorithms and highlight the effectiveness of joint state-action learned embeddings within a multi-agent model-based paradigm.

</details>


### [108] [Constraint-Rectified Training for Efficient Chain-of-Thought](https://arxiv.org/abs/2602.12526)
*Qinhang Wu,Sen Lin,Ming Zhang,Yingbin Liang,Ness B. Shroff*

Main category: cs.LG

TL;DR: CRT is a principled post-training framework that optimizes reasoning efficiency in LLMs by minimizing reasoning length while maintaining accuracy through constrained optimization, reducing token usage without sacrificing answer quality.


<details>
  <summary>Details</summary>
Motivation: Current CoT reasoning methods with RL-based post-training suffer from high inference costs due to redundant steps (overthinking), and existing heuristic approaches for efficiency balancing cause accuracy drops and hyperparameter sensitivity.

Method: CRT uses reference-guarded constrained optimization that alternates between minimizing reasoning length and rectifying accuracy only when performance falls below reference. It employs a two-stage training scheme: first discovering shortest reliable reasoning patterns, then refining accuracy under a learnt length budget.

Result: CRT consistently reduces token usage while maintaining answer quality, improves reasoning efficiency by shortening responses and reducing internal language redundancy, and yields intermediate checkpoints enabling fine-grained control over reasoning verbosity without retraining.

Conclusion: CRT provides a stable, interpretable framework for efficient reasoning that addresses overthinking in LLMs, offering a principled alternative to heuristic approaches while enabling flexible control over reasoning verbosity.

Abstract: Chain-of-Thought (CoT) has significantly enhanced the reasoning capabilities of Large Language Models (LLMs), especially when combined with reinforcement learning (RL) based post-training methods. While longer reasoning traces can improve answer quality and unlock abilities such as self-correction, they also incur high inference costs and often introduce redundant steps, known as overthinking. Recent research seeks to develop efficient reasoning strategies that balance reasoning length and accuracy, either through length-aware reward design or prompt-based calibration. However, these heuristic-based approaches may suffer from severe accuracy drop and be very sensitive to hyperparameters. To address these problems, we introduce CRT (Constraint-Rectified Training), a principled post-training framework based on reference-guarded constrained optimization, yielding a more stable and interpretable formulation for efficient reasoning. CRT alternates between minimizing reasoning length and rectifying accuracy only when performance falls below the reference, enabling stable and effective pruning of redundant reasoning. We further extend CRT with a two-stage training scheme that first discovers the shortest reliable reasoning patterns and then refines accuracy under a learnt length budget, preventing the re-emergence of verbose CoT. Our comprehensive evaluation shows that this framework consistently reduces token usage while maintaining answer quality at a robust and reliable level. Further analysis reveals that CRT improves reasoning efficiency not only by shortening responses but also by reducing internal language redundancy, leading to a new evaluation metric. Moreover, CRT-based training naturally yields a sequence of intermediate checkpoints that span a spectrum of explanation lengths while preserving correctness, enabling fine-grained control over reasoning verbosity without retraining.

</details>


### [109] [Analytical Results for Two Exponential Family Distributions in Hierarchical Dirichlet Processes](https://arxiv.org/abs/2602.12527)
*Naiqi Li*

Main category: cs.LG

TL;DR: The paper extends the Hierarchical Dirichlet Process (HDP) framework beyond Dirichlet-multinomial models by deriving analytic results for Gamma-Poisson and Normal-Gamma-Normal conjugate pairs, providing closed-form expressions for these exponential family distributions within HDP.


<details>
  <summary>Details</summary>
Motivation: While HDP is a flexible Bayesian nonparametric framework for grouped data, existing applications mainly focus on Dirichlet-multinomial conjugate structure. The framework is more general and can accommodate broader conjugate prior-likelihood pairs, particularly exponential family distributions which offer unified analytic tractability.

Method: The authors investigate analytic results for Poisson and normal distributions within HDP framework. They derive explicit closed-form expressions for Gamma-Poisson and Normal-Gamma-Normal conjugate pairs under hierarchical Dirichlet process construction, providing detailed derivations and proofs to clarify the mathematical structure.

Result: The paper provides practical analytic results for researchers using hierarchical Bayesian nonparametrics, extending HDP applicability beyond Dirichlet-multinomial setting. The derived closed-form expressions enable systematic exploitation of conjugacy in hierarchical nonparametric models.

Conclusion: This work successfully extends the HDP framework to accommodate important exponential family distributions (Poisson and normal), providing analytic tractability and expanding the practical utility of hierarchical Bayesian nonparametric methods for researchers.

Abstract: The Hierarchical Dirichlet Process (HDP) provides a flexible Bayesian nonparametric framework for modeling grouped data with a shared yet unbounded collection of mixture components. While existing applications of the HDP predominantly focus on the Dirichlet-multinomial conjugate structure, the framework itself is considerably more general and, in principle, accommodates a broad class of conjugate prior-likelihood pairs. In particular, exponential family distributions offer a unified and analytically tractable modeling paradigm that encompasses many commonly used distributions. In this paper, we investigate analytic results for two important members of the exponential family within the HDP framework: the Poisson distribution and the normal distribution. We derive explicit closed-form expressions for the corresponding Gamma-Poisson and Normal-Gamma-Normal conjugate pairs under the hierarchical Dirichlet process construction. Detailed derivations and proofs are provided to clarify the underlying mathematical structure and to demonstrate how conjugacy can be systematically exploited in hierarchical nonparametric models. Our work extends the applicability of the HDP beyond the Dirichlet-multinomial setting and furnishes practical analytic results for researchers employing hierarchical Bayesian nonparametrics.

</details>


### [110] [Flow-Factory: A Unified Framework for Reinforcement Learning in Flow-Matching Models](https://arxiv.org/abs/2602.12529)
*Bowen Ping,Chengyou Jia,Minnan Luo,Hangwei Qian,Ivor Tsang*

Main category: cs.LG

TL;DR: Flow-Factory is a unified framework for aligning diffusion and flow-matching models with human preferences via reinforcement learning, offering modular architecture, production-ready optimizations, and support for multiple algorithms and models.


<details>
  <summary>Details</summary>
Motivation: Current reinforcement learning approaches for aligning diffusion/flow-matching models suffer from fragmented codebases, model-specific implementations, and engineering complexity, making it difficult for practitioners to experiment and innovate.

Method: Flow-Factory uses a modular, registry-based architecture that decouples algorithms, models, and rewards, enabling seamless integration of new components. It supports algorithms like GRPO, DiffusionNFT, and AWM across various models including Flux, Qwen-Image, and WAN video models.

Result: The framework provides production-ready memory optimization, flexible multi-reward training, and seamless distributed training support, minimizing implementation overhead and enabling rapid prototyping and scaling of innovations.

Conclusion: Flow-Factory empowers researchers to rapidly prototype and scale future innovations in reinforcement learning for diffusion and flow-matching models by providing a unified, modular framework that addresses current fragmentation and complexity issues.

Abstract: Reinforcement learning has emerged as a promising paradigm for aligning diffusion and flow-matching models with human preferences, yet practitioners face fragmented codebases, model-specific implementations, and engineering complexity. We introduce Flow-Factory, a unified framework that decouples algorithms, models, and rewards through through a modular, registry-based architecture. This design enables seamless integration of new algorithms and architectures, as demonstrated by our support for GRPO, DiffusionNFT, and AWM across Flux, Qwen-Image, and WAN video models. By minimizing implementation overhead, Flow-Factory empowers researchers to rapidly prototype and scale future innovations with ease. Flow-Factory provides production-ready memory optimization, flexible multi-reward training, and seamless distributed training support. The codebase is available at https://github.com/X-GenGroup/Flow-Factory.

</details>


### [111] [AMPS: Adaptive Modality Preference Steering via Functional Entropy](https://arxiv.org/abs/2602.12533)
*Zihan Huang,Xintong Li,Rohan Surana,Tong Yu,Rui Wang,Julian McAuley,Jingbo Shang,Junda Wu*

Main category: cs.LG

TL;DR: Instance-aware steering for MLLMs modulates modality preference with sample-specific intensity, outperforming uniform steering while minimizing generation errors.


<details>
  <summary>Details</summary>
Motivation: MLLMs exhibit modality preference (over-relying on either linguistic or visual information), and uniform steering approaches are problematic - strong steering impairs standard inference while weak steering is ineffective, plus steering sensitivity varies across instances.

Method: 1) Develop instance-aware diagnostic metric to quantify each modality's information contribution and sample-specific steering susceptibility; 2) Propose scaling strategy that reduces steering for sensitive samples; 3) Design learnable module to infer scaling patterns for instance-aware control.

Result: Instance-aware steering outperforms conventional uniform steering in modulating modality preference, achieving effective adjustment while keeping generation error rates low.

Conclusion: Instance-aware control of modality preference through sample-specific steering intensity is more effective than uniform approaches, balancing preference adjustment with minimal disruption to standard inference.

Abstract: Multimodal Large Language Models (MLLMs) often exhibit significant modality preference, which is a tendency to favor one modality over another. Depending on the input, they may over-rely on linguistic priors relative to visual evidence, or conversely over-attend to visually salient but facts in textual contexts. Prior work has applied a uniform steering intensity to adjust the modality preference of MLLMs. However, strong steering can impair standard inference and increase error rates, whereas weak steering is often ineffective. In addition, because steering sensitivity varies substantially across multimodal instances, a single global strength is difficult to calibrate. To address this limitation with minimal disruption to inference, we introduce an instance-aware diagnostic metric that quantifies each modality's information contribution and reveals sample-specific susceptibility to steering. Building on these insights, we propose a scaling strategy that reduces steering for sensitive samples and a learnable module that infers scaling patterns, enabling instance-aware control of modality preference. Experimental results show that our instance-aware steering outperforms conventional steering in modulating modality preference, achieving effective adjustment while keeping generation error rates low.

</details>


### [112] [Exploring Accurate and Transparent Domain Adaptation in Predictive Healthcare via Concept-Grounded Orthogonal Inference](https://arxiv.org/abs/2602.12542)
*Pengfei Hu,Chang Lu,Feifan Liu,Yue Ning*

Main category: cs.LG

TL;DR: ExtraCare: A transparent domain adaptation method for clinical event prediction that decomposes patient representations into invariant and covariant components, offering both improved accuracy and human-understandable explanations.


<details>
  <summary>Details</summary>
Motivation: Deep learning models for clinical event prediction suffer performance degradation when deployed under different data distributions, and existing domain adaptation methods lack transparency needed for clinical trust and safety.

Method: Decomposes patient representations into invariant and covariant components, supervises both components, enforces their orthogonality during training, maps sparse latent dimensions to medical concepts, and quantifies contributions via targeted ablations.

Result: Superior performance across multiple domain partition settings on two real-world EHR datasets, with enhanced transparency demonstrated through accurate predictions and explanations from extensive case studies.

Conclusion: ExtraCare provides both improved accuracy over feature alignment models and the transparency needed for clinical adoption by offering human-understandable explanations through concept mapping and contribution quantification.

Abstract: Deep learning models for clinical event prediction on electronic health records (EHR) often suffer performance degradation when deployed under different data distributions. While domain adaptation (DA) methods can mitigate such shifts, its "black-box" nature prevents widespread adoption in clinical practice where transparency is essential for trust and safety. We propose ExtraCare to decompose patient representations into invariant and covariant components. By supervising these two components and enforcing their orthogonality during training, our model preserves label information while exposing domain-specific variation at the same time for more accurate predictions than most feature alignment models. More importantly, it offers human-understandable explanations by mapping sparse latent dimensions to medical concepts and quantifying their contributions via targeted ablations. ExtraCare is evaluated on two real-world EHR datasets across multiple domain partition settings, demonstrating superior performance along with enhanced transparency, as evidenced by its accurate predictions and explanations from extensive case studies.

</details>


### [113] [SD-MoE: Spectral Decomposition for Effective Expert Specialization](https://arxiv.org/abs/2602.12556)
*Ruijun Huang,Fang Dong,Xin Zhang,Hengjie Cao,Zhendong Huang,Anrui Chen,Jixian Zhou,Mengyi Chen,Yifeng Yang,Mingzhi Dong,Yujiang Wang,Jinlong Hou,Qin Lv,Robert P. Dick,Yuan Cheng,Fan Yang,Tun Lu,Chun Zhang,Li Shang*

Main category: cs.LG

TL;DR: SD-MoE addresses expert specialization failure in Mixture-of-Experts models by spectral decoupling of parameters and gradients, improving performance across tasks with minimal computation overhead.


<details>
  <summary>Details</summary>
Motivation: Current MoE architectures suffer from expert specialization failure - some experts become functionally similar while others act as shared experts, limiting effective capacity and model performance. This occurs due to overlapping spectral components in parameters and aligned gradient subspaces driven by low-rank structure in human corpus.

Method: Proposes Spectral-Decoupled MoE (SD-MoE) which decomposes both parameter and gradient in the spectral space. The method identifies and addresses overlapping dominant spectral components in expert parameters and strongly aligned gradient subspaces across experts.

Result: SD-MoE improves performance across downstream tasks, enables effective expert specialization, incurs minimal additional computation, and can be seamlessly integrated into a wide range of existing MoE architectures including Qwen and DeepSeek.

Conclusion: Spectral analysis reveals fundamental limitations in MoE specialization, and SD-MoE provides an effective solution through spectral decoupling that enhances expert specialization and model performance while maintaining computational efficiency.

Abstract: Mixture-of-Experts (MoE) architectures scale Large Language Models via expert specialization induced by conditional computation. In practice, however, expert specialization often fails: some experts become functionally similar, while others functioning as de facto shared experts, limiting the effective capacity and model performance. In this work, we analysis from a spectral perspective on parameter and gradient spaces, uncover that (1) experts share highly overlapping dominant spectral components in their parameters, (2) dominant gradient subspaces are strongly aligned across experts, driven by ubiquitous low-rank structure in human corpus, and (3) gating mechanisms preferentially route inputs along these dominant directions, further limiting specialization. To address this, we propose Spectral-Decoupled MoE (SD-MoE), which decomposes both parameter and gradient in the spectral space. SD-MoE improves performance across downstream tasks, enables effective expert specialization, incurring minimal additional computation, and can be seamlessly integrated into a wide range of existing MoE architectures, including Qwen and DeepSeek.

</details>


### [114] [Fractional Order Federated Learning for Battery Electric Vehicle Energy Consumption Modeling](https://arxiv.org/abs/2602.12567)
*Mohammad Partohaghighi,Roummel Marcia,Bruce J. West,YangQuan Chen*

Main category: cs.LG

TL;DR: FO-RI-FedAvg improves federated learning stability for electric vehicles by combining roughness-informed regularization and fractional-order optimization to handle connectivity issues and data heterogeneity.


<details>
  <summary>Details</summary>
Motivation: Federated learning on connected electric vehicles faces severe instability due to intermittent connectivity, time-varying client participation, and pronounced client-to-client variation from diverse operating conditions. Conventional FedAvg and advanced methods suffer from excessive drift and degraded convergence under these realistic constraints.

Method: FO-RI-FedAvg introduces two complementary client-side mechanisms: (1) adaptive roughness-informed proximal regularization that dynamically tunes the pull toward the global model based on local loss-landscape roughness, and (2) non-integer-order local optimization that incorporates short-term memory to smooth conflicting update directions. The approach preserves standard FedAvg server aggregation and adds only element-wise operations.

Result: Experiments on two real-world BEV energy prediction datasets (VED and eVED) show that FO-RI-FedAvg achieves improved accuracy and more stable convergence compared to strong federated baselines, particularly under reduced client participation.

Conclusion: FO-RI-FedAvg is a lightweight, modular extension of FedAvg that effectively addresses federated learning instability in electric vehicle applications through roughness-informed regularization and fractional-order optimization, maintaining good performance even with limited client participation.

Abstract: Federated learning on connected electric vehicles (BEVs) faces severe instability due to intermittent connectivity, time-varying client participation, and pronounced client-to-client variation induced by diverse operating conditions. Conventional FedAvg and many advanced methods can suffer from excessive drift and degraded convergence under these realistic constraints. This work introduces Fractional-Order Roughness-Informed Federated Averaging (FO-RI-FedAvg), a lightweight and modular extension of FedAvg that improves stability through two complementary client-side mechanisms: (i) adaptive roughness-informed proximal regularization, which dynamically tunes the pull toward the global model based on local loss-landscape roughness, and (ii) non-integer-order local optimization, which incorporates short-term memory to smooth conflicting update directions. The approach preserves standard FedAvg server aggregation, adds only element-wise operations with amortizable overhead, and allows independent toggling of each component. Experiments on two real-world BEV energy prediction datasets, VED and its extended version eVED, show that FO-RI-FedAvg achieves improved accuracy and more stable convergence compared to strong federated baselines, particularly under reduced client participation.

</details>


### [115] [VI-CuRL: Stabilizing Verifier-Independent RL Reasoning via Confidence-Guided Variance Reduction](https://arxiv.org/abs/2602.12579)
*Xin-Qiang Cai,Masashi Sugiyama*

Main category: cs.LG

TL;DR: VI-CuRL is a verifier-free curriculum RL framework that uses model confidence to manage bias-variance trade-off, preventing training collapse in LLM reasoning tasks.


<details>
  <summary>Details</summary>
Motivation: RLVR's reliance on external verifiers limits scalability, and verifier-free methods like GRPO suffer from destructive gradient variance leading to training collapse.

Method: VI-CuRL leverages model's intrinsic confidence to construct a curriculum, prioritizing high-confidence samples to reduce action and problem variance while maintaining asymptotic unbiasedness.

Result: VI-CuRL promotes training stability and outperforms verifier-independent baselines across six challenging benchmarks, both with and without verifiers.

Conclusion: VI-CuRL provides an effective verifier-independent curriculum RL framework that addresses gradient variance issues and enables scalable LLM reasoning enhancement without external verifiers.

Abstract: Reinforcement Learning with Verifiable Rewards (RLVR) has emerged as a dominant paradigm for enhancing Large Language Models (LLMs) reasoning, yet its reliance on external verifiers limits its scalability. Recent findings suggest that RLVR primarily functions by eliciting latent capabilities, motivating the development of verifier-free algorithms. However, in such settings, standard methods like Group Relative Policy Optimization face a critical challenge: destructive gradient variance that often leads to training collapse. To address this issue, we introduceVerifier-Independent Curriculum Reinforcement Learning (VI-CuRL), a framework that leverages the model's intrinsic confidence to construct a curriculum independent from external verifiers. By prioritizing high-confidence samples, VI-CuRL effectively manages the bias-variance trade-off, specifically targeting the reduction of action and problem variance. We provide a rigorous theoretical analysis, proving that our estimator guarantees asymptotic unbiasedness. Empirically, VI-CuRL promotes stability and consistently outperforms verifier-independent baselines across six challenging benchmarks with/without verifiers.

</details>


### [116] [Multi-Head Attention as a Source of Catastrophic Forgetting in MoE Transformers](https://arxiv.org/abs/2602.12587)
*Anrui Chen,Ruijun Huang,Xin Zhang,Fang Dong,Hengjie Cao,Zhendong Huang,Yifeng Yang,Mengyi Chen,Jixian Zhou,Mingzhi Dong,Yujiang Wang,Jinlong Hou,Qin Lv,Robert P. Dick,Yuan Cheng,Tun Lu,Fan Yang,Li Shang*

Main category: cs.LG

TL;DR: MH-MoE improves continual learning in Mixture-of-Experts Transformers by addressing pre-routing bottlenecks through head-wise routing to reduce composition collisions and forgetting.


<details>
  <summary>Details</summary>
Motivation: Despite sparse routing in MoE architectures theoretically reducing interference for continual learning, MoE Transformers still experience substantial forgetting. The authors identify a pre-routing bottleneck where multi-head attention concatenates head-specific signals, forcing routing to act on co-occurring feature compositions rather than separable head channels.

Method: Proposes MH-MoE (Multi-Head Mixture-of-Experts) which performs head-wise routing over sub-representations to increase routing granularity. This approach reduces composition collisions by routing at the head level rather than after concatenation of all attention heads.

Result: MH-MoE effectively mitigates forgetting on TRACE benchmark with Qwen3 models. On Qwen3-0.6B, it reduces Backward Transfer (BWT) from 11.2% (LoRAMoE baseline) to 4.5%, showing significant improvement in continual learning performance.

Conclusion: Head-wise routing in MH-MoE addresses the composition collision problem in MoE Transformers, enabling better continual learning by increasing routing granularity and reducing interference between different feature compositions.

Abstract: Mixture-of-Experts (MoE) architectures are often considered a natural fit for continual learning because sparse routing should localize updates and reduce interference, yet MoE Transformers still forget substantially even with sparse, well-balanced expert utilization. We attribute this gap to a pre-routing bottleneck: multi-head attention concatenates head-specific signals into a single post-attention router input, forcing routing to act on co-occurring feature compositions rather than separable head channels. We show that this router input simultaneously encodes multiple separately decodable semantic and structural factors with uneven head support, and that different feature compositions induce weakly aligned parameter-gradient directions; as a result, routing maps many distinct compositions to the same route. We quantify this collision effect via a route-wise effective composition number $N_{eff}$ and find that higher $N_{eff}$ is associated with larger old-task loss increases after continual training. Motivated by these findings, we propose MH-MoE, which performs head-wise routing over sub-representations to increase routing granularity and reduce composition collisions. On TRACE with Qwen3-0.6B/8B, MH-MoE effectively mitigates forgetting, reducing BWT on Qwen3-0.6B from 11.2% (LoRAMoE) to 4.5%.

</details>


### [117] [Vehicle behaviour estimation for abnormal event detection using distributed fiber optic sensing](https://arxiv.org/abs/2602.12591)
*Hemant Prasad,Daisuke Ikefuji,Shin Tominaga,Hitoshi Sakurai,Manabu Otani*

Main category: cs.LG

TL;DR: A method to detect single-lane traffic abnormalities using distributed fiber-optic sensing by tracking vehicle paths and detecting lane changes through vibration analysis.


<details>
  <summary>Details</summary>
Motivation: While distributed fiber-optic sensing (DFOS) systems can detect traffic congestion, they struggle with identifying single-lane abnormalities that cause congestion. Detecting these early abnormalities through lane change behavior monitoring could provide more proactive traffic management.

Method: The method tracks individual vehicle paths using clustering techniques to estimate vehicle positions over time. It detects lane changes by monitoring changes in the spectral centroid of vehicle vibrations, using a reference vehicle tracking approach along highway sections.

Result: Evaluation with real traffic data showed 80% accuracy for detecting lane change events, which indicate the presence of single-lane abnormalities.

Conclusion: The proposed method successfully detects single-lane abnormalities by monitoring vehicle lane change behavior using DFOS technology, providing a practical solution for early congestion detection with existing fiber infrastructure.

Abstract: The distributed fiber-optic sensing (DFOS) system is a cost-effective wide-area traffic monitoring technology that utilizes existing fiber infrastructure to effectively detect traffic congestions. However, detecting single-lane abnormalities, that lead to congestions, is still a challenge. These single-lane abnormalities can be detected by monitoring lane change behaviour of vehicles, performed to avoid congestion along the monitoring section of a road. This paper presents a method to detect single-lane abnormalities by tracking individual vehicle paths and detecting vehicle lane changes along a section of a road. We propose a method to estimate the vehicle position at all time instances and fit a path using clustering techniques. We detect vehicle lane change by monitoring any change in spectral centroid of vehicle vibrations by tracking a reference vehicle along a highway. The evaluation of our proposed method with real traffic data showed 80% accuracy for lane change detection events that represent presence of abnormalities.

</details>


### [118] [Power Interpretable Causal ODE Networks: A Unified Model for Explainable Anomaly Detection and Root Cause Analysis in Power Systems](https://arxiv.org/abs/2602.12592)
*Yue Sun,Likai Wang,Rick S. Blum,Parv Venkitasubramaniam*

Main category: cs.LG

TL;DR: PICODE Networks: A causality-informed architecture for joint anomaly detection and explanation in power systems, providing root cause localization, anomaly type classification, and shape characterization without needing labeled data or external causal graphs.


<details>
  <summary>Details</summary>
Motivation: Existing ML models for time series anomaly detection in cyber-physical systems like power grids are black boxes that only give binary outputs without explanations (anomaly type, origin, or root causes). This lack of interpretability limits their practical utility for ensuring system safety and resilience.

Method: Proposes Power Interpretable Causality Ordinary Differential Equation (PICODE) Networks - a unified architecture that jointly performs anomaly detection with explanations. The method extracts causal graphs and uses ODE-based modeling to provide root cause localization, anomaly type classification, and anomaly shape characterization without requiring labeled data or external causal graphs.

Result: Experimental results in power systems show PICODE achieves competitive detection performance while offering improved interpretability and reduced reliance on labeled data or external causal graphs. Theoretical results demonstrate alignment between anomaly function shapes and changes in extracted causal graph weights.

Conclusion: PICODE provides a unified solution for interpretable anomaly detection in power systems, addressing the black-box limitation of existing approaches by jointly detecting anomalies and explaining why they're detected, including root causes, types, and shapes.

Abstract: Anomaly detection and root cause analysis (RCA) are critical for ensuring the safety and resilience of cyber-physical systems such as power grids. However, existing machine learning models for time series anomaly detection often operate as black boxes, offering only binary outputs without any explanation, such as identifying anomaly type and origin. To address this challenge, we propose Power Interpretable Causality Ordinary Differential Equation (PICODE) Networks, a unified, causality-informed architecture that jointly performs anomaly detection along with the explanation why it is detected as an anomaly, including root cause localization, anomaly type classification, and anomaly shape characterization. Experimental results in power systems demonstrate that PICODE achieves competitive detection performance while offering improved interpretability and reduced reliance on labeled data or external causal graphs. We provide theoretical results demonstrating the alignment between the shape of anomaly functions and the changes in the weights of the extracted causal graphs.

</details>


### [119] [HyperMLP: An Integrated Perspective for Sequence Modeling](https://arxiv.org/abs/2602.12601)
*Jiecheng Lu,Shihao Yang*

Main category: cs.LG

TL;DR: The paper proposes viewing attention as dynamic MLPs (HyperMLP/HyperGLU) instead of probabilistic query-key lookups, achieving better performance than softmax-attention baselines.


<details>
  <summary>Details</summary>
Motivation: To move beyond the conventional probabilistic view of self-attention as query-key lookups and provide a simpler, more unified perspective that treats attention heads as dynamic two-layer MLPs with context-dependent weights.

Method: Introduces HyperMLP and HyperGLU that learn dynamic mixing in both feature and sequence space using a reverse-offset layout to align temporal mixing with autoregressive semantics. Views attention scores as ever-growing hidden representations rather than probability distributions.

Result: HyperMLP/HyperGLU consistently outperform strong softmax-attention baselines under matched parameter budgets, with theoretical characterizations of expressivity and implications.

Conclusion: The dynamic MLP perspective provides a simpler, more unified framework for attention mechanisms that outperforms traditional probabilistic approaches while offering better theoretical understanding.

Abstract: Self-attention is often viewed as probabilistic query-key lookup, motivating designs that preserve normalized attention scores and fixed positional semantics. We advocate a simpler and more unified perspective: an autoregressive attention head can be viewed as a dynamic two-layer MLP whose weights are instantiated from the context history. From this view, attention scores form an ever-growing hidden representation, and standard MLP activations such as ReLU or GLU naturally implement input-conditioned selection over a context-dependent memory pool rather than a probability distribution. Based on this formulation, we introduce HyperMLP and HyperGLU, which learn dynamic mixing in both feature space and sequence space, using a reverse-offset (lag) layout to align temporal mixing with autoregressive semantics. We provide theoretical characterizations of the expressivity and implications of this structure, and empirically show that HyperMLP/HyperGLU consistently outperform strong softmax-attention baselines under matched parameter budgets.

</details>


### [120] [Dual-Granularity Contrastive Reward via Generated Episodic Guidance for Efficient Embodied RL](https://arxiv.org/abs/2602.12636)
*Xin Liu,Yixuan Li,Yuhui Chen,Yuxing Qin,Haoran Li,Dongbin Zhao*

Main category: cs.LG

TL;DR: DEG is a novel RL framework that uses large video generation models to create task guidance videos and dual-granularity contrastive rewards for sample-efficient learning without human annotations or extensive supervision.


<details>
  <summary>Details</summary>
Motivation: Traditional RL faces challenges with reward design for embodied manipulation: trajectory success rewards are sparse (limiting sample efficiency), while dense reward methods require high-quality human annotations or abundant expert supervision.

Method: DEG uses large video generation models with minimal expert videos for domain adaptation to generate task guidance per episode. It employs dual-granularity contrastive rewards (coarse-grained exploration + fine-grained matching) in self-supervised latent space to guide agents toward generated guidance videos.

Result: Extensive experiments on 18 diverse tasks across simulation and real-world settings show DEG effectively serves as exploration stimulus for discovering sparse success rewards and enables stable policy convergence independently.

Conclusion: DEG provides a novel framework for sample-efficient dense rewards without human annotations or extensive supervision, leveraging video generation models and dual-granularity contrastive learning for effective RL in embodied manipulation tasks.

Abstract: Designing suitable rewards poses a significant challenge in reinforcement learning (RL), especially for embodied manipulation. Trajectory success rewards are suitable for human judges or model fitting, but the sparsity severely limits RL sample efficiency. While recent methods have effectively improved RL via dense rewards, they rely heavily on high-quality human-annotated data or abundant expert supervision. To tackle these issues, this paper proposes Dual-granularity contrastive reward via generated Episodic Guidance (DEG), a novel framework to seek sample-efficient dense rewards without requiring human annotations or extensive supervision. Leveraging the prior knowledge of large video generation models, DEG only needs a small number of expert videos for domain adaptation to generate dedicated task guidance for each RL episode. Then, the proposed dual-granularity reward that balances coarse-grained exploration and fine-grained matching, will guide the agent to efficiently approximate the generated guidance video sequentially in the contrastive self-supervised latent space, and finally complete the target task. Extensive experiments on 18 diverse tasks across both simulation and real-world settings show that DEG can not only serve as an efficient exploration stimulus to help the agent quickly discover sparse success rewards, but also guide effective RL and stable policy convergence independently.

</details>


### [121] [RelBench v2: A Large-Scale Benchmark and Repository for Relational Data](https://arxiv.org/abs/2602.12606)
*Justin Gu,Rishabh Ranjan,Charilaos Kanatsoulis,Haiming Tang,Martin Jurkovic,Valter Hudovernik,Mark Znidar,Pranshu Chaturvedi,Parth Shroff,Fengyu Li,Jure Leskovec*

Main category: cs.LG

TL;DR: RelBench v2 expands the relational deep learning benchmark with 4 new large-scale datasets, introduces autocomplete tasks, integrates external benchmarks, and shows RDL models outperform single-table baselines across multiple task types.


<details>
  <summary>Details</summary>
Motivation: As relational deep learning evolves toward larger models and relational foundation models, there's a need for scalable and realistic benchmarks to enable systematic evaluation and progress in the field.

Method: Introduces RelBench v2 with four major expansions: 1) Adds 4 large-scale relational datasets across different domains, 2) Introduces autocomplete tasks for inferring missing attribute values within tables, 3) Integrates external benchmarks by translating Temporal Graph Benchmark event streams into relational schemas, 4) Interfaces with ReDeLEx for access to 70+ real-world databases, and 5) Incorporates 4DBInfer datasets for broader multi-table prediction coverage.

Result: RelBench v2 now comprises 11 datasets with over 22 million rows across 29 tables. Experimental results demonstrate that relational deep learning models consistently outperform single-table baselines across autocomplete, forecasting, and recommendation tasks.

Conclusion: RelBench v2 provides a comprehensive benchmark for evaluating relational deep learning models, highlighting the importance of explicitly modeling relational structure and enabling systematic progress toward relational foundation models.

Abstract: Relational deep learning (RDL) has emerged as a powerful paradigm for learning directly on relational databases by modeling entities and their relationships across multiple interconnected tables. As this paradigm evolves toward larger models and relational foundation models, scalable and realistic benchmarks are essential for enabling systematic evaluation and progress. In this paper, we introduce RelBench v2, a major expansion of the RelBench benchmark for RDL. RelBench v2 adds four large-scale relational datasets spanning scholarly publications, enterprise resource planning, consumer platforms, and clinical records, increasing the benchmark to 11 datasets comprising over 22 million rows across 29 tables. We further introduce autocomplete tasks, a new class of predictive objectives that require models to infer missing attribute values directly within relational tables while respecting temporal constraints, expanding beyond traditional forecasting tasks constructed via SQL queries. In addition, RelBench v2 expands beyond its native datasets by integrating external benchmarks and evaluation frameworks: we translate event streams from the Temporal Graph Benchmark into relational schemas for unified relational-temporal evaluation, interface with ReDeLEx to provide uniform access to 70+ real-world databases suitable for pretraining, and incorporate 4DBInfer datasets and tasks to broaden multi-table prediction coverage. Experimental results demonstrate that RDL models consistently outperform single-table baselines across autocomplete, forecasting, and recommendation tasks, highlighting the importance of modeling relational structure explicitly.

</details>


### [122] [Coden: Efficient Temporal Graph Neural Networks for Continuous Prediction](https://arxiv.org/abs/2602.12613)
*Zulun Zhu,Siqiang Luo*

Main category: cs.LG

TL;DR: Coden is a Temporal Graph Neural Network model designed for efficient continuous predictions on dynamic graphs, overcoming computational bottlenecks while maintaining accuracy.


<details>
  <summary>Details</summary>
Motivation: Existing TGNNs focus on one-time predictions, but real-world applications require continuous predictions over time. Direct adaptation of current models leads to computational overhead or quality issues, especially for large graphs.

Method: Coden introduces innovations that overcome key complexity bottlenecks in existing TGNNs while preserving predictive accuracy. It establishes duality relationships with both RNN-based and attention-based models.

Result: Evaluations across five dynamic datasets show Coden surpasses existing performance benchmarks in both efficiency and effectiveness for continuous prediction tasks.

Conclusion: Coden establishes itself as a superior solution for continuous prediction in evolving graph environments, with theoretical analyses substantiating its effectiveness and efficiency.

Abstract: Temporal Graph Neural Networks (TGNNs) are pivotal in processing dynamic graphs. However, existing TGNNs primarily target one-time predictions for a given temporal span, whereas many practical applications require continuous predictions, that predictions are issued frequently over time. Directly adapting existing TGNNs to continuous-prediction scenarios introduces either significant computational overhead or prediction quality issues especially for large graphs. This paper revisits the challenge of { continuous predictions} in TGNNs, and introduces {\sc Coden}, a TGNN model designed for efficient and effective learning on dynamic graphs. {\sc Coden} innovatively overcomes the key complexity bottleneck in existing TGNNs while preserving comparable predictive accuracy. Moreover, we further provide theoretical analyses that substantiate the effectiveness and efficiency of {\sc Coden}, and clarify its duality relationship with both RNN-based and attention-based models. Our evaluations across five dynamic datasets show that {\sc Coden} surpasses existing performance benchmarks in both efficiency and effectiveness, establishing it as a superior solution for continuous prediction in evolving graph environments.

</details>


### [123] [Efficient Personalized Federated PCA with Manifold Optimization for IoT Anomaly Detection](https://arxiv.org/abs/2602.12622)
*Xianchao Xiu,Chenyi Huang,Wei Zhang,Wanquan Liu*

Main category: cs.LG

TL;DR: FedEP: Personalized federated PCA for IoT anomaly detection using ℓ₁-norm for local personalization and ℓ₂,₁-norm for robustness, solved via manifold optimization with ADMM.


<details>
  <summary>Details</summary>
Motivation: IoT networks need privacy-preserving anomaly detection, but current federated PCA methods lack both personalization (adapting to local data distributions) and robustness (handling anomalies), which are critical for effective IoT security.

Method: Proposes FedEP: personalized federated PCA with ℓ₁-norm for element-wise sparsity (local personalization) and ℓ₂,₁-norm for row-wise sparsity (robustness). Uses manifold optimization with ADMM for solving the non-convex problem, with theoretical convergence guarantees.

Result: FedEP outperforms state-of-the-art FedPG, achieving excellent F1-scores and accuracy across various IoT security scenarios. Code available on GitHub.

Conclusion: FedEP effectively addresses the limitations of current federated PCA methods by integrating both personalization and robustness, making it suitable for practical IoT anomaly detection applications.

Abstract: Internet of things (IoT) networks face increasing security threats due to their distributed nature and resource constraints. Although federated learning (FL) has gained prominence as a privacy-preserving framework for distributed IoT environments, current federated principal component analysis (PCA) methods lack the integration of personalization and robustness, which are critical for effective anomaly detection. To address these limitations, we propose an efficient personalized federated PCA (FedEP) method for anomaly detection in IoT networks. The proposed model achieves personalization through introducing local representations with the $\ell_1$-norm for element-wise sparsity, while maintaining robustness via enforcing local models with the $\ell_{2,1}$-norm for row-wise sparsity. To solve this non-convex problem, we develop a manifold optimization algorithm based on the alternating direction method of multipliers (ADMM) with rigorous theoretical convergence guarantees. Experimental results confirm that the proposed FedEP outperforms the state-of-the-art FedPG, achieving excellent F1-scores and accuracy in various IoT security scenarios. Our code will be available at \href{https://github.com/xianchaoxiu/FedEP}{https://github.com/xianchaoxiu/FedEP}.

</details>


### [124] [Formalizing the Sampling Design Space of Diffusion-Based Generative Models via Adaptive Solvers and Wasserstein-Bounded Timesteps](https://arxiv.org/abs/2602.12624)
*Sangwoo Jo,Sungjoon Choi*

Main category: cs.LG

TL;DR: SDM is a principled framework that optimizes diffusion model sampling by aligning solver selection and scheduling with the geometric properties of the diffusion trajectory, achieving state-of-the-art results with fewer function evaluations.


<details>
  <summary>Details</summary>
Motivation: Diffusion models have high sampling costs that limit practical deployment. Current approaches use static heuristics for solver selection and scheduling, lacking a principled framework that considers the intrinsic geometric properties of the diffusion process.

Method: SDM analyzes ODE dynamics to show that low-order solvers suffice in early high-noise stages while higher-order solvers are needed for later non-linear stages. It introduces a Wasserstein-bounded optimization framework to derive adaptive timesteps that explicitly bound local discretization error.

Result: Achieves state-of-the-art performance: FID of 1.93 on CIFAR-10, 2.41 on FFHQ, and 1.98 on AFHQv2, with reduced function evaluations compared to existing samplers. No additional training or architectural modifications required.

Conclusion: SDM provides a principled geometric framework for diffusion model sampling that optimizes solver selection and scheduling, enabling efficient high-quality sampling without model retraining.

Abstract: Diffusion-based generative models have achieved remarkable performance across various domains, yet their practical deployment is often limited by high sampling costs. While prior work focuses on training objectives or individual solvers, the holistic design of sampling, specifically solver selection and scheduling, remains dominated by static heuristics. In this work, we revisit this challenge through a geometric lens, proposing SDM, a principled framework that aligns the numerical solver with the intrinsic properties of the diffusion trajectory. By analyzing the ODE dynamics, we show that efficient low-order solvers suffice in early high-noise stages while higher-order solvers can be progressively deployed to handle the increasing non-linearity of later stages. Furthermore, we formalize the scheduling by introducing a Wasserstein-bounded optimization framework. This method systematically derives adaptive timesteps that explicitly bound the local discretization error, ensuring the sampling process remains faithful to the underlying continuous dynamics. Without requiring additional training or architectural modifications, SDM achieves state-of-the-art performance across standard benchmarks, including an FID of 1.93 on CIFAR-10, 2.41 on FFHQ, and 1.98 on AFHQv2, with a reduced number of function evaluations compared to existing samplers. Our code is available at https://github.com/aiimaginglab/sdm.

</details>


### [125] [Unifying Model-Free Efficiency and Model-Based Representations via Latent Dynamics](https://arxiv.org/abs/2602.12643)
*Jashaswimalya Acharjee,Balaraman Ravindran*

Main category: cs.LG

TL;DR: ULD is a reinforcement learning algorithm that combines model-free efficiency with model-based representation power by embedding state-action pairs into a latent space where the value function is approximately linear, achieving strong performance across diverse domains with minimal tuning.


<details>
  <summary>Details</summary>
Motivation: To unify the efficiency of model-free RL methods with the representational strengths of model-based approaches, without the computational overhead of planning, enabling cross-domain competence with minimal hyperparameter tuning.

Method: Embeds state-action pairs into a latent space where the true value function is approximately linear; uses synchronized updates of encoder, value, and policy networks; includes auxiliary losses for short-horizon predictive dynamics and reward-scale normalization for stable learning.

Result: Matches or exceeds specialized model-free and general model-based baselines across 80 environments (Gym locomotion, DeepMind Control, Atari) with minimal tuning and reduced parameter footprint.

Conclusion: Value-aligned latent representations alone can deliver the adaptability and sample efficiency traditionally attributed to full model-based planning, enabling cross-domain competence without planning overhead.

Abstract: We present Unified Latent Dynamics (ULD), a novel reinforcement learning algorithm that unifies the efficiency of model-free methods with the representational strengths of model-based approaches, without incurring planning overhead. By embedding state-action pairs into a latent space in which the true value function is approximately linear, our method supports a single set of hyperparameters across diverse domains -- from continuous control with low-dimensional and pixel inputs to high-dimensional Atari games. We prove that, under mild conditions, the fixed point of our embedding-based temporal-difference updates coincides with that of a corresponding linear model-based value expansion, and we derive explicit error bounds relating embedding fidelity to value approximation quality. In practice, ULD employs synchronized updates of encoder, value, and policy networks, auxiliary losses for short-horizon predictive dynamics, and reward-scale normalization to ensure stable learning under sparse rewards. Evaluated on 80 environments spanning Gym locomotion, DeepMind Control (proprioceptive and visual), and Atari, our approach matches or exceeds the performance of specialized model-free and general model-based baselines -- achieving cross-domain competence with minimal tuning and a fraction of the parameter footprint. These results indicate that value-aligned latent representations alone can deliver the adaptability and sample efficiency traditionally attributed to full model-based planning.

</details>


### [126] [Uncovering spatial tissue domains and cell types in spatial omics through cross-scale profiling of cellular and genomic interactions](https://arxiv.org/abs/2602.12651)
*Rui Yan,Xiaohan Xing,Xun Wang,Zixia Zhou,Md Tauhidul Islam,Lei Xing*

Main category: cs.LG

TL;DR: CellScape is a deep learning framework that jointly models spatial interactions and genomic relationships in spatial transcriptomics data to improve pattern discovery and spatial domain segmentation.


<details>
  <summary>Details</summary>
Motivation: Spatial transcriptomics provides valuable spatial gene expression data but is noisy, large, and complex, making it difficult for existing methods to capture both spatial interactions and intrinsic genomic relationships, limiting biological pattern discovery.

Method: CellScape is a deep learning framework that jointly models cellular interactions in tissue space and genomic relationships among cells, producing comprehensive representations that integrate spatial signals with gene regulatory mechanisms.

Result: The technique uncovers biologically informative patterns that improve spatial domain segmentation and supports comprehensive spatial cellular analyses across diverse transcriptomics datasets.

Conclusion: CellScape offers an accurate and versatile framework for deep analysis and interpretation of spatial transcriptomics data by effectively integrating spatial and genomic information.

Abstract: Cellular identity and function are linked to both their intrinsic genomic makeup and extrinsic spatial context within the tissue microenvironment. Spatial transcriptomics (ST) offers an unprecedented opportunity to study this, providing in situ gene expression profiles at single-cell resolution and illuminating the spatial and functional organization of cells within tissues. However, a significant hurdle remains: ST data is inherently noisy, large, and structurally complex. This complexity makes it intractable for existing computational methods to effectively capture the interplay between spatial interactions and intrinsic genomic relationships, thus limiting our ability to discern critical biological patterns. Here, we present CellScape, a deep learning framework designed to overcome these limitations for high-performance ST data analysis and pattern discovery. CellScape jointly models cellular interactions in tissue space and genomic relationships among cells, producing comprehensive representations that seamlessly integrate spatial signals with underlying gene regulatory mechanisms. This technique uncovers biologically informative patterns that improve spatial domain segmentation and supports comprehensive spatial cellular analyses across diverse transcriptomics datasets, offering an accurate and versatile framework for deep analysis and interpretation of ST data.w

</details>


### [127] [SLA2: Sparse-Linear Attention with Learnable Routing and QAT](https://arxiv.org/abs/2602.12675)
*Jintao Zhang,Haoxu Wang,Kai Jiang,Kaiwen Zheng,Youhe Jiang,Ion Stoica,Jianfei Chen,Jun Zhu,Joseph E. Gonzalez*

Main category: cs.LG

TL;DR: SLA2 improves sparse-linear attention for diffusion models with learnable routing, better decomposition, and low-bit quantization for 18.6x speedup.


<details>
  <summary>Details</summary>
Motivation: Existing Sparse-Linear Attention (SLA) has two issues: (1) heuristic split based on attention-weight magnitude is suboptimal, and (2) formal analysis reveals mismatch between SLA and proper sparse-linear decomposition.

Method: SLA2 introduces three key improvements: (I) learnable router for dynamic sparse/linear branch selection, (II) more faithful sparse-linear formulation with learnable combination ratio, and (III) sparse + low-bit attention via quantization-aware fine-tuning.

Result: On video diffusion models, SLA2 achieves 97% attention sparsity, delivers 18.6x attention speedup while preserving generation quality.

Conclusion: SLA2 successfully addresses limitations of SLA through learnable components and better decomposition, enabling efficient attention computation without quality degradation in video generation.

Abstract: Sparse-Linear Attention (SLA) combines sparse and linear attention to accelerate diffusion models and has shown strong performance in video generation. However, (i) SLA relies on a heuristic split that assigns computations to the sparse or linear branch based on attention-weight magnitude, which can be suboptimal. Additionally, (ii) after formally analyzing the attention error in SLA, we identify a mismatch between SLA and a direct decomposition into sparse and linear attention. We propose SLA2, which introduces (I) a learnable router that dynamically selects whether each attention computation should use sparse or linear attention, (II) a more faithful and direct sparse-linear attention formulation that uses a learnable ratio to combine the sparse and linear attention branches, and (III) a sparse + low-bit attention design, where low-bit attention is introduced via quantization-aware fine-tuning to reduce quantization error. Experiments show that on video diffusion models, SLA2 can achieve 97% attention sparsity and deliver an 18.6x attention speedup while preserving generation quality.

</details>


### [128] [Flow Matching from Viewpoint of Proximal Operators](https://arxiv.org/abs/2602.12683)
*Kenji Fukumizu,Wei Huang,Han Bao,Shuntuo Xu,Nisha Chandramoothy*

Main category: cs.LG

TL;DR: OT-CFM reformulated with exact proximal formulation via extended Brenier potential, works for manifold-supported targets without density assumptions, and exhibits terminal normal hyperbolicity.


<details>
  <summary>Details</summary>
Motivation: To provide a rigorous mathematical foundation for Optimal Transport Conditional Flow Matching (OT-CFM) that doesn't require target distribution density assumptions and to understand its geometric properties for manifold-supported data.

Method: Reformulate OT-CFM using extended Brenier potential to obtain exact proximal formulation, analyze convergence of minibatch to population version, and use second epi-derivatives of convex potentials to prove terminal normal hyperbolicity.

Result: OT-CFM admits exact proximal formulation via extended Brenier potential, minibatch converges to population formulation, and for manifold-supported targets, the dynamics contracts exponentially in normal directions while remaining neutral along tangential directions after time rescaling.

Conclusion: The paper establishes rigorous mathematical foundations for OT-CFM, showing it can handle manifold-supported targets without density assumptions and exhibits desirable geometric contraction properties that make it suitable for learning data on manifolds.

Abstract: We reformulate Optimal Transport Conditional Flow Matching (OT-CFM), a class of dynamical generative models, showing that it admits an exact proximal formulation via an extended Brenier potential, without assuming that the target distribution has a density. In particular, the mapping to recover the target point is exactly given by a proximal operator, which yields an explicit proximal expression of the vector field. We also discuss the convergence of minibatch OT-CFM to the population formulation as the batch size increases. Finally, using second epi-derivatives of convex potentials, we prove that, for manifold-supported targets, OT-CFM is terminally normally hyperbolic: after time rescaling, the dynamics contracts exponentially in directions normal to the data manifold while remaining neutral along tangential directions.

</details>


### [129] [Trust the uncertain teacher: distilling dark knowledge via calibrated uncertainty](https://arxiv.org/abs/2602.12687)
*Jeonghyun Kim,SooKyung Kim,Richeng Xuan,Hyunsoo Cho*

Main category: cs.LG

TL;DR: CUD (Calibrated Uncertainty Distillation) improves knowledge distillation by addressing teacher overconfidence, making dark knowledge more accessible through calibrated uncertainty targets.


<details>
  <summary>Details</summary>
Motivation: Teachers trained with conventional cross-entropy produce overconfident, sharp probability distributions that lack informative uncertainty signals, especially problematic in high-cardinality tasks and under distribution shift, limiting effective knowledge transfer to students.

Method: CUD shapes teacher's predictive distribution before transfer, encouraging teachers to reveal informative uncertainty and guiding students to learn from calibrated targets rather than sharpened certainty, balancing accuracy and calibration.

Result: CUD yields students that are more accurate, better calibrated under distribution shift, and more reliable on ambiguous, long-tail inputs across diverse benchmarks.

Conclusion: By addressing teacher overconfidence through calibrated uncertainty distillation, CUD makes dark knowledge more faithfully accessible, improving student performance and robustness in real-world conditions.

Abstract: The core of knowledge distillation lies in transferring the teacher's rich 'dark knowledge'-subtle probabilistic patterns that reveal how classes are related and the distribution of uncertainties. While this idea is well established, teachers trained with conventional cross-entropy often fail to preserve such signals. Their distributions collapse into sharp, overconfident peaks that appear decisive but are in fact brittle, offering little beyond the hard label or subtly hindering representation-level transfer. This overconfidence is especially problematic in high-cardinality tasks, where the nuances among many plausible classes matter most for guiding a compact student. Moreover, such brittle targets reduce robustness under distribution shift, leaving students vulnerable to miscalibration in real-world conditions. To address this limitation, we revisit distillation from a distributional perspective and propose Calibrated Uncertainty Distillation (CUD), a framework designed to make dark knowledge more faithfully accessible. Instead of uncritically adopting the teacher's overconfidence, CUD encourages teachers to reveal uncertainty where it is informative and guides students to learn from targets that are calibrated rather than sharpened certainty. By directly shaping the teacher's predictive distribution before transfer, our approach balances accuracy and calibration, allowing students to benefit from both confident signals on easy cases and structured uncertainty on hard ones. Across diverse benchmarks, CUD yields students that are not only more accurate, but also more calibrated under shift and more reliable on ambiguous, long-tail inputs.

</details>


### [130] [Leverage-Weighted Conformal Prediction](https://arxiv.org/abs/2602.12693)
*Shreyas Fadnavis*

Main category: cs.LG

TL;DR: LWCP is a conformal prediction method that weights nonconformity scores by leverage scores to achieve adaptive prediction intervals without auxiliary models, maintaining marginal coverage while improving conditional coverage.


<details>
  <summary>Details</summary>
Motivation: Standard split conformal prediction produces constant-width intervals that overcover in low-variance regions and undercover in high-variance regions. Existing adaptive methods require training auxiliary models, which adds complexity and computational cost.

Method: Leverage-Weighted Conformal Prediction (LWCP) weights nonconformity scores by a function of the statistical leverage (diagonal of the hat matrix). This derives adaptivity from the geometry of the design matrix rather than from auxiliary model fitting. The method preserves finite-sample marginal validity for any weight function and adds negligible computational overhead to vanilla CP.

Result: LWCP achieves asymptotically optimal conditional coverage at essentially no width cost when heteroscedasticity factors through leverage. It recovers the form and width of classical prediction intervals under Gaussian assumptions while retaining distribution-free guarantees. Randomized leverage approximations preserve coverage exactly with controlled width perturbation. Experiments confirm substantial reductions in conditional coverage disparity.

Conclusion: LWCP provides an efficient, distribution-free method for adaptive prediction intervals that eliminates the persistent conditional coverage gap of vanilla CP without requiring auxiliary models or significant computational overhead.

Abstract: Split conformal prediction provides distribution-free prediction intervals with finite-sample marginal coverage, but produces constant-width intervals that overcover in low-variance regions and undercover in high-variance regions. Existing adaptive methods require training auxiliary models. We propose Leverage-Weighted Conformal Prediction (LWCP), which weights nonconformity scores by a function of the statistical leverage -- the diagonal of the hat matrix -- deriving adaptivity from the geometry of the design matrix rather than from auxiliary model fitting. We prove that LWCP preserves finite-sample marginal validity for any weight function; achieves asymptotically optimal conditional coverage at essentially no width cost when heteroscedasticity factors through leverage; and recovers the form and width of classical prediction intervals under Gaussian assumptions while retaining distribution-free guarantees. We further establish that randomized leverage approximations preserve coverage exactly with controlled width perturbation, and that vanilla CP suffers a persistent, sample-size-independent conditional coverage gap that LWCP eliminates. The method requires no hyperparameters beyond the choice of weight function and adds negligible computational overhead to vanilla CP. Experiments on synthetic and real data confirm the theoretical predictions, demonstrating substantial reductions in conditional coverage disparity across settings.

</details>


### [131] [SWING: Unlocking Implicit Graph Representations for Graph Random Features](https://arxiv.org/abs/2602.12703)
*Alessandro Manenti,Avinava Dubey,Arijit Sehanobish,Cesare Alippi,Krzysztof Choromanski*

Main category: cs.LG

TL;DR: SWING proposes space walks for implicit network graphs, enabling computations on graphs defined by node feature functions without materializing the graph structure.


<details>
  <summary>Details</summary>
Motivation: Many graphs in machine learning are implicitly defined through node feature vectors (e.g., ε-neighborhood graphs), but existing methods require materializing the graph structure, which is inefficient for large-scale applications.

Method: Instead of walking on graph nodes, SWING conducts walks in the continuous embedding space using Gumbel-softmax sampling with linearized kernels via random features and importance sampling.

Result: SWING provides accurate and efficient approximations of combinatorial calculations on implicit graphs, is accelerator-friendly, and avoids graph materialization.

Conclusion: SWING introduces a novel algorithmic framework for implicit graph computations by leveraging connections between implicitly defined graphs and Fourier analysis, enabling efficient processing without explicit graph construction.

Abstract: We propose SWING: Space Walks for Implicit Network Graphs, a new class of algorithms for computations involving Graph Random Features on graphs given by implicit representations (i-graphs), where edge-weights are defined as bi-variate functions of feature vectors in the corresponding nodes. Those classes of graphs include several prominent examples, such as: $ε$-neighborhood graphs, used on regular basis in machine learning. Rather than conducting walks on graphs' nodes, those methods rely on walks in continuous spaces, in which those graphs are embedded. To accurately and efficiently approximate original combinatorial calculations, SWING applies customized Gumbel-softmax sampling mechanism with linearized kernels, obtained via random features coupled with importance sampling techniques. This algorithm is of its own interest. SWING relies on the deep connection between implicitly defined graphs and Fourier analysis, presented in this paper. SWING is accelerator-friendly and does not require input graph materialization. We provide detailed analysis of SWING and complement it with thorough experiments on different classes of i-graphs.

</details>


### [132] [QTabGAN: A Hybrid Quantum-Classical GAN for Tabular Data Synthesis](https://arxiv.org/abs/2602.12704)
*Subhangi Kumari,Rakesh Achutha,Vignesh Sivaraman*

Main category: cs.LG

TL;DR: QTabGAN is a quantum-classical hybrid GAN framework for synthesizing realistic tabular data, achieving up to 54.07% improvement over state-of-the-art models on classification tasks.


<details>
  <summary>Details</summary>
Motivation: Tabular data synthesis is challenging due to heterogeneous feature types and high dimensionality, especially in scenarios with scarce real data or privacy constraints that limit data availability.

Method: Hybrid quantum-classical GAN framework that uses quantum circuits to learn complex data distributions, then maps these to tabular features using classical neural networks.

Result: QTabGAN achieves up to 54.07% improvement across various classification datasets and evaluation metrics compared to leading state-of-the-art generative models.

Conclusion: QTabGAN establishes a scalable quantum approach to tabular data synthesis and demonstrates significant potential for quantum-assisted generative modeling in data-scarce or privacy-constrained settings.

Abstract: Synthesizing realistic tabular data is challenging due to heterogeneous feature types and high dimensionality. We introduce QTabGAN, a hybrid quantum-classical generative adversarial framework for tabular data synthesis. QTabGAN is especially designed for settings where real data are scarce or restricted by privacy constraints. The model exploits the expressive power of quantum circuits to learn complex data distributions, which are then mapped to tabular features using classical neural networks. We evaluate QTabGAN on multiple classification and regression datasets and benchmark it against leading state-of-the-art generative models. Experiments show that QTabGAN achieves up to 54.07% improvement across various classification datasets and evaluation metrics, thus establishing a scalable quantum approach to tabular data synthesis and highlighting its potential for quantum-assisted generative modelling.

</details>


### [133] [Physics-Informed Laplace Neural Operator for Solving Partial Differential Equations](https://arxiv.org/abs/2602.12706)
*Heechang Kim,Qianying Cao,Hyomin Shin,Seungchul Lee,George Em Karniadakis,Minseok Choi*

Main category: cs.LG

TL;DR: PILNO enhances Laplace Neural Operators with physics-informed training using virtual inputs and temporal-causality weighting to improve accuracy in small-data regimes and OOD generalization.


<details>
  <summary>Details</summary>
Motivation: Data-driven neural operators require extensive training data and generalize poorly in small-data regimes and under unseen input functions. There's a need for more data-efficient and robust models that can handle out-of-distribution scenarios.

Method: Proposes Physics-Informed Laplace Neural Operator (PILNO) with: 1) Advanced LNO backbone replacing steady-state branch with FNO-style Fourier multiplier; 2) Virtual inputs - unlabeled ensemble spanning broad spectral range for physics-only supervision; 3) Temporal-causality weighting - time-decaying reweighting prioritizing early-time dynamics.

Result: PILNO consistently improves accuracy in small-data settings (N_train ≤ 27), reduces run-to-run variability across random seeds, and achieves stronger OOD generalization than purely data-driven baselines across four PDE benchmarks.

Conclusion: Physics-informed training with virtual inputs and temporal-causality weighting makes neural operators more data-efficient, robust, and better at handling out-of-distribution scenarios while maintaining computational advantages.

Abstract: Neural operators have emerged as fast surrogate solvers for parametric partial differential equations (PDEs). However, purely data-driven models often require extensive training data and can generalize poorly, especially in small-data regimes and under unseen (out-of-distribution) input functions that are not represented in the training data. To address these limitations, we propose the Physics-Informed Laplace Neural Operator (PILNO), which enhances the Laplace Neural Operator (LNO) by embedding governing physics into training through PDE, boundary condition, and initial condition residuals. To improve expressivity, we first introduce an Advanced LNO (ALNO) backbone that retains a pole-residue transient representation while replacing the steady-state branch with an FNO-style Fourier multiplier. To make physics-informed training both data-efficient and robust, PILNO further leverages (i) virtual inputs: an unlabeled ensemble of input functions spanning a broad spectral range that provides abundant physics-only supervision and explicitly targets out-of-distribution (OOD) regimes; and (ii) temporal-causality weighting: a time-decaying reweighting of the physics residual that prioritizes early-time dynamics and stabilizes optimization for time-dependent PDEs. Across four representative benchmarks -- Burgers' equation, Darcy flow, a reaction-diffusion system, and a forced KdV equation -- PILNO consistently improves accuracy in small-data settings (e.g., N_train <= 27), reduces run-to-run variability across random seeds, and achieves stronger OOD generalization than purely data-driven baselines.

</details>


### [134] [Mixture of Predefined Experts: Maximizing Data Usage on Vertical Federated Learning](https://arxiv.org/abs/2602.12708)
*Jon Irureta,Gorka Azkune,Jon Imaz,Aizea Lojo,Javier Fernandez-Marques*

Main category: cs.LG

TL;DR: Split-MoPE: A novel VFL framework combining Split Learning with Mixture of Predefined Experts to handle sample misalignment, achieving SOTA performance in single communication round with robustness and interpretability.


<details>
  <summary>Details</summary>
Motivation: Existing VFL frameworks assume full sample alignment, which rarely holds in real-world scenarios like finance and healthcare. There's a need for a practical solution that handles sample misalignment while maintaining privacy and efficiency.

Method: Proposes Split-MoPE framework integrating Split Learning with Mixture of Predefined Experts (MoPE). Unlike standard MoE with dynamic routing, MoPE uses predefined experts for specific data alignments, maximizing data usage without requiring full sample overlap. Leverages pretrained encoders and operates in single communication round.

Result: Achieves state-of-the-art performance on vision (CIFAR-10/100) and tabular (Breast Cancer Wisconsin) datasets. Consistently outperforms systems like LASER and Vertical SplitNN, especially in scenarios with high data missingness. Reduces communication footprint significantly compared to multi-round training.

Conclusion: Split-MoPE effectively bridges the gap between theoretical VFL assumptions and practical deployment by handling sample misalignment while providing robustness against malicious participants, interpretability of collaborator contributions, and communication efficiency.

Abstract: Vertical Federated Learning (VFL) has emerged as a critical paradigm for collaborative model training in privacy-sensitive domains such as finance and healthcare. However, most existing VFL frameworks rely on the idealized assumption of full sample alignment across participants, a premise that rarely holds in real-world scenarios. To bridge this gap, this work introduces Split-MoPE, a novel framework that integrates Split Learning with a specialized Mixture of Predefined Experts (MoPE) architecture. Unlike standard Mixture of Experts (MoE), where routing is learned dynamically, MoPE uses predefined experts to process specific data alignments, effectively maximizing data usage during both training and inference without requiring full sample overlap. By leveraging pretrained encoders for target data domains, Split-MoPE achieves state-of-the-art performance in a single communication round, significantly reducing the communication footprint compared to multi-round end-to-end training. Furthermore, unlike existing proposals that address sample misalignment, this novel architecture provides inherent robustness against malicious or noisy participants and offers per-sample interpretability by quantifying each collaborator's contribution to each prediction. Extensive evaluations on vision (CIFAR-10/100) and tabular (Breast Cancer Wisconsin) datasets demonstrate that Split-MoPE consistently outperforms state-of-the-art systems such as LASER and Vertical SplitNN, particularly in challenging scenarios with high data missingness.

</details>


### [135] [Can Neural Networks Provide Latent Embeddings for Telemetry-Aware Greedy Routing?](https://arxiv.org/abs/2602.12798)
*Andreas Boltres,Niklas Freymuth,Gerhard Neumann*

Main category: cs.LG

TL;DR: Placer uses Message Passing Networks to create node embeddings for explainable, greedy routing decisions without solving all-pairs shortest paths.


<details>
  <summary>Details</summary>
Motivation: Current ML-based routing approaches sacrifice explainability due to black-box neural modules, making it difficult to understand how network events influence routing decisions.

Method: Uses Message Passing Networks to transform network states into latent node embeddings, enabling quick greedy next-hop routing without solving the all-pairs shortest paths problem.

Result: Provides explainable routing decisions with visualization capabilities to show how network events shape routing choices, while maintaining efficiency through greedy routing.

Conclusion: Placer offers a novel approach that combines the benefits of ML-based routing with explainability, addressing the black-box problem in current neural routing systems.

Abstract: Telemetry-Aware routing promises to increase efficacy and responsiveness to traffic surges in computer networks. Recent research leverages Machine Learning to deal with the complex dependency between network state and routing, but sacrifices explainability of routing decisions due to the black-box nature of the proposed neural routing modules. We propose \emph{Placer}, a novel algorithm using Message Passing Networks to transform network states into latent node embeddings. These embeddings facilitate quick greedy next-hop routing without directly solving the all-pairs shortest paths problem, and let us visualize how certain network events shape routing decisions.

</details>


### [136] [ADEPT: RL-Aligned Agentic Decoding of Emotion via Evidence Probing Tools -- From Consensus Learning to Ambiguity-Driven Emotion Reasoning](https://arxiv.org/abs/2602.12714)
*Esther Sun,Bo-Hao Su,Abinay Reddy Naini,Shinji Watanabe,Carlos Busso*

Main category: cs.LG

TL;DR: ADEPT is a framework that transforms Speech LLMs into agents for emotion recognition through multi-turn inquiry with acoustic/semantic probing tools, shifting from consensus learning to ambiguity-driven reasoning.


<details>
  <summary>Details</summary>
Motivation: Current Speech LLMs produce ungrounded, text-biased emotion judgments without verifiable acoustic evidence, while self-supervised speech encoders provide strong acoustic representations but lack interpretability. There's a need to bridge this gap and better handle the inherent complexity and co-occurrence of emotions in human affect.

Method: ADEPT reframes emotion recognition as a multi-turn inquiry process where an SLLM agent maintains an evolving candidate emotion set and adaptively invokes dedicated semantic and acoustic probing tools. It uses a structured pipeline of candidate generation, evidence collection, and adjudication, integrating Group Relative Policy Optimization (GRPO) with an Evidence Trust Gate to couple tool-usage with prediction quality.

Result: ADEPT improves primary emotion accuracy in most settings while substantially improving minor emotion characterization, producing explanations grounded in auditable acoustic and semantic evidence.

Conclusion: The framework enables a paradigm shift from consensus learning to ambiguity-driven emotion reasoning, treating minority annotations as informative perceptual signals rather than noise, and provides evidence-grounded reasoning for emotion recognition in speech.

Abstract: Speech Large Language Models (SLLMs) enable high-level emotion reasoning but often produce ungrounded, text-biased judgments without verifiable acoustic evidence. In contrast, self-supervised speech encoders such as WavLM provide strong acoustic representations yet remain opaque discriminative models with limited interpretability. To bridge this gap, we introduce ADEPT (Agentic Decoding of Emotion via Evidence Probing Tools), a framework that reframes emotion recognition as a multi-turn inquiry process rather than a single-pass prediction. ADEPT transforms an SLLM into an agent that maintains an evolving candidate emotion set and adaptively invokes dedicated semantic and acoustic probing tools within a structured pipeline of candidate generation, evidence collection, and adjudication. Crucially, ADEPT enables a paradigm shift from consensus learning to ambiguity-driven emotion reasoning. Since human affect exhibits inherent complexity and frequent co-occurrence of emotions, we treat minority annotations as informative perceptual signals rather than discarding them as noise. Finally, we integrate Group Relative Policy Optimization (GRPO) with an Evidence Trust Gate to explicitly couple tool-usage behaviors with prediction quality and enforce evidence-grounded reasoning. Experiments show that ADEPT improves primary emotion accuracy in most settings while substantially improving minor emotion characterization, producing explanations grounded in auditable acoustic and semantic evidence.

</details>


### [137] [GRAIL: Geometry-Aware Retrieval-Augmented Inference with LLMs over Hyperbolic Representations of Patient Trajectories](https://arxiv.org/abs/2602.12828)
*Zhan Qu,Michael Färber*

Main category: cs.LG

TL;DR: GRAIL framework improves next-visit clinical event prediction using structured geometric representations and structure-aware retrieval to handle sparse EHR data and hierarchical medical vocabularies.


<details>
  <summary>Details</summary>
Motivation: Predicting clinical events from EHRs is challenging due to sparse multi-type events, hierarchical medical vocabularies, and LLM hallucination issues when reasoning over long structured histories.

Method: GRAIL constructs a unified clinical graph combining coding-system hierarchies with temporal associations, embeds it in hyperbolic space, summarizes visits as probabilistic Central Events, and uses structure-aware retrieval with optional LLM reranking.

Result: Experiments on MIMIC-IV show GRAIL consistently improves multi-type next-visit prediction and yields more hierarchy-consistent forecasts.

Conclusion: GRAIL effectively addresses EHR prediction challenges through structured geometric representations and retrieval-based approaches, outperforming existing methods while maintaining clinical plausibility.

Abstract: Predicting future clinical events from longitudinal electronic health records (EHRs) is challenging due to sparse multi-type clinical events, hierarchical medical vocabularies, and the tendency of large language models (LLMs) to hallucinate when reasoning over long structured histories. We study next-visit event prediction, which aims to forecast a patient's upcoming clinical events based on prior visits. We propose GRAIL, a framework that models longitudinal EHRs using structured geometric representations and structure-aware retrieval. GRAIL constructs a unified clinical graph by combining deterministic coding-system hierarchies with data-driven temporal associations across event types, embeds this graph in hyperbolic space, and summarizes each visit as a probabilistic Central Event that denoises sparse observations. At inference time, GRAIL retrieves a structured set of clinically plausible future events aligned with hierarchical and temporal progression, and optionally refines their ranking using an LLM as a constrained inference-time reranker. Experiments on MIMIC-IV show that GRAIL consistently improves multi-type next-visit prediction and yields more hierarchy-consistent forecasts.

</details>


### [138] [Adaptive Structured Pruning of Convolutional Neural Networks for Time Series Classification](https://arxiv.org/abs/2602.12744)
*Javidan Abdullayev,Maxime Devanne,Cyril Meyer,Ali Ismail-Fawaz,Jonathan Weber,Germain Forestier*

Main category: cs.LG

TL;DR: Dynamic Structured Pruning (DSP) is an automatic pruning framework for time series classification models that removes redundant filters without manual hyperparameter tuning, achieving 58-75% compression while maintaining accuracy.


<details>
  <summary>Details</summary>
Motivation: Deep learning models for Time Series Classification have high computational/memory requirements that limit deployment on resource-constrained devices. Existing pruning methods rely on manually tuned hyperparameters (pruning ratios) which limit scalability and generalization across datasets.

Method: DSP introduces an instance-wise sparsity loss during training to induce channel-level sparsity, followed by global activation analysis to identify and prune redundant filters automatically without needing predefined pruning ratios.

Result: Validated on 128 UCR datasets using LITETime and InceptionTime architectures. Achieved average compression of 58% for LITETime and 75% for InceptionTime while maintaining classification accuracy. Redundancy analyses confirm DSP produces compact and informative representations.

Conclusion: DSP offers a practical path for scalable and efficient deep TSC deployment by automatically pruning redundant filters without manual hyperparameter tuning, addressing computational bottlenecks for resource-constrained devices.

Abstract: Deep learning models for Time Series Classification (TSC) have achieved strong predictive performance but their high computational and memory requirements often limit deployment on resource-constrained devices. While structured pruning can address these issues by removing redundant filters, existing methods typically rely on manually tuned hyperparameters such as pruning ratios which limit scalability and generalization across datasets. In this work, we propose Dynamic Structured Pruning (DSP), a fully automatic, structured pruning framework for convolution-based TSC models. DSP introduces an instance-wise sparsity loss during training to induce channel-level sparsity, followed by a global activation analysis to identify and prune redundant filters without needing any predefined pruning ratio. This work tackles computational bottlenecks of deep TSC models for deployment on resource-constrained devices. We validate DSP on 128 UCR datasets using two different deep state-of-the-art architectures: LITETime and InceptionTime. Our approach achieves an average compression of 58% for LITETime and 75% for InceptionTime architectures while maintaining classification accuracy. Redundancy analyses confirm that DSP produces compact and informative representations, offering a practical path for scalable and efficient deep TSC deployment.

</details>


### [139] [FLAC: Maximum Entropy RL via Kinetic Energy Regularized Bridge Matching](https://arxiv.org/abs/2602.12829)
*Lei Lv,Yunfei Li,Yu Luo,Fuchun Sun,Xiao Ma*

Main category: cs.LG

TL;DR: FLAC is a likelihood-free RL framework that uses kinetic energy regularization instead of explicit action densities for maximum entropy control with generative policies.


<details>
  <summary>Details</summary>
Motivation: Generative policies like diffusion models are expressive for continuous control but complicate Maximum Entropy RL because their action log-densities are not directly accessible, requiring a likelihood-free approach.

Method: Formulates policy optimization as a Generalized Schrödinger Bridge problem relative to a high-entropy reference process, using kinetic energy of the velocity field as a proxy for divergence from reference, with energy-regularized policy iteration and Lagrangian dual mechanism.

Result: FLAC achieves superior or comparable performance on high-dimensional benchmarks relative to strong baselines while avoiding explicit density estimation.

Conclusion: FLAC provides a physically grounded, likelihood-free framework for maximum entropy RL with generative policies by using kinetic energy regularization as a proxy for divergence from high-entropy reference processes.

Abstract: Iterative generative policies, such as diffusion models and flow matching, offer superior expressivity for continuous control but complicate Maximum Entropy Reinforcement Learning because their action log-densities are not directly accessible. To address this, we propose Field Least-Energy Actor-Critic (FLAC), a likelihood-free framework that regulates policy stochasticity by penalizing the kinetic energy of the velocity field. Our key insight is to formulate policy optimization as a Generalized Schrödinger Bridge (GSB) problem relative to a high-entropy reference process (e.g., uniform). Under this view, the maximum-entropy principle emerges naturally as staying close to a high-entropy reference while optimizing return, without requiring explicit action densities. In this framework, kinetic energy serves as a physically grounded proxy for divergence from the reference: minimizing path-space energy bounds the deviation of the induced terminal action distribution. Building on this view, we derive an energy-regularized policy iteration scheme and a practical off-policy algorithm that automatically tunes the kinetic energy via a Lagrangian dual mechanism. Empirically, FLAC achieves superior or comparable performance on high-dimensional benchmarks relative to strong baselines, while avoiding explicit density estimation.

</details>


### [140] [Hierarchical Successor Representation for Robust Transfer](https://arxiv.org/abs/2602.12753)
*Changmin Yu,Máté Lengyel*

Main category: cs.LG

TL;DR: The paper proposes Hierarchical Successor Representation (HSR) with NMF to create stable, sparse, policy-agnostic state representations that enable efficient task transfer and exploration in complex environments.


<details>
  <summary>Details</summary>
Motivation: Classical Successor Representation (SR) has limitations: 1) policy dependence makes representations obsolete when policies change, 2) suffers from spectral diffusion in complex environments leading to dense, overlapping features that scale poorly.

Method: Proposes Hierarchical Successor Representation (HSR) that incorporates temporal abstractions into predictive representations. Applies non-negative matrix factorisation (NMF) to HSR to obtain sparse, low-rank state representations.

Result: HSR-NMF learns stable state features robust to policy changes, enables highly sample-efficient transfer to novel tasks in multi-compartmental environments, discovers interpretable topological structures, and provides a policy-agnostic hierarchical map.

Conclusion: HSR bridges model-free optimality and model-based flexibility, provides useful basis for task-transfer, and its temporally extended predictive structure can drive efficient exploration in large, procedurally generated environments.

Abstract: The successor representation (SR) provides a powerful framework for decoupling predictive dynamics from rewards, enabling rapid generalisation across reward configurations. However, the classical SR is limited by its inherent policy dependence: policies change due to ongoing learning, environmental non-stationarities, and changes in task demands, making established predictive representations obsolete. Furthermore, in topologically complex environments, SRs suffer from spectral diffusion, leading to dense and overlapping features that scale poorly. Here we propose the Hierarchical Successor Representation (HSR) for overcoming these limitations. By incorporating temporal abstractions into the construction of predictive representations, HSR learns stable state features which are robust to task-induced policy changes. Applying non-negative matrix factorisation (NMF) to the HSR yields a sparse, low-rank state representation that facilitates highly sample-efficient transfer to novel tasks in multi-compartmental environments. Further analysis reveals that HSR-NMF discovers interpretable topological structures, providing a policy-agnostic hierarchical map that effectively bridges model-free optimality and model-based flexibility. Beyond providing a useful basis for task-transfer, we show that HSR's temporally extended predictive structure can also be leveraged to drive efficient exploration, effectively scaling to large, procedurally generated environments.

</details>


### [141] [TRACE: Temporal Reasoning via Agentic Context Evolution for Streaming Electronic Health Records (EHRs)](https://arxiv.org/abs/2602.12833)
*Zhan Qu,Michael Färber*

Main category: cs.LG

TL;DR: TRACE enables frozen LLMs to perform temporal clinical reasoning using a dual-memory architecture and agentic components, improving accuracy and safety on longitudinal patient data without fine-tuning.


<details>
  <summary>Details</summary>
Motivation: LLMs struggle with longitudinal patient trajectories due to evolving clinical states, irregular timing, and heterogeneous events. Existing adaptation methods (fine-tuning, retrieval) have computational, privacy, and stability limitations in long contexts.

Method: TRACE uses a dual-memory architecture: static Global Protocol (institutional rules) and dynamic Individual Protocol (patient-specific state). Four agentic components (Router, Reasoner, Auditor, Steward) coordinate over this structured memory to support temporal inference and state evolution with bounded inference cost.

Result: On MIMIC-IV longitudinal clinical event streams, TRACE significantly improves next-event prediction accuracy, protocol adherence, and clinical safety over long-context and retrieval-augmented baselines, while producing interpretable reasoning traces.

Conclusion: TRACE enables effective temporal clinical reasoning with frozen LLMs through structured context maintenance rather than parameter updates or extended context windows, offering improved performance, safety, and interpretability for longitudinal patient care.

Abstract: Large Language Models (LLMs) encode extensive medical knowledge but struggle to apply it reliably to longitudinal patient trajectories, where evolving clinical states, irregular timing, and heterogeneous events degrade performance over time. Existing adaptation strategies rely on fine-tuning or retrieval-based augmentation, which introduce computational overhead, privacy constraints, or instability under long contexts. We introduce TRACE (Temporal Reasoning via Agentic Context Evolution), a framework that enables temporal clinical reasoning with frozen LLMs by explicitly structuring and maintaining context rather than extending context windows or updating parameters. TRACE operates over a dual-memory architecture consisting of a static Global Protocol encoding institutional clinical rules and a dynamic Individual Protocol tracking patient-specific state. Four agentic components, Router, Reasoner, Auditor, and Steward, coordinate over this structured memory to support temporal inference and state evolution. The framework maintains bounded inference cost via structured state compression and selectively audits safety-critical clinical decisions. Evaluated on longitudinal clinical event streams from MIMIC-IV, TRACE significantly improves next-event prediction accuracy, protocol adherence, and clinical safety over long-context and retrieval-augmented baselines, while producing interpretable and auditable reasoning traces.

</details>


### [142] [Closing the Loop: A Control-Theoretic Framework for Provably Stable Time Series Forecasting with LLMs](https://arxiv.org/abs/2602.12756)
*Xingyu Zhang,Hanyun Du,Zeen Song,Jianqi Zhang,Changwen Zheng,Wenwen Qiang*

Main category: cs.LG

TL;DR: F-LLM introduces a closed-loop feedback control framework for LLM-based time series forecasting to mitigate error accumulation in autoregressive generation.


<details>
  <summary>Details</summary>
Motivation: Existing LLM-based time series forecasting methods suffer from exposure bias and error accumulation due to their open-loop autoregressive generation, where minor early errors cascade into significant trajectory drift over long horizons.

Method: Reformulates autoregressive forecasting through control theory, proposing F-LLM with a closed-loop framework featuring a learnable residual estimator (Observer) and feedback controller to actively stabilize trajectories.

Result: Theoretical guarantee of uniformly bounded error under local Lipschitz constraints, with extensive experiments showing F-LLM significantly mitigates error propagation and achieves strong performance on time series benchmarks.

Conclusion: F-LLM's closed-loop feedback mechanism provides a theoretically grounded solution to error accumulation in LLM-based forecasting, offering improved stability and performance over traditional open-loop approaches.

Abstract: Large Language Models (LLMs) have recently shown exceptional potential in time series forecasting, leveraging their inherent sequential reasoning capabilities to model complex temporal dynamics. However, existing approaches typically employ a naive autoregressive generation strategy. We identify a critical theoretical flaw in this paradigm: during inference, the model operates in an open-loop manner, consuming its own generated outputs recursively. This leads to inevitable error accumulation (exposure bias), where minor early deviations cascade into significant trajectory drift over long horizons. In this paper, we reformulate autoregressive forecasting through the lens of control theory, proposing \textbf{F-LLM} (Feedback-driven LLM), a novel closed-loop framework. Unlike standard methods that passively propagate errors, F-LLM actively stabilizes the trajectory via a learnable residual estimator (Observer) and a feedback controller. Furthermore, we provide a theoretical guarantee that our closed-loop mechanism ensures uniformly bounded error, provided the base model satisfies a local Lipschitz constraint. Extensive experiments demonstrate that F-LLM significantly mitigates error propagation, achieving good performance on time series benchmarks.

</details>


### [143] [Amortized Reasoning Tree Search: Decoupling Proposal and Decision in Large Language Models](https://arxiv.org/abs/2602.12846)
*Zesheng Hong,Jiadong Yu,Hui Pan*

Main category: cs.LG

TL;DR: RLVR suppresses rare reasoning paths; ARTS decouples generation from verification using Flow Matching to preserve diversity and recover performance on long-tail problems.


<details>
  <summary>Details</summary>
Motivation: RLVR (Reinforcement Learning with Verifiable Rewards) systematically suppresses valid but rare reasoning paths in LLMs, causing a "Normalization Squeeze" that drives rare correct traces to statistical extinction.

Method: Proposes Amortized Reasoning Tree Search (ARTS) that decouples generation from verification. Uses Flow Matching objective to repurpose the verifier to estimate probability flow conservation, enabling navigation through sparse, high-entropy search spaces.

Result: ARTS achieves 74.6% performance on MATH-500 benchmark (matching fully fine-tuned policies at 74.7%). Crucially, recovers significant performance on long-tail subset where RL optimization collapses to 0%, demonstrating robustness for complex reasoning tasks.

Conclusion: Disentangling verification from generation offers a more robust pathway for solving complex reasoning tasks by preserving the base model's latent diversity and preventing suppression of rare but valid reasoning paths.

Abstract: Reinforcement Learning with Verifiable Rewards (RLVR) has established itself as the dominant paradigm for instilling rigorous reasoning capabilities in Large Language Models. While effective at amplifying dominant behaviors, we identify a critical pathology in this alignment process: the systematic suppression of valid but rare (low-likelihood under the base model distribution) reasoning paths. We theoretically characterize this phenomenon as a "Normalization Squeeze," where the interplay between mode-seeking policy gradients and finite sampling acts as a high-pass likelihood filter, driving the probability of rare correct traces to statistical extinction. To counteract this collapse without discarding the base model's latent diversity, we propose Amortized Reasoning Tree Search (ARTS). Unlike standard approaches that force internalization via parameter updates, ARTS prioritizes deliberation by decoupling generation from verification. We introduce a Flow Matching objective that repurposes the verifier to estimate the conservation of probability flow, enabling robust navigation through sparse, high-entropy search spaces where traditional discriminative objectives fail. Extensive experiments on the MATH-500 benchmark demonstrate that ARTS achieves a performance of 74.6% (BoN@16), effectively matching fully fine-tuned policies (74.7%) without modifying the generative backbone. Crucially, on the long-tail subset where coupled RL optimization collapses to 0% pass@k, ARTS uniquely recovers significant performance, suggesting that disentangling verification from generation offers a more robust pathway for solving complex reasoning tasks.

</details>


### [144] [X-VORTEX: Spatio-Temporal Contrastive Learning for Wake Vortex Trajectory Forecasting](https://arxiv.org/abs/2602.12869)
*Zhan Qu,Michael Färber*

Main category: cs.LG

TL;DR: X-VORTEX is a spatio-temporal contrastive learning framework that learns physics-aware representations from unlabeled LiDAR point cloud sequences to track aircraft wake vortices, achieving superior performance with only 1% of labeled data.


<details>
  <summary>Details</summary>
Motivation: Wake vortices pose major safety and capacity challenges for air traffic management. Current approaches treat each LiDAR scan independently as supervised segmentation problems, which overlooks temporal structure and don't scale to vast unlabeled archives. Point-wise annotation is prohibitively expensive, and vortex signatures fade over time due to atmospheric turbulence.

Method: X-VORTEX uses spatio-temporal contrastive learning grounded in Augmentation Overlap Theory. It constructs paired inputs from the same flight event: a weakly perturbed sequence and a strongly augmented counterpart via temporal subsampling and spatial masking. The architecture includes a time-distributed geometric encoder for per-scan features and a sequential aggregator to model evolving vortex states across variable-length sequences.

Result: Evaluated on a real-world dataset of over one million LiDAR scans, X-VORTEX achieves superior vortex center localization while using only 1% of the labeled data required by supervised baselines. The learned representations also support accurate trajectory forecasting.

Conclusion: X-VORTEX demonstrates that spatio-temporal contrastive learning can effectively address the challenges of sensor sparsity and time-varying vortex dynamics in wake vortex tracking, enabling scalable analysis of vast unlabeled LiDAR archives with minimal supervision.

Abstract: Wake vortices are strong, coherent air turbulences created by aircraft, and they pose a major safety and capacity challenge for air traffic management. Tracking how vortices move, weaken, and dissipate over time from LiDAR measurements is still difficult because scans are sparse, vortex signatures fade as the flow breaks down under atmospheric turbulence and instabilities, and point-wise annotation is prohibitively expensive. Existing approaches largely treat each scan as an independent, fully supervised segmentation problem, which overlooks temporal structure and does not scale to the vast unlabeled archives collected in practice. We present X-VORTEX, a spatio-temporal contrastive learning framework grounded in Augmentation Overlap Theory that learns physics-aware representations from unlabeled LiDAR point cloud sequences. X-VORTEX addresses two core challenges: sensor sparsity and time-varying vortex dynamics. It constructs paired inputs from the same underlying flight event by combining a weakly perturbed sequence with a strongly augmented counterpart produced via temporal subsampling and spatial masking, encouraging the model to align representations across missing frames and partial observations. Architecturally, a time-distributed geometric encoder extracts per-scan features and a sequential aggregator models the evolving vortex state across variable-length sequences. We evaluate on a real-world dataset of over one million LiDAR scans. X-VORTEX achieves superior vortex center localization while using only 1% of the labeled data required by supervised baselines, and the learned representations support accurate trajectory forecasting.

</details>


### [145] [Transporting Task Vectors across Different Architectures without Training](https://arxiv.org/abs/2602.12952)
*Filippo Rinaldi,Aniello Panariello,Giacomo Salici,Angelo Porrello,Simone Calderara*

Main category: cs.LG

TL;DR: Theseus enables training-free transfer of task-specific updates across models of different widths by matching functional effects on representations rather than direct parameter matching.


<details>
  <summary>Details</summary>
Motivation: Current methods for transferring task-specific updates between models only work with identical architectures, creating a limitation for practical deployment across heterogeneous model variants. There's a need to enable transfer across models of different widths without additional training.

Method: Theseus characterizes task updates by their functional effect on intermediate representations rather than direct parameter matching. It formalizes task-vector transport as a functional matching problem on observed activations, using orthogonal Procrustes analysis to align representation spaces, then provides a stable closed-form solution that preserves update geometry.

Result: Theseus shows consistent improvements over strong baselines on vision and language models across different widths, without requiring additional training or backpropagation. Task updates can be meaningfully transferred across architectures when defined functionally rather than parametrically.

Conclusion: Task identity should be defined functionally rather than parametrically, enabling effective transfer of task-specific updates across heterogeneous model architectures through functional matching of representation effects.

Abstract: Adapting large pre-trained models to downstream tasks often produces task-specific parameter updates that are expensive to relearn for every model variant. While recent work has shown that such updates can be transferred between models with identical architectures, transferring them across models of different widths remains largely unexplored. In this work, we introduce Theseus, a training-free method for transporting task-specific updates across heterogeneous models. Rather than matching parameters directly, we characterize a task update by the functional effect it induces on intermediate representations. We formalize task-vector transport as a functional matching problem on observed activations and show that, after aligning representation spaces via orthogonal Procrustes analysis, it admits a stable closed-form solution that preserves the geometry of the update. We evaluate Theseus on vision and language models across different widths, showing consistent improvements over strong baselines without additional training or backpropagation. Our results show that task updates can be meaningfully transferred across architectures when task identity is defined functionally rather than parametrically.

</details>


### [146] [Extending confidence calibration to generalised measures of variation](https://arxiv.org/abs/2602.12975)
*Andrew Thompson,Vivek Desai*

Main category: cs.LG

TL;DR: Proposes Variation Calibration Error (VCE) as an extension of Expected Calibration Error (ECE) that can assess calibration of any variation metric (like entropy), not just maximum probability, and shows it properly approaches zero for perfectly calibrated distributions.


<details>
  <summary>Details</summary>
Motivation: Existing calibration metrics like ECE only assess calibration of maximum probability/confidence, ignoring the full probability distribution. Other variation metrics like entropy capture more information but need proper calibration assessment methods.

Method: Extends the ECE framework to assess calibration of any variation metric, proposing VCE as a general metric. Tests on synthetic perfectly calibrated predictions to validate properties.

Result: VCE approaches zero as sample size increases for perfectly calibrated distributions, unlike the previously proposed UCE (uncertainty calibration error) which doesn't have this desired property.

Conclusion: VCE provides a principled way to assess calibration of variation metrics like entropy, properly handling the full probability distribution and exhibiting correct asymptotic behavior for calibrated predictions.

Abstract: We propose the Variation Calibration Error (VCE) metric for assessing the calibration of machine learning classifiers. The metric can be viewed as an extension of the well-known Expected Calibration Error (ECE) which assesses the calibration of the maximum probability or confidence. Other ways of measuring the variation of a probability distribution exist which have the advantage of taking into account the full probability distribution, for example the Shannon entropy. We show how the ECE approach can be extended from assessing confidence calibration to assessing the calibration of any metric of variation. We present numerical examples upon synthetic predictions which are perfectly calibrated by design, demonstrating that, in this scenario, the VCE has the desired property of approaching zero as the number of data samples increases, in contrast to another entropy-based calibration metric (the UCE) which has been proposed in the literature.

</details>


### [147] [Ca-MCF: Category-level Multi-label Causal Feature selection](https://arxiv.org/abs/2602.12961)
*Wanfu Gao,Yanan Wang,Yonghao Li*

Main category: cs.LG

TL;DR: Ca-MCF is a category-level multi-label causal feature selection method that decomposes labels into specific categories for fine-grained causal modeling, using competition-based recovery mechanisms to identify causal features obscured by label correlations.


<details>
  <summary>Details</summary>
Motivation: Current multi-label causal feature selection methods operate at the label level, treating each label variable as monolithic and overlooking fine-grained causal mechanisms unique to individual categories within labels.

Method: Uses label category flattening to decompose label variables into specific category nodes, explanatory competition-based category-aware recovery mechanism with SCSMI and DCSMI metrics, structural symmetry checks, and cross-dimensional redundancy removal.

Result: Extensive experiments across seven real-world datasets show Ca-MCF significantly outperforms state-of-the-art benchmarks, achieving superior predictive accuracy with reduced feature dimensionality.

Conclusion: Ca-MCF enables precise modeling of causal structures at the category level, effectively salvaging causal features obscured by label correlations while ensuring robustness and compactness of identified Markov Blankets.

Abstract: Multi-label causal feature selection has attracted extensive attention in recent years. However, current methods primarily operate at the label level, treating each label variable as a monolithic entity and overlooking the fine-grained causal mechanisms unique to individual categories. To address this, we propose a Category-level Multi-label Causal Feature selection method named Ca-MCF. Ca-MCF utilizes label category flattening to decompose label variables into specific category nodes, enabling precise modeling of causal structures within the label space. Furthermore, we introduce an explanatory competition-based category-aware recovery mechanism that leverages the proposed Specific Category-Specific Mutual Information (SCSMI) and Distinct Category-Specific Mutual Information (DCSMI) to salvage causal features obscured by label correlations. The method also incorporates structural symmetry checks and cross-dimensional redundancy removal to ensure the robustness and compactness of the identified Markov Blankets. Extensive experiments across seven real-world datasets demonstrate that Ca-MCF significantly outperforms state-of-the-art benchmarks, achieving superior predictive accuracy with reduced feature dimensionality.

</details>


### [148] [Drift-Aware Variational Autoencoder-based Anomaly Detection with Two-level Ensembling](https://arxiv.org/abs/2602.12976)
*Jin Li,Kleanthis Malialis,Christos G. Panayiotou,Marios M. Polycarpou*

Main category: cs.LG

TL;DR: VAE++ESDD: A novel method using incremental learning with two-level ensembling (VAE ensemble for anomaly detection + drift detector ensemble) to handle concept drift in streaming data with extremely low anomaly rates.


<details>
  <summary>Details</summary>
Motivation: Address challenges of anomaly detection in streaming data with extremely low anomaly rates in nonstationary environments where concept drift causes model performance deterioration over time.

Method: VAE++ESDD employs incremental learning with two-level ensembling: ensemble of Variational AutoEncoders for anomaly prediction, and ensemble of statistical-based concept drift detectors.

Result: Comprehensive experimental study on real-world and synthetic datasets with severely/extremely low anomaly rates shows VAE++ESDD significantly outperforms both strong baselines and state-of-the-art methods.

Conclusion: The proposed VAE++ESDD method effectively addresses concept drift challenges in anomaly detection for streaming data with extremely low anomaly rates, demonstrating superior performance over existing approaches.

Abstract: In today's digital world, the generation of vast amounts of streaming data in various domains has become ubiquitous. However, many of these data are unlabeled, making it challenging to identify events, particularly anomalies. This task becomes even more formidable in nonstationary environments where model performance can deteriorate over time due to concept drift. To address these challenges, this paper presents a novel method, VAE++ESDD, which employs incremental learning and two-level ensembling: an ensemble of Variational AutoEncoder(VAEs) for anomaly prediction, along with an ensemble of concept drift detectors. Each drift detector utilizes a statistical-based concept drift mechanism. To evaluate the effectiveness of VAE++ESDD, we conduct a comprehensive experimental study using real-world and synthetic datasets characterized by severely or extremely low anomalous rates and various drift characteristics. Our study reveals that the proposed method significantly outperforms both strong baselines and state-of-the-art methods.

</details>


### [149] [MAUNet-Light: A Concise MAUNet Architecture for Bias Correction and Downscaling of Precipitation Estimates](https://arxiv.org/abs/2602.12980)
*Sumanta Chandra Mishra Sharma,Adway Mitra,Auroop Ratan Ganguly*

Main category: cs.LG

TL;DR: MAUNet-Light: A lightweight neural network for bias correction and downscaling of precipitation data, developed via teacher-student learning from MAUNet to reduce computational costs while maintaining accuracy.


<details>
  <summary>Details</summary>
Motivation: Satellite data and climate models often have systematic biases compared to ground measurements. Traditional deep learning models for bias correction and downscaling (like MAUNet) are effective but computationally expensive. There's a need for lightweight alternatives that maintain accuracy while reducing computational requirements.

Method: Proposes MAUNet-Light, a compact neural network architecture developed using teacher-student learning paradigm. Knowledge is transferred from trained MAUNet (teacher) to create a lightweight model (student) that performs both bias correction and spatial downscaling of precipitation data.

Result: MAUNet-Light achieves reduced computational and memory requirements while maintaining comparable accuracy to state-of-the-art models like MAUNet for both bias correction and downscaling tasks.

Conclusion: Teacher-student learning enables development of efficient lightweight neural networks for climate data processing. MAUNet-Light provides a practical solution for operational weather forecasting systems by balancing accuracy with computational efficiency for precipitation bias correction and downscaling.

Abstract: Satellite-derived data products and climate model simulations of geophysical variables like precipitation, often exhibit systematic biases compared to in-situ measurements. Bias correction and spatial downscaling are fundamental components to develop operational weather forecast systems, as they seek to improve the consistency between coarse-resolution climate model simulations or satellite-based estimates and ground-based observations. In recent years, deep learning-based models have been increasingly replaced traditional statistical methods to generate high-resolution, bias free projections of climate variables. For example, Max-Average U-Net (MAUNet) architecture has been demonstrated for its ability to downscale precipitation estimates. The versatility and adaptability of these neural models make them highly effective across a range of applications, though this often come at the cost of high computational and memory requirements. The aim of this research is to develop light-weight neural network architectures for both bias correction and downscaling of precipitation, for which the teacher-student based learning paradigm is explored. This research demonstrates the adaptability of MAUNet to the task of bias correction, and further introduces a compact, lightweight neural network architecture termed MAUNet-Light.The proposed MAUNet-Light model is developed by transferring knowledge from the trained MAUNet, and it is designed to perform both downscaling and bias correction with reduced computational requirements without any significant loss in accuracy compared to state-of-the-art.

</details>


### [150] [Prior-Guided Symbolic Regression: Towards Scientific Consistency in Equation Discovery](https://arxiv.org/abs/2602.13021)
*Jing Xiao,Xinhai Chen,Jiaming Peng,Qinglin Wang,Menghan Jia,Zhiquan Lai,Guangping Yu,Dongsheng Li,Tiejun Li,Jie Liu*

Main category: cs.LG

TL;DR: PG-SR is a prior-guided symbolic regression framework that prevents pseudo-equations by incorporating domain priors as executable constraints through a three-stage pipeline with progressive constraint enforcement.


<details>
  <summary>Details</summary>
Motivation: Existing symbolic regression methods often produce equations that fit data well but violate fundamental scientific principles (pseudo-equation trap), primarily due to over-reliance on empirical risk minimization without explicit scientific consistency constraints.

Method: Three-stage pipeline (warm-up, evolution, refinement) with prior constraint checker encoding domain priors as executable programs, and Prior Annealing Constrained Evaluation (PACE) mechanism that progressively steers evolution toward scientifically consistent regions.

Result: PG-SR outperforms state-of-the-art baselines across diverse domains, maintains robustness to varying prior quality, noisy data, and data scarcity, and theoretically reduces Rademacher complexity for better generalization bounds.

Conclusion: PG-SR successfully bridges the gap between data fitting and scientific consistency in symbolic regression, providing a theoretically-grounded framework that prevents pseudo-equations while maintaining strong empirical performance.

Abstract: Symbolic Regression (SR) aims to discover interpretable equations from observational data, with the potential to reveal underlying principles behind natural phenomena. However, existing approaches often fall into the Pseudo-Equation Trap: producing equations that fit observations well but remain inconsistent with fundamental scientific principles. A key reason is that these approaches are dominated by empirical risk minimization, lacking explicit constraints to ensure scientific consistency. To bridge this gap, we propose PG-SR, a prior-guided SR framework built upon a three-stage pipeline consisting of warm-up, evolution, and refinement. Throughout the pipeline, PG-SR introduces a prior constraint checker that explicitly encodes domain priors as executable constraint programs, and employs a Prior Annealing Constrained Evaluation (PACE) mechanism during the evolution stage to progressively steer discovery toward scientifically consistent regions. Theoretically, we prove that PG-SR reduces the Rademacher complexity of the hypothesis space, yielding tighter generalization bounds and establishing a guarantee against pseudo-equations. Experimentally, PG-SR outperforms state-of-the-art baselines across diverse domains, maintaining robustness to varying prior quality, noisy data, and data scarcity.

</details>


### [151] [Multi-Dimensional Visual Data Recovery: Scale-Aware Tensor Modeling and Accelerated Randomized Computation](https://arxiv.org/abs/2602.12982)
*Wenjin Qin,Hailin Wang,Jiangjun Peng,Jianjun Wang,Tingwen Huang*

Main category: cs.LG

TL;DR: Proposes improved FCTN decomposition methods for multi-dimensional data recovery with nonconvex regularization, quantized observations, and randomized compression for computational efficiency.


<details>
  <summary>Details</summary>
Motivation: Existing FCTN-based multi-dimensional data recovery methods need improvement in computational efficiency and modeling capability, especially for large-scale data.

Method: 1) FCTN-based generalized nonconvex regularization via gradient mapping; 2) Models for quantized observations; 3) ADMM optimization with convergence guarantees; 4) Randomized compression using sketching techniques for large-scale data acceleration.

Result: Theoretical guarantees on approximation error bounds and convergence. Extensive experiments show superiority over state-of-the-art methods in quantitative metrics, visual quality, and running time.

Conclusion: Proposed framework effectively addresses computational efficiency and modeling limitations of existing FCTN methods, providing reliable and scalable multi-dimensional data recovery with theoretical guarantees.

Abstract: The recently proposed fully-connected tensor network (FCTN) decomposition has demonstrated significant advantages in correlation characterization and transpositional invariance, and has achieved notable achievements in multi-dimensional data processing and analysis. However, existing multi-dimensional data recovery methods leveraging FCTN decomposition still have room for further enhancement, particularly in computational efficiency and modeling capability. To address these issues, we first propose a FCTN-based generalized nonconvex regularization paradigm from the perspective of gradient mapping. Then, reliable and scalable multi-dimensional data recovery models are investigated, where the model formulation is shifted from unquantized observations to coarse-grained quantized observations. Based on the alternating direction method of multipliers (ADMM) framework, we derive efficient optimization algorithms with convergence guarantees to solve the formulated models. To alleviate the computational bottleneck encountered when processing large-scale multi-dimensional data, fast and efficient randomized compression algorithms are devised in virtue of sketching techniques in numerical linear algebra. These dimensionality-reduction techniques serve as the computational acceleration core of our proposed algorithm framework. Theoretical results on approximation error upper bounds and convergence analysis for the proposed method are derived. Extensive numerical experiments illustrate the effectiveness and superiority of the proposed algorithm over other state-of-the-art methods in terms of quantitative metrics, visual quality, and running time.

</details>


### [152] [Look Inward to Explore Outward: Learning Temperature Policy from LLM Internal States via Hierarchical RL](https://arxiv.org/abs/2602.13035)
*Yixiao Zhou,Yang Li,Dongzhou Cheng,Hehe Fan,Yu Cheng*

Main category: cs.LG

TL;DR: RLVR framework learns to control sampling temperature during LLM generation, outperforming fixed/heuristic baselines on math reasoning tasks.


<details>
  <summary>Details</summary>
Motivation: Existing methods use static or heuristic temperature settings decoupled from task rewards, missing opportunities to optimize exploration-exploitation trade-off during generation.

Method: Hierarchical RL framework where model selects temperature based on hidden state at each decoding step, then samples token from resulting distribution. Joint optimization via coordinate ascent.

Result: Learned temperature policies outperform fixed and heuristic baselines on mathematical reasoning benchmarks, showing interpretable exploration behaviors aligned with reasoning uncertainty.

Conclusion: Sampling temperature should be learned as part of RL from verifiable rewards, enabling adaptive exploration strategies that improve performance on complex reasoning tasks.

Abstract: Reinforcement Learning from Verifiable Rewards (RLVR) trains large language models (LLMs) from sampled trajectories, making decoding strategy a core component of learning rather than a purely inference-time choice. Sampling temperature directly controls the exploration--exploitation trade-off by modulating policy entropy, yet existing methods rely on static values or heuristic adaptations that are decoupled from task-level rewards. We propose Introspective LLM, a hierarchical reinforcement learning framework that learns to control sampling temperature during generation. At each decoding step, the model selects a temperature based on its hidden state and samples the next token from the resulting distribution. Temperature and token policies are jointly optimized from downstream rewards using a coordinate ascent scheme. Experiments on mathematical reasoning benchmarks show that learned temperature policies outperform fixed and heuristic baselines, while exhibiting interpretable exploration behaviors aligned with reasoning uncertainty.

</details>


### [153] [Uncertainty in Federated Granger Causality: From Origins to Systemic Consequences](https://arxiv.org/abs/2602.13004)
*Ayush Mohanty,Nazal Mohamed,Nagi Gebraeel*

Main category: cs.LG

TL;DR: First methodology for quantifying uncertainty in federated Granger causality, differentiating aleatoric vs epistemic uncertainty, with closed-form recursions and convergence analysis.


<details>
  <summary>Details</summary>
Motivation: Existing federated GC algorithms only provide deterministic point estimates of causality without uncertainty quantification, limiting reliability and interpretability in distributed applications like smart grids with data-sovereignty constraints.

Method: Systematically classify uncertainty sources (aleatoric vs epistemic), derive closed-form recursions modeling uncertainty evolution through client-server interactions, identify four novel cross-covariance components coupling data and model uncertainties, define convergence conditions, and obtain explicit steady-state variances.

Result: Convergence analysis shows steady-state variances depend exclusively on client data statistics, eliminating dependence on initial epistemic priors. Empirical evaluations on synthetic and real-world industrial datasets demonstrate improved reliability and interpretability.

Conclusion: Explicit uncertainty characterization significantly improves federated causal inference reliability and interpretability, with the methodology providing rigorous uncertainty quantification that enhances robustness in distributed applications.

Abstract: Granger Causality (GC) provides a rigorous framework for learning causal structures from time-series data. Recent federated variants of GC have targeted distributed infrastructure applications (e.g., smart grids) with distributed clients that generate high-dimensional data bound by data-sovereignty constraints. However, Federated GC algorithms only yield deterministic point estimates of causality and neglect uncertainty. This paper establishes the first methodology for rigorously quantifying uncertainty and its propagation within federated GC frameworks. We systematically classify sources of uncertainty, explicitly differentiating aleatoric (data noise) from epistemic (model variability) effects. We derive closed-form recursions that model the evolution of uncertainty through client-server interactions and identify four novel cross-covariance components that couple data uncertainties with model parameter uncertainties across the federated architecture. We also define rigorous convergence conditions for these uncertainty recursions and obtain explicit steady-state variances for both server and client model parameters. Our convergence analysis demonstrates that steady-state variances depend exclusively on client data statistics, thus eliminating dependence on initial epistemic priors and enhancing robustness. Empirical evaluations on synthetic benchmarks and real-world industrial datasets demonstrate that explicitly characterizing uncertainty significantly improves the reliability and interpretability of federated causal inference.

</details>


### [154] [Geometric Manifold Rectification for Imbalanced Learning](https://arxiv.org/abs/2602.13045)
*Xubin Wang,Qing Li,Weijia Jia*

Main category: cs.LG

TL;DR: GMR is a novel undersampling framework for imbalanced tabular data that uses geometric confidence estimation and asymmetric cleaning to better preserve minority class structure while removing noisy majority samples.


<details>
  <summary>Details</summary>
Motivation: Imbalanced classification is challenging with noisy tabular data where majority class intrusion obscures decision boundaries. Traditional undersampling methods like ENN use symmetric rules that fail to capture local manifold structure and often remove informative minority samples.

Method: GMR uses two key techniques: (1) Geometric confidence estimation with inverse-distance weighted kNN voting and adaptive distance metrics to assess local reliability, and (2) Asymmetric cleaning that is strict on majority samples while conservatively protecting minority samples via a safe-guarding cap on minority removal.

Result: Extensive experiments on multiple benchmark datasets show that GMR is competitive with strong sampling baselines.

Conclusion: GMR provides a robust framework for handling imbalanced structured data by exploiting local geometric priors to better preserve minority class manifold structure while effectively cleaning noisy majority samples.

Abstract: Imbalanced classification presents a formidable challenge in machine learning, particularly when tabular datasets are plagued by noise and overlapping class boundaries. From a geometric perspective, the core difficulty lies in the topological intrusion of the majority class into the minority manifold, which obscures the true decision boundary. Traditional undersampling techniques, such as Edited Nearest Neighbours (ENN), typically employ symmetric cleaning rules and uniform voting, failing to capture the local manifold structure and often inadvertently removing informative minority samples. In this paper, we propose GMR (Geometric Manifold Rectification), a novel framework designed to robustly handle imbalanced structured data by exploiting local geometric priors. GMR makes two contributions: (1) Geometric confidence estimation that uses inverse-distance weighted kNN voting with an adaptive distance metric to capture local reliability; and (2) asymmetric cleaning that is strict on majority samples while conservatively protecting minority samples via a safe-guarding cap on minority removal. Extensive experiments on multiple benchmark datasets show that GMR is competitive with strong sampling baselines.

</details>


### [155] [Machine Learning-Based Classification of Jhana Advanced Concentrative Absorption Meditation (ACAM-J) using 7T fMRI](https://arxiv.org/abs/2602.13008)
*Puneet Kumar,Winson F. Z. Yang,Alakhsimar Singh,Xiaobai Li,Matthew D. Sacchet*

Main category: cs.LG

TL;DR: Machine learning classifiers using fMRI regional homogeneity patterns can distinguish advanced concentration meditation states from non-meditative states with 66.82% accuracy, with prefrontal and anterior cingulate regions being most important.


<details>
  <summary>Details</summary>
Motivation: Advanced concentration absorption meditation (ACAM-J) produces profound changes in consciousness and cognition, making it important to study its neural correlates for insights into consciousness and well-being.

Method: Used fMRI-derived regional homogeneity (ReHo) patterns from 20 advanced meditators to train machine learning classifiers, then tested on intensive single-case data from an advanced practitioner performing ACAM-J vs control tasks. Features extracted from predefined brain ROIs, with stratified cross-validation.

Result: Ensemble models achieved 66.82% (p < 0.05) accuracy in distinguishing ACAM-J from control conditions. Feature importance analysis showed prefrontal and anterior cingulate areas contributed most to classification decisions.

Conclusion: Machine learning is feasible for classifying advanced meditation states, with findings supporting future research on neuromodulation and mechanistic models of advanced meditation.

Abstract: Jhana advanced concentration absorption meditation (ACAM-J) is related to profound changes in consciousness and cognitive processing, making the study of their neural correlates vital for insights into consciousness and well-being. This study evaluates whether functional MRI-derived regional homogeneity (ReHo) can be used to classify ACAM-J using machine-learning approaches. We collected group-level fMRI data from 20 advanced meditators to train the classifiers, and intensive single-case data from an advanced practitioner performing ACAM-J and control tasks to evaluate generalization. ReHo maps were computed, and features were extracted from predefined brain regions of interest. We trained multiple machine learning classifiers using stratified cross-validation to evaluate whether ReHo patterns distinguish ACAM-J from non-meditative states. Ensemble models achieved 66.82% (p < 0.05) accuracy in distinguishing ACAM-J from control conditions. Feature-importance analysis indicated that prefrontal and anterior cingulate areas contributed most to model decisions, aligning with established involvement of these regions in attentional regulation and metacognitive processes. Moreover, moderate agreement reflected in Cohen's kappa supports the feasibility of using machine learning to distinguish ACAM-J from non-meditative states. These findings advocate machine-learning's feasibility in classifying advanced meditation states, future research on neuromodulation and mechanistic models of advanced meditation.

</details>


### [156] [Probabilistic Wind Power Forecasting with Tree-Based Machine Learning and Weather Ensembles](https://arxiv.org/abs/2602.13010)
*Max Bruninx,Diederik van Binsbergen,Timothy Verstraeten,Ann Nowé,Jan Helsen*

Main category: cs.LG

TL;DR: This paper compares three probabilistic machine learning methods for wind power forecasting using weather ensembles, showing up to 53% improvement over traditional engineering methods.


<details>
  <summary>Details</summary>
Motivation: Accurate wind power forecasts are essential for integrating renewable energy into the grid, requiring probabilistic approaches that account for uncertainty.

Method: Comparative analysis of three probabilistic methods (conformalised quantree regression, natural gradient boosting, conditional diffusion models) combined with gradient boosting trees using weather forecast ensembles, validated on 4 years of Belgian offshore wind farm data.

Result: Machine learning methods improved MAE by up to 53% vs power curve and 33% vs calibrated wake model. Conditional diffusion models performed best overall. Weather ensemble improved point forecast accuracy by up to 23%.

Conclusion: Conditional diffusion models with gradient boosting trees and weather ensembles provide superior probabilistic and point forecasts for wind power generation compared to traditional engineering methods.

Abstract: Accurate production forecasts are essential to continue facilitating the integration of renewable energy sources into the power grid. This paper illustrates how to obtain probabilistic day-ahead forecasts of wind power generation via gradient boosting trees using an ensemble of weather forecasts. To this end, we perform a comparative analysis across three state-of-the-art probabilistic prediction methods-conformalised quantile regression, natural gradient boosting and conditional diffusion models-all of which can be combined with tree-based machine learning. The methods are validated using four years of data for all wind farms present within the Belgian offshore zone. Additionally, the point forecasts are benchmarked against deterministic engineering methods, using either the power curve or an advanced approach incorporating a calibrated analytical wake model. The experimental results show that the machine learning methods improve the mean absolute error by up to 53% and 33% compared to the power curve and the calibrated wake model. Considering the three probabilistic prediction methods, the conditional diffusion model is found to yield the best overall probabilistic and point estimate of wind power generation. Moreover, the findings suggest that the use of an ensemble of weather forecasts can improve point forecast accuracy by up to 23%.

</details>


### [157] [Diverging Flows: Detecting Extrapolations in Conditional Generation](https://arxiv.org/abs/2602.13061)
*Constantinos Tsakonas,Serena Ivaldi,Jean-Baptiste Mouret*

Main category: cs.LG

TL;DR: Diverging Flows enables flow models to detect extrapolations while maintaining predictive performance by enforcing inefficient transport for off-manifold inputs.


<details>
  <summary>Details</summary>
Motivation: Flow Matching models have extrapolation hazards in safety-critical applications - they produce plausible outputs even for off-manifold conditions, leading to silent failures that are indistinguishable from valid predictions.

Method: Diverging Flows structurally enforces inefficient transport for off-manifold inputs, allowing a single model to perform both conditional generation and native extrapolation detection simultaneously.

Result: The method achieves effective detection of extrapolations without compromising predictive fidelity or inference latency, demonstrated on synthetic manifolds, cross-domain style transfer, and weather temperature forecasting.

Conclusion: Diverging Flows provides a robust solution for trustworthy flow models, enabling reliable deployment in safety-critical domains like medicine, robotics, and climate science.

Abstract: The ability of Flow Matching (FM) to model complex conditional distributions has established it as the state-of-the-art for prediction tasks (e.g., robotics, weather forecasting). However, deployment in safety-critical settings is hindered by a critical extrapolation hazard: driven by smoothness biases, flow models yield plausible outputs even for off-manifold conditions, resulting in silent failures indistinguishable from valid predictions. In this work, we introduce Diverging Flows, a novel approach that enables a single model to simultaneously perform conditional generation and native extrapolation detection by structurally enforcing inefficient transport for off-manifold inputs. We evaluate our method on synthetic manifolds, cross-domain style transfer, and weather temperature forecasting, demonstrating that it achieves effective detection of extrapolations without compromising predictive fidelity or inference latency. These results establish Diverging Flows as a robust solution for trustworthy flow models, paving the way for reliable deployment in domains such as medicine, robotics, and climate science.

</details>


### [158] [Bus-Conditioned Zero-Shot Trajectory Generation via Task Arithmetic](https://arxiv.org/abs/2602.13071)
*Shuai Liu,Ning Cao,Yile Chen,Yue Jiang,Gao Cong*

Main category: cs.LG

TL;DR: MobTA enables zero-shot trajectory generation for target cities without mobility data, using only source city trajectories and public bus timetables via task arithmetic.


<details>
  <summary>Details</summary>
Motivation: Real mobility trajectory data is often difficult to obtain, and existing methods require at least some target city data, limiting applicability in data-inaccessible scenarios.

Method: Proposes bus-conditioned zero-shot trajectory generation using task arithmetic. Models parameter shift from bus-timetable-based to mobility trajectory generation in source city, then applies this shift to target city through arithmetic operations on task vectors.

Result: MobTA significantly outperforms existing methods and achieves performance close to models finetuned using actual target city mobility trajectories.

Conclusion: MobTA enables effective trajectory generation for target cities without requiring any real mobility data, making it highly applicable in data-inaccessible scenarios.

Abstract: Mobility trajectory data provide essential support for smart city applications. However, such data are often difficult to obtain. Meanwhile, most existing trajectory generation methods implicitly assume that at least a subset of real mobility data from target city is available, which limits their applicability in data-inaccessible scenarios. In this work, we propose a new problem setting, called bus-conditioned zero-shot trajectory generation, where no mobility trajectories from a target city are accessible. The generation process relies solely on source city mobility data and publicly available bus timetables from both cities. Under this setting, we propose MobTA, the first approach to introduce task arithmetic into trajectory generation. MobTA models the parameter shift from bus-timetable-based trajectory generation to mobility trajectory generation in source city, and applies this shift to target city through arithmetic operations on task vectors. This enables trajectory generation that reflects target-city mobility patterns without requiring any real mobility data from it. Furthermore, we theoretically analyze MobTA's stability across base and instruction-tuned LLMs. Extensive experiments show that MobTA significantly outperforms existing methods, and achieves performance close to models finetuned using target city mobility trajectories.

</details>


### [159] [Resource-Efficient Gesture Recognition through Convexified Attention](https://arxiv.org/abs/2602.13030)
*Daniel Schwartz,Dario Salvucci,Yusuf Osmanlioglu,Richard Vallett,Genevieve Dion,Ali Shokoufandeh*

Main category: cs.LG

TL;DR: A convexified attention mechanism for e-textile gesture recognition achieves 100% accuracy with only 120-360 parameters (97% reduction) and sub-millisecond inference times, enabling on-device processing without external hardware.


<details>
  <summary>Details</summary>
Motivation: Wearable e-textile interfaces need gesture recognition but face severe power, computational, and form factor constraints that make traditional deep learning impractical. Existing lightweight architectures still require thousands of parameters, limiting deployment on textile-integrated platforms.

Method: Introduces a convexified attention mechanism using Euclidean projection onto the probability simplex (instead of non-convex softmax) combined with multi-class hinge loss, ensuring global convergence guarantees. Implemented on a textile-based capacitive sensor with four connection points.

Result: Achieves 100.00% accuracy on both tap and swipe gestures across 10-fold cross-validation and held-out tests. Requires only 120-360 parameters (97% reduction), sub-millisecond inference times (290-296μs), and minimal storage (<7KB).

Conclusion: Demonstrates how convex optimization enables efficient on-device machine learning for textile interfaces. While tested in controlled lab conditions with single-user data, real-world deployment would need validation across multiple users, environments, and more complex gestures.

Abstract: Wearable e-textile interfaces require gesture recognition capabilities but face severe constraints in power consumption, computational capacity, and form factor that make traditional deep learning impractical. While lightweight architectures like MobileNet improve efficiency, they still demand thousands of parameters, limiting deployment on textile-integrated platforms. We introduce a convexified attention mechanism for wearable applications that dynamically weights features while preserving convexity through nonexpansive simplex projection and convex loss functions. Unlike conventional attention mechanisms using non-convex softmax operations, our approach employs Euclidean projection onto the probability simplex combined with multi-class hinge loss, ensuring global convergence guarantees. Implemented on a textile-based capacitive sensor with four connection points, our approach achieves 100.00\% accuracy on tap gestures and 100.00\% on swipe gestures -- consistent across 10-fold cross-validation and held-out test evaluation -- while requiring only 120--360 parameters, a 97\% reduction compared to conventional approaches. With sub-millisecond inference times (290--296$μ$s) and minimal storage requirements ($<$7KB), our method enables gesture interfaces directly within e-textiles without external processing. Our evaluation, conducted in controlled laboratory conditions with a single-user dataset, demonstrates feasibility for basic gesture interactions. Real-world deployment would require validation across multiple users, environmental conditions, and more complex gesture vocabularies. These results demonstrate how convex optimization can enable efficient on-device machine learning for textile interfaces.

</details>


### [160] [EXCODER: EXplainable Classification Of DiscretE time series Representations](https://arxiv.org/abs/2602.13087)
*Yannik Hahn,Antonin Königsfeld,Hasan Tercan,Tobias Meisen*

Main category: cs.LG

TL;DR: Discrete latent representations (VQ-VAE/DVAE) enhance explainability in time series classification by reducing noise and redundancy, enabling more concise XAI explanations without performance loss, with a new SSA metric to validate explanation quality.


<details>
  <summary>Details</summary>
Motivation: Deep learning models for time series classification lack explainability, and existing XAI techniques struggle with high-dimensional, noisy raw time series data. There's a need for more transparent and interpretable approaches.

Method: Transform time series into discrete latent representations using VQ-VAE and DVAE to compress data and reduce redundancy. Apply XAI methods to these compressed representations, and propose Similar Subsequence Accuracy (SSA) metric to quantitatively assess explanation alignment with training data patterns.

Result: Discrete latent representations preserve classification performance while enabling more concise, structured, and faithful explanations. The SSA metric provides systematic validation of XAI-identified salient features, showing they align with learned classification patterns.

Conclusion: Discrete latent representations offer a pathway to more compact, interpretable, and computationally efficient explanations in time series analysis, balancing performance with explainability through structured compression and quantitative validation metrics.

Abstract: Deep learning has significantly improved time series classification, yet the lack of explainability in these models remains a major challenge. While Explainable AI (XAI) techniques aim to make model decisions more transparent, their effectiveness is often hindered by the high dimensionality and noise present in raw time series data. In this work, we investigate whether transforming time series into discrete latent representations-using methods such as Vector Quantized Variational Autoencoders (VQ-VAE) and Discrete Variational Autoencoders (DVAE)-not only preserves but enhances explainability by reducing redundancy and focusing on the most informative patterns. We show that applying XAI methods to these compressed representations leads to concise and structured explanations that maintain faithfulness without sacrificing classification performance. Additionally, we propose Similar Subsequence Accuracy (SSA), a novel metric that quantitatively assesses the alignment between XAI-identified salient subsequences and the label distribution in the training data. SSA provides a systematic way to validate whether the features highlighted by XAI methods are truly representative of the learned classification patterns. Our findings demonstrate that discrete latent representations not only retain the essential characteristics needed for classification but also offer a pathway to more compact, interpretable, and computationally efficient explanations in time series analysis.

</details>


### [161] [Which Algorithms Can Graph Neural Networks Learn?](https://arxiv.org/abs/2602.13106)
*Solveig Wittig,Antonis Vasileiou,Robert R. Nerem,Timo Stoll,Floris Geerts,Yusu Wang,Christopher Morris*

Main category: cs.LG

TL;DR: The paper proposes a theoretical framework for understanding when and how message-passing graph neural networks can learn algorithms from small training instances and generalize to arbitrary-sized inputs, with both positive results for certain algorithms and impossibility results for others.


<details>
  <summary>Details</summary>
Motivation: There's growing interest in neural algorithmic reasoning to integrate algorithmic capabilities into neural pipelines. While many architectures use MPNNs, existing work lacks formal guarantees about generalization beyond finite training sets, focusing either on empirical results or expressivity alone.

Method: The authors propose a general theoretical framework characterizing sufficient conditions for MPNNs to learn algorithms from small training instances and generalize to arbitrary-sized inputs. They analyze a broad class of algorithms, establish impossibility results for certain tasks, derive more expressive MPNN-like architectures, and refine analysis specifically for Bellman-Ford algorithm.

Result: The framework applies to algorithms like single-source shortest paths, minimum spanning trees, and dynamic programming problems. The paper shows impossibility results for some algorithmic tasks with standard MPNNs, proposes more expressive architectures, and refines Bellman-Ford analysis to require smaller training sets and allow differentiable regularization loss.

Conclusion: The work provides theoretical foundations for neural algorithmic reasoning with MPNNs, offering both positive generalization guarantees and impossibility results, while proposing enhanced architectures and empirical validation of theoretical findings.

Abstract: In recent years, there has been growing interest in understanding neural architectures' ability to learn to execute discrete algorithms, a line of work often referred to as neural algorithmic reasoning. The goal is to integrate algorithmic reasoning capabilities into larger neural pipelines. Many such architectures are based on (message-passing) graph neural networks (MPNNs), owing to their permutation equivariance and ability to deal with sparsity and variable-sized inputs. However, existing work is either largely empirical and lacks formal guarantees or it focuses solely on expressivity, leaving open the question of when and how such architectures generalize beyond a finite training set. In this work, we propose a general theoretical framework that characterizes the sufficient conditions under which MPNNs can learn an algorithm from a training set of small instances and provably approximate its behavior on inputs of arbitrary size. Our framework applies to a broad class of algorithms, including single-source shortest paths, minimum spanning trees, and general dynamic programming problems, such as the $0$-$1$ knapsack problem. In addition, we establish impossibility results for a wide range of algorithmic tasks, showing that standard MPNNs cannot learn them, and we derive more expressive MPNN-like architectures that overcome these limitations. Finally, we refine our analysis for the Bellman-Ford algorithm, yielding a substantially smaller required training set and significantly extending the recent work of Nerem et al. [2025] by allowing for a differentiable regularization loss. Empirical results largely support our theoretical findings.

</details>


### [162] [TCRL: Temporal-Coupled Adversarial Training for Robust Constrained Reinforcement Learning in Worst-Case Scenarios](https://arxiv.org/abs/2602.13040)
*Wentao Xu,Zhongming Yao,Weihao Li,Zhenghang Song,Yumeng Song,Tianyi Li,Yushuai Li*

Main category: cs.LG

TL;DR: TCRL introduces a temporal-coupled adversarial training framework for robust constrained RL that handles worst-case temporally coupled perturbations through novel constraint functions and dual-constraint defense mechanisms.


<details>
  <summary>Details</summary>
Motivation: Existing robust CRL approaches focus on single-step perturbations and temporally independent adversarial models, lacking explicit modeling of robustness against temporally coupled perturbations, which is crucial for safety-critical applications like autonomous driving and robotics.

Method: TCRL introduces: 1) worst-case-perceived cost constraint function that estimates safety costs under temporally coupled perturbations without explicit adversarial modeling, and 2) dual-constraint defense mechanism on rewards to counter temporally coupled adversaries while maintaining reward unpredictability.

Result: Experimental results show TCRL consistently outperforms existing methods in robustness against temporally coupled perturbation attacks across various CRL tasks.

Conclusion: TCRL provides an effective framework for robust constrained reinforcement learning that addresses the limitations of existing approaches by explicitly handling temporally coupled perturbations in worst-case scenarios.

Abstract: Constrained Reinforcement Learning (CRL) aims to optimize decision-making policies under constraint conditions, making it highly applicable to safety-critical domains such as autonomous driving, robotics, and power grid management. However, existing robust CRL approaches predominantly focus on single-step perturbations and temporally independent adversarial models, lacking explicit modeling of robustness against temporally coupled perturbations. To tackle these challenges, we propose TCRL, a novel temporal-coupled adversarial training framework for robust constrained reinforcement learning (TCRL) in worst-case scenarios. First, TCRL introduces a worst-case-perceived cost constraint function that estimates safety costs under temporally coupled perturbations without the need to explicitly model adversarial attackers. Second, TCRL establishes a dual-constraint defense mechanism on the reward to counter temporally coupled adversaries while maintaining reward unpredictability. Experimental results demonstrate that TCRL consistently outperforms existing methods in terms of robustness against temporally coupled perturbation attacks across a variety of CRL tasks.

</details>


### [163] [GPTZero: Robust Detection of LLM-Generated Texts](https://arxiv.org/abs/2602.13042)
*George Alexandru Adam,Alexander Cui,Edwin Thomas,Emily Napier,Nazar Shmatko,Jacob Schnell,Jacob Junqi Tian,Alekhya Dronavalli,Edward Tian,Dongwon Lee*

Main category: cs.LG

TL;DR: GPTZero is an AI detection tool that distinguishes human-written from AI-generated text using hierarchical multi-task architecture, achieving high accuracy and robustness against attacks.


<details>
  <summary>Details</summary>
Motivation: The rise of LLMs has created new challenges in text authenticity, including undermining skill evaluations, mass-production of low-quality content, and misinformation proliferation. Traditional plagiarism detection is insufficient for distinguishing human vs AI text.

Method: Introduces GPTZero with hierarchical multi-task architecture enabling flexible taxonomy of human and AI texts. Uses multi-tiered automated red teaming for robustness against adversarial attacks and paraphrasing.

Result: Demonstrates state-of-the-art accuracy across various domains with granular predictions. Achieves superior robustness to adversarial attacks and paraphrasing. Provides accurate, explainable detection with educational components for responsible use.

Conclusion: GPTZero offers a reliable industrial AI detection solution that ensures fair and transparent assessment of text authenticity, addressing critical concerns in the LLM era while promoting responsible use.

Abstract: While historical considerations surrounding text authenticity revolved primarily around plagiarism, the advent of large language models (LLMs) has introduced a new challenge: distinguishing human-authored from AI-generated text. This shift raises significant concerns, including the undermining of skill evaluations, the mass-production of low-quality content, and the proliferation of misinformation. Addressing these issues, we introduce GPTZero a state-of-the-art industrial AI detection solution, offering reliable discernment between human and LLM-generated text. Our key contributions include: introducing a hierarchical, multi-task architecture enabling a flexible taxonomy of human and AI texts, demonstrating state-of-the-art accuracy on a variety of domains with granular predictions, and achieving superior robustness to adversarial attacks and paraphrasing via multi-tiered automated red teaming. GPTZero offers accurate and explainable detection, and educates users on its responsible use, ensuring fair and transparent assessment of text.

</details>


### [164] [Backdoor Attacks on Contrastive Continual Learning for IoT Systems](https://arxiv.org/abs/2602.13062)
*Alfous Tim,Kuniyilh Simi D*

Main category: cs.LG

TL;DR: This paper analyzes backdoor attack vulnerabilities in contrastive continual learning (CCL) for IoT systems, examining how geometric contrastive objectives combined with replay mechanisms create persistent security threats that survive through system updates.


<details>
  <summary>Details</summary>
Motivation: IoT systems increasingly rely on continual learning to adapt to dynamic environments (sensor drift, changing user behavior, device aging, adversarial dynamics), but contrastive continual learning introduces new security vulnerabilities through embedding alignment and replay reinforcement that enable persistent backdoor attacks.

Method: The authors formalize embedding-level attack objectives, examine persistence mechanisms unique to IoT deployments, develop a layered taxonomy tailored to IoT systems, compare vulnerabilities across learning paradigms, and evaluate defense strategies under IoT constraints (limited memory, edge computing, federated aggregation).

Result: The analysis reveals that while CCL effectively enhances adaptive IoT intelligence, it also elevates long-lived representation-level threats, with backdoor attacks exploiting geometric contrastive objectives and replay-based rehearsal to implant persistent malicious behaviors that endure through updates and deployment cycles.

Conclusion: CCL is valuable for IoT adaptation but requires enhanced security measures to address representation-level backdoor vulnerabilities; IoT-specific constraints (memory limitations, edge computing, federated learning) must be considered when designing defenses against these persistent threats.

Abstract: The Internet of Things (IoT) systems increasingly depend on continual learning to adapt to non-stationary environments. These environments can include factors such as sensor drift, changing user behavior, device aging, and adversarial dynamics. Contrastive continual learning (CCL) combines contrastive representation learning with incremental adaptation, enabling robust feature reuse across tasks and domains. However, the geometric nature of contrastive objectives, when paired with replay-based rehearsal and stability-preserving regularization, introduces new security vulnerabilities. Notably, backdoor attacks can exploit embedding alignment and replay reinforcement, enabling the implantation of persistent malicious behaviors that endure through updates and deployment cycles. This paper provides a comprehensive analysis of backdoor attacks on CCL within IoT systems. We formalize the objectives of embedding-level attacks, examine persistence mechanisms unique to IoT deployments, and develop a layered taxonomy tailored to IoT. Additionally, we compare vulnerabilities across various learning paradigms and evaluate defense strategies under IoT constraints, including limited memory, edge computing, and federated aggregation. Our findings indicate that while CCL is effective for enhancing adaptive IoT intelligence, it may also elevate long-lived representation-level threats if not adequately secured.

</details>


### [165] [Memory-Efficient Structured Backpropagation for On-Device LLM Fine-Tuning](https://arxiv.org/abs/2602.13069)
*Juneyoung Park,Yuri Hong,Seongwan Kim,Jaeho Lee*

Main category: cs.LG

TL;DR: MeSP enables exact gradient computation for LoRA fine-tuning with 49% memory reduction compared to standard backpropagation, making on-device personalization feasible on memory-constrained mobile devices.


<details>
  <summary>Details</summary>
Motivation: On-device fine-tuning of LLMs for privacy-preserving personalization is limited by mobile device memory constraints (6-12GB). Existing approaches force a trade-off between exact gradients with high memory (MeBP) and low memory with noisy estimates (MeZO).

Method: Memory-efficient Structured Backpropagation (MeSP) manually derives backward passes that exploit LoRA's low-rank structure. The key insight is that the intermediate projection h = xA can be recomputed during backward at minimal cost since rank r << d_in, eliminating the need to store it.

Result: MeSP achieves 49% average memory reduction compared to MeBP on Qwen2.5 models (0.5B-3B) while computing mathematically identical gradients. Reduces peak memory from 361MB to 136MB for Qwen2.5-0.5B, enabling previously infeasible fine-tuning scenarios. Analysis shows MeZO's gradient estimates have near-zero correlation with true gradients (cosine similarity ≈0.001).

Conclusion: MeSP bridges the gap between exact gradients and memory efficiency for on-device fine-tuning, making privacy-preserving personalization of LLMs practical on memory-constrained mobile devices without sacrificing gradient accuracy.

Abstract: On-device fine-tuning enables privacy-preserving personalization of large language models, but mobile devices impose severe memory constraints, typically 6--12GB shared across all workloads. Existing approaches force a trade-off between exact gradients with high memory (MeBP) and low memory with noisy estimates (MeZO). We propose Memory-efficient Structured Backpropagation (MeSP), which bridges this gap by manually deriving backward passes that exploit LoRA's low-rank structure. Our key insight is that the intermediate projection $h = xA$ can be recomputed during backward at minimal cost since rank $r \ll d_{in}$, eliminating the need to store it. MeSP achieves 49\% average memory reduction compared to MeBP on Qwen2.5 models (0.5B--3B) while computing mathematically identical gradients. Our analysis also reveals that MeZO's gradient estimates show near-zero correlation with true gradients (cosine similarity $\approx$0.001), explaining its slow convergence. MeSP reduces peak memory from 361MB to 136MB for Qwen2.5-0.5B, enabling fine-tuning scenarios previously infeasible on memory-constrained devices.

</details>


### [166] [LCSB: Layer-Cyclic Selective Backpropagation for Memory-Efficient On-Device LLM Fine-Tuning](https://arxiv.org/abs/2602.13073)
*Juneyoung Park,Eunbeen Yoon,Seongwan Kim. Jaeho Lee*

Main category: cs.LG

TL;DR: LCSB is a memory-efficient backpropagation method that selectively computes gradients for only a subset of transformer layers per step, achieving up to 1.4× speedup with minimal quality loss while improving stability in quantized settings.


<details>
  <summary>Details</summary>
Motivation: Existing memory-efficient backpropagation (MeBP) methods require backward computation through all transformer layers at every step, with weight decompression alone consuming 32-42% of backward time. This computational overhead limits efficiency for fine-tuning LLMs on resource-constrained devices.

Method: Layer-Cyclic Selective Backpropagation (LCSB) computes gradients for only a subset of layers per training step. It leverages residual connections to maintain gradient flow through identity paths, while AdamW momentum provides implicit updates for non-selected layers. The method is theoretically interpreted as Block Coordinate Descent on the LoRA parameter space.

Result: LCSB achieves up to 1.40× speedup with less than 2% quality degradation across five models and three tasks. In 4-bit quantized settings, it demonstrates superior stability - a 3B model that diverges under full backpropagation converges smoothly with LCSB, suggesting implicit regularization from selective gradient computation.

Conclusion: LCSB provides an efficient alternative to full backpropagation for fine-tuning LLMs, offering significant speed improvements with minimal accuracy loss while enhancing training stability in low-precision settings through implicit regularization effects.

Abstract: Memory-efficient backpropagation (MeBP) has enabled first-order fine-tuning of large language models (LLMs) on mobile devices with less than 1GB memory. However, MeBP requires backward computation through all transformer layers at every step, where weight decompression alone accounts for 32--42% of backward time. We propose Layer-Cyclic Selective Backpropagation (LCSB), which computes gradients for only a subset of layers per step. Our key insight is that residual connections guarantee gradient flow through identity paths, while AdamW momentum provides implicit updates for non-selected layers. We interpret LCSB as Block Coordinate Descent on the LoRA parameter space, providing theoretical justification for convergence. LCSB achieves up to 1.40$\times$ speedup with less than 2\% quality degradation across five models and three tasks. Surprisingly, in 4-bit quantized settings, LCSB exhibits superior stability: a 3B model that completely diverges under full backpropagation converges smoothly with LCSB, suggesting an implicit regularization effect from selective gradient computation.

</details>


### [167] [Unified Multi-Domain Graph Pre-training for Homogeneous and Heterogeneous Graphs via Domain-Specific Expert Encoding](https://arxiv.org/abs/2602.13075)
*Chundong Liang,Yongqi Huang,Dongxiao He,Peiyuan Li,Yawen Li,Di Jin,Weixiong Zhang*

Main category: cs.LG

TL;DR: GPH² is a unified graph pre-training method that handles both homogeneous and heterogeneous graphs simultaneously, using domain-specific experts and adaptive fusion for downstream tasks.


<details>
  <summary>Details</summary>
Motivation: Most existing graph pre-training methods are designed for either homogeneous OR heterogeneous graphs, but real-world applications often involve mixed graph types with distribution shifts between pre-training and deployment. There's a need for unified graph modeling across diverse graph types.

Method: 1) Unified Multi-View Graph Construction that encodes both homogeneous and heterogeneous graphs without explicit graph-type-specific designs. 2) Domain-specific expert encoding where each expert is independently pre-trained on a single graph to capture domain-specific knowledge. 3) Task-oriented Expert Fusion Strategy that adaptively integrates multiple experts based on their discriminative strengths for downstream tasks.

Result: Extensive experiments on mixed graphs demonstrate that GPH² enables stable transfer across graph types and domains, significantly outperforming existing graph pre-training methods.

Conclusion: A balanced mixture of homogeneous and heterogeneous graph pre-training benefits downstream tasks, and GPH² provides an effective unified framework for cross-graph-type pre-training with domain adaptation capabilities.

Abstract: Graph pre-training has achieved remarkable success in recent years, delivering transferable representations for downstream adaptation. However, most existing methods are designed for either homogeneous or heterogeneous graphs, thereby hindering unified graph modeling across diverse graph types. This separation contradicts real-world applications, where mixed homogeneous and heterogeneous graphs are ubiquitous, and distribution shifts between upstream pre-training and downstream deployment are common. In this paper, we empirically demonstrate that a balanced mixture of homogeneous and heterogeneous graph pre-training benefits downstream tasks and propose a unified multi-domain \textbf{G}raph \textbf{P}re-training method across \textbf{H}omogeneous and \textbf{H}eterogeneous graphs ($\mathbf{GPH^{2}}$). To address the lack of a unified encoder for homogeneous and heterogeneous graphs, we propose a Unified Multi-View Graph Construction that simultaneously encodes both without explicit graph-type-specific designs. To cope with the increased cross-domain distribution discrepancies arising from mixed graphs, we introduce domain-specific expert encoding. Each expert is independently pre-trained on a single graph to capture domain-specific knowledge, thereby shielding the pre-training encoder from the adverse effects of cross-domain discrepancies. For downstream tasks, we further design a Task-oriented Expert Fusion Strategy that adaptively integrates multiple experts based on their discriminative strengths. Extensive experiments on mixed graphs demonstrate that $\text{GPH}^{2}$ enables stable transfer across graph types and domains, significantly outperforming existing graph pre-training methods.

</details>


### [168] [R-Diverse: Mitigating Diversity Illusion in Self-Play LLM Training](https://arxiv.org/abs/2602.13103)
*Gengsheng Li,Jinghan He,Shijie Wang,Dan Zhang,Ruiqi Liu,Renrui Zhang,Zijun Yao,Junfeng Fang,Haiyun Guo,Jinqiao Wang*

Main category: cs.LG

TL;DR: R-Diverse addresses Diversity Illusion in self-play LLM reasoning by introducing Memory-Augmented Penalty and Skill-Aware Measurement to sustain improvement across iterations.


<details>
  <summary>Details</summary>
Motivation: Existing self-play frameworks like R-Zero suffer from non-sustained improvement where early gains degrade over time due to Diversity Illusion - training signals appear diverse but collapse into recurring patterns.

Method: Proposes R-Diverse with two innovations: 1) Memory-Augmented Penalty (MAP) uses persistent memory bank to discourage recycling across iterations, 2) Skill-Aware Measurement (SAM) evaluates diversity by reasoning skills exercised rather than surface question variation.

Result: Across 10 math and general reasoning benchmarks, R-Diverse sustains gains over more iterations and consistently outperforms prior self-play methods.

Conclusion: R-Diverse effectively mitigates Diversity Illusion in self-play LLM reasoning, enabling sustained improvement through better diversity management across training iterations.

Abstract: Self-play bootstraps LLM reasoning through an iterative Challenger-Solver loop: the Challenger is trained to generate questions that target the Solver's capabilities, and the Solver is optimized on the generated data to expand its reasoning skills. However, existing frameworks like R-Zero often exhibit non-sustained improvement, where early gains degrade as self-play continues. We identify a key failure mode, Diversity Illusion, where the Solver's training signals appear diverse yet collapse into recurring underlying patterns. It manifests as (1) Local Diversity Illusion, where diversity is enforced only within-batch, inducing cross-iteration mode cycling; and (2) Surface Diversity Illusion, where questions vary superficially but require near-identical reasoning skills. To mitigate them, we propose R-Diverse with two aligned innovations: Memory-Augmented Penalty (MAP), which uses a persistent memory bank to discourage recycling across iterations, and Skill-Aware Measurement (SAM), which evaluates diversity by the reasoning skills exercised rather than surface variation of questions. Across 10 math and general reasoning benchmarks, R-Diverse sustains gains over more iterations and consistently outperforms prior self-play methods. Code is available at https://github.com/Gengsheng-Li/R-Diverse.

</details>


### [169] [Eventizing Traditionally Opaque Binary Neural Networks as 1-safe Petri net Models](https://arxiv.org/abs/2602.13128)
*Mohamed Tarraf,Alex Chan,Alex Yakovlev,Rishad Shafik*

Main category: cs.LG

TL;DR: A Petri net framework for analyzing and verifying Binary Neural Networks by modeling their operations as event-driven processes to enable causal transparency and formal verification.


<details>
  <summary>Details</summary>
Motivation: BNNs are difficult to explain, validate, and formally verify due to their discrete, highly non-linear behavior, limiting their suitability for safety-critical applications where transparency and behavioral guarantees are essential.

Method: Develop a Petri net-based framework that captures BNN internal operations as event-driven processes. Construct modular PN blueprints for core BNN components (activation, gradient computation, weight updates) and compose them into a complete system-level model.

Result: Validated the composed PN against reference software-based BNN, verified reachability and structural properties (1-safeness, deadlock-freeness, mutual exclusion, correct-by-construction causal sequencing), and assessed scalability/complexity using Workcraft tools.

Conclusion: The framework enables causal introspection of transparent, event-driven BNNs that are amenable to formal reasoning and verification, addressing the opacity problem of BNNs for safety-critical applications.

Abstract: Binary Neural Networks (BNNs) offer a low-complexity and energy-efficient alternative to traditional full-precision neural networks by constraining their weights and activations to binary values. However, their discrete, highly non-linear behavior makes them difficult to explain, validate and formally verify. As a result, BNNs remain largely opaque, limiting their suitability in safety-critical domains, where causal transparency and behavioral guarantees are essential. In this work, we introduce a Petri net (PN)-based framework that captures the BNN's internal operations as event-driven processes. By "eventizing" their operations, we expose their causal relationships and dependencies for a fine-grained analysis of concurrency, ordering, and state evolution. Here, we construct modular PN blueprints for core BNN components including activation, gradient computation and weight updates, and compose them into a complete system-level model. We then validate the composed PN against a reference software-based BNN, verify it against reachability and structural checks to establish 1-safeness, deadlock-freeness, mutual exclusion and correct-by-construction causal sequencing, before we assess its scalability and complexity at segment, component, and system levels using the automated measurement tools in Workcraft. Overall, this framework enables causal introspection of transparent and event-driven BNNs that are amenable to formal reasoning and verification.

</details>


### [170] [Order Matters in Retrosynthesis: Structure-aware Generation via Reaction-Center-Guided Discrete Flow Matching](https://arxiv.org/abs/2602.13136)
*Chenguang Wang,Zihan Zhou,Lei Bai,Tianshu Yu*

Main category: cs.LG

TL;DR: A structure-aware template-free retrosynthesis method using positional inductive bias and discrete flow matching achieves SOTA performance with efficient generation.


<details>
  <summary>Details</summary>
Motivation: Template-free methods are inefficient black-box sequence generators, while semi-template approaches rely on rigid reaction libraries that limit generalization. There's a gap that can be addressed by recognizing that atom ordering in neural representations matters for chemical reactions.

Method: Proposes a structure-aware template-free framework that encodes the two-stage nature of chemical reactions as positional inductive bias. Places reaction center atoms at sequence head to transform implicit chemical knowledge into explicit positional patterns. Uses RetroDiT backbone (graph transformer with rotary position embeddings) to exploit this ordering, combined with discrete flow matching to decouple training from sampling.

Result: Achieves state-of-the-art performance on USPTO-50k (61.2% top-1) and USPTO-Full (51.3% top-1) with predicted reaction centers. With oracle centers: 71.1% and 63.4% respectively, surpassing foundation models trained on 10B reactions while using orders of magnitude less data. Generation in 20-50 steps vs 500 for prior diffusion methods. Ablation shows structural priors outperform scaling: 280K-parameter model with proper ordering matches 65M-parameter model without it.

Conclusion: Proper atom ordering as positional inductive bias significantly improves retrosynthesis performance and efficiency. Structural priors are more effective than brute-force scaling, enabling high performance with smaller models and less data.

Abstract: Template-free retrosynthesis methods treat the task as black-box sequence generation, limiting learning efficiency, while semi-template approaches rely on rigid reaction libraries that constrain generalization. We address this gap with a key insight: atom ordering in neural representations matters. Building on this insight, we propose a structure-aware template-free framework that encodes the two-stage nature of chemical reactions as a positional inductive bias. By placing reaction center atoms at the sequence head, our method transforms implicit chemical knowledge into explicit positional patterns that the model can readily capture. The proposed RetroDiT backbone, a graph transformer with rotary position embeddings, exploits this ordering to prioritize chemically critical regions. Combined with discrete flow matching, our approach decouples training from sampling and enables generation in 20--50 steps versus 500 for prior diffusion methods. Our method achieves state-of-the-art performance on both USPTO-50k (61.2% top-1) and the large-scale USPTO-Full (51.3% top-1) with predicted reaction centers. With oracle centers, performance reaches 71.1% and 63.4% respectively, surpassing foundation models trained on 10 billion reactions while using orders of magnitude less data. Ablation studies further reveal that structural priors outperform brute-force scaling: a 280K-parameter model with proper ordering matches a 65M-parameter model without it.

</details>


### [171] [FlashSchNet: Fast and Accurate Coarse-Grained Neural Network Molecular Dynamics](https://arxiv.org/abs/2602.13140)
*Pingzhi Li,Hongxuan Li,Zirui Liu,Xingcheng Lin,Tianlong Chen*

Main category: cs.LG

TL;DR: FlashSchNet is an IO-aware GNN-MD framework that optimizes GPU memory access patterns to achieve 6.5x speedup over baseline while maintaining SchNet-level accuracy for molecular dynamics simulations.


<details>
  <summary>Details</summary>
Motivation: Current GNN potentials like SchNet improve molecular dynamics simulation accuracy but remain slower than classical force fields due to inefficient GPU memory utilization from fragmented kernels and memory-bound pipelines.

Method: Four IO-aware techniques: (1) flash radial basis - fuses distance computation, basis expansion, and envelope into single tiled pass; (2) flash message passing - fuses cutoff, neighbor gather, filter multiplication, and reduction; (3) flash aggregation - reformulates scatter-add via CSR segment reduce; (4) channel-wise 16-bit quantization of MLP weights.

Result: Achieves 1000 ns/day aggregate throughput over 64 parallel replicas on coarse-grained protein (6.5x faster than baseline with 80% memory reduction), surpassing classical force fields like MARTINI while maintaining SchNet accuracy.

Conclusion: Making GNN-MD IO-aware through careful memory access optimization enables significant performance improvements, allowing GNN potentials to surpass classical force fields in speed while retaining their accuracy advantages.

Abstract: Graph neural network (GNN) potentials such as SchNet improve the accuracy and transferability of molecular dynamics (MD) simulation by learning many-body interactions, but remain slower than classical force fields due to fragmented kernels and memory-bound pipelines that underutilize GPUs. We show that a missing principle is making GNN-MD IO-aware, carefully accounting for reads and writes between GPU high-bandwidth memory (HBM) and on-chip SRAM. We present FlashSchNet, an efficient and accurate IO-aware SchNet-style GNN-MD framework built on four techniques: (1) flash radial basis, which fuses pairwise distance computation, Gaussian basis expansion, and cosine envelope into a single tiled pass, computing each distance once and reusing it across all basis functions; (2) flash message passing, which fuses cutoff, neighbor gather, filter multiplication, and reduction to avoid materializing edge tensors in HBM; (3) flash aggregation, which reformulates scatter-add via CSR segment reduce, reducing atomic writes by a factor of feature dimension and enabling contention-free accumulation in both forward and backward passes; (4) channel-wise 16-bit quantization that exploits the low per-channel dynamic range in SchNet MLP weights to further improve throughput with negligible accuracy loss. On a single NVIDIA RTX PRO 6000, FlashSchNet achieves 1000 ns/day aggregate simulation throughput over 64 parallel replicas on coarse-grained (CG) protein containing 269 beads (6.5x faster than CGSchNet baseline with 80% reduction of peak memory), surpassing classical force fields (e.g. MARTINI) while retaining SchNet-level accuracy and transferability.

</details>


### [172] [Quantization-Robust LLM Unlearning via Low-Rank Adaptation](https://arxiv.org/abs/2602.13151)
*João Vitor Boer Abitante,Joana Meneguzzo Pasquali,Luan Fonseca Garcia,Ewerton de Oliveira,Thomas da Silva Paula,Rodrigo C. Barros,Lucas S. Kupssinskü*

Main category: cs.LG

TL;DR: LLM unlearning with LoRA preserves unlearning effects after 4-bit quantization, preventing quantized models from reverting to pre-unlearning behavior.


<details>
  <summary>Details</summary>
Motivation: Standard LLM unlearning methods fail when models are quantized for efficient deployment, as aggressive low-bit quantization can erase the small parameter changes from full-parameter fine-tuning, causing quantized models to revert to pre-unlearning behavior.

Method: Propose quantization-robust unlearning via low-rank adaptation (LoRA): freeze the base model and concentrate unlearning updates into trainable adapters, preserving the effective update after quantization.

Result: LoRA improves 4-bit utility by up to 7.93 points on BOOKS dataset and 4.76 points on NEWS dataset, substantially reduces privacy leakage under 4-bit PTQ (PrivLeak moves from -25.68 to -5.86), while maintaining strong forgetting performance (VerMem and KnowMem near 0).

Conclusion: Using LoRA for machine unlearning is beneficial for deployment scenarios requiring quantization, as it preserves unlearning effects in quantized models that would otherwise revert to pre-unlearning behavior.

Abstract: Large Language Model (LLM) unlearning aims to remove targeted knowledge from a trained model, but practical deployments often require post-training quantization (PTQ) for efficient inference. However, aggressive low-bit PTQ can mask or erase unlearning updates, causing quantized models to revert to pre-unlearning behavior. We show that standard full-parameter fine-tuning often induce parameter changes that are too small to survive 4-bit quantization. We propose quantization-robust unlearning via low-rank adaptation (LoRA): we freeze the base model and concentrate unlearning into trainable adapters so that the effective update is preserved after quantization. On Llama-2-7B evaluated with MUSE dataset (BOOKS and NEWS), LoRA improves 4-bit utility by up to 7.93 points (NPO+GDR on BOOKS: 50.17 to 58.10) and yields higher 4-bit utility on NEWS for GA+GDR (40.06 to 44.82, increase of 4.76). LoRA also substantially reduces privacy leakage under 4-bit PTQ, e.g., for GA+KLR on BOOKS, PrivLeak moves from -25.68 to -5.86 (closer to ideal 0), while maintaining strong forgetting (VerMem and KnowMem near 0). Thus, using LoRA for Machine Unlearning is beneficial for scenarios where quantization is necessary for model deployment.

</details>


### [173] [Learning to Approximate Uniform Facility Location via Graph Neural Networks](https://arxiv.org/abs/2602.13155)
*Chendi Qian,Christopher Morris,Stefanie Jegelka,Christian Sohler*

Main category: cs.LG

TL;DR: A differentiable MPNN model for Uniform Facility Location that combines approximation algorithm principles with neural networks, achieving provable guarantees and outperforming classical approximation methods without needing supervised training data.


<details>
  <summary>Details</summary>
Motivation: To bridge the gap between learning-based methods (which adapt to data but lack guarantees) and classical approximation algorithms (which have guarantees but can't adapt to data distributions), using Uniform Facility Location as a fundamental example.

Method: Develop a fully differentiable message-passing neural network (MPNN) that embeds approximation-algorithmic principles, avoiding the need for solver supervision or discrete relaxations. The model learns to solve UniFL problems while maintaining provable guarantees.

Result: The approach achieves provable approximation and size generalization guarantees to larger instances than seen during training. Empirically, it outperforms standard non-learned approximation algorithms in solution quality, closing the gap with computationally intensive integer linear programming approaches.

Conclusion: This work provides a step toward bridging learning-based methods and approximation algorithms for discrete optimization, combining the adaptability of neural networks with the theoretical guarantees of approximation algorithms.

Abstract: There has been a growing interest in using neural networks, especially message-passing neural networks (MPNNs), to solve hard combinatorial optimization problems heuristically. However, existing learning-based approaches for hard combinatorial optimization tasks often rely on supervised training data, reinforcement learning, or gradient estimators, leading to significant computational overhead, unstable training, or a lack of provable performance guarantees. In contrast, classical approximation algorithms offer such performance guarantees under worst-case inputs but are non-differentiable and unable to adaptively exploit structural regularities in natural input distributions. We address this dichotomy with the fundamental example of Uniform Facility Location (UniFL), a variant of the combinatorial facility location problem with applications in clustering, data summarization, logistics, and supply chain design. We develop a fully differentiable MPNN model that embeds approximation-algorithmic principles while avoiding the need for solver supervision or discrete relaxations. Our approach admits provable approximation and size generalization guarantees to much larger instances than seen during training. Empirically, we show that our approach outperforms standard non-learned approximation algorithms in terms of solution quality, closing the gap with computationally intensive integer linear programming approaches. Overall, this work provides a step toward bridging learning-based methods and approximation algorithms for discrete optimization.

</details>


### [174] [Learning functional components of PDEs from data using neural networks](https://arxiv.org/abs/2602.13174)
*Torkel E. Loman,Yurij Salmaniw,Antonio Leon Villares,Jose A. Carrillo,Ruth E. Baker*

Main category: cs.LG

TL;DR: Neural networks embedded in PDEs can recover unknown functions from data, demonstrated with nonlocal aggregation-diffusion equations to recover interaction kernels and external potentials.


<details>
  <summary>Details</summary>
Motivation: Partial differential equations often contain unknown functions that are difficult or impossible to measure directly, which hampers our ability to derive predictions from the model. While workflows for recovering scalar PDE parameters from data are well studied, there's a need to extend these approaches to recover functions.

Method: Embed neural networks into PDEs and train them on data to approximate unknown functions with arbitrary accuracy. Use nonlocal aggregation-diffusion equations as a case study to recover interaction kernels and external potentials from steady state data.

Result: The approach successfully recovers functions from data and investigates how factors such as the number of available solutions, their properties, sampling density, and measurement noise affect recovery ability.

Conclusion: The neural network embedding approach is advantageous because it can utilize standard parameter-fitting workflows, and the trained PDE can be treated as a normal PDE for purposes such as generating system predictions.

Abstract: Partial differential equations often contain unknown functions that are difficult or impossible to measure directly, hampering our ability to derive predictions from the model. Workflows for recovering scalar PDE parameters from data are well studied: here we show how similar workflows can be used to recover functions from data. Specifically, we embed neural networks into the PDE and show how, as they are trained on data, they can approximate unknown functions with arbitrary accuracy. Using nonlocal aggregation-diffusion equations as a case study, we recover interaction kernels and external potentials from steady state data. Specifically, we investigate how a wide range of factors, such as the number of available solutions, their properties, sampling density, and measurement noise, affect our ability to successfully recover functions. Our approach is advantageous because it can utilise standard parameter-fitting workflows, and in that the trained PDE can be treated as a normal PDE for purposes such as generating system predictions.

</details>


<div id='eess.SP'></div>

# eess.SP [[Back]](#toc)

### [175] [A Gradient Boosted Mixed-Model Machine Learning Framework for Vessel Speed in the U.S. Arctic](https://arxiv.org/abs/2602.12292)
*Mauli Pant,Linda Fernandez,Indranil Sahoo*

Main category: eess.SP

TL;DR: A two-stage ML framework analyzes Arctic vessel speeds using AIS data, separating zero vs positive speeds, with distance to coast and bathymetry as primary drivers.


<details>
  <summary>Details</summary>
Motivation: Understanding how environmental and operational conditions affect vessel speed is crucial for characterizing Arctic navigational conditions, but analyzing AIS data is challenging due to many zero-speed records that obscure patterns when treated as continuous.

Method: Two-stage machine learning framework: first models probability of SOG > 0 (binary classifier), then models SOG conditional on being positive. Uses gradient boosted decision trees with random effects to handle nonlinear environmental responses and repeated observations. Integrates AIS data with sea ice, wind, bathymetry, distance to coast, vessel group, and navigational status. Uses SHAP values for interpretability.

Result: Positive SOG classifier achieved strong discrimination (AUC = 0.85). Conditional speed model explained ~77% of out-of-fold variance. Distance to coast and bathymetric depth were dominant determinants of both speed likelihood and magnitude. Course, vessel group, and navigational status introduced secondary variation. Wind and sea ice effects were modest.

Conclusion: The two-stage ML framework effectively characterizes Arctic vessel operating regimes, revealing that coastal proximity and bathymetry are primary speed determinants, with implications for speed management and corridor-level assessment in the Arctic.

Abstract: Understanding how environmental and operational conditions influence vessel speed is crucial for characterizing navigational conditions in the Arctic. We analyzed Automatic Identification System (AIS) data from 2010-2019 to examine vessel speed over ground (SOG). Over half of the AIS records showed zero SOG, and treating zero and positive SOG as a single continuous process can obscure important patterns. We therefore applied a two-stage machine learning framework, first modeling the probability of SOG greater than zero and then modeling SOG conditional on being positive. AIS observations were integrated with sea ice concentration, course over ground, wind, bathymetric depth, distance to coast, vessel group, and navigational status. Gradient boosted decision trees with random effects captured nonlinear environmental responses while accounting for repeated observations. The positive SOG classifier achieved strong discrimination (AUC = 0.85), while the conditional speed model explained approximately 77 percent of out-of-fold variance. SHAP values quantified covariate effects by decomposing model predictions into additive contributions from individual variables. Distance to coast and bathymetric depth were dominant determinants of both the likelihood and magnitude of vessel speed, while changes in course, vessel group, and navigational status introduced secondary variation. Wind and sea ice effects were modest. Together, these results empirically characterize Arctic vessel operating regimes relevant to speed management and corridor-level assessment.

</details>


### [176] [Interference-Robust Non-Coherent Over-the-Air Computation for Decentralized Optimization](https://arxiv.org/abs/2602.12426)
*Nicolò Michelusi*

Main category: eess.SP

TL;DR: Proposes interference-robust NCOTA scheme using coordinated random rotations and pseudo-random pilots to maintain unbiased consensus estimates under external interference.


<details>
  <summary>Details</summary>
Motivation: NCOTA computation enables efficient decentralized optimization but is inherently susceptible to external interference, which can bias consensus estimates and deteriorate algorithm convergence.

Method: Apply coordinated random rotation of frame of reference across all nodes and transmit pseudo-random pilot signal to transform external interference into circularly symmetric distribution with zero mean relative to rotated frame.

Result: IR-NCOTA exhibits superior performance over baseline NCOTA algorithm in presence of external interference, demonstrated through numerical results on classification task.

Conclusion: The proposed IR-NCOTA scheme ensures consensus estimates remain unbiased despite external interference, preserving convergence guarantees of underlying optimization algorithms.

Abstract: Non-coherent over-the-air (NCOTA) computation enables low-latency and bandwidth-efficient decentralized optimization by exploiting the average energy superposition property of wireless channels. It has recently been proposed as a powerful tool for executing consensus-based optimization algorithms in fully decentralized systems. A key advantage of NCOTA is that it enables unbiased consensus estimation without channel state information at either transmitters or receivers, requires no transmission scheduling, and scales efficiently to dense network deployments. However, NCOTA is inherently susceptible to external interference, which can bias the consensus estimate and deteriorate the convergence of the underlying decentralized optimization algorithm. In this paper, we propose a novel interference-robust (IR-)NCOTA scheme. The core idea is to apply a coordinated random rotation of the frame of reference across all nodes, and transmit a pseudo-random pilot signal, allowing to transform external interference into a circularly symmetric distribution with zero mean relative to the rotated frame. This ensures that the consensus estimates remain unbiased, preserving the convergence guarantees of the underlying optimization algorithm. Through numerical results on a classification task, it is demonstrated that IR-NCOTA exhibits superior performance over the baseline NCOTA algorithm in the presence of external interference.

</details>


### [177] [Practical RIS Gain without the Pain: Randomization and Opportunistic Scheduling in 5G NR](https://arxiv.org/abs/2602.12437)
*L. Yashvanth,Raju Malleboina,Venkatareddy Akumalla,Nekkanti Guna Sai Kiran,Debdeep Sarkar,Chandra R. Murthy*

Main category: eess.SP

TL;DR: Experimental demonstration shows random RIS phase configurations with 5G NR PF scheduling achieve near-optimal throughput comparable to optimized RIS designs.


<details>
  <summary>Details</summary>
Motivation: To experimentally evaluate the performance gains of reconfigurable intelligent surfaces (RIS) integrated with real-time 5G NR systems, and to determine if simple random RIS configurations can achieve performance comparable to optimized designs.

Method: Built an in-house RIS integrated with a real-time 5G NR system using OpenAirInterface (OAI) framework. Quantified throughput gains with RIS, tested random RIS phase configurations combined with 5G NR's proportional fair (PF) scheduling, and evaluated key metrics including RSRP, BLER, MCS index, and throughput.

Result: Random RIS configurations with PF scheduling yield near-optimal throughput when PF scheduler's averaging window is chosen properly. Performance metrics show comparable results to optimized RIS designs, demonstrating that random RIS with negligible overhead can deliver similar performance in real-world 5G systems.

Conclusion: Randomly configured RIS combined with 5G NR's inherent PF scheduling mechanism can achieve performance comparable to optimized RIS designs, offering a practical low-overhead solution for real-world 5G wireless communication systems.

Abstract: We experimentally demonstrate the performance gains achieved by an in-house built reconfigurable intelligent surface (RIS) integrated with a real-time 5G new radio (NR) system implemented using the OpenAirInterface (OAI) framework. We first quantify the gain in throughput achievable by integrating an RIS with a 5G system. Next, we show that randomly setting the RIS phase configuration and leveraging the inherent proportional fair (PF) scheduling mechanism of 5G NR can yield near-optimal throughput, provided the throughput averaging window of the PF scheduler is chosen judiciously. This occurs because, in each time slot, the PF scheduler naturally prioritizes data transmission to the user equipment (UE) that experiences the best channel conditions, namely, the UE to which the randomly configured RIS is aligned. Subsequently, we experimentally evaluate key performance metrics, including the reference signal received power (RSRP), block error rate (BLER), modulation and coding scheme (MCS) index, and throughput, under random RIS configurations. These results confirm that even a randomly configured RIS with negligible overhead can deliver performance comparable to optimized RIS designs, in real-world 5G NR wireless communication systems.

</details>


### [178] [Task- and Metric-Specific Signal Quality Indices for Medical Time Series](https://arxiv.org/abs/2602.12478)
*Jad Haidamous,Christoph Hoog Antink*

Main category: eess.SP

TL;DR: Proposes perturbation-based signal quality index (pSQI) that measures algorithm performance degradation under worst-case noise, outperforming existing methods for ECG/PPG analysis without training.


<details>
  <summary>Details</summary>
Motivation: Existing signal quality indices are task-agnostic and don't account for specific algorithms and performance metrics, while automated medical signal analysis increasingly informs clinical decisions requiring reliable quality assessment.

Method: Formalizes signal quality as task- and metric-dependent concept; defines pSQI as worst-case performance metric value under additive colored Gaussian noise perturbation with lower-bounded signal-to-noise ratio; introduces formal requirements for SQIs including monotonicity and maximal separation.

Result: pSQI consistently outperforms existing feature- and deep learning-based SQIs in identifying unreliable inputs for R-peak detection and atrial fibrillation classification benchmarks, without requiring training.

Conclusion: Task- and metric-specific signal quality assessment via perturbation-based approach provides superior reliability detection for medical time series analysis compared to traditional methods.

Abstract: Medical time series such as electrocardiograms (ECGs) and photoplethysmograms (PPGs) are frequently affected by measurement artifacts due to challenging acquisition environments, such as in ambulances and during routine daily activities. Since automated algorithms for analyzing such signals increasingly inform clinically relevant decisions, identifying signal segments on which these algorithms may produce unreliable outputs is of critical importance. Signal quality indices (SQIs) are commonly used for this purpose. However, most existing SQIs are task agnostic and do not account for the specific algorithm and performance metric used downstream. In this work, we formalize signal quality as a task- and metric-dependent concept and propose a perturbation-based SQI (pSQI) that aims to detect an algorithm's performance degradation on an input signal with respect to a metric. The pSQI is defined as the worst-case value of the performance metric under an additive, colored Gaussian noise perturbation with a lower-bounded signal-to-noise ratio. We introduce formal requirements for task- and metric-specific SQIs, including monotonicity of the metric in expectation and maximal separation under thresholding. Experiments on R-peak detection and atrial fibrillation classification benchmarks demonstrate that the proposed pSQI consistently outperforms existing feature- and deep learning-based SQIs in identifying unreliable inputs without requiring training.

</details>


### [179] [Generative Site-Specific Beamforming via Information-Maximizing Codebook](https://arxiv.org/abs/2602.12552)
*Cheng-Jie Zhao,Zhaolin Wang,Yuanwei Liu*

Main category: eess.SP

TL;DR: GenSSBF framework combines SIM codebook with CFM-based beam generator to create site-specific beams from coarse RSRP feedback, achieving near-optimal performance with minimal overhead.


<details>
  <summary>Details</summary>
Motivation: Traditional beamforming requires extensive beam sweeping overhead and detailed channel state information. The paper aims to reduce feedback overhead while maintaining high beamforming performance by learning site-specific propagation environments.

Method: 1) Construct SIM codebook to maximize mutual information between RSRP feedback and site-specific channel, reducing initial beam sweeping. 2) Develop CFM-based generative model using RSRP feedback as conditional prior to capture beam generation uncertainty. 3) Generate small set of candidate beams from posterior distribution, with final beam selected by UE.

Result: Achieves beamforming performance nearly identical to maximum ratio transmission while requiring only 8 probing beams and 8 candidate beams, significantly reducing overhead compared to traditional methods.

Conclusion: The GenSSBF framework effectively learns site-specific radio environments and generates high-fidelity beams from coarse feedback, offering a practical solution for efficient beamforming with minimal overhead.

Abstract: A novel generative site-specific beamforming (GenSSBF) framework is proposed, which integrates a site-information-maximizing (SIM) codebook with a conditional flow matching (CFM)-based beam generator. By this framework, the site-specific radio propagation environment is learned at the base station (BS), enabling the generation of high fidelity communication beams from coarse reference-signal-received-power (RSRP) feedback provided by user equipments (UEs). In the proposed design, a low-dimensional SIM probing codebook is first constructed by maximizing the mutual information between the RSRP feedback and the site-specific channel. This design not only reduces the initial beam sweeping overhead, but also enhances the amount of channel state information conveyed through UE feedback. By treating the RSRP feedback as a conditional prior, a CFM-based generative model is further developed to explicitly capture the uncertainty in beam generation. Specifically, a small set of UE-specific candidate beams is generated by inferring the learned generative model and sampling from the corresponding posterior distribution, after which the final data transmission beam is selected by the UE. Extensive simulation results demonstrate the effectiveness of both the proposed SIM codebook and the CFM-based beam generator. The proposed GenSSBF framework achieves beamforming performance nearly identical to maximum ratio transmission while requiring only eight probing beams and eight candidate beams.

</details>


### [180] [GNSS Jamming Detection with Automatic Gain Control (AGC) and Carrier-to-Noise Ratio Density (CNO) Observables from a COTS receiver](https://arxiv.org/abs/2602.12688)
*Syed Ali Kazim,Anas Darwich,Juliette Marais*

Main category: eess.SP

TL;DR: The paper analyzes the impact of chirp interference on GPS L1 receiver observables and detection performance, focusing on GNSS vulnerabilities in rail automation systems.


<details>
  <summary>Details</summary>
Motivation: As rail transport moves toward higher automation (R2DATO project), accurate GNSS-based localization is essential for safety-critical applications like ATO and virtual coupling. However, GNSS signals are vulnerable to RFI (jamming/spoofing), which threatens system reliability and safety.

Method: The study evaluates impact of various interference signals (AM, FM, pulsed, frequency hopping, chirp) on GNSS observables (AGC, CNO) using Commercial Off-The-Shelf (COTS) equipment. This work specifically focuses on analyzing chirp interference effects on GPS L1 receiver observables and detection performance.

Result: The paper presents analysis of chirp interference impact on GPS L1 receiver observables, though specific quantitative results are not provided in the abstract. The study contributes to understanding GNSS vulnerability in rail applications.

Conclusion: GNSS interference poses significant risks to rail automation systems, and understanding these vulnerabilities is crucial for developing robust localization solutions. The analysis of chirp interference effects provides insights for improving interference detection and mitigation in safety-critical rail applications.

Abstract: As rail transport moves toward higher degrees of automation under initiatives like the R2DATO project [1], accurate and reliable train localization has become essential. Global Satellite Navigation System (GNSS) is considered as a main technology in enabling operational advancements including Automatic Train Operation (ATO), moving block signaling, and virtual coupling, which are the core components of the Horizon Europe 2024 rail digitalization agenda. However, GNSS signal integrity is increasingly threatened by intentional and unintentional radio frequency interference (RFI). This include jamming and spoofing, which are particularly concerning as the broadcasted signal can deliberately disrupt or manipulate the GNSS signal. - Jamming refers to an intentional form of interference that induces disturbances in the GNSS band, causing performance degradation or can even entirely block the receiver from acquiring the satellite signals. - Spoofing involves broadcasting counterfeit satellite signals to deceive the GNSS receiver, leading to inaccurate estimation of position, navigation and timing information. This concern about interference is not unique to rail applications. The aeronautical sector has long recognized the risks posed by GNSS interference, with extensive documentation on its impact on navigation, landing procedures, and surveillance systems. In recent years, awareness of these risks has expanded to other transport sectors. Within the automotive industry, particularly in Intelligent Transport Systems (ITS), several studies [2][3][4] have addressed the vulnerability of GNSS against interference. Similar concerns are now emerging in the rail domain [5][6][7], especially as GNSS is increasingly adopted in safety-critical applications. In literature, several levels of actions have been explored, ranging from merely the detection of a malicious signal at the initial phase to the application of advanced signal processing methods aimed at suppressing the effects of interference [8]. In alignment with the goal of the R2DATO project, we evaluated the impact of various classes of interference signals such as amplitude modulation (AM), frequency modulation (FM), pulsed, frequency hopping and chirp signals on the GNSS observables including Automatic Gain Control (AGC) and Carrier to Noise Ratio (CNO) as measured by a Commercial Off-The-Shelf (COTS). However, in this work, the analysis is only limited to impact of chirp interference on GPS L1 receiver observables and detection performance.

</details>


### [181] [Flexible RISs: Learning-based Array Manifold Estimation and Phase-shift Optimization](https://arxiv.org/abs/2602.12757)
*Mohamadreza Delbari,Ehsan Mohammadi,Mostafa Darabi,Arash Asadi,Alejandro Jiménez-Sáez,Vahid Jamali*

Main category: eess.SP

TL;DR: A deep learning framework for optimizing phase shifts of non-planar RIS using sparse power measurements to estimate surface geometry and achieve optimal beamforming.


<details>
  <summary>Details</summary>
Motivation: Practical RIS deployments often involve non-planar surfaces (structural columns, curved facades) where standard planar beamforming models fail, and existing solutions are limited to specific pre-defined geometries.

Method: Proposes a DL framework with: 1) low-dimensional parametric model to capture arbitrary surface curvature, 2) neural network using sparse received power measurements to estimate surface geometry and derive optimal phase configuration.

Result: Simulations show fast convergence and significant performance improvement over conventional planar beamforming designs, with robustness against arbitrary surface curvature. Also analyzes impact of measurement location errors.

Conclusion: The proposed DL framework effectively addresses the limitation of existing solutions by enabling optimization of non-planar RIS phase shifts using sparse measurements, demonstrating practical viability for real-world curved surface deployments.

Abstract: Reconfigurable intelligent surfaces (RISs) are envisioned as a key enabler for next-generation wireless networks, offering programmable control over propagation environments. While extensive research focuses on planar RIS architectures, practical deployments often involve non-planar surfaces, such as structural columns or curved facades, where standard planar beamforming models fail. Moreover, existing analytical solutions for curved RISs are often restricted to specific, pre-defined array manifold geometries. To address this limitation, this paper proposes a novel deep learning (DL) framework for optimizing the phase shifts of non-planar RISs. We first introduce a low-dimensional parametric model to capture arbitrary surface curvature effectively. Based on this, we design a neural network (NN) that utilizes a sparse set of received power measurements to estimate the surface geometry and derive the optimal phase configuration. Simulation results demonstrate that the proposed algorithm converges fast and significantly outperforms conventional planar beamforming designs, validating its robustness against arbitrary surface curvature. We also analyze the impact of the measurement location error on the algorithm's performance.

</details>


### [182] [Comparison of OTFS and OFDM for RIS-aided Systems in the Presence of Phase Noise](https://arxiv.org/abs/2602.12804)
*Stephen McWade,Arman Farhang*

Main category: eess.SP

TL;DR: RIS-aided OTFS outperforms RIS-aided OFDM in phase noise environments, with a proposed joint channel/phase noise estimation method achieving up to 3dB BER improvement.


<details>
  <summary>Details</summary>
Motivation: Phase noise limits RIS-aided OFDM performance, while OTFS shows resilience to time-varying channels but lacks analysis of phase noise effects in RIS-aided systems.

Method: Proposed joint RIS channel and phase noise estimation using Wiener filtering approach that exploits statistical properties of both phase noise and Doppler spread channel.

Result: RIS-aided OTFS offers significant gains over RIS-aided OFDM in phase noise environments, and the proposed estimation method achieves up to 3dB BER improvement over existing methods.

Conclusion: OTFS is superior to OFDM for RIS-aided systems in phase noise conditions, and the proposed Wiener-based joint estimation effectively addresses phase noise challenges in such systems.

Abstract: In this paper, we investigate the performance of RIS-aided orthogonal time frequency space (OTFS) and orthogonal frequency division multiplexing (OFDM) systems in the presence of oscillator phase noise. OFDM is known to be sensitive to phase noise, which could limit the potential gains promised by RIS systems. OTFS, on the other hand, is a compelling potential waveform for RIS-aided systems in the presence of phase noise due to it's resilience to time-varying channels. However, the effect of phase noise on OTFS has not been fully analyzed in the literature as of yet. Additionally, no existing works in the literature consider the effect of phase noise on an RIS-aided OTFS system. Hence, we propose a joint RIS channel and phase noise estimation technique using a Wiener filtering approach. Our proposed method exploits the statistical nature of both the phase noise and the Doppler spread channel in a setup with RIS. Our numerical analysis demonstrates the significant gain of RIS-aided OTFS offers compared to RIS-aided OFDM in the presence in the presence of phase noise. Additionally, our results demonstrate the superiority of our proposed estimation technique, with gains of up to 3~dB in terms of bit error rate (BER), over existing methods in the literature.

</details>


### [183] [HoRAMA: Holistic Reconstruction with Automated Material Assignment for Ray Tracing using NYURay](https://arxiv.org/abs/2602.12942)
*Mingjun Ying,Guanyue Qian,Xinquan Wang,Peijie Ma,Dipankar Shakya,Theodore S. Rappaport*

Main category: eess.SP

TL;DR: HoRAMA automates 3D model reconstruction for wireless ray tracing from smartphone videos, reducing reconstruction time from 2 months to 16 hours while maintaining comparable prediction accuracy.


<details>
  <summary>Details</summary>
Motivation: Manual 3D model reconstruction for wireless ray tracing requires weeks of expert effort, creating scalability bottlenecks. Traditional vision-based methods produce geometrically defective meshes without material properties needed for accurate propagation prediction.

Method: HoRAMA integrates MASt3R-SLAM dense point cloud generation with vision language model-assisted material assignment to create RT-compatible 3D models from RGB video captured by smartphones or low-cost cameras.

Result: HoRAMA achieves 2.28 dB RMSE for matched multipath component power predictions, comparable to manual 3D model baseline (2.18 dB), while reducing reconstruction time from two months to 16 hours in a 700 square meter factory test.

Conclusion: HoRAMA enables scalable wireless digital twin creation for 5G/6G network planning, infrastructure deployment, and beam management, with potential for real-time edge implementation.

Abstract: Next-generation wireless networks at upper mid-band and millimeter-wave frequencies require accurate site-specific deterministic channel propagation prediction. Wireless ray tracing (RT) provides site-specific predictions but demands high-fidelity three-dimensional (3D) environment models with material properties. Manual 3D model reconstruction achieves high accuracy but requires weeks of expert effort, creating scalability bottlenecks for large environment reconstruction. Traditional vision-based 3D reconstruction methods lack RT compatibility due to geometrically defective meshes and missing material properties. This paper presents Holistic Reconstruction with Automated Material Assignment (HoRAMA) for wireless propagation prediction using NYURay. HoRAMA generates RT-compatible 3D models from RGB video readily captured using a smartphone or low-cost portable camera, by integrating MASt3R-SLAM dense point cloud generation with vision language model-assisted material assignment. The HoRAMA 3D reconstruction method is verified by comparing NYURay RT predictions, using both manually created and HoRAMA-generated 3D models, against field measurements at 6.75 GHz and 16.95 GHz across 12 TX-RX locations in a 700 square meter factory. HoRAMA ray tracing predictions achieve a 2.28 dB RMSE for matched multipath component (MPC) power predictions, comparable to the manually created 3D model baseline (2.18 dB), while reducing 3D reconstruction time from two months to 16 hours. HoRAMA enables scalable wireless digital twin creation for RT network planning, infrastructure deployment, and beam management in 5G/6G systems, as well as eventual real-time implementation at the edge.

</details>


### [184] [RIS Nearfield Position and Velocity Estimation Using a Validated Propagation Model](https://arxiv.org/abs/2602.12979)
*Thomas Zemen,Musa Furkan Keskin,Moustafa Rahal,Thomas Wilding,Hamed Radpour,Markus Hofer,Benoit Denis,Henk Wymeersch*

Main category: eess.SP

TL;DR: RIS-based indoor positioning using multi-step algorithm achieves 7mm position and 0.12m/s velocity errors at 2m distance.


<details>
  <summary>Details</summary>
Motivation: To address NLOS indoor positioning challenges using reconfigurable intelligent surfaces for accurate position and velocity estimation.

Method: Multi-step estimation algorithm using compound RIS prototype with 1-bit phase control per unit cell, with modified robust algorithm to handle near-field effects and antenna pattern considerations.

Result: Modified three-step algorithm achieves 7 mm position error and 0.12 m/s velocity error at 2 m distance to RIS center under realistic propagation model.

Conclusion: RIS-based positioning is feasible for indoor NLOS scenarios, but antenna patterns and near-field effects must be addressed through robust algorithms for accurate estimation.

Abstract: We investigate reconfigurable intelligent surfaces (RISs) for the task of position and velocity estimation in non-LOS (NLOS) indoor scenarios, using a snapshot based multi-step estimation algorithm. We evaluate a compound RIS structure prototype composed of four RIS tiles with 1-bit phase control per RIS unit cell. Numerical simulation results taking the antenna patterns into account are presented for an 3 m x 3 m area of interest. We demonstrate that the initial grid search step using the far field assumption is not robust enough for small distances to the RIS center and propose a more robust algorithm. Furthermore, we show that the effect of the antenna pattern causes an increased position and velocity error. Our modified three-step algorithm achieves a position error of 7 mm and a velocity error of 0.12 m/s at a distance of 2 m to the RIS center under a realistic numerical propagation model.

</details>


### [185] [Represent Micro-Doppler Signature in Orders](https://arxiv.org/abs/2602.12985)
*Weicheng Gao*

Main category: eess.SP

TL;DR: Proposes Chebyshev-time map method using polynomial orders to characterize micro-Doppler signatures for distinguishing armed vs unarmed human activities in through-the-wall radar, achieving better accuracy with compressed data dimensions.


<details>
  <summary>Details</summary>
Motivation: Current through-the-wall radar systems struggle to distinguish similar indoor human activities (like gun carrying vs normal walking) due to minimal micro-Doppler signature differences, and large time-frequency spectrogram images create training/inference efficiency challenges.

Method: Establishes parametric kinematic models for human motion and TWR echo model, then proposes time-frequency feature representation using orthogonal Chebyshev polynomial decomposition. Extracts kinematic envelopes of torso/limbs and maps time-frequency spectrum slices into Chebyshev-time coefficient space to preserve multi-order morphological details.

Result: Numerical simulations and experiments verify the method effectively characterizes armed/unarmed indoor human activities while compressing time-frequency spectrum scale, achieving balance between recognition accuracy and input data dimensions.

Conclusion: Chebyshev-time map method successfully addresses the challenge of distinguishing similar human activities in through-the-wall radar by providing robust feature representation with compressed data dimensions, enabling more efficient and accurate non-line-of-sight human activity sensing.

Abstract: Non-line-of-sight sensing of human activities in complex environments is enabled by multiple-input multiple-output through-the-wall radar (TWR). However, the distinctiveness of micro-Doppler signature between similar indoor human activities such as gun carrying and normal walking is minimal, while the large scale of input images required for effective identification utilizing time-frequency spectrograms creates challenges for model training and inference efficiency. To address this issue, the Chebyshev-time map is proposed in this paper, which is a method characterizing micro-Doppler signature using polynomial orders. The parametric kinematic models for human motion and the TWR echo model are first established. Then, a time-frequency feature representation method based on orthogonal Chebyshev polynomial decomposition is proposed. The kinematic envelopes of the torso and limbs are extracted, and the time-frequency spectrum slices are mapped into a robust Chebyshev-time coefficient space, preserving the multi-order morphological detail information of time-frequency spectrum. Numerical simulations and experiments are conducted to verify the effectiveness of the proposed method, which demonstrates the capability to characterize armed and unarmed indoor human activities while effectively compressing the scale of the time-frequency spectrum to achieve a balance between recognition accuracy and input data dimensions. The open-source code of this paper can be found in: https://github.com/JoeyBGOfficial/Represent-Micro-Doppler-Signature-in-Orders.

</details>


### [186] [Near-Field Beampointing with Low Exposure Regions: a Dominant Subspace Projection Approach](https://arxiv.org/abs/2602.13023)
*Laurence Defraigne,Gilles Monnoyer,Jérôme Louveaux,Luc Vandendorpe*

Main category: eess.SP

TL;DR: A low-complexity algorithm for near-field beam pattern design with low exposure region constraints using SVD-based subspace representation.


<details>
  <summary>Details</summary>
Motivation: Near-field spherical wavefronts enable advanced beamforming, but designing beam patterns with continuous low exposure region constraints leads to prohibitive computational complexity due to spatial discretization.

Method: Proposes a novel low-complexity algorithm using singular value decomposition to create a low-dimensional subspace representation of the low exposure region, enabling computationally tractable beam pattern design.

Result: The approach achieves low complexity while providing power received at target user close to optimal achievable power, with uniform power mitigation over the low exposure region.

Conclusion: The SVD-based subspace representation effectively addresses computational complexity challenges in near-field beam pattern design with continuous region constraints.

Abstract: The spherical nature of the wavefronts exhibited in the near-field of antenna arrays enables advanced beamforming capabilities, such as beampointing and beamnulling. In this paper, we exploit these properties to design a near-field beam pattern under a low exposure region constraint. We address the continuous region constraint through spatial discretization, which results in a large number of constraints that lead to prohibitive computational complexity. We propose a novel low-complexity algorithm that enables a computationally tractable beam pattern design. It uses a low-dimensional subspace representation of the low exposure region based on a singular value decomposition. Our approach achieves low complexity while providing a power received at a target user close to the optimal achievable power, yet with uniform power mitigation over the low exposure region.

</details>


### [187] [Properties of Near Field Focusing for Three-Dimensional Large Intelligent Surface](https://arxiv.org/abs/2602.13115)
*Jiawang Li,Mats Gustafsson,Alireza Saberkari,Buon Kiong Lau*

Main category: eess.SP

TL;DR: 3D LIS near-field focusing across frequencies/polarizations with optimal excitation strategies under different power constraints.


<details>
  <summary>Details</summary>
Motivation: Investigate near-field focusing using 3D distributed LIS elements in a corridor rather than planar apertures, examining how different power constraints affect optimal focusing solutions across frequencies and polarizations.

Method: Formulate optimization problems under local and global power constraints for continuous apertures, then validate with discretized Hertzian dipole arrays using analytical results and full-wave simulation.

Result: Under global constraint: optimal current matches time-reversal solution. Under local constraint: matches conjugate-phase solution. When both active: larger excitation magnitudes to elements closer to illumination field. Behavior invariant with frequency/polarization for fixed-size LIS. CP method yields identical transverse/longitudinal resolution across polarizations, while TR method shows differences.

Conclusion: 3D LIS near-field focusing strategies depend on power constraints, with optimal solutions showing consistent behavior across frequencies and polarizations, providing insights for practical implementation of distributed aperture systems.

Abstract: This work investigates near-field focusing using a three-dimensional (3D) large intelligent surface (LIS) across frequencies and polarizations. Specifically, the LIS elements are distributed in 3D space within a long corridor, rather than being confined to a single planar aperture, and the focal point is located at a prescribed position in the radiating near field. By formulating optimization problems under both local and global power constraints, we obtain the corresponding optima. For continuous apertures, the optimal current magnitude distribution matches time-reversal (TR) solution under the global constraint and conjugate-phase (CP) solution when the local constraint dominates. When both constraints are active, the solution assigns larger excitation magnitudes to elements closer to the illumination field. This behavior remains invariant with respect to frequency and polarization for a fixed-size LIS. These findings are consistent to the more practical case of using discretized apertures in the form of Hertzian dipole arrays, studied using both analytical results and full-wave simulation. In addition, with the CP method, specific polarizations lead to identical transverse and longitudinal resolution, in contrast, under the TR method, these quantities can differ across polarizations.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [188] [LatentAM: Real-Time, Large-Scale Latent Gaussian Attention Mapping via Online Dictionary Learning](https://arxiv.org/abs/2602.12314)
*Junwoon Lee,Yulun Tian*

Main category: cs.RO

TL;DR: LatentAM is an online 3D Gaussian Splatting framework for open-vocabulary robotic perception that builds scalable latent feature maps from RGB-D streams using model-agnostic dictionary learning instead of VLM distillation.


<details>
  <summary>Details</summary>
Motivation: Current methods for open-vocabulary 3D scene understanding rely on distilling high-dimensional Vision-Language Model embeddings using model-specific decoders, which limits flexibility and requires pretraining. There's a need for a more flexible, model-agnostic approach that can work with different VLMs at test time without extensive pretraining.

Method: LatentAM associates each Gaussian primitive with a compact query vector that converts to approximate VLM embeddings via attention with a learnable dictionary. The dictionary is initialized from streaming observations and optimized online with trust-region regularization. For scalability, it uses voxel hashing for map management, restricting optimization to active local GPU maps while storing the global map on CPU.

Result: LatentAM achieves significantly better feature reconstruction fidelity than state-of-the-art methods on public benchmarks and a large-scale custom dataset, while maintaining near-real-time performance (12-35 FPS).

Conclusion: LatentAM provides an effective, scalable solution for open-vocabulary 3D mapping that is model-agnostic, pretraining-free, and achieves high-quality feature reconstruction with real-time performance suitable for robotic applications.

Abstract: We present LatentAM, an online 3D Gaussian Splatting (3DGS) mapping framework that builds scalable latent feature maps from streaming RGB-D observations for open-vocabulary robotic perception. Instead of distilling high-dimensional Vision-Language Model (VLM) embeddings using model-specific decoders, LatentAM proposes an online dictionary learning approach that is both model-agnostic and pretraining-free, enabling plug-and-play integration with different VLMs at test time. Specifically, our approach associates each Gaussian primitive with a compact query vector that can be converted into approximate VLM embeddings using an attention mechanism with a learnable dictionary. The dictionary is initialized efficiently from streaming observations and optimized online to adapt to evolving scene semantics under trust-region regularization. To scale to long trajectories and large environments, we further propose an efficient map management strategy based on voxel hashing, where optimization is restricted to an active local map on the GPU, while the global map is stored and indexed on the CPU to maintain bounded GPU memory usage. Experiments on public benchmarks and a large-scale custom dataset demonstrate that LatentAM attains significantly better feature reconstruction fidelity compared to state-of-the-art methods, while achieving near-real-time speed (12-35 FPS) on the evaluated datasets. Our project page is at: https://junwoonlee.github.io/projects/LatentAM

</details>


### [189] [ForeAct: Steering Your VLA with Efficient Visual Foresight Planning](https://arxiv.org/abs/2602.12322)
*Zhuoyang Zhang,Shang Yang,Qinghao Hu,Luke J. Huang,James Hou,Yufei Sun,Yao Lu,Song Han*

Main category: cs.RO

TL;DR: ForeAct is a Visual Foresight Planning framework that uses imagined future observations to guide Vision-Language-Action models, achieving 87.4% success rate on real-world tasks with 40.9% improvement over baseline.


<details>
  <summary>Details</summary>
Motivation: VLA models struggle with converting high-level language instructions into executable actions in open-world environments. Current approaches lack efficient visual foresight planning that can guide VLAs step-by-step using imagined future observations.

Method: ForeAct combines an efficient foresight image generator (predicts 640×480 future observations in 0.33s) with a vision-language model. The generator is pretrained on 1M+ multi-task episodes, and the system produces subtask descriptions for both the generator and VLA. VLAs can integrate it seamlessly by augmenting visual inputs.

Result: Achieves 87.4% average success rate on 11 diverse real-world tasks, with +40.9% absolute improvement over baseline (46.5%) and +30.3% improvement over baseline with textual subtask guidance (57.1%).

Conclusion: Visual foresight planning significantly improves VLA performance by enabling step-by-step guidance through imagined future observations, allowing VLAs to focus on visuo-motor inference rather than high-level reasoning, leading to better accuracy and generalization.

Abstract: Vision-Language-Action (VLA) models convert high-level language instructions into concrete, executable actions, a task that is especially challenging in open-world environments. We present Visual Foresight Planning (ForeAct), a general and efficient planner that guides a VLA step-by-step using imagined future observations and subtask descriptions. With an imagined future observation, the VLA can focus on visuo-motor inference rather than high-level semantic reasoning, leading to improved accuracy and generalization. Our planner comprises a highly efficient foresight image generation module that predicts a high-quality 640$\times$480 future observation from the current visual input and language instruction within only 0.33s on an H100 GPU, together with a vision-language model that reasons over the task and produces subtask descriptions for both the generator and the VLA. Importantly, state-of-the-art VLAs can integrate our planner seamlessly by simply augmenting their visual inputs, without any architectural modification. The foresight generator is pretrained on over 1 million multi-task, cross-embodiment episodes, enabling it to learn robust embodied dynamics. We evaluate our framework on a benchmark that consists of 11 diverse, multi-step real-world tasks. It achieves an average success rate of 87.4%, demonstrating a +40.9% absolute improvement over the $π_0$ baseline (46.5%) and a +30.3% absolute improvement over $π_0$ augmented with textual subtask guidance (57.1%).

</details>


### [190] [Schur-MI: Fast Mutual Information for Robotic Information Gathering](https://arxiv.org/abs/2602.12346)
*Kalvik Jakkala,Jason O'Kane,Srinivas Akella*

Main category: cs.RO

TL;DR: Schur-MI: A faster Gaussian process mutual information formulation for real-time robotic information gathering using Schur complements and precomputation.


<details>
  <summary>Details</summary>
Motivation: Mutual information (MI) is theoretically strong for robotic information gathering but computationally expensive for real-time planning due to repeated log-determinant evaluations.

Method: Schur-MI leverages iterative planning structure to precompute/reuse expensive intermediate quantities and uses Schur-complement factorization to avoid large determinant computations.

Result: Reduces MI computation from O(|V|³) to O(|A|³), achieving up to 12.7× speedup on real-world datasets and validated in field trials with autonomous surface vehicles.

Conclusion: Schur-MI makes MI computation tractable for online planning, bridging the gap between information-theoretic objectives and real-time robotic exploration.

Abstract: Mutual information (MI) is a principled and widely used objective for robotic information gathering (RIG), providing strong theoretical guarantees for sensor placement (SP) and informative path planning (IPP). However, its high computational cost, dominated by repeated log-determinant evaluations, has limited its use in real-time planning. This letter presents Schur-MI, a Gaussian process (GP) MI formulation that (i) leverages the iterative structure of RIG to precompute and reuse expensive intermediate quantities across planning steps, and (ii) uses a Schur-complement factorization to avoid large determinant computations. Together, these methods reduce the per-evaluation cost of MI from $\mathcal{O}(|\mathcal{V}|^3)$ to $\mathcal{O}(|\mathcal{A}|^3)$, where $\mathcal{V}$ and $\mathcal{A}$ denote the candidate and selected sensing locations, respectively. Experiments on real-world bathymetry datasets show that Schur-MI achieves up to a $12.7\times$ speedup over the standard MI formulation. Field trials with an autonomous surface vehicle (ASV) performing adaptive IPP further validate its practicality. By making MI computation tractable for online planning, Schur-MI helps bridge the gap between information-theoretic objectives and real-time robotic exploration.

</details>


### [191] [LongNav-R1: Horizon-Adaptive Multi-Turn RL for Long-Horizon VLA Navigation](https://arxiv.org/abs/2602.12351)
*Yue Hu,Avery Xi,Qixin Xiao,Seth Isaacson,Henry X. Liu,Ram Vasudevan,Maani Ghaffari*

Main category: cs.RO

TL;DR: LongNav-R1 is a multi-turn RL framework that optimizes VLA models for long-horizon navigation by treating navigation as continuous conversation with the environment, enabling causal reasoning and online learning.


<details>
  <summary>Details</summary>
Motivation: Existing single-turn paradigms for visual-language-action models are insufficient for long-horizon navigation tasks. They lack the ability to reason about historical interactions and sequential future outcomes, and often suffer from behavioral rigidity imposed by human demonstrations.

Method: Develops LongNav-R1, an end-to-end multi-turn RL framework that reformulates navigation as continuous conversation between VLA policy and environment. Introduces Horizon-Adaptive Policy Optimization for accurate temporal credit assignment over extended sequences.

Result: With only 4,000 rollout trajectories, LongNav-R1 boosts Qwen3-VL-2B success rate from 64.3% to 73.0% on object navigation benchmarks, demonstrating superior sample efficiency and outperforming state-of-the-art methods.

Conclusion: LongNav-R1 effectively addresses long-horizon navigation challenges through multi-turn RL, enabling causal reasoning and diverse behavior generation. The framework shows strong generalization to real-world settings and will be open-sourced.

Abstract: This paper develops LongNav-R1, an end-to-end multi-turn reinforcement learning (RL) framework designed to optimize Visual-Language-Action (VLA) models for long-horizon navigation. Unlike existing single-turn paradigm, LongNav-R1 reformulates the navigation decision process as a continuous multi-turn conversation between the VLA policy and the embodied environment. This multi-turn RL framework offers two distinct advantages: i) it enables the agent to reason about the causal effects of historical interactions and sequential future outcomes; and ii) it allows the model to learn directly from online interactions, fostering diverse trajectory generation and avoiding the behavioral rigidity often imposed by human demonstrations. Furthermore, we introduce Horizon-Adaptive Policy Optimization. This mechanism explicitly accounts for varying horizon lengths during advantage estimation, facilitating accurate temporal credit assignment over extended sequences. Consequently, the agent develops diverse navigation behaviors and resists collapse during long-horizon tasks. Experiments on object navigation benchmarks validate the framework's efficacy: With 4,000 rollout trajectories, LongNav-R1 boosts the Qwen3-VL-2B success rate from 64.3% to 73.0%. These results demonstrate superior sample efficiency and significantly outperform state-of-the-art methods. The model's generalizability and robustness are further validated by its zero-shot performance in long-horizon real-world navigation settings. All source code will be open-sourced upon publication.

</details>


### [192] [Predicting Dynamic Map States from Limited Field-of-View Sensor Data](https://arxiv.org/abs/2602.12360)
*Knut Peterson,David Han*

Main category: cs.RO

TL;DR: Using deep learning to predict dynamic map states from limited field-of-view time series data by converting sensor data into single-image representations.


<details>
  <summary>Details</summary>
Motivation: Autonomous systems often face limited field-of-view constraints due to design limitations, occlusions, or sensor failures. When large FOV is unavailable, systems need to infer environmental information and predict nearby states to maintain safe and accurate operation.

Method: Represent dynamic sensor data in a simple single-image format that captures both spatial and temporal information, then use existing image-to-image learning models to predict map states.

Result: The approach enables accurate prediction of map states in diverse sensing scenarios using limited FOV time series data.

Conclusion: Deep learning with single-image representations of spatiotemporal sensor data is effective for dynamic map state prediction under limited field-of-view conditions.

Abstract: When autonomous systems are deployed in real-world scenarios, sensors are often subject to limited field-of-view (FOV) constraints, either naturally through system design, or through unexpected occlusions or sensor failures. In conditions where a large FOV is unavailable, it is important to be able to infer information about the environment and predict the state of nearby surroundings based on available data to maintain safe and accurate operation. In this work, we explore the effectiveness of deep learning for dynamic map state prediction based on limited FOV time series data. We show that by representing dynamic sensor data in a simple single-image format that captures both spatial and temporal information, we can effectively use a wide variety of existing image-to-image learning models to predict map states with high accuracy in a diverse set of sensing scenarios.

</details>


### [193] [Zero-Shot Adaptation to Robot Structural Damage via Natural Language-Informed Kinodynamics Modeling](https://arxiv.org/abs/2602.12385)
*Anuj Pokhrel,Aniket Datar,Mohammad Nazeri,Francesco Cancelliere,Xuesu Xiao*

Main category: cs.RO

TL;DR: ZLIK uses natural language descriptions of structural damage to learn kinodynamic models that adapt zero-shot to various vehicle damages, reducing prediction errors by up to 81% and generalizing across sim-to-real and scale gaps.


<details>
  <summary>Details</summary>
Motivation: Autonomous mobile robots operating in challenging environments inevitably suffer structural damage that alters their kinodynamic behavior. Traditional methods struggle to quantify heterogeneous structural failures, but natural language can describe this variety of damages effectively.

Method: Proposes Zero-shot Language Informed Kinodynamics (ZLIK) using self-supervised learning to ground semantic damage descriptions in kinodynamic behaviors. Collects data from structurally compromised vehicles in BeamNG.tech physics simulator to learn forward kinodynamics models in a data-driven manner.

Result: The learned model achieves zero-shot adaptation to different damages with up to 81% reduction in kinodynamics error. It also generalizes effectively across sim-to-real and full-to-1/10th scale gaps.

Conclusion: Natural language descriptions can effectively capture structural damage variations, enabling zero-shot adaptation of kinodynamic models that maintain performance across different damage types and scale/simulation gaps.

Abstract: High-performance autonomous mobile robots endure significant mechanical stress during in-the-wild operations, e.g., driving at high speeds or over rugged terrain. Although these platforms are engineered to withstand such conditions, mechanical degradation is inevitable. Structural damage manifests as consistent and notable changes in kinodynamic behavior compared to a healthy vehicle. Given the heterogeneous nature of structural failures, quantifying various damages to inform kinodynamics is challenging. We posit that natural language can describe and thus capture this variety of damages. Therefore, we propose Zero-shot Language Informed Kinodynamics (ZLIK), which employs self-supervised learning to ground semantic information of damage descriptions in kinodynamic behaviors to learn a forward kinodynamics model in a data-driven manner. Using the high-fidelity soft-body physics simulator BeamNG.tech, we collect data from a variety of structurally compromised vehicles. Our learned model achieves zero-shot adaptation to different damages with up to 81% reduction in kinodynamics error and generalizes across the sim-to-real and full-to-1/10$^{\text{th}}$ scale gaps.

</details>


### [194] [Self-Refining Vision Language Model for Robotic Failure Detection and Reasoning](https://arxiv.org/abs/2602.12405)
*Carl Qi,Xiaojie Wang,Silong Yong,Stephen Sheng,Huitan Mao,Sriram Srinivasan,Manikantan Nambi,Amy Zhang,Yesh Dattatreya*

Main category: cs.RO

TL;DR: ARMOR is a multi-task self-refinement model for robotic failure detection and reasoning that learns from sparse binary labels and limited rich reasoning annotations, achieving significant improvements over previous approaches.


<details>
  <summary>Details</summary>
Motivation: Real-world robotic failures are subtle, combinatorial, and difficult to enumerate, while rich reasoning labels are expensive to acquire. Current approaches either treat failure reasoning as closed-set classification or require extensive human annotations.

Method: ARMOR formulates detection and reasoning as a multi-task self-refinement process where the model iteratively predicts detection outcomes and natural language reasoning conditioned on past outputs. It learns from heterogeneous supervision (sparse binary labels + limited reasoning annotations) via offline and online imitation learning.

Result: ARMOR achieves state-of-the-art performance, improving over previous approaches by up to 30% on failure detection rate and up to 100% in reasoning (measured through LLM fuzzy match score). It demonstrates robustness to heterogeneous supervision and open-ended reasoning beyond predefined failure modes.

Conclusion: ARMOR provides an effective approach for robotic failure detection and reasoning that handles the challenges of subtle, combinatorial failures with limited supervision, enabling more reliable and trustworthy robotic systems.

Abstract: Reasoning about failures is crucial for building reliable and trustworthy robotic systems. Prior approaches either treat failure reasoning as a closed-set classification problem or assume access to ample human annotations. Failures in the real world are typically subtle, combinatorial, and difficult to enumerate, whereas rich reasoning labels are expensive to acquire. We address this problem by introducing ARMOR: Adaptive Round-based Multi-task mOdel for Robotic failure detection and reasoning. We formulate detection and reasoning as a multi-task self-refinement process, where the model iteratively predicts detection outcomes and natural language reasoning conditioned on past outputs. During training, ARMOR learns from heterogeneous supervision - large-scale sparse binary labels and small-scale rich reasoning annotations - optimized via a combination of offline and online imitation learning. At inference time, ARMOR generates multiple refinement trajectories and selects the most confident prediction via a self-certainty metric. Experiments across diverse environments show that ARMOR achieves state-of-the-art performance by improving over the previous approaches by up to 30% on failure detection rate and up to 100% in reasoning measured through LLM fuzzy match score, demonstrating robustness to heterogeneous supervision and open-ended reasoning beyond predefined failure modes. We provide dditional visualizations on our website: https://sites.google.com/utexas.edu/armor

</details>


### [195] [MiDAS: A Multimodal Data Acquisition System and Dataset for Robot-Assisted Minimally Invasive Surgery](https://arxiv.org/abs/2602.12407)
*Keshara Weerasinghe,Seyed Hamid Reza Roodabeh,Andrew Hawkins,Zhaomeng Zhang,Zachary Schrader,Homa Alemzadeh*

Main category: cs.RO

TL;DR: MiDAS is an open-source, platform-agnostic system for non-invasive multimodal data acquisition in robot-assisted surgery, enabling synchronized data collection without proprietary robot interfaces.


<details>
  <summary>Details</summary>
Motivation: Proprietary robot telemetry access is a major barrier for RMIS research, limiting multimodal data collection and reproducibility across different surgical robotic platforms.

Method: MiDAS integrates electromagnetic and RGB-D hand tracking, foot pedal sensing, and surgical video capturing without requiring proprietary interfaces. Validated on Raven-II and da Vinci Xi systems with peg transfer and hernia repair suturing tasks performed by surgical residents.

Result: External hand and foot sensing closely approximated internal robot kinematics, and non-invasive motion signals achieved gesture recognition performance comparable to proprietary telemetry.

Conclusion: MiDAS enables reproducible multimodal RMIS data collection and is released with annotated datasets, including the first multimodal dataset capturing hernia repair suturing on high-fidelity simulation models.

Abstract: Background: Robot-assisted minimally invasive surgery (RMIS) research increasingly relies on multimodal data, yet access to proprietary robot telemetry remains a major barrier. We introduce MiDAS, an open-source, platform-agnostic system enabling time-synchronized, non-invasive multimodal data acquisition across surgical robotic platforms.
  Methods: MiDAS integrates electromagnetic and RGB-D hand tracking, foot pedal sensing, and surgical video capturing without requiring proprietary robot interfaces. We validated MiDAS on the open-source Raven-II and the clinical da Vinci Xi by collecting multimodal datasets of peg transfer and hernia repair suturing tasks performed by surgical residents. Correlation analysis and downstream gesture recognition experiments were conducted.
  Results: External hand and foot sensing closely approximated internal robot kinematics and non-invasive motion signals achieved gesture recognition performance comparable to proprietary telemetry.
  Conclusion: MiDAS enables reproducible multimodal RMIS data collection and is released with annotated datasets, including the first multimodal dataset capturing hernia repair suturing on high-fidelity simulation models.

</details>


### [196] [Control Barrier Functions with Audio Risk Awareness for Robot Safe Navigation on Construction Sites](https://arxiv.org/abs/2602.12416)
*Johannes Mootz,Reza Akhavian*

Main category: cs.RO

TL;DR: Audio-enhanced CBF safety filter for construction robots uses jackhammer detection to dynamically adjust safety margins, improving obstacle avoidance in cluttered environments.


<details>
  <summary>Details</summary>
Motivation: Construction sites are dynamic and visually occluded environments where audio information is underutilized in robot autonomy. Current systems lack robust safety mechanisms that can adapt to audio cues indicating potential hazards.

Method: Proposes a control barrier function (CBF)-based safety filter augmented with a lightweight, real-time jackhammer detector using signal envelope and periodicity analysis. The audio-derived risk cue modulates the barrier function in the controller. Two CBF formulations (circular and goal-aligned elliptical) are tested with a unicycle robot in simulation.

Result: The CBF safety filter eliminates safety violations across all trials. The elliptical formulation achieves 76.5% success rate reaching targets vs 40.2% for circular, as it better avoids deadlock situations in cluttered construction environments.

Conclusion: Integrating audio perception into CBF-based controllers enables richer multimodal safety reasoning for autonomous robots in safety-critical, dynamic environments like construction sites, demonstrating a pathway toward more robust autonomy.

Abstract: Construction automation increasingly requires autonomous mobile robots, yet robust autonomy remains challenging on construction sites. These environments are dynamic and often visually occluded, which complicates perception and navigation. In this context, valuable information from audio sources remains underutilized in most autonomy stacks. This work presents a control barrier function (CBF)-based safety filter that provides safety guarantees for obstacle avoidance while adapting safety margins during navigation using an audio-derived risk cue. The proposed framework augments the CBF with a lightweight, real-time jackhammer detector based on signal envelope and periodicity. Its output serves as an exogenous risk that is directly enforced in the controller by modulating the barrier function. The approach is evaluated in simulation with two CBF formulations (circular and goal-aligned elliptical) with a unicycle robot navigating a cluttered construction environment. Results show that the CBF safety filter eliminates safety violations across all trials while reaching the target in 40.2% (circular) vs. 76.5% (elliptical), as the elliptical formulation better avoids deadlock. This integration of audio perception into a CBF-based controller demonstrates a pathway toward richer multimodal safety reasoning in autonomous robots for safety-critical and dynamic environments.

</details>


### [197] [An Autonomous, End-to-End, Convex-Based Framework for Close-Range Rendezvous Trajectory Design and Guidance with Hardware Testbed Validation](https://arxiv.org/abs/2602.12421)
*Minduli C. Wijayatunga,Julian Guinane,Nathan D. Wallace,Xiaofeng Wu*

Main category: cs.RO

TL;DR: CORTEX is a real-time autonomous trajectory design and guidance framework for satellite rendezvous that combines deep learning perception with convex optimization, validated through high-fidelity simulations and hardware testing.


<details>
  <summary>Details</summary>
Motivation: Autonomous satellite servicing requires close-range rendezvous under strict safety constraints while being computationally tractable for onboard use and robust to uncertainties in sensing, actuation, and dynamics.

Method: Integrates deep-learning perception pipeline with convex-optimization-based trajectory design and guidance, including reference regeneration and abort-to-safe-orbit logic to handle sensor faults and engine failures.

Result: In Monte-Carlo simulations with uncertainties, achieved terminal docking errors of 36.85±44.46 mm position and 1.25±2.26 mm/s velocity. On planar air-bearing testbed, achieved 8.09±5.29 mm position and 2.23±1.72 mm/s velocity errors across 18 cases (including off-nominal scenarios).

Conclusion: CORTEX demonstrates robust autonomous rendezvous capability through integrated perception and optimization, validated in both high-fidelity simulation and hardware testing under various uncertainties and failure scenarios.

Abstract: Autonomous satellite servicing missions must execute close-range rendezvous under stringent safety and operational constraints while remaining computationally tractable for onboard use and robust to uncertainty in sensing, actuation, and dynamics. This paper presents CORTEX (Convex Optimization for Rendezvous Trajectory Execution), an autonomous, perception-enabled, real-time trajectory design and guidance framework for close-range rendezvous. CORTEX integrates a deep-learning perception pipeline with convex-optimisation-based trajectory design and guidance, including reference regeneration and abort-to-safe-orbit logic to recover from large deviations caused by sensor faults and engine failures.
  CORTEX is validated in high-fidelity software simulation and hardware-in-the-loop experiments. The software pipeline (Basilisk) models high-fidelity relative dynamics, realistic thruster execution, perception, and attitude control. Hardware testing uses (i) an optical navigation testbed to assess perception-to-estimation performance and (ii) a planar air-bearing testbed to evaluate the end-to-end guidance loop under representative actuation and subsystem effects. A Monte-Carlo campaign in simulation includes initial-state uncertainty, thrust-magnitude errors, and missed-thrust events; under the strongest case investigated, CORTEX achieves terminal docking errors of $36.85 \pm 44.46$ mm in relative position and $1.25 \pm 2.26$ mm/s in relative velocity. On the planar air-bearing testbed, 18 cases are executed (10 nominal; 8 off-nominal requiring recomputation and/or abort due to simulated engine failure and sensor malfunctions), yielding terminal errors of $8.09 \pm 5.29$ mm in position and $2.23 \pm 1.72$ mm/s in velocity.

</details>


### [198] [Gradient-Enhanced Partitioned Gaussian Processes for Real-Time Quadrotor Dynamics Modeling](https://arxiv.org/abs/2602.12487)
*Xinhuan Sang,Adam Rozman,Sheryl Grace,Roberto Tron*

Main category: cs.RO

TL;DR: Real-time quadrotor dynamics prediction using partitioned Gaussian Processes with gradient information and aerodynamic effects from mid-fidelity simulations.


<details>
  <summary>Details</summary>
Motivation: Traditional GP-based approaches provide reliable Bayesian predictions with uncertainty quantification but are computationally expensive for real-time simulations, making them unsuitable for real-time applications.

Method: Integrate gradient information to improve accuracy and introduce novel partitioning/approximation strategy with local GPs for non-overlapping regions. Use Schur complements to pre-compute matrix inversions offline. Generate training data using CHARM mid-fidelity aerodynamic solver for SUI Endurance quadrotor, with derivative information via finite differences.

Result: Proposed partitioned GP with gradient conditioning achieves higher accuracy than standard partitioned GPs without gradient information, while enabling real-time inference at frequencies above 30 Hz on standard desktop hardware.

Conclusion: The framework provides an efficient foundation for real-time aerodynamic prediction and control algorithms in complex and unsteady environments.

Abstract: We present a quadrotor dynamics Gaussian Process (GP) with gradient information that achieves real-time inference via state-space partitioning and approximation, and that includes aerodynamic effects using data from mid-fidelity potential flow simulations. While traditional GP-based approaches provide reliable Bayesian predictions with uncertainty quantification, they are computationally expensive and thus unsuitable for real-time simulations. To address this challenge, we integrate gradient information to improve accuracy and introduce a novel partitioning and approximation strategy to reduce online computational cost. In particular, for the latter, we associate a local GP with each non-overlapping region; by splitting the training data into local near and far subsets, and by using Schur complements, we show that a large part of the matrix inversions required for inference can be performed offline, enabling real-time inference at frequencies above 30 Hz on standard desktop hardware. To generate a training dataset that captures aerodynamic effects, such as rotor-rotor interactions and apparent wind direction, we use the CHARM code, which is a mid-fidelity aerodynamic solver. It is applied to the SUI Endurance quadrotor to predict force and torque, along with noise at three specified locations. The derivative information is obtained via finite differences. Experimental results demonstrate that the proposed partitioned GP with gradient conditioning achieves higher accuracy than standard partitioned GPs without gradient information, while greatly reducing computational time. This framework provides an efficient foundation for real-time aerodynamic prediction and control algorithms in complex and unsteady environments.

</details>


### [199] [Composable Model-Free RL for Navigation with Input-Affine Systems](https://arxiv.org/abs/2602.12492)
*Xinhuan Sang,Abdelrahman Abdelgawad,Roberto Tron*

Main category: cs.RO

TL;DR: Model-free RL method learns individual value functions for environment elements (goals/obstacles) and composes them online via QCQP for safe navigation with formal guarantees.


<details>
  <summary>Details</summary>
Motivation: Autonomous robots need to navigate safely in complex, dynamic environments where anticipating all possible behaviors is infeasible. Existing methods often lack formal safety guarantees or require accurate models.

Method: 1) Derive continuous-time HJB equation for value functions with quadratic advantage structure; 2) Model-free actor-critic algorithm learns policies/value functions for static/moving obstacles via gradient descent; 3) Compose multiple reach/avoid models using quadratically constrained quadratic program (QCQP).

Result: Method provides formal obstacle-avoidance guarantees via value-function level sets, offering model-free alternative to CLF/CBF controllers. Simulations show improved performance over PPO baseline on discrete-time approximation.

Conclusion: Composable model-free RL approach enables safe navigation with formal guarantees in continuous-time systems with unknown nonlinear dynamics, bridging RL and control-theoretic safety methods.

Abstract: As autonomous robots move into complex, dynamic real-world environments, they must learn to navigate safely in real time, yet anticipating all possible behaviors is infeasible. We propose a composable, model-free reinforcement learning method that learns a value function and an optimal policy for each individual environment element (e.g., goal or obstacle) and composes them online to achieve goal reaching and collision avoidance. Assuming unknown nonlinear dynamics that evolve in continuous time and are input-affine, we derive a continuous-time Hamilton-Jacobi-Bellman (HJB) equation for the value function and show that the corresponding advantage function is quadratic in the action and optimal policy. Based on this structure, we introduce a model-free actor-critic algorithm that learns policies and value functions for static or moving obstacles using gradient descent. We then compose multiple reach/avoid models via a quadratically constrained quadratic program (QCQP), yielding formal obstacle-avoidance guarantees in terms of value-function level sets, providing a model-free alternative to CLF/CBF-based controllers. Simulations demonstrate improved performance over a PPO baseline applied to a discrete-time approximation.

</details>


### [200] [Monocular Reconstruction of Neural Tactile Fields](https://arxiv.org/abs/2602.12508)
*Pavan Mantripragada,Siddhanth Deshmukh,Eadom Dessalene,Manas Desai,Yiannis Aloimonos*

Main category: cs.RO

TL;DR: Neural tactile fields predict 3D tactile response from single RGB image, enabling robots to plan paths through deformable environments by distinguishing between high and low resistance regions.


<details>
  <summary>Details</summary>
Motivation: Robots need to operate in real-world environments that deform and reconfigure under contact, requiring interaction-aware 3D representations beyond static geometric occupancy to plan through yielding materials like foliage.

Method: Introduces neural tactile fields - a 3D representation mapping spatial locations to expected tactile response upon contact. Predicts these fields from a single monocular RGB image, then integrates with off-the-shelf path planners.

Result: Improves volumetric 3D reconstruction by 85.8% and surface reconstruction by 26.7% compared to state-of-the-art monocular 3D reconstruction methods (LRM and Direct3D). Enables robots to avoid high-resistance objects while routing through low-resistance regions.

Conclusion: Neural tactile fields provide an interaction-aware 3D representation that enables more intelligent path planning in deformable environments, moving beyond treating all occupied space as equally impassable.

Abstract: Robots operating in the real world must plan through environments that deform, yield, and reconfigure under contact, requiring interaction-aware 3D representations that extend beyond static geometric occupancy. To address this, we introduce neural tactile fields, a novel 3D representation that maps spatial locations to the expected tactile response upon contact. Our model predicts these neural tactile fields from a single monocular RGB image -- the first method to do so. When integrated with off-the-shelf path planners, neural tactile fields enable robots to generate paths that avoid high-resistance objects while deliberately routing through low-resistance regions (e.g. foliage), rather than treating all occupied space as equally impassable. Empirically, our learning framework improves volumetric 3D reconstruction by $85.8\%$ and surface reconstruction by $26.7\%$ compared to state-of-the-art monocular 3D reconstruction methods (LRM and Direct3D).

</details>


### [201] [CRAFT: Adapting VLA Models to Contact-rich Manipulation via Force-aware Curriculum Fine-tuning](https://arxiv.org/abs/2602.12532)
*Yike Zhang,Yaonan Wang,Xinxin Sun,Kaizhen Huang,Zhiyuan Xu,Junjie Ji,Zhengping Che,Jian Tang,Jingtao Sun*

Main category: cs.RO

TL;DR: CRAFT introduces a force-aware curriculum fine-tuning framework for VLA models to improve contact-rich manipulation by addressing the imbalance between high-entropy vision/language inputs and low-entropy force signals.


<details>
  <summary>Details</summary>
Motivation: Current Vision-Language-Action (VLA) models struggle with contact-rich manipulation tasks that require precise alignment, stable contact maintenance, and handling of deformable objects. The fundamental challenge is the imbalance between high-entropy vision/language inputs and low-entropy but critical force signals, leading to over-reliance on perception and unstable control.

Method: CRAFT uses a force-aware curriculum fine-tuning framework with a variational information bottleneck module to regulate vision and language embeddings during early training. The curriculum strategy prioritizes force signals initially, then progressively restores access to full multimodal information. A homologous leader-follower teleoperation system collects synchronized vision, language, and force data across diverse contact-rich tasks.

Result: Real-world experiments show CRAFT consistently improves task success, generalizes to unseen objects and novel task variations, and adapts effectively across diverse VLA architectures, enabling robust and generalizable contact-rich manipulation.

Conclusion: CRAFT successfully addresses the force-perception imbalance in VLA models through curriculum learning and information bottleneck techniques, enabling more robust contact-rich manipulation that generalizes well to new scenarios and different model architectures.

Abstract: Vision-Language-Action (VLA) models have shown a strong capability in enabling robots to execute general instructions, yet they struggle with contact-rich manipulation tasks, where success requires precise alignment, stable contact maintenance, and effective handling of deformable objects. A fundamental challenge arises from the imbalance between high-entropy vision and language inputs and low-entropy but critical force signals, which often leads to over-reliance on perception and unstable control. To address this, we introduce CRAFT, a force-aware curriculum fine-tuning framework that integrates a variational information bottleneck module to regulate vision and language embeddings during early training. This curriculum strategy encourages the model to prioritize force signals initially, before progressively restoring access to the full multimodal information. To enable force-aware learning, we further design a homologous leader-follower teleoperation system that collects synchronized vision, language, and force data across diverse contact-rich tasks. Real-world experiments demonstrate that CRAFT consistently improves task success, generalizes to unseen objects and novel task variations, and adapts effectively across diverse VLA architectures, enabling robust and generalizable contact-rich manipulation.

</details>


### [202] [Eva-Tracker: ESDF-update-free, Visibility-aware Planning with Target Reacquisition for Robust Aerial Tracking](https://arxiv.org/abs/2602.12549)
*Yue Lin,Yang Liu,Dong Wang,Huchuan Lu*

Main category: cs.RO

TL;DR: Eva-Tracker is a visibility-aware aerial tracking framework that eliminates ESDF updates by using precomputed FoV-ESDF for rapid visibility evaluation and incorporates recovery-capable path generation for target reacquisition.


<details>
  <summary>Details</summary>
Motivation: Traditional ESDF-based visibility evaluation in aerial tracking requires frequent updates that introduce significant computational overhead, limiting real-time performance and efficiency.

Method: Three key components: 1) Target trajectory prediction with visibility-aware initial path generation, 2) Precomputed Field of View ESDF (FoV-ESDF) for rapid visibility evaluation without updates, 3) Differentiable FoV-ESDF-based trajectory optimization for continuous visibility.

Result: Extensive simulations and real-world experiments show more robust tracking results with lower computational effort compared to state-of-the-art methods.

Conclusion: Eva-Tracker successfully addresses ESDF computational overhead while maintaining visibility-aware tracking, offering an efficient solution for aerial tracking with target reacquisition capabilities.

Abstract: The Euclidean Signed Distance Field (ESDF) is widely used in visibility evaluation to prevent occlusions and collisions during tracking. However, frequent ESDF updates introduce considerable computational overhead. To address this issue, we propose Eva-Tracker, a visibility-aware trajectory planning framework for aerial tracking that eliminates ESDF updates and incorporates a recovery-capable path generation method for target reacquisition. First, we design a target trajectory prediction method and a visibility-aware initial path generation algorithm that maintain an appropriate observation distance, avoid occlusions, and enable rapid replanning to reacquire the target when it is lost. Then, we propose the Field of View ESDF (FoV-ESDF), a precomputed ESDF tailored to the tracker's field of view, enabling rapid visibility evaluation without requiring updates. Finally, we optimize the trajectory using differentiable FoV-ESDF-based objectives to ensure continuous visibility throughout the tracking process. Extensive simulations and real-world experiments demonstrate that our approach delivers more robust tracking results with lower computational effort than existing state-of-the-art methods. The source code is available at https://github.com/Yue-0/Eva-Tracker.

</details>


### [203] [Hemispherical Angular Power Mapping of Installed mmWave Radar Modules Under Realistic Deployment Constraints](https://arxiv.org/abs/2602.12584)
*Maaz Qureshi,Mohammad Omid Bagheri,William Melek,George Shaker*

Main category: cs.RO

TL;DR: A method for in-situ hemispherical angular power mapping of installed mmWave radar modules using calibrated probes and geometry-consistent positioning to characterize installation-dependent radiation patterns.


<details>
  <summary>Details</summary>
Motivation: Conventional antenna measurement methods (chamber- and turntable-based) become impractical once mmWave radar modules are installed in real-world environments where packaging, mounting hardware, and nearby structures significantly alter radiation patterns.

Method: Hemispherical angular received-power mapping using calibrated receiving probes placed at prescribed (phi, theta, r) locations with geometry-consistent positioning and quasi-static acquisition. Amplitude-only received-power is recorded using standard RF instrumentation to generate angular power maps.

Result: Proof-of-concept measurements on a 60-GHz radar module demonstrate repeatable hemispherical mapping with angular trends in good agreement with full-wave simulation, validating the methodology.

Conclusion: The presented approach enables practical on-site characterization of embedded mmWave transmitters, supporting in-situ EM validation under realistic deployment constraints where conventional measurement methods are impractical.

Abstract: Characterizing the angular radiation behavior of installed millimeter-wave (mmWave) radar modules is increasingly important in practical sensing platforms, where packaging, mounting hardware, and nearby structures can significantly alter the effective emission profile. However, once a device is embedded in its host environment, conventional chamber- and turntable-based antenna measurements are often impractical. This paper presents a hemispherical angular received-power mapping methodology for in-situ EM validation of installed mmWave modules under realistic deployment constraints. The approach samples the accessible half-space around a stationary device-under-test by placing a calibrated receiving probe at prescribed (phi, theta, r) locations using geometry-consistent positioning and quasi-static acquisition. Amplitude-only received-power is recorded using standard RF instrumentation to generate hemispherical angular power maps that capture installation-dependent radiation characteristics. Proof-of-concept measurements on a 60-GHz radar module demonstrate repeatable hemi-spherical mapping with angular trends in good agreement with full-wave simulation, supporting practical on-site characterization of embedded mmWave transmitters.

</details>


### [204] [PISHYAR: A Socially Intelligent Smart Cane for Indoor Social Navigation and Multimodal Human-Robot Interaction for Visually Impaired People](https://arxiv.org/abs/2602.12597)
*Mahdi Haghighat Joo,Maryam Karimi Jafari,Alireza Taheri*

Main category: cs.RO

TL;DR: PISHYAR is a socially intelligent smart cane combining social navigation with multimodal AI interaction to assist visually impaired users with both mobility and interactive support.


<details>
  <summary>Details</summary>
Motivation: To create an assistive mobility aid that goes beyond basic navigation to provide socially aware navigation and interactive assistance for visually impaired and low-vision users, addressing both physical mobility needs and social interaction capabilities.

Method: Two-component system: (1) Social navigation framework using Raspberry Pi 5 with RGB-D perception (OAK-D Lite), YOLOv8 object detection, COMPOSER activity recognition, D* Lite path planning, and haptic feedback; (2) Agentic multimodal LLM-VLM interaction framework integrating speech recognition, vision language models, LLMs, and TTS with dynamic routing between voice-only and vision-only modes.

Result: 80% overall system accuracy in social navigation, robust group activity recognition across crowd scenarios, and positive user feedback from 8 visually impaired participants showing high acceptance, usability, trust, and perceived sociability.

Conclusion: PISHYAR demonstrates potential as a multimodal assistive mobility aid that successfully extends beyond navigation to provide socially interactive support for visually impaired users through combined social navigation and AI-powered interaction capabilities.

Abstract: This paper presents PISHYAR, a socially intelligent smart cane designed by our group to combine socially aware navigation with multimodal human-AI interaction to support both physical mobility and interactive assistance. The system consists of two components: (1) a social navigation framework implemented on a Raspberry Pi 5 that integrates real-time RGB-D perception using an OAK-D Lite camera, YOLOv8-based object detection, COMPOSER-based collective activity recognition, D* Lite dynamic path planning, and haptic feedback via vibration motors for tasks such as locating a vacant seat; and (2) an agentic multimodal LLM-VLM interaction framework that integrates speech recognition, vision language models, large language models, and text-to-speech, with dynamic routing between voice-only and vision-only modes to enable natural voice-based communication, scene description, and object localization from visual input. The system is evaluated through a combination of simulation-based tests, real-world field experiments, and user-centered studies. Results from simulated and real indoor environments demonstrate reliable obstacle avoidance and socially compliant navigation, achieving an overall system accuracy of approximately 80% under different social conditions. Group activity recognition further shows robust performance across diverse crowd scenarios. In addition, a preliminary exploratory user study with eight visually impaired and low-vision participants evaluates the agentic interaction framework through structured tasks and a UTAUT-based questionnaire reveals high acceptance and positive perceptions of usability, trust, and perceived sociability during our experiments. The results highlight the potential of PISHYAR as a multimodal assistive mobility aid that extends beyond navigation to provide socially interactive support for such users.

</details>


### [205] [RLinf-Co: Reinforcement Learning-Based Sim-Real Co-Training for VLA Models](https://arxiv.org/abs/2602.12628)
*Liangzhi Shi,Shuaihang Chen,Feng Gao,Yinuo Chen,Kang Chen,Tonghe Zhang,Hongzhi Zhang,Weinan Zhang,Chao Yu,Yu Wang*

Main category: cs.RO

TL;DR: RL-based sim-real co-training framework (RL-Co) that combines supervised fine-tuning with reinforcement learning in simulation while anchoring with real-world data, achieving significant improvements in real-world robot manipulation tasks.


<details>
  <summary>Details</summary>
Motivation: Current sim-real co-training methods rely on supervised fine-tuning (SFT) which treats simulation as static demonstration source, limiting real-world gains and generalization. There's a need to leverage large-scale closed-loop interaction in simulation while preserving real-world capabilities.

Method: Two-stage framework: 1) Warm-start policy with SFT on mixture of real and simulated demonstrations, 2) Fine-tune with reinforcement learning in simulation while adding auxiliary supervised loss on real-world data to anchor policy and prevent catastrophic forgetting.

Result: Consistent improvements over real-only fine-tuning and SFT-based co-training: +24% real-world success on OpenVLA and +20% on π₀.₅. Better generalization to unseen task variations and substantially improved real-world data efficiency.

Conclusion: RL-based co-training provides practical and scalable pathway for leveraging simulation to enhance real-robot deployment, offering higher success rates, stronger generalization, and improved data efficiency compared to SFT-based approaches.

Abstract: Simulation offers a scalable and low-cost way to enrich vision-language-action (VLA) training, reducing reliance on expensive real-robot demonstrations. However, most sim-real co-training methods rely on supervised fine-tuning (SFT), which treats simulation as a static source of demonstrations and does not exploit large-scale closed-loop interaction. Consequently, real-world gains and generalization are often limited. In this paper, we propose an \underline{\textit{RL}}-based sim-real \underline{\textit{Co}}-training \modify{(RL-Co)} framework that leverages interactive simulation while preserving real-world capabilities. Our method follows a generic two-stage design: we first warm-start the policy with SFT on a mixture of real and simulated demonstrations, then fine-tune it with reinforcement learning in simulation while adding an auxiliary supervised loss on real-world data to anchor the policy and mitigate catastrophic forgetting. We evaluate our framework on four real-world tabletop manipulation tasks using two representative VLA architectures, OpenVLA and $π_{0.5}$, and observe consistent improvements over real-only fine-tuning and SFT-based co-training, including +24% real-world success on OpenVLA and +20% on $π_{0.5}$. Beyond higher success rates, RL co-training yields stronger generalization to unseen task variations and substantially improved real-world data efficiency, providing a practical and scalable pathway for leveraging simulation to enhance real-robot deployment.

</details>


### [206] [Real-to-Sim for Highly Cluttered Environments via Physics-Consistent Inter-Object Reasoning](https://arxiv.org/abs/2602.12633)
*Tianyi Xiang,Jiahang Cao,Sikai Guo,Guoyang Zhao,Andrew F. Luo,Jun Ma*

Main category: cs.RO

TL;DR: A physics-constrained Real-to-Sim pipeline that reconstructs physically consistent 3D scenes from single-view RGB-D data for contact-rich robotic manipulation.


<details>
  <summary>Details</summary>
Motivation: Standard perception pipelines for 3D scene reconstruction often neglect physical constraints, resulting in invalid states (floating objects, inter-penetration) that make downstream simulation unreliable for contact-rich robotic manipulation tasks.

Method: A differentiable optimization pipeline that models spatial dependencies via a contact graph and jointly refines object poses and physical properties through differentiable rigid-body simulation.

Result: Extensive evaluations in both simulation and real-world settings show that reconstructed scenes achieve high physical fidelity and faithfully replicate real-world contact dynamics.

Conclusion: The proposed physics-constrained Real-to-Sim pipeline enables stable and reliable contact-rich manipulation by reconstructing physically consistent 3D scenes from single-view observations.

Abstract: Reconstructing physically valid 3D scenes from single-view observations is a prerequisite for bridging the gap between visual perception and robotic control. However, in scenarios requiring precise contact reasoning, such as robotic manipulation in highly cluttered environments, geometric fidelity alone is insufficient. Standard perception pipelines often neglect physical constraints, resulting in invalid states, e.g., floating objects or severe inter-penetration, rendering downstream simulation unreliable. To address these limitations, we propose a novel physics-constrained Real-to-Sim pipeline that reconstructs physically consistent 3D scenes from single-view RGB-D data. Central to our approach is a differentiable optimization pipeline that explicitly models spatial dependencies via a contact graph, jointly refining object poses and physical properties through differentiable rigid-body simulation. Extensive evaluations in both simulation and real-world settings demonstrate that our reconstructed scenes achieve high physical fidelity and faithfully replicate real-world contact dynamics, enabling stable and reliable contact-rich manipulation.

</details>


### [207] [PMG: Parameterized Motion Generator for Human-like Locomotion Control](https://arxiv.org/abs/2602.12656)
*Chenxi Han,Yuheng Min,Zihao Huang,Ao Hong,Hang Liu,Yi Cheng,Houde Liu*

Main category: cs.RO

TL;DR: PMG is a real-time motion generator that synthesizes humanoid locomotion using compact parameterized motion data and high-dimensional control commands, enabling natural movement, VR teleoperation, and efficient sim-to-real transfer.


<details>
  <summary>Details</summary>
Motivation: Current whole-body reference-guided methods for humanoid control require large datasets, are brittle across different speed/pose regimes, and are sensitive to robot-specific calibration, making them difficult to adapt to higher-level commands and diverse tasks.

Method: Proposes Parameterized Motion Generator (PMG) grounded in human motion structure analysis, using compact parameterized motion data with high-dimensional control commands. Combined with imitation-learning pipeline and optimization-based sim-to-real motor parameter identification module.

Result: Validated on humanoid prototype ZERITH Z1, PMG produces natural human-like locomotion, responds precisely to high-dimensional control inputs including VR teleoperation, and enables efficient verifiable sim-to-real transfer.

Conclusion: Establishes a practical, experimentally validated pathway toward natural and deployable humanoid control by addressing limitations of current data-driven approaches.

Abstract: Recent advances in data-driven reinforcement learning and motion tracking have substantially improved humanoid locomotion, yet critical practical challenges remain. In particular, while low-level motion tracking and trajectory-following controllers are mature, whole-body reference-guided methods are difficult to adapt to higher-level command interfaces and diverse task contexts: they require large, high-quality datasets, are brittle across speed and pose regimes, and are sensitive to robot-specific calibration. To address these limitations, we propose the Parameterized Motion Generator (PMG), a real-time motion generator grounded in an analysis of human motion structure that synthesizes reference trajectories using only a compact set of parameterized motion data together with High-dimensional control commands. Combined with an imitation-learning pipeline and an optimization-based sim-to-real motor parameter identification module, we validate the complete approach on our humanoid prototype ZERITH Z1 and show that, within a single integrated system, PMG produces natural, human-like locomotion, responds precisely to high-dimensional control inputs-including VR-based teleoperation-and enables efficient, verifiable sim-to-real transfer. Together, these results establish a practical, experimentally validated pathway toward natural and deployable humanoid control.

</details>


### [208] [Xiaomi-Robotics-0: An Open-Sourced Vision-Language-Action Model with Real-Time Execution](https://arxiv.org/abs/2602.12684)
*Rui Cai,Jun Guo,Xinze He,Piaopiao Jin,Jie Li,Bingxuan Lin,Futeng Liu,Wei Liu,Fei Ma,Kun Ma,Feng Qiu,Heng Qu,Yifei Su,Qiao Sun,Dong Wang,Donghao Wang,Yunhong Wang,Rujie Wu,Diyun Xiang,Yu Yang,Hangjun Ye,Yuan Zhang,Quanyun Zhou*

Main category: cs.RO

TL;DR: Xiaomi-Robotics-0 is a vision-language-action model optimized for real-time robot execution, achieving SOTA performance through specialized training and deployment strategies for smooth, fast operation on consumer-grade GPUs.


<details>
  <summary>Details</summary>
Motivation: The paper addresses the need for VLA models that can execute smoothly in real-time on robots, overcoming inference latency issues during real-robot rollouts while maintaining high performance in manipulation tasks.

Method: The method involves: 1) Pre-training on large-scale cross-embodiment robot trajectories and vision-language data to build generalizable action-generation capabilities, 2) Post-training techniques for asynchronous execution to address inference latency, and 3) Careful alignment of timesteps during deployment for continuous real-time rollouts.

Result: The model achieves state-of-the-art performance across all simulation benchmarks and demonstrates high success rates and throughput on two challenging real-robot bimanual manipulation tasks using consumer-grade GPUs.

Conclusion: Xiaomi-Robotics-0 successfully combines high performance with real-time execution capabilities, providing an effective VLA solution for robotic manipulation that works efficiently on accessible hardware, with code and models open-sourced for community use.

Abstract: In this report, we introduce Xiaomi-Robotics-0, an advanced vision-language-action (VLA) model optimized for high performance and fast and smooth real-time execution. The key to our method lies in a carefully designed training recipe and deployment strategy. Xiaomi-Robotics-0 is first pre-trained on large-scale cross-embodiment robot trajectories and vision-language data, endowing it with broad and generalizable action-generation capabilities while avoiding catastrophic forgetting of the visual-semantic knowledge of the underlying pre-trained VLM. During post-training, we propose several techniques for training the VLA model for asynchronous execution to address the inference latency during real-robot rollouts. During deployment, we carefully align the timesteps of consecutive predicted action chunks to ensure continuous and seamless real-time rollouts. We evaluate Xiaomi-Robotics-0 extensively in simulation benchmarks and on two challenging real-robot tasks that require precise and dexterous bimanual manipulation. Results show that our method achieves state-of-the-art performance across all simulation benchmarks. Moreover, Xiaomi-Robotics-0 can roll out fast and smoothly on real robots using a consumer-grade GPU, achieving high success rates and throughput on both real-robot tasks. To facilitate future research, code and model checkpoints are open-sourced at https://xiaomi-robotics-0.github.io

</details>


### [209] [SignScene: Visual Sign Grounding for Mapless Navigation](https://arxiv.org/abs/2602.12686)
*Nicky Zimmerman,Joel Loo,Benjamin Koh,Zishuo Wang,David Hsu*

Main category: cs.RO

TL;DR: SignScene enables robots to navigate using real-world signs by mapping semantic instructions to 3D scene elements, achieving 88% grounding accuracy and enabling mapless navigation on Spot robots.


<details>
  <summary>Details</summary>
Motivation: Humans navigate unfamiliar environments using signs without maps, but robots struggle with interpreting diverse real-world signs and grounding their semantic content to local 3D scenes for navigation.

Method: Proposes SignScene, a sign-centric spatial-semantic representation that captures navigation-relevant scene elements and sign information, presented to Vision-Language Models (VLMs) in a form conducive to effective reasoning for sign grounding.

Result: Achieved 88% grounding accuracy on a dataset of 114 queries across nine diverse environment types, significantly outperforming baselines, and demonstrated real-world mapless navigation on a Spot robot using only signs.

Conclusion: SignScene effectively enables robots to exploit real-world signs for mapless navigation by providing VLMs with appropriate spatial-semantic representations for sign grounding, bridging the gap between abstract semantic instructions and concrete navigational actions.

Abstract: Navigational signs enable humans to navigate unfamiliar environments without maps. This work studies how robots can similarly exploit signs for mapless navigation in the open world. A central challenge lies in interpreting signs: real-world signs are diverse and complex, and their abstract semantic contents need to be grounded in the local 3D scene. We formalize this as sign grounding, the problem of mapping semantic instructions on signs to corresponding scene elements and navigational actions. Recent Vision-Language Models (VLMs) offer the semantic common-sense and reasoning capabilities required for this task, but are sensitive to how spatial information is represented. We propose SignScene, a sign-centric spatial-semantic representation that captures navigation-relevant scene elements and sign information, and presents them to VLMs in a form conducive to effective reasoning. We evaluate our grounding approach on a dataset of 114 queries collected across nine diverse environment types, achieving 88% grounding accuracy and significantly outperforming baselines. Finally, we demonstrate that it enables real-world mapless navigation on a Spot robot using only signs.

</details>


### [210] [ALOE: Action-Level Off-Policy Evaluation for Vision-Language-Action Model Post-Training](https://arxiv.org/abs/2602.12691)
*Rushuai Yang,Hecheng Wang,Chiming Liu,Xiaohan Yan,Yunlong Wang,Xuan Du,Shuoyu Yue,Yongcheng Liu,Chuheng Zhang,Lizhe Qi,Yi Chen,Wei Shan,Maoqing Yao*

Main category: cs.RO

TL;DR: ALOE is an action-level off-policy evaluation framework for vision-language-action systems that uses chunking-based temporal-difference bootstrapping to improve RL learning efficiency in real-world manipulation tasks.


<details>
  <summary>Details</summary>
Motivation: Current VLA systems use conservative on-policy estimation for stability, which avoids direct evaluation of high-capacity policies and limits learning effectiveness. Off-policy evaluation from mixed data sources (historical policies + human interventions) is needed but challenging.

Method: ALOE applies chunking-based temporal-difference bootstrapping to evaluate individual action sequences rather than predicting final task outcomes. This enables better credit assignment to critical action chunks under sparse rewards and supports stable policy improvement.

Result: ALOE improves learning efficiency without compromising execution speed across three real-world manipulation tasks: smartphone packing (high-precision), laundry folding (long-horizon deformable-object), and bimanual pick-and-place (multi-object perception).

Conclusion: Off-policy RL can be reliably reintroduced for real-world VLA post-training through action-level evaluation with chunking-based temporal-difference bootstrapping, enabling more effective learning from mixed data sources.

Abstract: We study how to improve large foundation vision-language-action (VLA) systems through online reinforcement learning (RL) in real-world settings. Central to this process is the value function, which provides learning signals to guide VLA learning from experience. In practice, the value function is estimated from trajectory fragments collected from different data sources, including historical policies and intermittent human interventions. Estimating the value function of current behavior quality from the mixture data is inherently an off-policy evaluation problem. However, prior work often adopts conservative on-policy estimation for stability, which avoids direct evaluation of the current high-capacity policy and limits learning effectiveness. In this paper, we propose ALOE, an action-level off-policy evaluation framework for VLA post-training. ALOE applies chunking-based temporal-difference bootstrapping to evaluate individual action sequences instead of predicting final task outcomes. This design improves effective credit assignment to critical action chunks under sparse rewards and supports stable policy improvement. We evaluate our method on three real-world manipulation tasks, including smartphone packing as a high-precision task, laundry folding as a long-horizon deformable-object task, and bimanual pick-and-place involving multi-object perception. Across all tasks, ALOE improves learning efficiency without compromising execution speed, showing that off-policy RL can be reintroduced in a reliable manner for real-world VLA post-training. Videos and additional materials are available at our project website.

</details>


### [211] [Constrained PSO Six-Parameter Fuzzy PID Tuning Method for Balanced Optimization of Depth Tracking Performance in Underwater Vehicles](https://arxiv.org/abs/2602.12700)
*Yanxi Ding,Tingyue Jia*

Main category: cs.RO

TL;DR: Constrained PSO optimizes 6-parameter fuzzy PID for underwater vehicle depth control, balancing performance with energy constraints.


<details>
  <summary>Details</summary>
Motivation: Traditional fuzzy PID tuning relies on empirical methods, making it difficult to achieve stable, reproducible balance between performance enhancement and control cost in underwater vehicle depth control applications.

Method: Proposes constrained particle swarm optimization (PSO) for tuning six-parameter fuzzy PID controllers. Adjusts benchmark PID parameters alongside fuzzy controller's input quantization factor and output proportional gain. Introduces time-weighted absolute error integral, adjustment time, relative overshoot control energy, and saturation occupancy rate with control energy constraints to construct constraint-driven comprehensive evaluation system.

Result: Significant performance improvements: time-weighted absolute error integral decreased from 0.2631 to 0.1473, settling time shortened from 2.301 s to 1.613 s, relative overshoot reduced from 0.1494 to 0.01839. Control energy varied from 7980 to 7935 (satisfying constraints), saturation occupancy decreased from 0.004 to 0.003.

Conclusion: The constrained six-parameter joint tuning strategy effectively enhances underwater vehicle depth control performance while maintaining energy constraints, validating its engineering significance for navigation scenarios.

Abstract: Depth control of underwater vehicles in engineering applications must simultaneously satisfy requirements for rapid tracking, low overshoot, and actuator constraints. Traditional fuzzy PID tuning often relies on empirical methods, making it difficult to achieve a stable and reproducible equilibrium solution between performance enhancement and control cost. This paper proposes a constrained particle swarm optimization (PSO) method for tuning six-parameter fuzzy PID controllers. By adjusting the benchmark PID parameters alongside the fuzzy controller's input quantization factor and output proportional gain, it achieves synergistic optimization of the overall tuning strength and dynamic response characteristics of the fuzzy PID system. To ensure engineering feasibility of the optimization results, a time-weighted absolute error integral, adjustment time, relative overshoot control energy, and saturation occupancy rate are introduced. Control energy constraints are applied to construct a constraint-driven comprehensive evaluation system, suppressing pseudo-improvements achieved solely by increasing control inputs. Simulation results demonstrate that, while maintaining consistent control energy and saturation levels, the proposed method significantly enhances deep tracking performance: the time-weighted absolute error integral decreases from 0.2631 to 0.1473, the settling time shortens from 2.301 s to 1.613 s, and the relative overshoot reduces from 0.1494 to 0.01839. Control energy varied from 7980 to 7935, satisfying the energy constraint, while saturation occupancy decreased from 0.004 to 0.003. These results validate the effectiveness and engineering significance of the proposed constrained six-parameter joint tuning strategy for depth control in underwater vehicle navigation scenarios.

</details>


### [212] [TRANS: Terrain-aware Reinforcement Learning for Agile Navigation of Quadruped Robots under Social Interactions](https://arxiv.org/abs/2602.12724)
*Wei Zhu,Irfan Tito Kurniawan,Ye Zhao,Mistuhiro Hayashibe*

Main category: cs.RO

TL;DR: TRANS is a two-stage DRL framework for quadrupedal social navigation over unstructured terrains, combining terrain-aware locomotion and social navigation in a unified system.


<details>
  <summary>Details</summary>
Motivation: Existing quadrupedal navigation methods either separate motion planning from locomotion (neglecting whole-body constraints) or use end-to-end approaches requiring high-frequency sensing. Most also assume static environments, limiting use in human-populated settings.

Method: Two-stage training with three DRL pipelines: (1) TRANS-Loco uses asymmetric actor-critic for terrain traversal without explicit terrain observations, (2) TRANS-Nav uses symmetric actor-critic for social navigation with LiDAR data, (3) TRANS integrates both for terrain-aware social navigation.

Result: Comprehensive benchmarks show TRANS outperforms locomotion and social navigation baselines. Hardware experiments confirm potential for sim-to-real transfer.

Conclusion: TRANS enables effective quadrupedal navigation in both uneven terrains and socially interactive environments, addressing limitations of conventional approaches through integrated DRL framework.

Abstract: This study introduces TRANS: Terrain-aware Reinforcement learning for Agile Navigation under Social interactions, a deep reinforcement learning (DRL) framework for quadrupedal social navigation over unstructured terrains. Conventional quadrupedal navigation typically separates motion planning from locomotion control, neglecting whole-body constraints and terrain awareness. On the other hand, end-to-end methods are more integrated but require high-frequency sensing, which is often noisy and computationally costly. In addition, most existing approaches assume static environments, limiting their use in human-populated settings. To address these limitations, we propose a two-stage training framework with three DRL pipelines. (1) TRANS-Loco employs an asymmetric actor-critic (AC) model for quadrupedal locomotion, enabling traversal of uneven terrains without explicit terrain or contact observations. (2) TRANS-Nav applies a symmetric AC framework for social navigation, directly mapping transformed LiDAR data to ego-agent actions under differential-drive kinematics. (3) A unified pipeline, TRANS, integrates TRANS-Loco and TRANS-Nav, supporting terrain-aware quadrupedal navigation in uneven and socially interactive environments. Comprehensive benchmarks against locomotion and social navigation baselines demonstrate the effectiveness of TRANS. Hardware experiments further confirm its potential for sim-to-real transfer.

</details>


### [213] [Scaling Single Human Demonstrations for Imitation Learning using Generative Foundational Models](https://arxiv.org/abs/2602.12734)
*Nick Heppert,Minh Quang Nguyen,Abhinav Valada*

Main category: cs.RO

TL;DR: Real2Gen trains robot manipulation policies from single human demonstrations by transferring them to simulation for unlimited synthetic data generation via flow matching.


<details>
  <summary>Details</summary>
Motivation: Collecting robot demonstrations is tedious and time-consuming, while human demonstrations are abundant but transfer to robots is non-trivial. Need efficient way to leverage human demonstrations for robot learning.

Method: Extract information from single human demo, transfer to simulation environment, use programmable expert agent to generate unlimited synthetic demonstrations, train flow matching policy on this abundant data.

Result: 26.6% average increase in success rate compared to baseline, better generalization due to diverse training data, zero-shot real-world deployment of simulation-trained policy.

Conclusion: Real2Gen effectively bridges human-to-robot transfer gap by leveraging simulation for unlimited data generation, enabling efficient policy learning from minimal human demonstrations.

Abstract: Imitation learning is a popular paradigm to teach robots new tasks, but collecting robot demonstrations through teleoperation or kinesthetic teaching is tedious and time-consuming. In contrast, directly demonstrating a task using our human embodiment is much easier and data is available in abundance, yet transfer to the robot can be non-trivial. In this work, we propose Real2Gen to train a manipulation policy from a single human demonstration. Real2Gen extracts required information from the demonstration and transfers it to a simulation environment, where a programmable expert agent can demonstrate the task arbitrarily many times, generating an unlimited amount of data to train a flow matching policy. We evaluate Real2Gen on human demonstrations from three different real-world tasks and compare it to a recent baseline. Real2Gen shows an average increase in the success rate of 26.6% and better generalization of the trained policy due to the abundance and diversity of training data. We further deploy our purely simulation-trained policy zero-shot in the real world. We make the data, code, and trained models publicly available at real2gen.cs.uni-freiburg.de.

</details>


### [214] [SafeFlowMPC: Predictive and Safe Trajectory Planning for Robot Manipulators with Learning-based Policies](https://arxiv.org/abs/2602.12794)
*Thies Oelerich,Gerald Ebmer,Christian Hartl-Nesic,Andreas Kugi*

Main category: cs.RO

TL;DR: SafeFlowMPC combines flow matching with online optimization to create safe, real-time robot control that generalizes from demonstrations while providing safety guarantees.


<details>
  <summary>Details</summary>
Motivation: Robots in everyday life need flexibility and real-time reactivity. Learning-based methods (like flow matching) provide generalization but lack interpretability and safety guarantees. Optimization-based methods provide safety guarantees but lack flexibility and generalization. There's a need to combine both approaches.

Method: SafeFlowMPC integrates flow matching (for learning from demonstrations) with model-predictive control (MPC) optimization. It uses a suboptimal MPC formulation to meet real-time execution demands while maintaining safety guarantees at all times.

Result: The method was tested on a KUKA 7-DoF manipulator in three real-world experiments: two grasping tasks and a dynamic human-robot object handover. It achieved strong performance while guaranteeing safety.

Conclusion: SafeFlowMPC successfully combines the strengths of learning-based and optimization-based approaches, providing both generalization from demonstrations and rigorous safety guarantees for real-time robot control in everyday applications.

Abstract: The emerging integration of robots into everyday life brings several major challenges. Compared to classical industrial applications, more flexibility is needed in combination with real-time reactivity. Learning-based methods can train powerful policies based on demonstrated trajectories, such that the robot generalizes a task to similar situations. However, these black-box models lack interpretability and rigorous safety guarantees. Optimization-based methods provide these guarantees but lack the required flexibility and generalization capabilities. This work proposes SafeFlowMPC, a combination of flow matching and online optimization to combine the strengths of learning and optimization. This method guarantees safety at all times and is designed to meet the demands of real-time execution by using a suboptimal model-predictive control formulation. SafeFlowMPC achieves strong performance in three real-world experiments on a KUKA 7-DoF manipulator, namely two grasping experiment and a dynamic human-robot object handover experiment. A video of the experiments is available at http://www.acin.tuwien.ac.at/42d6. The code is available at https://github.com/TU-Wien-ACIN-CDS/SafeFlowMPC.

</details>


### [215] [SKYSURF: A Self-learning Framework for Persistent Surveillance using Cooperative Aerial Gliders](https://arxiv.org/abs/2602.12838)
*Houssem Eddine Mohamadi,Nadjia Kara*

Main category: cs.RO

TL;DR: A cooperative UAV surveillance system using soaring techniques for energy harvesting, with behavioral management, collision avoidance, and optimized path tracking to extend flight time and improve target detection.


<details>
  <summary>Details</summary>
Motivation: Small UAVs have limited on-board power, which restricts surveillance persistence. The paper seeks to extend flight time by harvesting energy from rising buoyant air (soaring) while maintaining effective surveillance coverage.

Method: Proposes a local-global behavioral management approach for cooperative soaring-capable UAVs modeled as non-deterministic finite state-based rational agents. Includes mission planning, dynamic navigation with visibility/prediction concepts for collision avoidance, and delayed learning strategy to optimize path tracking controller gains.

Result: The approach significantly outperforms baselines: maintains surveillance persistence (staying aloft longer), detects twice as many targets as non-cooperative/semi-cooperative approaches, and consumes only 6% battery in six hours.

Conclusion: The proposed cooperative soaring approach effectively extends UAV surveillance persistence while improving target detection efficiency and minimizing power consumption, validated through rigorous comparative analyses with multiple benchmarks.

Abstract: The success of surveillance applications involving small unmanned aerial vehicles (UAVs) depends on how long the limited on-board power would persist. To cope with this challenge, alternative renewable sources of lift are sought. One promising solution is to extract energy from rising masses of buoyant air. This paper proposes a local-global behavioral management and decision-making approach for the autonomous deployment of soaring-capable UAVs. The cooperative UAVs are modeled as non-deterministic finite state-based rational agents. In addition to a mission planning module for assigning tasks and issuing dynamic navigation waypoints for a new path planning scheme, in which the concepts of visibility and prediction are applied to avoid the collisions. Moreover, a delayed learning and tuning strategy is employed optimize the gains of the path tracking controller. Rigorous comparative analyses carried out with three benchmarking baselines and 15 evolutionary algorithms highlight the adequacy of the proposed approach for maintaining the surveillance persistency (staying aloft for longer periods without landing) and maximizing the detection of targets (two times better than non-cooperative and semi-cooperative approaches) with less power consumption (almost 6% of battery consumed in six hours).

</details>


### [216] [Adding internal audio sensing to internal vision enables human-like in-hand fabric recognition with soft robotic fingertips](https://arxiv.org/abs/2602.12918)
*Iris Andrussow,Jans Solano,Benjamin A. Richardson,Georg Martius,Katherine J. Kuchenbecker*

Main category: cs.RO

TL;DR: A robotic system uses dual tactile sensors (Minsight for force/deformation and Minsound for vibrations) to classify fabrics with 97% accuracy on 20 fabrics, showing audio-based sensing is highly effective.


<details>
  <summary>Details</summary>
Motivation: Humans easily distinguish fabrics by integrating force patterns and vibrations, but robots struggle because tactile sensors can't achieve both high spatial resolution and high temporal sampling rate simultaneously.

Method: Robotic hand with soft tactile sensors: Minsight (camera-based, 50Hz force/deformation) and Minsound (MEMS microphone, 50Hz-15kHz vibrations). Robot actively encloses and rubs folded fabrics between fingers, inspired by human movements. Transformer-based classification method.

Result: Achieves 97% fabric classification accuracy on 20 common fabrics. Audio-based sensor shows high utility. External microphone improves robustness in loud noise. System learns general representations of fabric properties (stretchiness, thickness, roughness) beyond training data.

Conclusion: Audio-visual tactile sensing approach effectively replicates human haptic perception of fabrics, with vibration sensing proving particularly valuable for robotic tactile perception and generalization to fabric properties.

Abstract: Distinguishing the feel of smooth silk from coarse cotton is a trivial everyday task for humans. When exploring such fabrics, fingertip skin senses both spatio-temporal force patterns and texture-induced vibrations that are integrated to form a haptic representation of the explored material. It is challenging to reproduce this rich, dynamic perceptual capability in robots because tactile sensors typically cannot achieve both high spatial resolution and high temporal sampling rate. In this work, we present a system that can sense both types of haptic information, and we investigate how each type influences robotic tactile perception of fabrics. Our robotic hand's middle finger and thumb each feature a soft tactile sensor: one is the open-source Minsight sensor that uses an internal camera to measure fingertip deformation and force at 50 Hz, and the other is our new sensor Minsound that captures vibrations through an internal MEMS microphone with a bandwidth from 50 Hz to 15 kHz. Inspired by the movements humans make to evaluate fabrics, our robot actively encloses and rubs folded fabric samples between its two sensitive fingers. Our results test the influence of each sensing modality on overall classification performance, showing high utility for the audio-based sensor. Our transformer-based method achieves a maximum fabric classification accuracy of 97 % on a dataset of 20 common fabrics. Incorporating an external microphone away from Minsound increases our method's robustness in loud ambient noise conditions. To show that this audio-visual tactile sensing approach generalizes beyond the training data, we learn general representations of fabric stretchiness, thickness, and roughness.

</details>


### [217] [INHerit-SG: Incremental Hierarchical Semantic Scene Graphs with RAG-Style Retrieval](https://arxiv.org/abs/2602.12971)
*YukTungSamuel Fang,Zhikang Shi,Jiabin Qiu,Zixuan Chen,Jieqi Shi,Hao Xu,Jing Huo,Yang Gao*

Main category: cs.RO

TL;DR: INHerit-SG is a semantic scene graph system for robot navigation that uses explicit natural-language semantic anchors and an asynchronous architecture to enable interpretable human-intent reasoning with long-term consistency and low computational overhead.


<details>
  <summary>Details</summary>
Motivation: Existing semantic scene graph approaches for robot navigation are misaligned with embodied tasks because they rely on offline batch processing or implicit feature embeddings, making them unsuitable for interpretable human-intent reasoning in complex environments.

Method: INHerit-SG redefines the map as a structured, RAG-ready knowledge base with explicit natural-language semantic anchors. It uses an asynchronous dual-process architecture with a Floor-Room-Area-Object hierarchy to decouple geometric segmentation from semantic reasoning. An event-triggered update mechanism reorganizes the graph only when meaningful semantic events occur. For retrieval, multi-role LLMs decompose queries into atomic constraints and handle logical negations, with a hard-to-soft filtering strategy.

Result: The system achieves state-of-the-art performance on complex queries on the HM3DSem-SQR dataset and demonstrates scalability for downstream navigation tasks in real-world environments.

Conclusion: INHerit-SG provides explicit interpretability that improves success rate and reliability of complex retrievals, enabling adaptation to a broader spectrum of human interaction tasks in robot navigation.

Abstract: Driven by advancements in foundation models, semantic scene graphs have emerged as a prominent paradigm for high-level 3D environmental abstraction in robot navigation. However, existing approaches are fundamentally misaligned with the needs of embodied tasks. As they rely on either offline batch processing or implicit feature embeddings, the maps can hardly support interpretable human-intent reasoning in complex environments. To address these limitations, we present INHerit-SG. We redefine the map as a structured, RAG-ready knowledge base where natural-language descriptions are introduced as explicit semantic anchors to better align with human intent. An asynchronous dual-process architecture, together with a Floor-Room-Area-Object hierarchy, decouples geometric segmentation from time-consuming semantic reasoning. An event-triggered map update mechanism reorganizes the graph only when meaningful semantic events occur. This strategy enables our graph to maintain long-term consistency with relatively low computational overhead. For retrieval, we deploy multi-role Large Language Models (LLMs) to decompose queries into atomic constraints and handle logical negations, and employ a hard-to-soft filtering strategy to ensure robust reasoning. This explicit interpretability improves the success rate and reliability of complex retrievals, enabling the system to adapt to a broader spectrum of human interaction tasks. We evaluate INHerit-SG on a newly constructed dataset, HM3DSem-SQR, and in real-world environments. Experiments demonstrate that our system achieves state-of-the-art performance on complex queries, and reveal its scalability for downstream navigation tasks. Project Page: https://fangyuktung.github.io/INHeritSG.github.io/

</details>


### [218] [Learning Native Continuation for Action Chunking Flow Policies](https://arxiv.org/abs/2602.12978)
*Yufeng Liu,Hang Yu,Juntu Zhao,Bocheng Li,Di Zhang,Mingzhu Li,Wenxuan Wu,Yingdong Hu,Junyuan Xie,Junliang Guo,Dequan Wang,Yang Gao*

Main category: cs.RO

TL;DR: Legato is a training-time continuation method for action-chunked VLA models that improves real-time execution smoothness by initializing denoising from schedule-shaped mixtures and reshaping flow dynamics.


<details>
  <summary>Details</summary>
Motivation: Current action chunking methods for Vision Language Action (VLA) models suffer from discontinuities at chunk boundaries. Real-Time Chunking (RTC) helps but is external to the policy, causing spurious multimodal switching and trajectories that aren't intrinsically smooth.

Method: Legato uses three key techniques: 1) Initializes denoising from a schedule-shaped mixture of known actions and noise to expose models to partial action information, 2) Reshapes learned flow dynamics to ensure denoising consistency between training and inference under per-step guidance, and 3) Uses randomized schedule conditioning during training to support varying inference delays and achieve controllable smoothness.

Result: Legato produces smoother trajectories, reduces spurious multimodal switching, decreases hesitation, and shortens task completion time. In real-world experiments across five manipulation tasks, it consistently outperforms RTC with approximately 10% improvements in both trajectory smoothness and task completion time.

Conclusion: Legato effectively addresses discontinuities in action-chunked VLA models by integrating continuation methods into training, resulting in intrinsically smoother policies that outperform external smoothing approaches like RTC.

Abstract: Action chunking enables Vision Language Action (VLA) models to run in real time, but naive chunked execution often exhibits discontinuities at chunk boundaries. Real-Time Chunking (RTC) alleviates this issue but is external to the policy, leading to spurious multimodal switching and trajectories that are not intrinsically smooth. We propose Legato, a training-time continuation method for action-chunked flow-based VLA policies. Specifically, Legato initializes denoising from a schedule-shaped mixture of known actions and noise, exposing the model to partial action information. Moreover, Legato reshapes the learned flow dynamics to ensure that the denoising process remains consistent between training and inference under per-step guidance. Legato further uses randomized schedule condition during training to support varying inference delays and achieve controllable smoothness. Empirically, Legato produces smoother trajectories and reduces spurious multimodal switching during execution, leading to less hesitation and shorter task completion time. Extensive real-world experiments show that Legato consistently outperforms RTC across five manipulation tasks, achieving approximately 10% improvements in both trajectory smoothness and task completion time.

</details>


### [219] [How Swarms Differ: Challenges in Collective Behaviour Comparison](https://arxiv.org/abs/2602.13016)
*André Fialho Jesus,Jonas Kuckling*

Main category: cs.RO

TL;DR: The paper investigates the robustness of feature sets and similarity measures for quantifying swarm behaviors, showing that some combinations better distinguish behavioral groups and proposing a self-organizing map approach to identify ambiguous feature regions.


<details>
  <summary>Details</summary>
Motivation: Current approaches for measuring swarm behaviors often use ad-hoc feature sets designed for specific contexts without considering their resilience in broader applications. The development of automatic methods for designing swarm behaviors depends on the ability to quantitatively measure behavioral similarity, necessitating robust feature sets.

Method: The study selects existing swarm feature sets and similarity measures from prior robotics works, assesses their robustness across different behavioral contexts, and proposes a self-organizing map-based approach to identify regions of feature space where behaviors cannot be easily distinguished.

Result: The research demonstrates that the combination of feature set and similarity measure significantly impacts the ability to distinguish groups of similar behaviors. Some combinations prove more suitable for behavioral discrimination than others.

Conclusion: The interplay between feature sets and similarity measures is crucial for robust swarm behavior analysis. The proposed self-organizing map approach helps identify ambiguous feature regions, contributing to more reliable quantitative assessment of collective behaviors.

Abstract: Collective behaviours often need to be expressed through numerical features, e.g., for classification or imitation learning. This problem is often addressed by proposing an ad-hoc feature set for a particular swarm behaviour context, usually without further consideration of the solution's resilience outside of the conceived context. Yet, the development of automatic methods to design swarm behaviours is dependent on the ability to measure quantitatively the similarity of swarm behaviours. Hence, we investigate the impact of feature sets for collective behaviours. We select swarm feature sets and similarity measures from prior swarm robotics works, which mainly considered a narrow behavioural context and assess their robustness. We demonstrate that the interplay of feature set and similarity measure makes some combinations more suitable to distinguish groups of similar behaviours. We also propose a self-organised map-based approach to identify regions of the feature space where behaviours cannot be easily distinguished.

</details>


### [220] [SENSE-STEP: Learning Sim-to-Real Locomotion for a Sensory-Enabled Soft Quadruped Robot](https://arxiv.org/abs/2602.13078)
*Storm de Kam,Ebrahim Shahabi,Cosimo Della Santina*

Main category: cs.RO

TL;DR: Learning-based control framework enables closed-loop locomotion for soft quadruped robots using tactile suction-cup feet, achieving 41-91% speed improvements over open-loop baselines.


<details>
  <summary>Details</summary>
Motivation: Soft quadruped robots face challenges in robust closed-loop locomotion due to high-dimensional dynamics, actuator hysteresis, difficult-to-model contact interactions, and limited proprioceptive information about ground contact.

Method: A learning-based control framework with tactile suction-cup feet, trained in simulation through staged learning from reference gait under randomized environmental conditions, mapping proprioceptive and tactile feedback to pneumatic actuation and suction-cup commands.

Result: Closed-loop policy outperforms open-loop baseline by 41% faster on flat surfaces and 91% faster on 5-degree inclines; ablation studies show tactile force estimates and inertial feedback improve performance up to 56%.

Conclusion: The learning-based approach with tactile sensing enables robust closed-loop locomotion for soft quadruped robots on varied terrain, demonstrating significant performance improvements over conventional methods.

Abstract: Robust closed-loop locomotion remains challenging for soft quadruped robots due to high-dimensional dynamics, actuator hysteresis, and difficult-to-model contact interactions, while conventional proprioception provides limited information about ground contact. In this paper, we present a learning-based control framework for a pneumatically actuated soft quadruped equipped with tactile suction-cup feet, and we validate the approach experimentally on physical hardware. The control policy is trained in simulation through a staged learning process that starts from a reference gait and is progressively refined under randomized environmental conditions. The resulting controller maps proprioceptive and tactile feedback to coordinated pneumatic actuation and suction-cup commands, enabling closed-loop locomotion on flat and inclined surfaces. When deployed on the real robot, the closed-loop policy outperforms an open-loop baseline, increasing forward speed by 41% on a flat surface and by 91% on a 5-degree incline. Ablation studies further demonstrate the role of tactile force estimates and inertial feedback in stabilizing locomotion, with performance improvements of up to 56% compared to configurations without sensory feedback.

</details>


### [221] [Agentic AI for Robot Control: Flexible but still Fragile](https://arxiv.org/abs/2602.13081)
*Oscar Lima,Marc Vinci,Martin Günther,Marian Renz,Alexander Sung,Sebastian Stock,Johannes Brust,Lennart Niecksch,Zongyao Yi,Felix Igelbrink,Benjamin Kisliuk,Martin Atzmueller,Joachim Hertzberg*

Main category: cs.RO

TL;DR: LLM-based agentic control system for robots shows flexibility across domains but exhibits fragility in execution with non-deterministic behavior and prompt sensitivity.


<details>
  <summary>Details</summary>
Motivation: To leverage reasoning capabilities and commonsense priors of generative models for robot control in real-world settings with uncertainty, partial observability, and ambiguous natural language commands.

Method: An agentic control system where a reasoning-capable language model plans and executes tasks through an iterative planner-executor loop, selecting and invoking robot skills, with structured introspection, explicit event checks, and operator intervention support.

Result: Proof-of-concept experiments on two physical platforms (Mobipick indoor manipulation and Valdemar agricultural navigation) revealed substantial fragility including non-deterministic suboptimal behavior, instruction-following errors, and high prompt sensitivity, but demonstrated flexibility in domain transfer with minimal system updates.

Conclusion: While the LLM-based agentic architecture shows promise for flexible robot control across domains, current implementations exhibit significant fragility that must be addressed for reliable real-world deployment.

Abstract: Recent work leverages the capabilities and commonsense priors of generative models for robot control. In this paper, we present an agentic control system in which a reasoning-capable language model plans and executes tasks by selecting and invoking robot skills within an iterative planner and executor loop. We deploy the system on two physical robot platforms in two settings: (i) tabletop grasping, placement, and box insertion in indoor mobile manipulation (Mobipick) and (ii) autonomous agricultural navigation and sensing (Valdemar). Both settings involve uncertainty, partial observability, sensor noise, and ambiguous natural-language commands. The system exposes structured introspection of its planning and decision process, reacts to exogenous events via explicit event checks, and supports operator interventions that modify or redirect ongoing execution. Across both platforms, our proof-of-concept experiments reveal substantial fragility, including non-deterministic suboptimal behavior, instruction-following errors, and high sensitivity to prompt specification. At the same time, the architecture is flexible: transfer to a different robot and task domain largely required updating the system prompt (domain model, affordances, and action catalogue) and re-binding the same tool interface to the platform-specific skill API.

</details>


### [222] [UniManip: General-Purpose Zero-Shot Robotic Manipulation with Agentic Operational Graph](https://arxiv.org/abs/2602.13086)
*Haichao Liu,Yuanjiang Xue,Yuheng Zhou,Haoyuan Deng,Yinan Liang,Lihua Xie,Ziwei Wang*

Main category: cs.RO

TL;DR: UniManip is a robotic manipulation framework using a Bi-level Agentic Operational Graph to unify semantic reasoning with physical grounding for robust zero-shot generalization across unseen objects and tasks.


<details>
  <summary>Details</summary>
Motivation: Existing robotic manipulation approaches struggle with zero-shot generalization - end-to-end VLA models lack precision for long-horizon tasks, while hierarchical planners are too rigid for open-world variations. There's a need to bridge high-level semantic intent with low-level physical interaction in unstructured environments.

Method: UniManip uses a Bi-level Agentic Operational Graph (AOG) with two layers: a high-level Agentic Layer for task orchestration and a low-level Scene Layer for dynamic state representation. It operates as a dynamic agentic loop that instantiates object-centric scene graphs from perception, parameterizes them into collision-free trajectories via a safety-aware local planner, and uses structured memory for autonomous failure diagnosis and recovery.

Result: The system achieves 22.5% and 25.0% higher success rates compared to state-of-the-art VLA and hierarchical baselines respectively on unseen objects and tasks. It enables direct zero-shot transfer from fixed-base to mobile manipulation without fine-tuning or reconfiguration.

Conclusion: UniManip successfully bridges the gap between semantic reasoning and physical grounding through its bi-level agentic architecture, enabling robust zero-shot generalization in robotic manipulation across diverse environments and tasks.

Abstract: Achieving general-purpose robotic manipulation requires robots to seamlessly bridge high-level semantic intent with low-level physical interaction in unstructured environments. However, existing approaches falter in zero-shot generalization: end-to-end Vision-Language-Action (VLA) models often lack the precision required for long-horizon tasks, while traditional hierarchical planners suffer from semantic rigidity when facing open-world variations. To address this, we present UniManip, a framework grounded in a Bi-level Agentic Operational Graph (AOG) that unifies semantic reasoning and physical grounding. By coupling a high-level Agentic Layer for task orchestration with a low-level Scene Layer for dynamic state representation, the system continuously aligns abstract planning with geometric constraints, enabling robust zero-shot execution. Unlike static pipelines, UniManip operates as a dynamic agentic loop: it actively instantiates object-centric scene graphs from unstructured perception, parameterizes these representations into collision-free trajectories via a safety-aware local planner, and exploits structured memory to autonomously diagnose and recover from execution failures. Extensive experiments validate the system's robust zero-shot capability on unseen objects and tasks, demonstrating a 22.5% and 25.0% higher success rate compared to state-of-the-art VLA and hierarchical baselines, respectively. Notably, the system enables direct zero-shot transfer from fixed-base setups to mobile manipulation without fine-tuning or reconfiguration. Our open-source project page can be found at https://henryhcliu.github.io/unimanip.

</details>


### [223] [Temporally-Sampled Efficiently Adaptive State Lattices for Autonomous Ground Robot Navigation in Partially Observed Environments](https://arxiv.org/abs/2602.13159)
*Ashwin Satish Menon,Eric R. Damm,Eli S. Lancaster,Felix A. Sanchez,Jason M. Gregory,Thomas M. Howard*

Main category: cs.RO

TL;DR: TSEASL is a regional planner arbitration architecture that improves navigation safety in partially observable off-road environments by considering both current and previously optimized trajectories to reduce guidance instability.


<details>
  <summary>Details</summary>
Motivation: In off-road environments with partial observability, traditional navigation architectures suffer from rapidly changing regional plan guidance due to continuous map updates. This causes unsafe navigation behavior requiring manual safety interventions during autonomous traversals.

Method: Proposes Temporally-Sampled Efficiently Adaptive State Lattices (TSEASL), a regional planner arbitration architecture that considers updated/optimized versions of previously generated trajectories alongside the currently generated trajectory to provide more stable guidance.

Result: Testing on a Clearpath Robotics Warthog UGV with real map data showed that TSEASL eliminated manual interventions in locations where baseline planner failed, and demonstrated higher levels of planner stability compared to baseline.

Conclusion: TSEASL effectively addresses navigation instability in partially observable off-road environments, with future work focused on improving generalizability to various off-road autonomy scenarios.

Abstract: Due to sensor limitations, environments that off-road mobile robots operate in are often only partially observable. As the robots move throughout the environment and towards their goal, the optimal route is continuously revised as the sensors perceive new information. In traditional autonomous navigation architectures, a regional motion planner will consume the environment map and output a trajectory for the local motion planner to use as a reference. Due to the continuous revision of the regional plan guidance as a result of changing map information, the reference trajectories which are passed down to the local planner can differ significantly across sequential planning cycles. This rapidly changing guidance can result in unsafe navigation behavior, often requiring manual safety interventions during autonomous traversals in off-road environments. To remedy this problem, we propose Temporally-Sampled Efficiently Adaptive State Lattices (TSEASL), which is a regional planner arbitration architecture that considers updated and optimized versions of previously generated trajectories against the currently generated trajectory. When tested on a Clearpath Robotics Warthog Unmanned Ground Vehicle as well as real map data collected from the Warthog, results indicate that when running TSEASL, the robot did not require manual interventions in the same locations where the robot was running the baseline planner. Additionally, higher levels of planner stability were recorded with TSEASL over the baseline. The paper concludes with a discussion of further improvements to TSEASL in order to make it more generalizable to various off-road autonomy scenarios.

</details>


### [224] [Human Emotion-Mediated Soft Robotic Arts: Exploring the Intersection of Human Emotions, Soft Robotics and Arts](https://arxiv.org/abs/2602.13163)
*Saitarun Nadipineni,Chenhao Hong,Tanishtha Ramlall,Chapa Sirithunge,Kaspar Althoefer,Fumiya Iida,Thilina Dulantha Lalitharatne*

Main category: cs.RO

TL;DR: Researchers created soft robotic art displays (character and flower) that respond to human emotions measured via EEG alpha waves, demonstrating emotion-mediated interactive art.


<details>
  <summary>Details</summary>
Motivation: To explore the intersection of human emotions, soft robotics, and art to create new forms of human emotion-mediated soft robotic art that can embody emotional states for artistic expression.

Method: Developed two soft embodiments (character and flower) that respond to brain signals; measured human emotions via EEG alpha waves; mapped alpha waves to dynamic movements of soft robots; conducted experiments to demonstrate the concept.

Result: Successfully created soft robotic art displays that dynamically respond to different emotion levels reflected in alpha waves, demonstrating how soft robotics can embody human emotional states.

Conclusion: Soft robotics offers a new medium for insightful artistic expression and interaction by embodying human emotional states, expanding possibilities for emotion-mediated art displays.

Abstract: Soft robotics has emerged as a versatile field with applications across various domains, from healthcare to industrial automation, and more recently, art and interactive installations. The inherent flexibility, adaptability, and safety of soft robots make them ideal for applications that require delicate, organic, and lifelike movement, allowing for immersive and responsive interactions. This study explores the intersection of human emotions, soft robotics, and art to establish and create new forms of human emotion-mediated soft robotic art. In this paper, we introduce two soft embodiments: a soft character and a soft flower as an art display that dynamically responds to brain signals based on alpha waves, reflecting different emotion levels. We present how human emotions can be measured as alpha waves based on brain/EEG signals, how we map the alpha waves to the dynamic movements of the two soft embodiments, and demonstrate our proposed concept using experiments. The findings of this study highlight how soft robotics can embody human emotional states, offering a new medium for insightful artistic expression and interaction, and demonstrating how art displays can be embodied.

</details>


### [225] [Steerable Vision-Language-Action Policies for Embodied Reasoning and Hierarchical Control](https://arxiv.org/abs/2602.13193)
*William Chen,Jagdeep Singh Bhatia,Catherine Glossop,Nikhil Mathihalli,Ria Doshi,Andy Tang,Danny Driess,Karl Pertsch,Sergey Levine*

Main category: cs.RO

TL;DR: Steerable Policies enable better grounding of vision-language model knowledge in robot control by training vision-language-action models on rich synthetic commands at multiple abstraction levels, improving low-level controllability and task generalization.


<details>
  <summary>Details</summary>
Motivation: Current hierarchical approaches using VLMs for high-level reasoning and VLAs for low-level execution are limited by natural language task instructions as the interface, which restricts how much VLM reasoning can steer low-level robot behavior.

Method: Introduce Steerable Policies: VLAs trained on rich synthetic commands at various abstraction levels (subtasks, motions, grounded pixel coordinates). These policies can be controlled by both learned high-level embodied reasoners and off-the-shelf VLMs prompted via in-context learning to reason over command abstractions.

Result: The approach outperforms prior embodied reasoning VLAs and VLM-based hierarchical baselines across extensive real-world manipulation experiments, including challenging generalization and long-horizon tasks.

Conclusion: Steerable Policies bridge the gap between high-level VLM reasoning and low-level robot control by improving low-level controllability, enabling better utilization of pretrained vision-language knowledge for improved robotic task generalization.

Abstract: Pretrained vision-language models (VLMs) can make semantic and visual inferences across diverse settings, providing valuable common-sense priors for robotic control. However, effectively grounding this knowledge in robot behaviors remains an open challenge. Prior methods often employ a hierarchical approach where VLMs reason over high-level commands to be executed by separate low-level policies, e.g., vision-language-action models (VLAs). The interface between VLMs and VLAs is usually natural language task instructions, which fundamentally limits how much VLM reasoning can steer low-level behavior. We thus introduce Steerable Policies: VLAs trained on rich synthetic commands at various levels of abstraction, like subtasks, motions, and grounded pixel coordinates. By improving low-level controllability, Steerable Policies can unlock pretrained knowledge in VLMs, enabling improved task generalization. We demonstrate this benefit by controlling our Steerable Policies with both a learned high-level embodied reasoner and an off-the-shelf VLM prompted to reason over command abstractions via in-context learning. Across extensive real-world manipulation experiments, these two novel methods outperform prior embodied reasoning VLAs and VLM-based hierarchical baselines, including on challenging generalization and long-horizon tasks.
  Website: steerable-policies.github.io

</details>


### [226] [Imitating What Works: Simulation-Filtered Modular Policy Learning from Human Videos](https://arxiv.org/abs/2602.13197)
*Albert J. Zhai,Kuo-Hao Zeng,Jiasen Lu,Ali Farhadi,Shenlong Wang,Wei-Chiu Ma*

Main category: cs.RO

TL;DR: PSI framework trains modular manipulation policies using human video data processed through simulation to learn task-compatible grasping and post-grasp motions without robot data.


<details>
  <summary>Details</summary>
Motivation: Human videos provide scalable data for robot learning but are insufficient for teaching grasping behaviors to robots with non-human-like hands. Modular policies with grasp generators often produce stable but task-incompatible grasps that hinder downstream motions.

Method: Perceive-Simulate-Imitate (PSI) framework processes human video motion data through paired grasp-trajectory filtering in simulation. Simulation extends trajectory data with grasp suitability labels, enabling supervised learning of task-oriented grasping capabilities in a modular policy design.

Result: Real-world experiments show PSI can learn precise manipulation skills efficiently without any robot data, achieving significantly more robust performance than naive grasp generator approaches.

Conclusion: PSI successfully bridges the gap between human video data and robot manipulation by using simulation to generate task-compatible grasp labels, enabling effective learning of both grasping and post-grasp motions from human demonstrations.

Abstract: The ability to learn manipulation skills by watching videos of humans has the potential to unlock a new source of highly scalable data for robot learning. Here, we tackle prehensile manipulation, in which tasks involve grasping an object before performing various post-grasp motions. Human videos offer strong signals for learning the post-grasp motions, but they are less useful for learning the prerequisite grasping behaviors, especially for robots without human-like hands. A promising way forward is to use a modular policy design, leveraging a dedicated grasp generator to produce stable grasps. However, arbitrary stable grasps are often not task-compatible, hindering the robot's ability to perform the desired downstream motion. To address this challenge, we present Perceive-Simulate-Imitate (PSI), a framework for training a modular manipulation policy using human video motion data processed by paired grasp-trajectory filtering in simulation. This simulation step extends the trajectory data with grasp suitability labels, which allows for supervised learning of task-oriented grasping capabilities. We show through real-world experiments that our framework can be used to learn precise manipulation skills efficiently without any robot data, resulting in significantly more robust performance than using a grasp generator naively.

</details>


<div id='eess.IV'></div>

# eess.IV [[Back]](#toc)

### [227] [Quantum walk inspired JPEG compression of images](https://arxiv.org/abs/2602.12306)
*Abhishek Verma,Sahil Tomar,Sandeep Kumar*

Main category: eess.IV

TL;DR: Quantum-inspired optimization improves JPEG compression by learning optimal quantization tables using quantum walk search, achieving 3-6 dB PSNR gains while maintaining JPEG compatibility.


<details>
  <summary>Details</summary>
Motivation: To enhance classical JPEG compression by introducing an optimized quantization framework that improves compression efficiency and reconstruction quality while maintaining backward compatibility with standard JPEG decoders.

Method: Proposes a quantum-inspired adaptive quantization framework using Quantum Walk Inspired Optimization (QWIO) to search continuous parameter space of frequency band scaling factors, optimizing a unified rate-distortion objective that jointly considers reconstruction fidelity and compression efficiency.

Result: Experimental evaluation on MNIST, CIFAR10, and ImageNet shows average gains of 3-6 dB PSNR, better structural preservation of edges/contours/luminance transitions, improved SSIM and BPP metrics, while maintaining JPEG decoder compatibility.

Conclusion: The quantum-inspired optimization framework successfully enhances JPEG compression quality and efficiency without modifying decoder compatibility, making it practical for deployment and research using accessible scientific packages.

Abstract: This work proposes a quantum inspired adaptive quantization framework that enhances the classical JPEG compression by introducing a learned, optimized Qtable derived using a Quantum Walk Inspired Optimization (QWIO) search strategy. The optimizer searches a continuous parameter space of frequency band scaling factors under a unified rate distortion objective that jointly considers reconstruction fidelity and compression efficiency. The proposed framework is evaluated on MNIST, CIFAR10, and ImageNet subsets, using Peak Signal to Noise Ratio (PSNR), Structural Similarity Index (SSIM), Bits Per Pixel (BPP), and error heatmap visual analysis as evaluation metrics. Experimental results show average gains ranging from 3 to 6 dB PSNR, along with better structural preservation of edges, contours, and luminance transitions, without modifying decoder compatibility. The structure remains JPEG compliant and can be implemented using accessible scientific packages making it ideal for deployment and practical research use.

</details>


### [228] [Visible and Hyperspectral Imaging for Quality Assessment of Milk: Property Characterisation and Identification](https://arxiv.org/abs/2602.12313)
*Massimo Martinelli,Elena Tomassi,Nafiou Arouna,Morena Gabriele,Laryssa Perez Fabbri,Luisa Pozzo,Giuseppe Conte,Davide Moroni,Laura Pucci*

Main category: eess.IV

TL;DR: Visible and hyperspectral imaging combined with machine learning can rapidly and non-destructively assess milk quality with high accuracy, outperforming conventional chemical analyses.


<details>
  <summary>Details</summary>
Motivation: Need for rapid, non-destructive, and cost-effective alternatives to conventional chemical analyses for milk quality assessment to ensure nutritional value and food safety.

Method: Analyzed 52 milk samples using conventional chemical methods (spectrophotometry, GLC/HPLC) alongside visible (smartphone) and hyperspectral imaging. Used 11 machine learning algorithms to correlate imaging features with biochemical measurements.

Result: Visible imaging achieved 100% accuracy for distinguishing fresh vs. 12-day stored samples and antibiotic-treated vs. untreated groups. XGBoost perfectly predicted polyphenols and antioxidant capacity. Hyperspectral imaging achieved >95% accuracy for fatty acid classification and 94.8% for treatment groups using Random Forest.

Conclusion: Both visible and hyperspectral imaging coupled with machine learning are powerful, non-invasive tools for rapid assessment of milk's chemical and nutritional profiles, demonstrating strong potential for milk quality assessment.

Abstract: Rapid and non-destructive assessment of milk quality is crucial to ensuring both nutritional value and food safety. In this study, we investigated the potential of visible and hyperspectral imaging as cost-effective and quick-response alternatives to conventional chemical analyses for characterizing key properties of cowś milk. A total of 52 milk samples were analysed to determine their biochemical composition (polyphenols, antioxidant capacity, and fatty acids) using spectrophotometer methods and standard gas-liquid and high-performance liquid chromatography (GLC/HPLC). Concurrently, visible (RGB) images were captured using a standard smartphone, and hyperspectral data were acquired in the near-infrared range. A comprehensive analytical framework, including eleven different machine learning algorithms, was employed to correlate imaging features with biochemical measurements. Analysis of visible images accurately distinguished between fresh samples and those stored for 12 days (100 percent accuracy) and achieved perfect discrimination between antibiotic-treated and untreated groups (100 percent accuracy). Moreover, image-derived features enabled perfect prediction of the polyphenols content and the antioxidant capacity using an XGBoost model. Hyperspectral imaging further achieved classification accuracies exceeding 95 percent for several individual fatty acids and 94.8 percent for treatment groups using a Random Forest model. These findings demonstrate that both visible and hyperspectral imaging, when coupled with machine learning, are powerful, non-invasive tools for the rapid assessment of milkś chemical and nutritional profiles, highlighting the strong potential of imaging-based approaches for milk quality assessment.

</details>


### [229] [Conference Proceedings of the Inaugural Conference of the International Society for Tractography (IST 2025 Bordeaux)](https://arxiv.org/abs/2602.12410)
*Flavio Dell Acqua,Maxime Descoteaux,Graham Little,Laurent Petit,Dogu Baran Aydogan,Stephanie Forkel,Alexander Leemans,Simona Schiavi,Michel Thiebaut de Schotten*

Main category: eess.IV

TL;DR: Conference proceedings abstracts from the inaugural International Society for Tractography conference covering neuroanatomy, tractography methods, and clinical applications.


<details>
  <summary>Details</summary>
Motivation: To foster interdisciplinary exchange and collaboration in tractography research, bringing together experts from neuroanatomy, methods development, and clinical applications to advance the field.

Method: Conference format with poster sessions, power pitches, and oral presentations featuring abstracts on latest research in tractography, diffusion MRI, and related applications.

Result: Collection of abstracts showcasing advancements in tractography methods, applications to neurological/psychiatric disorders, deep brain stimulation targeting, and brain development research.

Conclusion: The inaugural IST conference successfully brought together world-leading experts to discuss challenges and chart future directions for tractography research and clinical applications.

Abstract: This collection comprises the abstracts presented during poster, power pitch and oral sessions at the Inaugural Conference of the International Society for Tractography (IST Conference 2025), held in Bordeaux, France, from October 13-16, 2025. The conference was designed to foster meaningful exchange and collaboration between disparate fields. The overall focus was on advancing research, innovation, and community in the common fields of interest: neuroanatomy, tractography methods and scientific/clinical applications of tractography. The included abstracts cover the latest advancements in tractography, Diffusion MRI, and related fields including new work on; neurological and psychiatric disorders, deep brain stimulation targeting, and brain development. This landmark event brought together world-leading experts to discuss critical challenges and chart the future direction of the field.

</details>


### [230] [Lung nodule classification on CT scan patches using 3D convolutional neural networks](https://arxiv.org/abs/2602.12750)
*Volodymyr Sydorskyi*

Main category: eess.IV

TL;DR: The paper presents three methodological improvements for lung nodule classification in CT scans, achieving state-of-the-art performance on the LIDC-IDRI dataset.


<details>
  <summary>Details</summary>
Motivation: Lung cancer is a deadly disease where early detection is crucial but challenging due to the large number of CT scans to review, multiple nodules, and small nodule sizes. Automated systems are needed to assist radiologists in accurate and efficient lung nodule classification.

Method: Three key improvements: (1) advanced CT scan cropping strategy to focus on target nodules and reduce computational cost, (2) target filtering techniques to remove noisy labels, and (3) novel augmentation methods to improve model robustness. These are integrated into a classification subsystem within a Clinical Decision Support System.

Result: Achieved Macro ROC AUC of 0.9176 and Macro F1-score of 0.7658 for multiclass classification, and Binary ROC AUC of 0.9383 and Binary F1-score of 0.8668 on the LIDC-IDRI dataset, outperforming previous approaches.

Conclusion: The proposed techniques enable robust lung nodule classification that works across diverse acquisition protocols, scanner types, and upstream models, demonstrating state-of-the-art performance for this critical medical task.

Abstract: Lung cancer remains one of the most common and deadliest forms of cancer worldwide. The likelihood of successful treatment depends strongly on the stage at which the disease is diagnosed. Therefore, early detection of lung cancer represents a critical medical challenge. However, this task poses significant difficulties for thoracic radiologists due to the large number of studies to review, the presence of multiple nodules within the lungs, and the small size of many nodules, which complicates visual assessment. Consequently, the development of automated systems that incorporate highly accurate and computationally efficient lung nodule detection and classification modules is essential. This study introduces three methodological improvements for lung nodule classification: (1) an advanced CT scan cropping strategy that focuses the model on the target nodule while reducing computational cost; (2) target filtering techniques for removing noisy labels; (3) novel augmentation methods to improve model robustness. The integration of these techniques enables the development of a robust classification subsystem within a comprehensive Clinical Decision Support System for lung cancer detection, capable of operating across diverse acquisition protocols, scanner types, and upstream models (segmentation or detection). The multiclass model achieved a Macro ROC AUC of 0.9176 and a Macro F1-score of 0.7658, while the binary model reached a Binary ROC AUC of 0.9383 and a Binary F1-score of 0.8668 on the LIDC-IDRI dataset. These results outperform several previously reported approaches and demonstrate state-of-the-art performance for this task.

</details>


### [231] [VineetVC: Adaptive Video Conferencing Under Severe Bandwidth Constraints Using Audio-Driven Talking-Head Reconstruction](https://arxiv.org/abs/2602.12758)
*Vineet Kumar Rakesh,Soumya Mazumdar,Tapas Samanta,Hemendra Kumar Pandey,Amitabha Das,Sarbajit Pal*

Main category: eess.IV

TL;DR: An adaptive video conferencing system that switches to AI-generated talking-head video during bandwidth saturation to maintain call quality with minimal bandwidth (32.80 kbps).


<details>
  <summary>Details</summary>
Motivation: Bandwidth depletion in consumer networks undermines real-time video conferencing stability, causing encoder saturation, packet loss, frame rate deterioration, and increased latency.

Method: Integrates WebRTC media delivery with an audio-driven talking-head reconstruction pathway and telemetry-driven mode regulation. System includes WebSocket signaling, optional SFU, browser client with real-time statistics extraction, and AI REST service that processes reference face image and audio to synthesize MP4 video.

Result: The system can substitute camera track with synthesized stream using median bandwidth of 32.80 kbps. Includes bandwidth-mode switching strategy and client-side mode-state logging.

Conclusion: Proposed adaptive conferencing system maintains video call quality during network congestion by switching to low-bandwidth AI-generated talking-head video when needed.

Abstract: Intense bandwidth depletion within consumer and constrained networks has the potential to undermine the stability of real-time video conferencing: encoder rate management becomes saturated, packet loss escalates, frame rates deteriorate, and end-to-end latency significantly increases. This work delineates an adaptive conferencing system that integrates WebRTC media delivery with a supplementary audio-driven talking-head reconstruction pathway and telemetry-driven mode regulation. The system consists of a WebSocket signaling service, an optional SFU for multi-party transmission, a browser client capable of real-time WebRTC statistics extraction and CSV telemetry export, and an AI REST service that processes a reference face image and recorded audio to produce a synthesized MP4; the browser can substitute its outbound camera track with the synthesized stream with a median bandwidth of 32.80 kbps. The solution incorporates a bandwidth-mode switching strategy and a client-side mode-state logger.

</details>


### [232] [3DLAND: 3D Lesion Abdominal Anomaly Localization Dataset](https://arxiv.org/abs/2602.12820)
*Mehran Advand,Zahra Dehghanian,Navid Faraji,Reza Barati,Seyed Amir Ahmad Safavi-Naini,Hamid R. Rabiee*

Main category: eess.IV

TL;DR: 3DLAND: A large-scale 3D abdominal CT dataset with 6,000+ volumes and 20,000+ lesion annotations linked to 7 organs, enabling robust medical AI evaluation.


<details>
  <summary>Details</summary>
Motivation: Existing abdominal CT datasets lack comprehensive 3D annotations, multi-organ coverage, and precise lesion-to-organ associations, limiting robust representation learning and clinical applications.

Method: A streamlined three-phase pipeline: 1) automated spatial reasoning, 2) prompt-optimized 2D segmentation, and 3) memory-guided 3D propagation, validated by expert radiologists with surface dice scores >0.75.

Result: Created 3DLAND dataset with over 6,000 contrast-enhanced CT volumes and 20,000+ high-fidelity 3D lesion annotations across 7 abdominal organs (liver, kidneys, pancreas, spleen, stomach, gallbladder).

Conclusion: 3DLAND establishes a new benchmark for evaluating organ-aware 3D segmentation models and enables scalable evaluation of anomaly detection, localization, and cross-organ transfer learning for medical AI.

Abstract: Existing medical imaging datasets for abdominal CT often lack three-dimensional annotations, multi-organ coverage, or precise lesion-to-organ associations, hindering robust representation learning and clinical applications. To address this gap, we introduce 3DLAND, a large-scale benchmark dataset comprising over 6,000 contrast-enhanced CT volumes with over 20,000 high-fidelity 3D lesion annotations linked to seven abdominal organs: liver, kidneys, pancreas, spleen, stomach, and gallbladder. Our streamlined three-phase pipeline integrates automated spatial reasoning, prompt-optimized 2D segmentation, and memory-guided 3D propagation, validated by expert radiologists with surface dice scores exceeding 0.75. By providing diverse lesion types and patient demographics, 3DLAND enables scalable evaluation of anomaly detection, localization, and cross-organ transfer learning for medical AI. Our dataset establishes a new benchmark for evaluating organ-aware 3D segmentation models, paving the way for advancements in healthcare-oriented AI. To facilitate reproducibility and further research, the 3DLAND dataset and implementation code are publicly available at https://mehrn79.github.io/3DLAND.

</details>


### [233] [Dual-Phase Cross-Modal Contrastive Learning for CMR-Guided ECG Representations for Cardiovascular Disease Assessment](https://arxiv.org/abs/2602.12883)
*Laura Alvarez-Florez,Angel Bujalance-Gomez,Femke Raijmakers,Samuel Ruiperez-Campillo,Maarten Z. H. Kolk,Jesse Wiers,Julia Vogt,Erik J. Bekkers,Ivana Išgum,Fleur V. Y. Tjong*

Main category: eess.IV

TL;DR: A contrastive learning framework that aligns ECG representations with 3D cardiac MRI volumes to extract clinically relevant cardiac phenotypes from ubiquitous ECG data, enabling scalable extraction of image-derived traits.


<details>
  <summary>Details</summary>
Motivation: CMR provides detailed cardiac structure/function evaluation but has limited accessibility, while ECG is ubiquitous and inexpensive but offers limited insight into underlying cardiac structure and mechanical function. There's a need to bridge this gap to extract clinically relevant cardiac phenotypes from widely available ECG data.

Method: Introduces a contrastive learning framework that aligns ECG representations with 3D CMR volumes at both end-diastole (ED) and end-systole (ES) phases. Uses a dual-phase contrastive loss to anchor each ECG jointly with both cardiac phases in a shared latent space, enabling flexible disentanglement of structural and functional cardiac properties.

Result: Using over 34,000 ECG-CMR pairs from UK Biobank, demonstrated improved extraction of image-derived phenotypes from ECG, particularly for functional parameters (↑9.2%), while improvements in clinical outcome prediction remained modest (↑0.7%).

Conclusion: This strategy could enable scalable and cost-effective extraction of image-derived traits from ECG, making detailed cardiac assessment more accessible through ubiquitous ECG data rather than limited CMR.

Abstract: Cardiac magnetic resonance imaging (CMR) offers detailed evaluation of cardiac structure and function, but its limited accessibility restricts use to selected patient populations. In contrast, the electrocardiogram (ECG) is ubiquitous and inexpensive, and provides rich information on cardiac electrical activity and rhythm, yet offers limited insight into underlying cardiac structure and mechanical function. To address this, we introduce a contrastive learning framework that improves the extraction of clinically relevant cardiac phenotypes from ECG by learning from paired ECG-CMR data. Our approach aligns ECG representations with 3D CMR volumes at end-diastole (ED) and end-systole (ES), with a dual-phase contrastive loss to anchor each ECG jointly with both cardiac phases in a shared latent space. Unlike prior methods limited to 2D CMR representations with or without a temporal component, our framework models 3D anatomy at both ED and ES phases as distinct latent representations, enabling flexible disentanglement of structural and functional cardiac properties. Using over 34,000 ECG-CMR pairs from the UK Biobank, we demonstrate improved extraction of image-derived phenotypes from ECG, particularly for functional parameters ($\uparrow$ 9.2\%), while improvements in clinical outcome prediction remained modest ($\uparrow$ 0.7\%). This strategy could enable scalable and cost-effective extraction of image-derived traits from ECG. The code for this research is publicly available.

</details>


### [234] [Efficient Plug-and-Play method for Dynamic Imaging Via Kalman Smoothing](https://arxiv.org/abs/2602.13043)
*Benjamin Hawkes,Mike Davies,Victor Elvira,Audrey Repetti*

Main category: eess.IV

TL;DR: PnP-KS-ADMM algorithm combines Kalman smoothing with ADMM and deep learning denoisers for improved computational efficiency in state-space models with spatial priors.


<details>
  <summary>Details</summary>
Motivation: Traditional Kalman smoothing lacks expressivity by not incorporating spatial prior information, while standard PnP-ADMM becomes inefficient for large numbers of timesteps in state-space models.

Method: Proposes a Plug-and-Play algorithm based on KS-ADMM iterations that efficiently handles state-space models through Kalman smoothing while enabling the use of powerful deep learning denoisers as priors.

Result: Simulations on 2D+t imaging problems show that PnP-KS-ADMM improves computational efficiency over standard PnP-ADMM for large numbers of timesteps.

Conclusion: The proposed method successfully combines the strengths of Kalman smoothing, ADMM optimization, and deep learning denoisers to create an efficient and expressive framework for state-space model problems with spatial priors.

Abstract: State-space models (SSM) are common in signal processing, where Kalman smoothing (KS) methods are state-of-the-art. However, traditional KS techniques lack expressivity as they do not incorporate spatial prior information. Recently, [1] proposed an ADMM algorithm that handles the state-space fidelity term with KS while regularizing the object via a sparsity-based prior with proximity operators. Plug-and-Play (PnP) methods are a popular type of iterative algorithms that replace proximal operators encoding prior knowledge with powerful denoisers such as deep neural networks. These methods are widely used in image processing, achieving state-of-the-art results. In this work, we build on the KS-ADMM method, combining it with deep learning to achieve higher expressivity. We propose a PnP algorithm based on KS-ADMM iterations, efficiently handling the SSM through KS, while enabling the use of powerful denoising networks. Simulations on a 2D+t imaging problem show that the proposed PnP-KS-ADMM algorithm improves the computational efficiency over standard PnP-ADMM for large numbers of timesteps.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [235] [GT-HarmBench: Benchmarking AI Safety Risks Through the Lens of Game Theory](https://arxiv.org/abs/2602.12316)
*Pepijn Cobben,Xuanqiang Angelo Huang,Thao Amelia Pham,Isabel Dahlgren,Terry Jingchen Zhang,Zhijing Jin*

Main category: cs.AI

TL;DR: GT-HarmBench is a new benchmark for evaluating AI safety in multi-agent environments, testing 15 frontier models across 2,009 high-stakes scenarios based on game theory structures like Prisoner's Dilemma, with agents making socially beneficial choices only 62% of the time.


<details>
  <summary>Details</summary>
Motivation: Existing AI safety benchmarks focus on single agents, leaving multi-agent risks like coordination failure and conflict poorly understood. There's a need to evaluate how frontier AI systems perform in realistic multi-agent scenarios where game-theoretic dynamics create potential for harmful outcomes.

Method: Created GT-HarmBench with 2,009 high-stakes scenarios drawn from realistic AI risk contexts in the MIT AI Risk Repository. Scenarios span classic game-theoretic structures (Prisoner's Dilemma, Stag Hunt, Chicken). Tested 15 frontier models, measured sensitivity to prompt framing and ordering, analyzed reasoning patterns, and evaluated game-theoretic interventions.

Result: Agents chose socially beneficial actions in only 62% of cases, frequently leading to harmful outcomes. Game-theoretic interventions improved socially beneficial outcomes by up to 18%. The study revealed substantial reliability gaps in multi-agent alignment.

Conclusion: The benchmark provides a standardized testbed for studying alignment in multi-agent environments and highlights significant gaps in current AI systems' ability to navigate game-theoretic scenarios safely. The findings emphasize the need for better multi-agent safety evaluation and alignment techniques.

Abstract: Frontier AI systems are increasingly capable and deployed in high-stakes multi-agent environments. However, existing AI safety benchmarks largely evaluate single agents, leaving multi-agent risks such as coordination failure and conflict poorly understood. We introduce GT-HarmBench, a benchmark of 2,009 high-stakes scenarios spanning game-theoretic structures such as the Prisoner's Dilemma, Stag Hunt and Chicken. Scenarios are drawn from realistic AI risk contexts in the MIT AI Risk Repository. Across 15 frontier models, agents choose socially beneficial actions in only 62% of cases, frequently leading to harmful outcomes. We measure sensitivity to game-theoretic prompt framing and ordering, and analyze reasoning patterns driving failures. We further show that game-theoretic interventions improve socially beneficial outcomes by up to 18%. Our results highlight substantial reliability gaps and provide a broad standardized testbed for studying alignment in multi-agent environments. The benchmark and code are available at https://github.com/causalNLP/gt-harmbench.

</details>


### [236] [A Theoretical Framework for Adaptive Utility-Weighted Benchmarking](https://arxiv.org/abs/2602.12356)
*Philip Waggoner*

Main category: cs.AI

TL;DR: The paper introduces a theoretical framework that reconceptualizes AI benchmarking as a multilayer adaptive network linking metrics, model components, and stakeholder groups, enabling dynamic, context-aware evaluation that embeds human tradeoffs.


<details>
  <summary>Details</summary>
Motivation: Traditional benchmarking practices in AI focus on shared tasks, metrics, and leaderboards but fail to account for the sociotechnical contexts in which AI systems operate. As AI systems are deployed in more varied and consequential settings, there's a need for more holistic evaluation that considers multiple stakeholders and their unique priorities.

Method: The paper proposes a theoretical framework that models benchmarking as a multilayer adaptive network. It links evaluation metrics, model components, and stakeholder groups through weighted interactions. The approach uses conjoint-derived utilities and a human-in-the-loop update rule to embed human tradeoffs into benchmark structure, allowing benchmarks to evolve dynamically while preserving stability and interpretability.

Result: The framework generalizes classical leaderboards as a special case and provides a foundation for building context-aware evaluation protocols. It offers new robust tools for analyzing the structural properties of benchmarks, enabling more accountable and human-aligned evaluation.

Conclusion: The proposed multilayer adaptive network framework represents a significant advancement in AI evaluation methodology, moving beyond traditional benchmarking to create more holistic, stakeholder-aware evaluation systems that can dynamically adapt to evolving priorities while maintaining interpretability and stability.

Abstract: Benchmarking has long served as a foundational practice in machine learning and, increasingly, in modern AI systems such as large language models, where shared tasks, metrics, and leaderboards offer a common basis for measuring progress and comparing approaches. As AI systems are deployed in more varied and consequential settings, though, there is growing value in complementing these established practices with a more holistic conceptualization of what evaluation should represent. Of note, recognizing the sociotechnical contexts in which these systems operate invites an opportunity for a deeper view of how multiple stakeholders and their unique priorities might inform what we consider meaningful or desirable model behavior. This paper introduces a theoretical framework that reconceptualizes benchmarking as a multilayer, adaptive network linking evaluation metrics, model components, and stakeholder groups through weighted interactions. Using conjoint-derived utilities and a human-in-the-loop update rule, we formalize how human tradeoffs can be embedded into benchmark structure and how benchmarks can evolve dynamically while preserving stability and interpretability. The resulting formulation generalizes classical leaderboards as a special case and provides a foundation for building evaluation protocols that are more context aware, resulting in new robust tools for analyzing the structural properties of benchmarks, which opens a path toward more accountable and human-aligned evaluation.

</details>


### [237] [Evolving Beyond Snapshots: Harmonizing Structure and Sequence via Entity State Tuning for Temporal Knowledge Graph Forecasting](https://arxiv.org/abs/2602.12389)
*Siyuan Li,Yunjia Wu,Yiyong Xiao,Pingyang Huang,Peize Li,Ruitong Liu,Yan Wen,Te Sun,Fangyi Pei*

Main category: cs.AI

TL;DR: EST is an encoder-agnostic framework that adds persistent, evolving entity states to TKG forecasters to overcome episodic amnesia and maintain long-term dependencies.


<details>
  <summary>Details</summary>
Motivation: Existing TKG forecasting methods are stateless, recomputing entity representations from limited query windows, causing episodic amnesia and rapid decay of long-term dependencies.

Method: EST maintains a global state buffer with closed-loop design: topology-aware state perceiver injects entity-state priors, unified temporal context module aggregates state-enhanced events, and dual-track evolution mechanism writes updated context back to memory.

Result: Experiments on multiple benchmarks show EST consistently improves diverse backbones and achieves state-of-the-art performance.

Conclusion: EST demonstrates the importance of state persistence for long-horizon TKG forecasting, effectively addressing limitations of stateless methods.

Abstract: Temporal knowledge graph (TKG) forecasting requires predicting future facts by jointly modeling structural dependencies within each snapshot and temporal evolution across snapshots. However, most existing methods are stateless: they recompute entity representations at each timestamp from a limited query window, leading to episodic amnesia and rapid decay of long-term dependencies. To address this limitation, we propose Entity State Tuning (EST), an encoder-agnostic framework that endows TKG forecasters with persistent and continuously evolving entity states. EST maintains a global state buffer and progressively aligns structural evidence with sequential signals via a closed-loop design. Specifically, a topology-aware state perceiver first injects entity-state priors into structural encoding. Then, a unified temporal context module aggregates the state-enhanced events with a pluggable sequence backbone. Subsequently, a dual-track evolution mechanism writes the updated context back to the global entity state memory, balancing plasticity against stability. Experiments on multiple benchmarks show that EST consistently improves diverse backbones and achieves state-of-the-art performance, highlighting the importance of state persistence for long-horizon TKG forecasting. The code is published at https://github.com/yuanwuyuan9/Evolving-Beyond-Snapshots

</details>


### [238] [Intent-Driven Smart Manufacturing Integrating Knowledge Graphs and Large Language Models](https://arxiv.org/abs/2602.12419)
*Takoua Jradi,John Violos,Dimitrios Spatharakis,Lydia Mavraidi,Ioannis Dimolitsas,Aris Leivadeas,Symeon Papavassiliou*

Main category: cs.AI

TL;DR: LLM + Knowledge Graph framework translates natural language intents into executable manufacturing actions with high accuracy.


<details>
  <summary>Details</summary>
Motivation: Smart manufacturing needs interfaces that can convert high-level human intents into machine-executable actions, especially in Manufacturing-as-a-Service ecosystems where complex interactions occur.

Method: Fine-tuned Mistral-7B-Instruct-V02 LLM on domain-specific data to translate natural language intents into structured JSON models, then semantically mapped these to a Neo4j-based knowledge graph aligned with ISA-95 manufacturing standards.

Result: Achieved 89.33% exact match accuracy and 97.27% overall accuracy, significantly outperforming zero-shot and 3-shot baselines.

Conclusion: The unified LLM+KG framework provides a foundation for scalable, explainable, and adaptive human-machine interaction in smart manufacturing environments.

Abstract: The increasing complexity of smart manufacturing environments demands interfaces that can translate high-level human intents into machine-executable actions. This paper presents a unified framework that integrates instruction-tuned Large Language Models (LLMs) with ontology-aligned Knowledge Graphs (KGs) to enable intent-driven interaction in Manufacturing-as-a-Service (MaaS) ecosystems. We fine-tune Mistral-7B-Instruct-V02 on a domain-specific dataset, enabling the translation of natural language intents into structured JSON requirement models. These models are semantically mapped to a Neo4j-based knowledge graph grounded in the ISA-95 standard, ensuring operational alignment with manufacturing processes, resources, and constraints. Our experimental results demonstrate significant performance gains over zero-shot and 3-shots baselines, achieving 89.33\% exact match accuracy and 97.27\% overall accuracy. This work lays the foundation for scalable, explainable, and adaptive human-machine

</details>


### [239] [Scaling Web Agent Training through Automatic Data Generation and Fine-grained Evaluation](https://arxiv.org/abs/2602.12544)
*Lajanugen Logeswaran,Jaekyeom Kim,Sungryull Sohn,Creighton Glasscock,Honglak Lee*

Main category: cs.AI

TL;DR: Scalable pipeline for generating high-quality web agent training data using constraint-based trajectory evaluation to leverage partially successful trajectories, achieving state-of-the-art performance on complex booking tasks.


<details>
  <summary>Details</summary>
Motivation: The paper addresses the challenge of efficiently creating diverse, realistic web interaction datasets for training web agents, particularly focusing on the difficulty of identifying high-quality training instances through trajectory evaluation.

Method: Introduces a novel constraint-based evaluation framework for fine-grained assessment of progress towards task completion, enabling the use of partially successful trajectories. Presents a scalable pipeline for automatically generating training data and proposes BookingArena benchmark with complex booking tasks across 20 popular websites.

Result: The distilled student model outperforms open-source approaches and matches or exceeds commercial systems on the BookingArena benchmark, while being significantly smaller in size. The method expands usable training data by leveraging partially successful trajectories.

Conclusion: The work provides an effective solution for creating web agent training data and a systematic evaluation methodology for complex structured web tasks, demonstrating that constraint-based evaluation enables better utilization of training trajectories for improved agent performance.

Abstract: We present a scalable pipeline for automatically generating high-quality training data for web agents. In particular, a major challenge in identifying high-quality training instances is trajectory evaluation - quantifying how much progress was made towards task completion. We introduce a novel constraint-based evaluation framework that provides fine-grained assessment of progress towards task completion. This enables us to leverage partially successful trajectories, which significantly expands the amount of usable training data. We evaluate our method on a new benchmark we propose called BookingArena, which consists of complex booking tasks across 20 popular websites, and demonstrate that our distilled student model outperforms open-source approaches and matches or exceeds commercial systems, while being a significantly smaller model. Our work addresses the challenge of efficiently creating diverse, realistic web interaction datasets and provides a systematic evaluation methodology for complex structured web tasks.

</details>


### [240] [To Mix or To Merge: Toward Multi-Domain Reinforcement Learning for Large Language Models](https://arxiv.org/abs/2602.12566)
*Haoqing Wang,Xiang Long,Ziheng Li,Yilong Xu,Tingguang Li,Yehui Tang*

Main category: cs.AI

TL;DR: The paper compares two RLVR training paradigms for multi-domain LLMs: mixed multi-task training vs separate training followed by model merging, finding minimal interference and synergistic effects between reasoning-intensive domains.


<details>
  <summary>Details</summary>
Motivation: To understand how Reinforcement Learning with Verifiable Rewards (RLVR) training paradigms affect multi-domain expert-level LLMs, as current state-of-the-art models use either mixed multi-task RLVR or separate RLVR with model merging without detailed comparison.

Method: Used multiple high-level tasks (math, coding, science, instruction following) with open-source datasets, conducted extensive qualitative and quantitative experiments comparing mixed multi-task RLVR training vs separate RLVR training followed by model merging.

Result: Found RLVR across domains exhibits few mutual interferences, with reasoning-intensive domains showing mutually synergistic effects. Analyzed internal mechanisms through weight space geometry, model prediction behavior, and information constraints.

Conclusion: The study provides insights into RLVR training paradigms for multi-domain LLMs, showing that domains can be trained together without significant interference and may even benefit each other, particularly in reasoning-intensive tasks.

Abstract: Reinforcement Learning with Verifiable Rewards (RLVR) plays a key role in stimulating the explicit reasoning capability of Large Language Models (LLMs). We can achieve expert-level performance in some specific domains via RLVR, such as coding or math. When a general multi-domain expert-level model is required, we need to carefully consider the collaboration of RLVR across different domains. The current state-of-the-art models mainly employ two different training paradigms for multi-domain RLVR: mixed multi-task RLVR and separate RLVR followed by model merging. However, most of the works did not provide a detailed comparison and analysis about these paradigms. To this end, we choose multiple commonly used high-level tasks (e.g., math, coding, science, and instruction following) as our target domains and design extensive qualitative and quantitative experiments using open-source datasets. We find the RLVR across domains exhibits few mutual interferences, and reasoning-intensive domains demonstrate mutually synergistic effects. Furthermore, we analyze the internal mechanisms of mutual gains from the perspectives of weight space geometry, model prediction behavior, and information constraints. This project is named as M2RL that means Mixed multi-task training or separate training followed by model Merging for Reinforcement Learning, and the homepage is at https://github.com/mosAI25/M2RL

</details>


### [241] [Can I Have Your Order? Monte-Carlo Tree Search for Slot Filling Ordering in Diffusion Language Models](https://arxiv.org/abs/2602.12586)
*Joshua Ong Jun Leang,Yu Zhao,Mihaela Cătălina Stoian,Wenda Li,Shay B. Cohen,Eleonora Giunchiglia*

Main category: cs.AI

TL;DR: McDiffuSE uses Monte Carlo Tree Search to optimize slot infilling order in Masked Diffusion Models, improving reasoning performance by systematically exploring generation orders through look-ahead simulations.


<details>
  <summary>Details</summary>
Motivation: Plan-and-infill decoding in Masked Diffusion Models shows promise for mathematical and code reasoning, but performance is highly sensitive to slot infilling order, causing substantial output variance.

Method: McDiffuSE formulates slot selection as decision making and optimizes infilling orders through Monte Carlo Tree Search (MCTS), using look-ahead simulations to evaluate partial completions before commitment.

Result: Average improvement of 3.2% over autoregressive baselines and 8.0% over baseline plan-and-infill, with notable gains of 19.5% on MBPP and 4.9% on MATH500. Larger exploration constants rather than increased simulations are needed to overcome model confidence biases.

Conclusion: MCTS-based planning is an effective approach for enhancing generation quality in MDMs, with non-sequential generation being essential for maximizing performance despite the model predominantly following sequential ordering.

Abstract: While plan-and-infill decoding in Masked Diffusion Models (MDMs) shows promise for mathematical and code reasoning, performance remains highly sensitive to slot infilling order, often yielding substantial output variance. We introduce McDiffuSE, a framework that formulates slot selection as decision making and optimises infilling orders through Monte Carlo Tree Search (MCTS). McDiffuSE uses look-ahead simulations to evaluate partial completions before commitment, systematically exploring the combinatorial space of generation orders. Experiments show an average improvement of 3.2% over autoregressive baselines and 8.0% over baseline plan-and-infill, with notable gains of 19.5% on MBPP and 4.9% on MATH500. Our analysis reveals that while McDiffuSE predominantly follows sequential ordering, incorporating non-sequential generation is essential for maximising performance. We observe that larger exploration constants, rather than increased simulations, are necessary to overcome model confidence biases and discover effective orderings. These findings establish MCTS-based planning as an effective approach for enhancing generation quality in MDMs.

</details>


### [242] [GeoAgent: Learning to Geolocate Everywhere with Reinforced Geographic Characteristics](https://arxiv.org/abs/2602.12617)
*Modi Jin,Yiming Zhang,Boyuan Sun,Dingwen Zhang,MingMing Cheng,Qibin Hou*

Main category: cs.AI

TL;DR: GeoAgent is a model for geolocation reasoning that outperforms existing methods by using expert-annotated data and specialized training rewards that account for geographic characteristics.


<details>
  <summary>Details</summary>
Motivation: Previous RL-based geolocation methods rely on AI-generated chain-of-thought data and training strategies that conflict with geographic characteristics, raising concerns about their reliability and alignment with geographic reasoning.

Method: 1) Introduces GeoSeek dataset with CoT data annotated by geographic experts and professional players. 2) Proposes geo-similarity reward and consistency reward assessed by a consistency agent to guide training. 3) Ensures model converges toward correct geographic answers while maintaining reasoning integrity.

Result: GeoAgent outperforms existing methods and general VLLMs across multiple grains (levels of granularity) and generates reasoning that closely aligns with human thinking.

Conclusion: The approach of using expert-annotated data and specialized geographic rewards enables more accurate and human-aligned geolocation reasoning, addressing limitations of previous RL-based methods.

Abstract: This paper presents GeoAgent, a model capable of reasoning closely with humans and deriving fine-grained address conclusions. Previous RL-based methods have achieved breakthroughs in performance and interpretability but still remain concerns because of their reliance on AI-generated chain-of-thought (CoT) data and training strategies, which conflict with geographic characteristics. To address these issues, we first introduce GeoSeek, a new geolocation dataset comprising CoT data annotated by geographic experts and professional players. We further thoroughly explore the inherent characteristics of geographic tasks and propose a geo-similarity reward and a consistency reward assessed by a consistency agent to assist training. This encourages the model to converge towards correct answers from a geographic perspective while ensuring the integrity and consistency of its reasoning process. Experimental results show that GeoAgent outperforms existing methods and a series of general VLLMs across multiple grains, while generating reasoning that closely aligns with humans.

</details>


### [243] [AI Agents for Inventory Control: Human-LLM-OR Complementarity](https://arxiv.org/abs/2602.12631)
*Jackie Baek,Yaopeng Fu,Will Ma,Tianyi Peng*

Main category: cs.AI

TL;DR: LLM-augmented operations research methods outperform either approach alone in inventory control, and human-AI collaboration yields higher profits than either working independently.


<details>
  <summary>Details</summary>
Motivation: Traditional OR algorithms for inventory control rely on rigid assumptions and perform poorly with demand shifts or missing contextual information, while LLMs offer flexible reasoning but lack established integration methods.

Method: Created InventoryBench with 1,000+ inventory instances across synthetic/real data to test methods under demand shifts, seasonality, and lead time uncertainty. Conducted classroom experiment embedding LLM recommendations in human-in-the-loop decision pipelines.

Result: OR-augmented LLM methods outperform either method alone. Human-AI teams achieve higher average profits than humans or AI agents operating independently. Substantial fraction of individuals benefit from AI collaboration.

Conclusion: OR algorithms and LLMs are complementary rather than substitutes, and human-AI collaboration can enhance inventory decision-making performance beyond what either can achieve alone.

Abstract: Inventory control is a fundamental operations problem in which ordering decisions are traditionally guided by theoretically grounded operations research (OR) algorithms. However, such algorithms often rely on rigid modeling assumptions and can perform poorly when demand distributions shift or relevant contextual information is unavailable. Recent advances in large language models (LLMs) have generated interest in AI agents that can reason flexibly and incorporate rich contextual signals, but it remains unclear how best to incorporate LLM-based methods into traditional decision-making pipelines.
  We study how OR algorithms, LLMs, and humans can interact and complement each other in a multi-period inventory control setting. We construct InventoryBench, a benchmark of over 1,000 inventory instances spanning both synthetic and real-world demand data, designed to stress-test decision rules under demand shifts, seasonality, and uncertain lead times. Through this benchmark, we find that OR-augmented LLM methods outperform either method in isolation, suggesting that these methods are complementary rather than substitutes.
  We further investigate the role of humans through a controlled classroom experiment that embeds LLM recommendations into a human-in-the-loop decision pipeline. Contrary to prior findings that human-AI collaboration can degrade performance, we show that, on average, human-AI teams achieve higher profits than either humans or AI agents operating alone. Beyond this population-level finding, we formalize an individual-level complementarity effect and derive a distribution-free lower bound on the fraction of individuals who benefit from AI collaboration; empirically, we find this fraction to be substantial.

</details>


### [244] [Think Fast and Slow: Step-Level Cognitive Depth Adaptation for LLM Agents](https://arxiv.org/abs/2602.12662)
*Ruihan Yang,Fanghua Ye,Xiang We,Ruoqing Zhao,Kang Luo,Xinbo Xu,Bo Zhao,Ruotian Ma,Shanyi Wang,Zhaopeng Tu,Xiaolong Li,Deqing Yang,Linus*

Main category: cs.AI

TL;DR: CogRouter is a framework that trains LLM agents to dynamically adapt cognitive depth at each step of multi-turn decision-making tasks, achieving state-of-the-art performance with superior efficiency.


<details>
  <summary>Details</summary>
Motivation: Current LLM agents use fixed cognitive patterns - either immediate responses or uniform deep reasoning - which is inefficient for long-horizon tasks where cognitive demands vary significantly across steps. Some steps require strategic planning while others only need routine execution.

Method: CogRouter trains agents to dynamically adapt cognitive depth using four hierarchical cognitive levels based on ACT-R theory. It uses a two-stage training approach: Cognition-aware Supervised Fine-tuning (CoSFT) to instill level-specific patterns, and Cognition-aware Policy Optimization (CoPO) for step-level credit assignment via confidence-aware advantage reweighting.

Result: CogRouter achieves state-of-the-art performance on ALFWorld and ScienceWorld. With Qwen2.5-7B, it reaches 82.3% success rate, outperforming GPT-4o (+40.3%), OpenAI-o3 (+18.3%), and GRPO (+14.0%), while using 62% fewer tokens.

Conclusion: Dynamic adaptation of cognitive depth based on task demands leads to more efficient and effective LLM agents, with the key insight that appropriate cognitive depth should maximize the confidence of the resulting action.

Abstract: Large language models (LLMs) are increasingly deployed as autonomous agents for multi-turn decision-making tasks. However, current agents typically rely on fixed cognitive patterns: non-thinking models generate immediate responses, while thinking models engage in deep reasoning uniformly. This rigidity is inefficient for long-horizon tasks, where cognitive demands vary significantly from step to step, with some requiring strategic planning and others only routine execution. In this paper, we introduce CogRouter, a framework that trains agents to dynamically adapt cognitive depth at each step. Grounded in ACT-R theory, we design four hierarchical cognitive levels ranging from instinctive responses to strategic planning. Our two-stage training approach includes Cognition-aware Supervised Fine-tuning (CoSFT) to instill stable level-specific patterns, and Cognition-aware Policy Optimization (CoPO) for step-level credit assignment via confidence-aware advantage reweighting. The key insight is that appropriate cognitive depth should maximize the confidence of the resulting action. Experiments on ALFWorld and ScienceWorld demonstrate that CogRouter achieves state-of-the-art performance with superior efficiency. With Qwen2.5-7B, it reaches an 82.3% success rate, outperforming GPT-4o (+40.3%), OpenAI-o3 (+18.3%), and GRPO (+14.0%), while using 62% fewer tokens.

</details>


### [245] [Evaluating Robustness of Reasoning Models on Parameterized Logical Problems](https://arxiv.org/abs/2602.12665)
*Naïm Es-sebbani,Esteban Marquer,Yakoub Salhi,Zied Bouraoui*

Main category: cs.AI

TL;DR: The paper introduces a diagnostic benchmark for 2-SAT problems that isolates specific structural phenomena rather than surface-level difficulties, revealing brittleness in LLM-based reasoners through targeted interventions.


<details>
  <summary>Details</summary>
Motivation: Standard SAT benchmarks conflate surface difficulties (length, wording, clause order) with the structural phenomena that actually determine satisfiability, making it hard to properly evaluate LLM-based reasoners' true reasoning capabilities.

Method: Created parameterized families of structured 2-CNF formulas where satisfiability is characterized by implication graphs. Five diagnostic generators isolate distinct competencies: contradiction-cycle UNSAT cores, SAT instances with controlled solution multiplicity, planted backbones, late bridge clauses, and symmetry/duplication variants.

Result: LLM-based reasoners show sharp performance transitions under targeted structural interventions even when surface statistics are fixed, revealing brittleness regimes invisible to aggregate SAT accuracy. Performance varies significantly across different structural challenges.

Conclusion: The diagnostic benchmark successfully uncovers specific failure modes in LLM-based reasoners that standard SAT benchmarks miss, demonstrating the importance of isolating structural phenomena for proper evaluation of reasoning capabilities.

Abstract: Logic provides a controlled testbed for evaluating LLM-based reasoners, yet standard SAT-style benchmarks often conflate surface difficulty (length, wording, clause order) with the structural phenomena that actually determine satisfiability. We introduce a diagnostic benchmark for 2-SAT built from parameterized families of structured 2--CNF formulas, where satisfiability is characterized by the implication graph and can be tuned along interpretable axes. Our generators isolate distinct competencies and failure modes: (i) contradiction-cycle UNSAT cores with controllable size and imbalance, (ii) SAT instances with a prescribed fraction of free variables to control solution multiplicity, (iii) planted backbones that modulate propagation, (iv) late bridge clauses that couple otherwise monotone regions to probe sensitivity to ordering and revision, and (v) symmetry/duplication variants that test abstraction under renaming and redundant structure. We evaluate LLM-based reasoners on decision accuracy and assignment validity, and quantify robustness under semantics-preserving perturbations such as clause reordering, filler clauses, and variable renaming. Across models, we observe sharp performance transitions under targeted structural interventions even when surface statistics are held fixed, revealing brittleness regimes that are invisible to aggregate SAT accuracy.

</details>


### [246] [SkillsBench: Benchmarking How Well Agent Skills Work Across Diverse Tasks](https://arxiv.org/abs/2602.12670)
*Xiangyi Li,Wenbo Chen,Yimin Liu,Shenghan Zheng,Xiaokun Chen,Yifeng He,Yubo Li,Bingran You,Haotian Shen,Jiankai Sun,Shuyi Wang,Qunhong Zeng,Di Wang,Xuandong Zhao,Yuanli Wang,Roey Ben Chaim,Zonglin Di,Yipeng Gao,Junwei He,Yizhuo He,Liqiang Jing,Luyang Kong,Xin Lan,Jiachen Li,Songlin Li,Yijiang Li,Yueqian Lin,Xinyi Liu,Xuanqing Liu,Haoran Lyu,Ze Ma,Bowei Wang,Runhui Wang,Tianyu Wang,Wengao Ye,Yue Zhang,Hanwen Xing,Yiqi Xue,Steven Dillmann,Han-chung Lee*

Main category: cs.AI

TL;DR: SkillsBench benchmark shows curated Skills improve LLM agent performance by 16.2pp on average, but effects vary by domain and self-generated Skills provide no benefit.


<details>
  <summary>Details</summary>
Motivation: There is rapid adoption of Agent Skills (structured procedural knowledge packages) for LLM agents, but no standard way to measure whether they actually help improve agent performance.

Method: Created SkillsBench benchmark with 86 tasks across 11 domains, each paired with curated Skills and deterministic verifiers. Evaluated 7 agent-model configurations over 7,308 trajectories under three conditions: no Skills, curated Skills, and self-generated Skills.

Result: Curated Skills raise average pass rate by 16.2 percentage points, but effects vary widely by domain (+4.5pp for Software Engineering to +51.9pp for Healthcare). 16 of 84 tasks show negative deltas. Self-generated Skills provide no benefit on average. Focused Skills with 2-3 modules outperform comprehensive documentation, and smaller models with Skills can match larger models without them.

Conclusion: Curated Skills significantly improve LLM agent performance, but models cannot reliably author the procedural knowledge they benefit from consuming. Skills design matters - focused Skills work better than comprehensive documentation, enabling smaller models to match larger ones.

Abstract: Agent Skills are structured packages of procedural knowledge that augment LLM agents at inference time. Despite rapid adoption, there is no standard way to measure whether they actually help. We present SkillsBench, a benchmark of 86 tasks across 11 domains paired with curated Skills and deterministic verifiers. Each task is evaluated under three conditions: no Skills, curated Skills, and self-generated Skills. We test 7 agent-model configurations over 7,308 trajectories. Curated Skills raise average pass rate by 16.2 percentage points(pp), but effects vary widely by domain (+4.5pp for Software Engineering to +51.9pp for Healthcare) and 16 of 84 tasks show negative deltas. Self-generated Skills provide no benefit on average, showing that models cannot reliably author the procedural knowledge they benefit from consuming. Focused Skills with 2--3 modules outperform comprehensive documentation, and smaller models with Skills can match larger models without them.

</details>


### [247] [X-SYS: A Reference Architecture for Interactive Explanation Systems](https://arxiv.org/abs/2602.12748)
*Tobias Labarta,Nhi Hoang,Maximilian Dreyer,Jim Berend,Oleg Hein,Jackie Ma,Wojciech Samek,Sebastian Lapuschkin*

Main category: cs.AI

TL;DR: X-SYS: A reference architecture for interactive explanation systems that treats XAI as an information systems problem, organizing around STAR quality attributes (scalability, traceability, responsiveness, adaptability) with five-component decomposition to connect user interfaces with system capabilities.


<details>
  <summary>Details</summary>
Motivation: While XAI research has produced many technical methods, deploying explainability as operational systems remains challenging. Interactive explanation systems require both suitable algorithms and system capabilities that maintain usability across repeated queries, evolving models/data, and governance constraints. The paper argues that operationalizing XAI requires treating explainability as an information systems problem where user interaction demands induce specific system requirements.

Method: Introduces X-SYS, a reference architecture for interactive explanation systems with four quality attributes (STAR: scalability, traceability, responsiveness, adaptability) and five-component decomposition (XUI Services, Explanation Services, Model Services, Data Services, Orchestration and Governance). Maps interaction patterns to system capabilities to decouple UI evolution from backend computation. Implements X-SYS through SemanticLens, a system for semantic search and activation steering in vision-language models.

Result: X-SYS provides a reusable blueprint for interactive explanation systems. SemanticLens demonstrates how contract-based service boundaries enable independent evolution, offline/online separation ensures responsiveness, and persistent state management supports traceability. The implementation shows how the architecture supports end-to-end design under operational constraints.

Conclusion: This work provides both a theoretical reference architecture (X-SYS) and concrete instantiation (SemanticLens) for building interactive explanation systems. By treating XAI as an information systems problem and organizing around STAR quality attributes, it offers a practical approach to operationalizing explainability that supports evolving user needs, models, and governance requirements.

Abstract: The explainable AI (XAI) research community has proposed numerous technical methods, yet deploying explainability as systems remains challenging: Interactive explanation systems require both suitable algorithms and system capabilities that maintain explanation usability across repeated queries, evolving models and data, and governance constraints. We argue that operationalizing XAI requires treating explainability as an information systems problem where user interaction demands induce specific system requirements. We introduce X-SYS, a reference architecture for interactive explanation systems, that guides (X)AI researchers, developers and practitioners in connecting interactive explanation user interfaces (XUI) with system capabilities. X-SYS organizes around four quality attributes named STAR (scalability, traceability, responsiveness, and adaptability), and specifies a five-component decomposition (XUI Services, Explanation Services, Model Services, Data Services, Orchestration and Governance). It maps interaction patterns to system capabilities to decouple user interface evolution from backend computation. We implement X-SYS through SemanticLens, a system for semantic search and activation steering in vision-language models. SemanticLens demonstrates how contract-based service boundaries enable independent evolution, offline/online separation ensures responsiveness, and persistent state management supports traceability. Together, this work provides a reusable blueprint and concrete instantiation for interactive explanation systems supporting end-to-end design under operational constraints.

</details>


### [248] [WebClipper: Efficient Evolution of Web Agents with Graph-based Trajectory Pruning](https://arxiv.org/abs/2602.12852)
*Junjie Wang,Zequn Xie,Dan Yang,Jie Feng,Yue Shen,Duolin Sun,Meixiu Long,Yihan Jiao,Zhehao Tan,Jian Wang,Peng Wei,Jinjie Gu*

Main category: cs.AI

TL;DR: WebClipper: A framework that compresses web agent trajectories via graph-based pruning to improve search efficiency while maintaining accuracy.


<details>
  <summary>Details</summary>
Motivation: Current deep research web agents suffer from inefficient search patterns with long tool-call trajectories, cyclic reasoning loops, and exploration of unproductive branches, limiting their practical deployment despite strong potential for complex information-seeking tasks.

Method: Models agent's search process as a state graph and casts trajectory optimization as a minimum-necessary DAG mining problem. Uses graph-based pruning to eliminate redundant steps while preserving essential reasoning, followed by continued training on refined trajectories.

Result: Reduces tool-call rounds by about 20% while improving accuracy. Introduces F-AE Score metric to balance accuracy and efficiency. Demonstrates compressed tool-call rounds under excellent performance.

Conclusion: WebClipper provides practical insight into balancing effectiveness and efficiency in web agent design, enabling agents to evolve toward more efficient search patterns through trajectory compression and continued training.

Abstract: Deep Research systems based on web agents have shown strong potential in solving complex information-seeking tasks, yet their search efficiency remains underexplored. We observe that many state-of-the-art open-source web agents rely on long tool-call trajectories with cyclic reasoning loops and exploration of unproductive branches. To address this, we propose WebClipper, a framework that compresses web agent trajectories via graph-based pruning. Concretely, we model the agent's search process as a state graph and cast trajectory optimization as a minimum-necessary Directed Acyclic Graph (DAG) mining problem, yielding pruned trajectories that preserve essential reasoning while eliminating redundant steps. Continued training on these refined trajectories enables the agent to evolve toward more efficient search patterns and reduces tool-call rounds by about 20% while improving accuracy. Furthermore, we introduce a new metric called F-AE Score to measure the model's overall performance in balancing accuracy and efficiency. Experiments demonstrate that WebClipper compresses tool-call rounds under excellent performance, providing practical insight into balancing effectiveness and efficiency in web agent design.

</details>


### [249] [BrowseComp-$V^3$: A Visual, Vertical, and Verifiable Benchmark for Multimodal Browsing Agents](https://arxiv.org/abs/2602.12876)
*Huanyao Zhang,Jiepeng Zhou,Bo Li,Bowen Zhou,Yanzhe Dan,Haishan Lu,Zhiyong Cao,Jiaoyang Chen,Yuqian Han,Zinan Sheng,Zhengwei Tao,Hao Liang,Jialong Wu,Yang Shi,Yuanpeng He,Jiaye Lin,Qintong Zhang,Guochen Yan,Runhao Zhao,Zhengpin Li,Xiaohan Yu,Lang Mei,Chong Chen,Wentao Zhang,Bin Cui*

Main category: cs.AI

TL;DR: BrowseComp-V³ is a challenging multimodal web browsing benchmark with 300 complex questions requiring cross-modal, multi-hop reasoning across publicly accessible web content, featuring both final-answer and process evaluation.


<details>
  <summary>Details</summary>
Motivation: Existing multimodal browsing benchmarks lack task complexity, evidence accessibility, and evaluation granularity, limiting comprehensive assessment of deep search capabilities in MLLMs.

Method: Created BrowseComp-V³ with 300 curated questions emphasizing deep, multi-level, cross-modal reasoning across web pages. Introduced expert-validated subgoal-driven process evaluation and proposed OmniSeeker framework integrating web search and visual perception tools.

Result: State-of-the-art models achieve only 36% accuracy, revealing critical bottlenecks in multimodal information integration and fine-grained perception. The benchmark exposes significant capability gaps.

Conclusion: There's a fundamental gap between current MLLM capabilities and robust multimodal deep search in real-world settings, highlighting the need for improved multimodal reasoning and perception.

Abstract: Multimodal large language models (MLLMs), equipped with increasingly advanced planning and tool-use capabilities, are evolving into autonomous agents capable of performing multimodal web browsing and deep search in open-world environments. However, existing benchmarks for multimodal browsing remain limited in task complexity, evidence accessibility, and evaluation granularity, hindering comprehensive and reproducible assessments of deep search capabilities. To address these limitations, we introduce BrowseComp-$V^3$, a novel benchmark consisting of 300 carefully curated and challenging questions spanning diverse domains. The benchmark emphasizes deep, multi-level, and cross-modal multi-hop reasoning, where critical evidence is interleaved across textual and visual modalities within and across web pages. All supporting evidence is strictly required to be publicly searchable, ensuring fairness and reproducibility. Beyond final-answer accuracy, we incorporate an expert-validated, subgoal-driven process evaluation mechanism that enables fine-grained analysis of intermediate reasoning behaviors and systematic characterization of capability boundaries. In addition, we propose OmniSeeker, a unified multimodal browsing agent framework integrating diverse web search and visual perception tools. Comprehensive experiments demonstrate that even state-of-the-art models achieve only 36% accuracy on our benchmark, revealing critical bottlenecks in multimodal information integration and fine-grained perception. Our results highlight a fundamental gap between current model capabilities and robust multimodal deep search in real-world settings.

</details>


### [250] [Information-theoretic analysis of world models in optimal reward maximizers](https://arxiv.org/abs/2602.12963)
*Alfred Harwood,Jose Faustino,Alex Altair*

Main category: cs.AI

TL;DR: Optimal policies in controlled Markov processes reveal exactly n log m bits of information about the environment's transition dynamics.


<details>
  <summary>Details</summary>
Motivation: To understand how much internal world representation is needed for optimal behavior in AI systems, specifically quantifying the information an optimal policy provides about the underlying environment.

Method: Analyze Controlled Markov Processes (CMPs) with n states and m actions, assuming uniform prior over transition dynamics. Prove that observing a deterministic optimal policy for any non-constant reward function conveys exactly n log m bits of information about the environment using information theory.

Result: The mutual information between the environment and an optimal policy is exactly n log m bits. This holds across finite-horizon, infinite-horizon discounted, and time-averaged reward maximization objectives.

Conclusion: Provides a precise information-theoretic lower bound on the "implicit world model" necessary for optimality, showing optimal policies reveal substantial information about environment dynamics.

Abstract: An important question in the field of AI is the extent to which successful behaviour requires an internal representation of the world. In this work, we quantify the amount of information an optimal policy provides about the underlying environment. We consider a Controlled Markov Process (CMP) with $n$ states and $m$ actions, assuming a uniform prior over the space of possible transition dynamics. We prove that observing a deterministic policy that is optimal for any non-constant reward function then conveys exactly $n \log m$ bits of information about the environment. Specifically, we show that the mutual information between the environment and the optimal policy is $n \log m$ bits. This bound holds across a broad class of objectives, including finite-horizon, infinite-horizon discounted, and time-averaged reward maximization. These findings provide a precise information-theoretic lower bound on the "implicit world model'' necessary for optimality.

</details>


### [251] [Consistency of Large Reasoning Models Under Multi-Turn Attacks](https://arxiv.org/abs/2602.13093)
*Yubo Li,Ramayya Krishnan,Rema Padman*

Main category: cs.AI

TL;DR: Reasoning models show meaningful but incomplete robustness to adversarial attacks, with distinct vulnerability patterns and failure modes that differ from standard LLMs, requiring new defense approaches.


<details>
  <summary>Details</summary>
Motivation: While large reasoning models achieve SOTA performance on complex tasks, their robustness under multi-turn adversarial pressure remains underexplored, creating a gap in understanding how reasoning capabilities translate to adversarial resilience.

Method: Evaluated nine frontier reasoning models under adversarial attacks, conducted trajectory analysis to identify failure modes, and tested Confidence-Aware Response Generation (CARG) defense effectiveness on reasoning models.

Result: Reasoning models significantly outperform instruction-tuned baselines but show distinct vulnerabilities; identified five failure modes (Self-Doubt, Social Conformity, Suggestion Hijacking, Emotional Susceptibility, Reasoning Fatigue); CARG defense fails due to overconfidence from extended reasoning traces.

Conclusion: Reasoning capabilities don't automatically confer adversarial robustness, confidence-based defenses need fundamental redesign for reasoning models, and random confidence embedding surprisingly outperforms targeted extraction approaches.

Abstract: Large reasoning models with reasoning capabilities achieve state-of-the-art performance on complex tasks, but their robustness under multi-turn adversarial pressure remains underexplored. We evaluate nine frontier reasoning models under adversarial attacks. Our findings reveal that reasoning confers meaningful but incomplete robustness: most reasoning models studied significantly outperform instruction-tuned baselines, yet all exhibit distinct vulnerability profiles, with misleading suggestions universally effective and social pressure showing model-specific efficacy. Through trajectory analysis, we identify five failure modes (Self-Doubt, Social Conformity, Suggestion Hijacking, Emotional Susceptibility, and Reasoning Fatigue) with the first two accounting for 50% of failures. We further demonstrate that Confidence-Aware Response Generation (CARG), effective for standard LLMs, fails for reasoning models due to overconfidence induced by extended reasoning traces; counterintuitively, random confidence embedding outperforms targeted extraction. Our results highlight that reasoning capabilities do not automatically confer adversarial robustness and that confidence-based defenses require fundamental redesign for reasoning models.

</details>


### [252] [Constrained Assumption-Based Argumentation Frameworks](https://arxiv.org/abs/2602.13135)
*Emanuele De Angelis,Fabio Fioravanti,Maria Chiara Meo,Alberto Pettorossi,Maurizio Proietti,Francesca Toni*

Main category: cs.AI

TL;DR: The paper introduces Constrained ABA (CABA), which extends Assumption-based Argumentation to support constrained variables over infinite domains, lifting the restriction to propositional atoms.


<details>
  <summary>Details</summary>
Motivation: Standard ABA frameworks are limited to ground (variable-free) arguments and attacks built from propositional atoms, restricting their applicability. The authors aim to overcome this representational limitation.

Method: Proposes Constrained ABA (CABA) where components and arguments may include constrained variables ranging over possibly infinite domains. Defines non-ground semantics for CABA using various notions of non-ground attacks.

Result: The new CABA semantics conservatively generalize standard ABA semantics, meaning they preserve existing properties while extending the framework's expressive power.

Conclusion: CABA successfully lifts the groundness restriction of traditional ABA, enabling more expressive argumentation with constrained variables while maintaining compatibility with existing semantics.

Abstract: Assumption-based Argumentation (ABA) is a well-established form of structured argumentation. ABA frameworks with an underlying atomic language are widely studied, but their applicability is limited by a representational restriction to ground (variable-free) arguments and attacks built from propositional atoms. In this paper, we lift this restriction and propose a novel notion of constrained ABA (CABA), whose components, as well as arguments built from them, may include constrained variables, ranging over possibly infinite domains. We define non-ground semantics for CABA, in terms of various notions of non-ground attacks. We show that the new semantics conservatively generalise standard ABA semantics.

</details>


### [253] [Optimal Take-off under Fuzzy Clearances](https://arxiv.org/abs/2602.13166)
*Hugo Henry,Arthur Tsai,Kelly Cohen*

Main category: cs.AI

TL;DR: Hybrid obstacle avoidance architecture combining fuzzy logic with optimal control for unmanned aircraft, achieving 2-3 second computation times but encountering solver software incompatibility issues.


<details>
  <summary>Details</summary>
Motivation: Address limitations of classical optimal control under uncertainty and need for interpretable decision making in safety-critical aviation systems, while maintaining compliance with FAA/EASA regulations.

Method: Three-stage Takagi-Sugeno-Kang fuzzy layer modulates constraint parameters based on aviation regulations, integrated as soft constraints into optimal control problem solved using FALCON toolbox and IPOPT solver.

Result: Proof-of-concept shows 2-3 second computation times per iteration, suggesting near real-time feasibility, but reveals critical software incompatibility where Lagrangian penalty term remains zero, preventing proper constraint enforcement.

Conclusion: Approach demonstrates potential for adaptive obstacle avoidance but requires addressing solver regression issues and further validation with earlier software versions, evolutionary optimization of fuzzy functions, and extension to more complex models.

Abstract: This paper presents a hybrid obstacle avoidance architecture that integrates Optimal Control under clearance with a Fuzzy Rule Based System (FRBS) to enable adaptive constraint handling for unmanned aircraft. Motivated by the limitations of classical optimal control under uncertainty and the need for interpretable decision making in safety critical aviation systems, we design a three stage Takagi Sugeno Kang fuzzy layer that modulates constraint radii, urgency levels, and activation decisions based on regulatory separation minima and airworthiness guidelines from FAA and EASA. These fuzzy-derived clearances are then incorporated as soft constraints into an optimal control problem solved using the FALCON toolbox and IPOPT. The framework aims to reduce unnecessary recomputations by selectively activating obstacle avoidance updates while maintaining compliance with aviation procedures. A proof of concept implementation using a simplified aircraft model demonstrates that the approach can generate optimal trajectories with computation times of 2,3 seconds per iteration in a single threaded MATLAB environment, suggesting feasibility for near real time applications. However, our experiments revealed a critical software incompatibility in the latest versions of FALCON and IPOPT, in which the Lagrangian penalty term remained identically zero, preventing proper constraint enforcement. This behavior was consistent across scenarios and indicates a solver toolbox regression rather than a modeling flaw. Future work includes validating this effect by reverting to earlier software versions, optimizing the fuzzy membership functions using evolutionary methods, and extending the system to higher fidelity aircraft models and stochastic obstacle environments.

</details>
