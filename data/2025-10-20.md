<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 88]
- [eess.SP](#eess.SP) [Total: 12]
- [cs.AI](#cs.AI) [Total: 30]
- [cs.LG](#cs.LG) [Total: 87]
- [cs.IT](#cs.IT) [Total: 8]
- [eess.IV](#eess.IV) [Total: 4]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [GAZE:Governance-Aware pre-annotation for Zero-shot World Model Environments](https://arxiv.org/abs/2510.14992)
*Leela Krishna,Mengyang Zhao,Saicharithreddy Pasula,Harshit Rajgarhia,Abhishek Mukherji*

Main category: cs.CV

TL;DR: GAZE pipeline automates conversion of raw video into rich supervision for world-model training using AI pre-annotation and structured outputs, achieving efficiency gains and reduced human review.


<details>
  <summary>Details</summary>
Motivation: Manual annotation for world model training is slow and expensive, creating bottlenecks in creating large-scale multimodal datasets.

Method: Pipeline normalizes 360-degree video formats, applies AI models for dense multimodal pre-annotation (scene understanding, object tracking, audio transcription, etc.), and consolidates signals into structured outputs for human validation.

Result: Achieved ~19 minutes saved per review hour, reduced human review volume by >80% through auto-skipping low-salience segments, and generated high-fidelity privacy-aware datasets.

Conclusion: GAZE provides scalable blueprint for generating high-quality world model training data with improved efficiency, consistency, and governance.

Abstract: Training robust world models requires large-scale, precisely labeled
multimodal datasets, a process historically bottlenecked by slow and expensive
manual annotation. We present a production-tested GAZE pipeline that automates
the conversion of raw, long-form video into rich, task-ready supervision for
world-model training. Our system (i) normalizes proprietary 360-degree formats
into standard views and shards them for parallel processing; (ii) applies a
suite of AI models (scene understanding, object tracking, audio transcription,
PII/NSFW/minor detection) for dense, multimodal pre-annotation; and (iii)
consolidates signals into a structured output specification for rapid human
validation.
  The GAZE workflow demonstrably yields efficiency gains (~19 minutes saved per
review hour) and reduces human review volume by >80% through conservative
auto-skipping of low-salience segments. By increasing label density and
consistency while integrating privacy safeguards and chain-of-custody metadata,
our method generates high-fidelity, privacy-aware datasets directly consumable
for learning cross-modal dynamics and action-conditioned prediction. We detail
our orchestration, model choices, and data dictionary to provide a scalable
blueprint for generating high-quality world model training data without
sacrificing throughput or governance.

</details>


### [2] [PC-UNet: An Enforcing Poisson Statistics U-Net for Positron Emission Tomography Denoising](https://arxiv.org/abs/2510.14995)
*Yang Shi,Jingchao Wang,Liangsi Lu,Mingxuan Huang,Ruixin He,Yifeng Xie,Hanqian Liu,Minzhe Guo,Yangyang Liang,Weipeng Zhang,Zimeng Li,Xuhang Chen*

Main category: cs.CV

TL;DR: PC-UNet with PVMC-Loss improves PET image denoising by incorporating physical data constraints, addressing Poisson noise from low-dose imaging while maintaining image fidelity.


<details>
  <summary>Details</summary>
Motivation: PET imaging faces limitations due to high radiation doses from signal-to-noise requirements. Lower doses introduce Poisson noise that current denoising methods fail to handle properly, causing distortions and artifacts.

Method: Proposed Poisson Consistent U-Net (PC-UNet) with a novel Poisson Variance and Mean Consistency Loss (PVMC-Loss) that incorporates physical data constraints. PVMC-Loss is statistically unbiased in variance and gradient adaptation, acting as a Generalized Method of Moments implementation.

Result: Tests on PET datasets demonstrate that PC-UNet improves physical consistency and image fidelity compared to existing methods, effectively handling Poisson noise from low-dose imaging.

Conclusion: PC-UNet successfully integrates physical information into denoising, providing a robust solution for low-dose PET imaging that maintains image quality while reducing radiation exposure risks.

Abstract: Positron Emission Tomography (PET) is crucial in medicine, but its clinical
use is limited due to high signal-to-noise ratio doses increasing radiation
exposure. Lowering doses increases Poisson noise, which current denoising
methods fail to handle, causing distortions and artifacts. We propose a Poisson
Consistent U-Net (PC-UNet) model with a new Poisson Variance and Mean
Consistency Loss (PVMC-Loss) that incorporates physical data to improve image
fidelity. PVMC-Loss is statistically unbiased in variance and gradient
adaptation, acting as a Generalized Method of Moments implementation, offering
robustness to minor data mismatches. Tests on PET datasets show PC-UNet
improves physical consistency and image fidelity, proving its ability to
integrate physical information effectively.

</details>


### [3] [DeLeaker: Dynamic Inference-Time Reweighting For Semantic Leakage Mitigation in Text-to-Image Models](https://arxiv.org/abs/2510.15015)
*Mor Ventura,Michael Toker,Or Patashnik,Yonatan Belinkov,Roi Reichart*

Main category: cs.CV

TL;DR: DeLeaker is a lightweight, optimization-free method that mitigates semantic leakage in Text-to-Image models by dynamically reweighting attention maps during inference to suppress cross-entity interactions while preserving entity identities.


<details>
  <summary>Details</summary>
Motivation: Text-to-Image models suffer from semantic leakage where semantically related features unintentionally transfer between distinct entities, and existing mitigation approaches are often optimization-based or require external inputs.

Method: DeLeaker intervenes directly on the model's attention maps during diffusion process, dynamically reweighting them to suppress excessive cross-entity interactions while strengthening each entity's identity. It also introduces SLIM dataset with 1,130 human-verified samples for systematic evaluation.

Result: DeLeaker consistently outperforms all baselines, even those with external information, achieving effective leakage mitigation without compromising image fidelity or quality.

Conclusion: Attention control is valuable for mitigating semantic leakage, and DeLeaker paves the way for more semantically precise Text-to-Image models through its optimization-free, inference-time approach.

Abstract: Text-to-Image (T2I) models have advanced rapidly, yet they remain vulnerable
to semantic leakage, the unintended transfer of semantically related features
between distinct entities. Existing mitigation strategies are often
optimization-based or dependent on external inputs. We introduce DeLeaker, a
lightweight, optimization-free inference-time approach that mitigates leakage
by directly intervening on the model's attention maps. Throughout the diffusion
process, DeLeaker dynamically reweights attention maps to suppress excessive
cross-entity interactions while strengthening the identity of each entity. To
support systematic evaluation, we introduce SLIM (Semantic Leakage in IMages),
the first dataset dedicated to semantic leakage, comprising 1,130
human-verified samples spanning diverse scenarios, together with a novel
automatic evaluation framework. Experiments demonstrate that DeLeaker
consistently outperforms all baselines, even when they are provided with
external information, achieving effective leakage mitigation without
compromising fidelity or quality. These results underscore the value of
attention control and pave the way for more semantically precise T2I models.

</details>


### [4] [UrbanVerse: Scaling Urban Simulation by Watching City-Tour Videos](https://arxiv.org/abs/2510.15018)
*Mingxuan Liu,Honglin He,Elisa Ricci,Wayne Wu,Bolei Zhou*

Main category: cs.CV

TL;DR: UrbanVerse is a data-driven system that converts city-tour videos into physics-aware simulation scenes for training urban embodied AI agents, achieving improved performance in navigation tasks.


<details>
  <summary>Details</summary>
Motivation: Existing simulation environments for urban AI agents lack scalability and fail to capture real-world complexity, limiting effective training of delivery robots and other urban autonomous systems.

Method: UrbanVerse consists of UrbanVerse-100K (100k+ annotated urban 3D assets) and UrbanVerse-Gen (automatic pipeline that extracts scene layouts from videos and instantiates 3D simulations using retrieved assets).

Result: The system preserves real-world semantics and layouts with human-evaluated realism comparable to manually crafted scenes. Navigation policies trained in UrbanVerse show +6.3% improvement in simulation and +30.1% in zero-shot sim-to-real transfer, completing 300m real-world missions with only two interventions.

Conclusion: UrbanVerse enables scalable, realistic urban simulation for training embodied AI agents, demonstrating strong generalization capabilities and significant performance improvements over prior methods.

Abstract: Urban embodied AI agents, ranging from delivery robots to quadrupeds, are
increasingly populating our cities, navigating chaotic streets to provide
last-mile connectivity. Training such agents requires diverse, high-fidelity
urban environments to scale, yet existing human-crafted or procedurally
generated simulation scenes either lack scalability or fail to capture
real-world complexity. We introduce UrbanVerse, a data-driven real-to-sim
system that converts crowd-sourced city-tour videos into physics-aware,
interactive simulation scenes. UrbanVerse consists of: (i) UrbanVerse-100K, a
repository of 100k+ annotated urban 3D assets with semantic and physical
attributes, and (ii) UrbanVerse-Gen, an automatic pipeline that extracts scene
layouts from video and instantiates metric-scale 3D simulations using retrieved
assets. Running in IsaacSim, UrbanVerse offers 160 high-quality constructed
scenes from 24 countries, along with a curated benchmark of 10 artist-designed
test scenes. Experiments show that UrbanVerse scenes preserve real-world
semantics and layouts, achieving human-evaluated realism comparable to manually
crafted scenes. In urban navigation, policies trained in UrbanVerse exhibit
scaling power laws and strong generalization, improving success by +6.3% in
simulation and +30.1% in zero-shot sim-to-real transfer comparing to prior
methods, accomplishing a 300 m real-world mission with only two interventions.

</details>


### [5] [NANO3D: A Training-Free Approach for Efficient 3D Editing Without Masks](https://arxiv.org/abs/2510.15019)
*Junliang Ye,Shenghao Xie,Ruowen Zhao,Zhengyi Wang,Hongyu Yan,Wenqiang Zu,Lei Ma,Jun Zhu*

Main category: cs.CV

TL;DR: Nano3D is a training-free framework for precise 3D object editing that integrates FlowEdit with TRELLIS and introduces region-aware merging strategies to maintain structural fidelity without requiring masks.


<details>
  <summary>Details</summary>
Motivation: Current 3D editing methods are inefficient, inconsistent, and often damage unedited regions, relying on multi-view rendering reconstruction that introduces artifacts and limits practicality.

Method: Integrates FlowEdit into TRELLIS for localized edits guided by front-view renderings, and introduces Voxel/Slat-Merge region-aware merging strategies that adaptively preserve structural consistency between edited and unedited areas.

Result: Achieves superior 3D consistency and visual quality compared to existing methods, and creates the first large-scale 3D editing dataset Nano3D-Edit-100k with over 100,000 high-quality editing pairs.

Conclusion: Addresses long-standing challenges in 3D editing algorithm design and data availability, significantly improving generality and reliability while laying groundwork for feed-forward 3D editing models.

Abstract: 3D object editing is essential for interactive content creation in gaming,
animation, and robotics, yet current approaches remain inefficient,
inconsistent, and often fail to preserve unedited regions. Most methods rely on
editing multi-view renderings followed by reconstruction, which introduces
artifacts and limits practicality. To address these challenges, we propose
Nano3D, a training-free framework for precise and coherent 3D object editing
without masks. Nano3D integrates FlowEdit into TRELLIS to perform localized
edits guided by front-view renderings, and further introduces region-aware
merging strategies, Voxel/Slat-Merge, which adaptively preserve structural
fidelity by ensuring consistency between edited and unedited areas. Experiments
demonstrate that Nano3D achieves superior 3D consistency and visual quality
compared with existing methods. Based on this framework, we construct the first
large-scale 3D editing datasets Nano3D-Edit-100k, which contains over 100,000
high-quality 3D editing pairs. This work addresses long-standing challenges in
both algorithm design and data availability, significantly improving the
generality and reliability of 3D editing, and laying the groundwork for the
development of feed-forward 3D editing models. Project
Page:https://jamesyjl.github.io/Nano3D

</details>


### [6] [ClapperText: A Benchmark for Text Recognition in Low-Resource Archival Documents](https://arxiv.org/abs/2510.15557)
*Tingyu Lin,Marco Peer,Florian Kleber,Robert Sablatnig*

Main category: cs.CV

TL;DR: ClapperText is a benchmark dataset for handwritten and printed text recognition in degraded archival settings, derived from WWII-era clapperboard videos with 9,813 annotated frames and 94,573 word instances.


<details>
  <summary>Details</summary>
Motivation: To address challenges in historical document analysis where structured content appears in degraded, non-standard forms with motion blur, handwriting variation, and cluttered backgrounds.

Method: Dataset creation from 127 archival video segments with annotations including transcription, semantic category, text type, and occlusion status. Evaluation of six recognition and seven detection models under zero-shot and fine-tuned conditions.

Result: Fine-tuning on the small training set (18 videos) leads to substantial performance gains, demonstrating suitability for few-shot learning scenarios.

Conclusion: ClapperText provides a realistic and culturally grounded resource for advancing robust OCR and document understanding in low-resource archival contexts.

Abstract: This paper presents ClapperText, a benchmark dataset for handwritten and
printed text recognition in visually degraded and low-resource settings. The
dataset is derived from 127 World War II-era archival video segments containing
clapperboards that record structured production metadata such as date,
location, and camera-operator identity. ClapperText includes 9,813 annotated
frames and 94,573 word-level text instances, 67% of which are handwritten and
1,566 are partially occluded. Each instance includes transcription, semantic
category, text type, and occlusion status, with annotations available as
rotated bounding boxes represented as 4-point polygons to support spatially
precise OCR applications. Recognizing clapperboard text poses significant
challenges, including motion blur, handwriting variation, exposure
fluctuations, and cluttered backgrounds, mirroring broader challenges in
historical document analysis where structured content appears in degraded,
non-standard forms. We provide both full-frame annotations and cropped word
images to support downstream tasks. Using a consistent per-video evaluation
protocol, we benchmark six representative recognition and seven detection
models under zero-shot and fine-tuned conditions. Despite the small training
set (18 videos), fine-tuning leads to substantial performance gains,
highlighting ClapperText's suitability for few-shot learning scenarios. The
dataset offers a realistic and culturally grounded resource for advancing
robust OCR and document understanding in low-resource archival contexts. The
dataset and evaluation code are available at
https://github.com/linty5/ClapperText.

</details>


### [7] [Constantly Improving Image Models Need Constantly Improving Benchmarks](https://arxiv.org/abs/2510.15021)
*Jiaxin Ge,Grace Luo,Heekyung Lee,Nishant Malpani,Long Lian,XuDong Wang,Aleksander Holynski,Trevor Darrell,Sewon Min,David M. Chan*

Main category: cs.CV

TL;DR: ECHO is a framework for creating image generation benchmarks from real-world social media posts, addressing the gap between new model capabilities and existing evaluation methods.


<details>
  <summary>Details</summary>
Motivation: Existing benchmarks lag behind rapidly evolving image generation capabilities, failing to capture emerging real-world use cases and creating a disconnect between community perceptions and formal evaluation.

Method: Construct benchmarks directly from social media evidence of model use - collecting prompts and user judgments from posts. Applied to GPT-4o Image Gen to create a dataset of 31,000+ curated prompts.

Result: ECHO discovers novel tasks absent from existing benchmarks, better distinguishes state-of-the-art models, and surfaces community feedback for designing quality metrics (color shifts, identity, structure).

Conclusion: ECHO successfully bridges the gap between real-world model usage and formal evaluation by leveraging social media evidence to create more relevant and timely benchmarks.

Abstract: Recent advances in image generation, often driven by proprietary systems like
GPT-4o Image Gen, regularly introduce new capabilities that reshape how users
interact with these models. Existing benchmarks often lag behind and fail to
capture these emerging use cases, leaving a gap between community perceptions
of progress and formal evaluation. To address this, we present ECHO, a
framework for constructing benchmarks directly from real-world evidence of
model use: social media posts that showcase novel prompts and qualitative user
judgments. Applying this framework to GPT-4o Image Gen, we construct a dataset
of over 31,000 prompts curated from such posts. Our analysis shows that ECHO
(1) discovers creative and complex tasks absent from existing benchmarks, such
as re-rendering product labels across languages or generating receipts with
specified totals, (2) more clearly distinguishes state-of-the-art models from
alternatives, and (3) surfaces community feedback that we use to inform the
design of metrics for model quality (e.g., measuring observed shifts in color,
identity, and structure). Our website is at https://echo-bench.github.io.

</details>


### [8] [DGME-T: Directional Grid Motion Encoding for Transformer-Based Historical Camera Movement Classification](https://arxiv.org/abs/2510.15725)
*Tingyu Lin,Armin Dadras,Florian Kleber,Robert Sablatnig*

Main category: cs.CV

TL;DR: DGME-T enhances Video Swin Transformer with directional grid motion encoding to improve camera movement classification on both modern and archival film footage, achieving significant accuracy gains.


<details>
  <summary>Details</summary>
Motivation: Camera movement classification models trained on modern footage perform poorly on archival films due to noise, missing frames, and low contrast that obscure motion cues.

Method: Introduces DGME-T, a lightweight extension to Video Swin Transformer that injects directional grid motion encoding derived from optical flow via a learnable normalized late-fusion layer.

Result: DGME-T raises top-1 accuracy from 81.78% to 86.14% and macro F1 from 82.08% to 87.81% on modern clips, while improving WWII footage from 83.43% to 84.62% accuracy and 81.72% to 82.63% macro F1.

Conclusion: Structured motion priors and transformer representations are complementary, and even a small calibrated motion head can substantially enhance robustness in degraded film analysis.

Abstract: Camera movement classification (CMC) models trained on contemporary,
high-quality footage often degrade when applied to archival film, where noise,
missing frames, and low contrast obscure motion cues. We bridge this gap by
assembling a unified benchmark that consolidates two modern corpora into four
canonical classes and restructures the HISTORIAN collection into five balanced
categories. Building on this benchmark, we introduce DGME-T, a lightweight
extension to the Video Swin Transformer that injects directional grid motion
encoding, derived from optical flow, via a learnable and normalised late-fusion
layer. DGME-T raises the backbone's top-1 accuracy from 81.78% to 86.14% and
its macro F1 from 82.08% to 87.81% on modern clips, while still improving the
demanding World-War-II footage from 83.43% to 84.62% accuracy and from 81.72%
to 82.63% macro F1. A cross-domain study further shows that an intermediate
fine-tuning stage on modern data increases historical performance by more than
five percentage points. These results demonstrate that structured motion priors
and transformer representations are complementary and that even a small,
carefully calibrated motion head can substantially enhance robustness in
degraded film analysis. Related resources are available at
https://github.com/linty5/DGME-T.

</details>


### [9] [LoRAverse: A Submodular Framework to Retrieve Diverse Adapters for Diffusion Models](https://arxiv.org/abs/2510.15022)
*Mert Sonmezer,Matthew Zheng,Pinar Yanardag*

Main category: cs.CV

TL;DR: This paper proposes a submodular framework to select relevant and diverse LoRA models from large databases, addressing challenges in navigating and utilizing the vast number of available adapters.


<details>
  <summary>Details</summary>
Motivation: With over 100K LoRA adapters available on platforms like Civit.ai, users face difficulties in selecting suitable models due to volume, diversity, and lack of organization. The paper aims to solve the problem of efficient LoRA model selection.

Method: The task is framed as a combinatorial optimization problem and solved using a novel submodular framework for selecting the most relevant and diverse LoRA models.

Result: Quantitative and qualitative experiments show that the method generates diverse outputs across various domains, demonstrating effective selection of LoRA adapters.

Conclusion: The proposed submodular framework successfully addresses the challenge of selecting relevant and diverse LoRA models from large databases, enabling more effective utilization of available adapters.

Abstract: Low-rank Adaptation (LoRA) models have revolutionized the personalization of
pre-trained diffusion models by enabling fine-tuning through low-rank,
factorized weight matrices specifically optimized for attention layers. These
models facilitate the generation of highly customized content across a variety
of objects, individuals, and artistic styles without the need for extensive
retraining. Despite the availability of over 100K LoRA adapters on platforms
like Civit.ai, users often face challenges in navigating, selecting, and
effectively utilizing the most suitable adapters due to their sheer volume,
diversity, and lack of structured organization. This paper addresses the
problem of selecting the most relevant and diverse LoRA models from this vast
database by framing the task as a combinatorial optimization problem and
proposing a novel submodular framework. Our quantitative and qualitative
experiments demonstrate that our method generates diverse outputs across a wide
range of domains.

</details>


### [10] [MOBIUS: Big-to-Mobile Universal Instance Segmentation via Multi-modal Bottleneck Fusion and Calibrated Decoder Pruning](https://arxiv.org/abs/2510.15026)
*Mattia Segu,Marta Tintore Gazulla,Yongqin Xian,Luc Van Gool,Federico Tombari*

Main category: cs.CV

TL;DR: MOBIUS is a family of foundation models for universal instance segmentation designed for efficient edge deployment, reducing FLOPs by up to 55-75% while maintaining state-of-the-art performance with only one-third of training iterations.


<details>
  <summary>Details</summary>
Motivation: Existing foundation models for instance-level perception have high computational costs that limit adoption on resource-constrained platforms, creating a need for efficient models that don't compromise performance.

Method: Proposes three key techniques: (i) bottleneck pixel decoder for efficient multi-scale and multi-modal fusion, (ii) language-guided uncertainty calibration loss for adaptive decoder pruning, and (iii) streamlined unified training strategy.

Result: MOBIUS reduces pixel decoder FLOPs by up to 55% and transformer decoder FLOPs by up to 75% while maintaining state-of-the-art performance, achieving this in just one-third of the training iterations compared to conventional methods.

Conclusion: MOBIUS establishes a new benchmark for efficient segmentation across both high-performance computing platforms and mobile devices, enabling Pareto-optimal downscaling for deployment across diverse hardware.

Abstract: Scaling up model size and training data has advanced foundation models for
instance-level perception, achieving state-of-the-art in-domain and zero-shot
performance across object detection and segmentation. However, their high
computational cost limits adoption on resource-constrained platforms. We first
examine the limitations of existing architectures in enabling efficient edge
deployment without compromising performance. We then introduce MOBIUS, a family
of foundation models for universal instance segmentation, designed for
Pareto-optimal downscaling to support deployment across devices ranging from
high-end accelerators to mobile hardware. To reduce training and inference
demands, we propose: (i) a bottleneck pixel decoder for efficient multi-scale
and multi-modal fusion, (ii) a language-guided uncertainty calibration loss for
adaptive decoder pruning, and (iii) a streamlined, unified training strategy.
Unlike efficient baselines that trade accuracy for reduced complexity, MOBIUS
reduces pixel and transformer decoder FLOPs by up to 55% and 75%, respectively,
while maintaining state-of-the-art performance in just a third of the training
iterations. MOBIUS establishes a new benchmark for efficient segmentation on
both high-performance computing platforms and mobile devices.

</details>


### [11] [Composition-Grounded Instruction Synthesis for Visual Reasoning](https://arxiv.org/abs/2510.15040)
*Xinyi Gu,Jiayuan Mao,Zhang-Wei Hong,Zhuoran Yu,Pengyuan Li,Dhiraj Joshi,Rogerio Feris,Zexue He*

Main category: cs.CV

TL;DR: COGS is a data-efficient framework that equips MLLMs with advanced reasoning abilities for artificial image domains by decomposing seed questions into primitive factors and systematically recomposing them to generate synthetic training data with process supervision.


<details>
  <summary>Details</summary>
Motivation: Multi-modal LLMs lack reasoning capabilities for domains like charts, documents, and webpages where large-scale human annotations are difficult to collect, despite the abundance of such data in practice.

Method: Decompose seed questions into primitive perception and reasoning factors, then systematically recompose them with new images to generate synthetic question-answer pairs with subquestions and intermediate answers for factor-level process rewards in reinforcement learning.

Result: COGS substantially improves performance on unseen chart reasoning questions, especially on reasoning-heavy and compositional questions, and enables better transfer across multiple datasets through factor-level mixture training.

Conclusion: The framework induces generalizable reasoning capabilities rather than dataset-specific overfitting and extends beyond charts to other domains like webpages.

Abstract: Pretrained multi-modal large language models (MLLMs) demonstrate strong
performance on diverse multimodal tasks, but remain limited in reasoning
capabilities for domains where annotations are difficult to collect. In this
work, we focus on artificial image domains such as charts, rendered documents,
and webpages, which are abundant in practice yet lack large-scale human
annotated reasoning datasets. We introduce COGS (COmposition-Grounded
instruction Synthesis), a data-efficient framework for equipping MLLMs with
advanced reasoning abilities from a small set of seed questions. The key idea
is to decompose each seed question into primitive perception and reasoning
factors, which can then be systematically recomposed with new images to
generate large collections of synthetic question-answer pairs. Each generated
question is paired with subquestions and intermediate answers, enabling
reinforcement learning with factor-level process rewards. Experiments on chart
reasoning show that COGS substantially improves performance on unseen
questions, with the largest gains on reasoning-heavy and compositional
questions. Moreover, training with a factor-level mixture of different seed
data yields better transfer across multiple datasets, suggesting that COGS
induces generalizable capabilities rather than dataset-specific overfitting. We
further demonstrate that the framework extends beyond charts to other domains
such as webpages.

</details>


### [12] [Generalized Dynamics Generation towards Scannable Physical World Model](https://arxiv.org/abs/2510.15041)
*Yichen Li,Zhiyi Li,Brandon Feng,Dinghuai Zhang,Antonio Torralba*

Main category: cs.CV

TL;DR: GDGen is a framework that unifies rigid body, articulated body, and soft body dynamics using a potential energy perspective, treating the world as one holistic entity and inferring physical properties from motion observations.


<details>
  <summary>Details</summary>
Motivation: To develop generalist embodied agents in scannable environments with complex physical behaviors by creating a unified system for diverse physical dynamics.

Method: Uses potential energy principle, extends elastodynamics with directional stiffness, employs specialized network for material properties and neural field for deformation representation in geometry-agnostic manner.

Result: GDGen robustly unifies diverse simulation paradigms, offering versatile foundation for interactive virtual environments and robotic agent training.

Conclusion: The framework successfully integrates multiple physical dynamics into unified system, enabling creation of complex interactive environments for embodied AI.

Abstract: Digital twin worlds with realistic interactive dynamics presents a new
opportunity to develop generalist embodied agents in scannable environments
with complex physical behaviors. To this end, we present GDGen (Generalized
Representation for Generalized Dynamics Generation), a framework that takes a
potential energy perspective to seamlessly integrate rigid body, articulated
body, and soft body dynamics into a unified, geometry-agnostic system. GDGen
operates from the governing principle that the potential energy for any stable
physical system should be low. This fresh perspective allows us to treat the
world as one holistic entity and infer underlying physical properties from
simple motion observations. We extend classic elastodynamics by introducing
directional stiffness to capture a broad spectrum of physical behaviors,
covering soft elastic, articulated, and rigid body systems. We propose a
specialized network to model the extended material property and employ a neural
field to represent deformation in a geometry-agnostic manner. Extensive
experiments demonstrate that GDGen robustly unifies diverse simulation
paradigms, offering a versatile foundation for creating interactive virtual
environments and training robotic agents in complex, dynamically rich
scenarios.

</details>


### [13] [Comprehensive language-image pre-training for 3D medical image understanding](https://arxiv.org/abs/2510.15042)
*Tassilo Wald,Ibrahim Ethem Hamamci,Yuan Gao,Sam Bond-Taylor,Harshita Sharma,Maximilian Ilse,Cynthia Lo,Olesya Melnichenko,Noel C. F. Codella,Maria Teodora Wetscherek,Klaus H. Maier-Hein,Panagiotis Korfiatis,Valentina Salvatelli,Javier Alvarez-Valle,Fernando Pérez-García*

Main category: cs.CV

TL;DR: COLIPRI introduces vision-language pre-training with report generation and vision-only pre-training to overcome data limitations in 3D medical imaging, achieving state-of-the-art performance across multiple tasks.


<details>
  <summary>Details</summary>
Motivation: Current 3D vision-language encoders in medical imaging are limited by data availability, restricting their capabilities for tasks like retrieval and abnormality prediction that could support radiologists.

Method: Inject additional inductive biases through report generation objective and pairing vision-language pre-training with vision-only pre-training, leveraging both image-only and paired image-text 3D datasets.

Result: COLIPRI encoders achieve state-of-the-art performance in report generation, classification probing, and zero-shot classification, while remaining competitive for semantic segmentation.

Conclusion: The proposed approach effectively addresses data limitations in 3D medical vision-language pre-training and demonstrates superior performance across multiple medical imaging tasks.

Abstract: Vision-language pre-training, i.e., aligning images with paired text, is a
powerful paradigm to create encoders that can be directly used for tasks such
as classification and retrieval, and for downstream tasks such as segmentation
and report generation. In the 3D medical image domain, these capabilities allow
vision-language encoders (VLEs) to support radiologists by retrieving patients
with similar abnormalities or predicting likelihoods of abnormality. While the
methodology holds promise, data availability limits the capabilities of current
3D VLEs.
  In this paper, we alleviate the lack of data by injecting additional
inductive biases: introducing a report generation objective and pairing
vision-language pre-training with vision-only pre-training. This allows us to
leverage both image-only and paired image-text 3D datasets, increasing the
total amount of data to which our model is exposed. Through these additional
inductive biases, paired with best practices of the 3D medical imaging domain,
we develop the Comprehensive Language-image Pre-training (COLIPRI) encoder
family. Our COLIPRI encoders achieve state-of-the-art performance in report
generation, classification probing, and zero-shot classification, and remain
competitive for semantic segmentation.

</details>


### [14] [Directional Reasoning Injection for Fine-Tuning MLLMs](https://arxiv.org/abs/2510.15050)
*Chao Huang,Zeliang Zhang,Jiang Liu,Ximeng Sun,Jialian Wu,Xiaodong Yu,Ze Wang,Chenliang Xu,Emad Barsoum,Zicheng Liu*

Main category: cs.CV

TL;DR: DRIFT is a lightweight method that transfers reasoning knowledge from text-only LLMs to MLLMs through gradient-space injection, avoiding resource-intensive training while improving multimodal reasoning performance.


<details>
  <summary>Details</summary>
Motivation: MLLMs lag behind text-only LLMs in reasoning ability, and existing methods like supervised fine-tuning or reinforcement learning are resource-intensive. Model merging shows inconsistent results across different model families.

Method: DRIFT precomputes a reasoning prior as parameter-space difference between reasoning and multimodal variants, then uses it to bias gradients during multimodal fine-tuning, preserving multimodal alignment while transferring reasoning knowledge.

Result: DRIFT consistently improves reasoning performance over naive merging and supervised fine-tuning on benchmarks like MathVista and MathVerse, matching or surpassing training-heavy methods at much lower cost.

Conclusion: DRIFT provides an efficient alternative to resource-intensive methods for enhancing MLLM reasoning, achieving strong performance through gradient-space knowledge transfer while maintaining training simplicity.

Abstract: Multimodal large language models (MLLMs) are rapidly advancing, yet their
reasoning ability often lags behind that of strong text-only counterparts.
Existing methods to bridge this gap rely on supervised fine-tuning over
large-scale multimodal reasoning data or reinforcement learning, both of which
are resource-intensive. A promising alternative is model merging, which
interpolates parameters between reasoning-enhanced LLMs and multimodal
variants. However, our analysis shows that naive merging is not always a "free
lunch": its effectiveness varies drastically across model families, with some
(e.g., LLaVA, Idefics) benefiting while others (e.g., Qwen) suffer performance
degradation. To address this, we propose Directional Reasoning Injection for
Fine-Tuning (DRIFT) MLLMs, a lightweight method that transfers reasoning
knowledge in the gradient space, without destabilizing multimodal alignment.
DRIFT precomputes a reasoning prior as the parameter-space difference between
reasoning and multimodal variants, then uses it to bias gradients during
multimodal fine-tuning. This approach preserves the simplicity of standard
supervised fine-tuning pipelines while enabling efficient reasoning transfer.
Extensive experiments on multimodal reasoning benchmarks, including MathVista
and MathVerse, demonstrate that DRIFT consistently improves reasoning
performance over naive merging and supervised fine-tuning, while matching or
surpassing training-heavy methods at a fraction of the cost.

</details>


### [15] [A solution to generalized learning from small training sets found in everyday infant experiences](https://arxiv.org/abs/2510.15060)
*Frangil Ramirez,Elizabeth Clerkin,David J. Crandall,Linda B. Smith*

Main category: cs.CV

TL;DR: Infant visual experiences have a lumpy similarity structure that helps them learn object categories from limited data, and mimicking this structure improves machine learning generalization.


<details>
  <summary>Details</summary>
Motivation: To understand how infants can generalize object categories from limited visual experiences, despite typically requiring large datasets for robust learning.

Method: Analyzed egocentric images from 14 infants (7-11 months) and conducted computational experiments mimicking the lumpy similarity structure found in infant visual input.

Result: Infant visual input shows clusters of highly similar images interspersed with more variable ones across eight early-learned categories. Mimicking this structure improves machine learning generalization from small datasets.

Conclusion: The natural lumpiness of infant visual experiences supports early category learning and may offer principles for efficient learning across various problems and learners.

Abstract: Young children readily recognize and generalize visual objects labeled by
common nouns, suggesting that these basic level object categories may be given.
Yet if they are, how they arise remains unclear. We propose that the answer
lies in the statistics of infant daily life visual experiences. Whereas large
and diverse datasets typically support robust learning and generalization in
human and machine learning, infants achieve this generalization from limited
experiences. We suggest that the resolution of this apparent contradiction lies
in the visual diversity of daily life, repeated experiences with single object
instances. Analyzing egocentric images from 14 infants (aged 7 to 11 months) we
show that their everyday visual input exhibits a lumpy similarity structure,
with clusters of highly similar images interspersed with rarer, more variable
ones, across eight early-learned categories. Computational experiments show
that mimicking this structure in machines improves generalization from small
datasets in machine learning. The natural lumpiness of infant experience may
thus support early category learning and generalization and, more broadly,
offer principles for efficient learning across a variety of problems and kinds
of learners.

</details>


### [16] [SaLon3R: Structure-aware Long-term Generalizable 3D Reconstruction from Unposed Images](https://arxiv.org/abs/2510.15072)
*Jiaxin Guo,Tongfan Guan,Wenzhen Dong,Wenzhao Zheng,Wenting Wang,Yue Wang,Yeung Yam,Yun-Hui Liu*

Main category: cs.CV

TL;DR: SaLon3R is a novel framework for structure-aware, long-term 3D Gaussian Splatting reconstruction that eliminates redundancy through anchor primitives and resolves geometric inconsistencies using a 3D Point Transformer.


<details>
  <summary>Details</summary>
Motivation: Existing 3DGS methods for sequential view reconstruction suffer from substantial redundancies and geometric inconsistencies in long-duration video sequences, leading to inefficient scene representations.

Method: Uses compact anchor primitives with differentiable saliency-aware Gaussian quantization to eliminate redundancy, coupled with a 3D Point Transformer that refines anchor attributes and saliency to resolve cross-frame inconsistencies.

Result: Achieves reconstruction of over 50 views at over 10 FPS with 50% to 90% redundancy removal. Demonstrates state-of-the-art performance on novel view synthesis and depth estimation with superior efficiency and generalization.

Conclusion: SaLon3R effectively resolves artifacts and prunes redundant 3DGS in a single feed-forward pass without known camera parameters or test-time optimization, enabling efficient long-term generalizable 3D reconstruction.

Abstract: Recent advances in 3D Gaussian Splatting (3DGS) have enabled generalizable,
on-the-fly reconstruction of sequential input views. However, existing methods
often predict per-pixel Gaussians and combine Gaussians from all views as the
scene representation, leading to substantial redundancies and geometric
inconsistencies in long-duration video sequences. To address this, we propose
SaLon3R, a novel framework for Structure-aware, Long-term 3DGS Reconstruction.
To our best knowledge, SaLon3R is the first online generalizable GS method
capable of reconstructing over 50 views in over 10 FPS, with 50% to 90%
redundancy removal. Our method introduces compact anchor primitives to
eliminate redundancy through differentiable saliency-aware Gaussian
quantization, coupled with a 3D Point Transformer that refines anchor
attributes and saliency to resolve cross-frame geometric and photometric
inconsistencies. Specifically, we first leverage a 3D reconstruction backbone
to predict dense per-pixel Gaussians and a saliency map encoding regional
geometric complexity. Redundant Gaussians are compressed into compact anchors
by prioritizing high-complexity regions. The 3D Point Transformer then learns
spatial structural priors in 3D space from training data to refine anchor
attributes and saliency, enabling regionally adaptive Gaussian decoding for
geometric fidelity. Without known camera parameters or test-time optimization,
our approach effectively resolves artifacts and prunes the redundant 3DGS in a
single feed-forward pass. Experiments on multiple datasets demonstrate our
state-of-the-art performance on both novel view synthesis and depth estimation,
demonstrating superior efficiency, robustness, and generalization ability for
long-term generalizable 3D reconstruction. Project Page:
https://wrld.github.io/SaLon3R/.

</details>


### [17] [TGT: Text-Grounded Trajectories for Locally Controlled Video Generation](https://arxiv.org/abs/2510.15104)
*Guofeng Zhang,Angtian Wang,Jacob Zhiyuan Fang,Liming Jiang,Haotian Yang,Bo Liu,Yiding Yang,Guang Chen,Longyin Wen,Alan Yuille,Chongyang Ma*

Main category: cs.CV

TL;DR: TGT is a text-to-video generation framework that uses text-grounded trajectories to control both appearance and motion of multiple objects, achieving better visual quality and controllability than prior methods.


<details>
  <summary>Details</summary>
Motivation: Standard text-to-video methods have limited ability to control subject composition, especially in complex multi-object scenarios where existing localized control methods struggle with precision and entity correspondence.

Method: Proposes Text-Grounded Trajectories (TGT) framework with Location-Aware Cross-Attention (LACA) and dual-CFG scheme for separate local/global text guidance, trained on 2M annotated video clips with tracked entity trajectories.

Result: TGT achieves higher visual quality, more accurate text alignment, and improved motion controllability compared to prior approaches, enabling intuitive motion handles via point trajectories.

Conclusion: The TGT framework successfully addresses multi-object control challenges in text-to-video generation by combining text-grounded trajectories with specialized attention mechanisms and training data.

Abstract: Text-to-video generation has advanced rapidly in visual fidelity, whereas
standard methods still have limited ability to control the subject composition
of generated scenes. Prior work shows that adding localized text control
signals, such as bounding boxes or segmentation masks, can help. However, these
methods struggle in complex scenarios and degrade in multi-object settings,
offering limited precision and lacking a clear correspondence between
individual trajectories and visual entities as the number of controllable
objects increases. We introduce Text-Grounded Trajectories (TGT), a framework
that conditions video generation on trajectories paired with localized text
descriptions. We propose Location-Aware Cross-Attention (LACA) to integrate
these signals and adopt a dual-CFG scheme to separately modulate local and
global text guidance. In addition, we develop a data processing pipeline that
produces trajectories with localized descriptions of tracked entities, and we
annotate two million high quality video clips to train TGT. Together, these
components enable TGT to use point trajectories as intuitive motion handles,
pairing each trajectory with text to control both appearance and motion.
Extensive experiments show that TGT achieves higher visual quality, more
accurate text alignment, and improved motion controllability compared with
prior approaches. Website: https://textgroundedtraj.github.io.

</details>


### [18] [Deep generative priors for 3D brain analysis](https://arxiv.org/abs/2510.15119)
*Ana Lawry Aguila,Dina Zemlyanker,You Cheng,Sudeshna Das,Daniel C. Alexander,Oula Puonti,Annabel Sorby-Adams,W. Taylor Kimberly,Juan Eugenio Iglesias*

Main category: cs.CV

TL;DR: This paper introduces a novel framework that combines diffusion models as anatomical priors with Bayesian inverse problems for medical imaging tasks, enabling high-quality brain MRI analysis without requiring paired training data.


<details>
  <summary>Details</summary>
Motivation: While diffusion models show promise in medical imaging, they lack integration with domain knowledge. Bayesian inverse problems offer robust inference but use classical priors that fail to capture complex brain anatomy. The goal is to bridge this gap by using diffusion models as flexible anatomical priors.

Method: The approach uses score-based diffusion priors trained on diverse brain MRI data, combined with flexible forward models for tasks like super-resolution, bias field correction, and inpainting. The framework can also refine outputs from existing deep learning methods.

Result: Experiments on clinical and research MRI data show state-of-the-art performance, producing consistent, high-quality solutions without needing paired training datasets. The method effectively handles various image processing tasks and combinations thereof.

Conclusion: Diffusion priors demonstrate strong potential as versatile tools for brain MRI analysis, successfully combining data-driven modeling with domain knowledge to solve inverse problems while maintaining anatomical fidelity.

Abstract: Diffusion models have recently emerged as powerful generative models in
medical imaging. However, it remains a major challenge to combine these
data-driven models with domain knowledge to guide brain imaging problems. In
neuroimaging, Bayesian inverse problems have long provided a successful
framework for inference tasks, where incorporating domain knowledge of the
imaging process enables robust performance without requiring extensive training
data. However, the anatomical modeling component of these approaches typically
relies on classical mathematical priors that often fail to capture the complex
structure of brain anatomy. In this work, we present the first general-purpose
application of diffusion models as priors for solving a wide range of medical
imaging inverse problems. Our approach leverages a score-based diffusion prior
trained extensively on diverse brain MRI data, paired with flexible forward
models that capture common image processing tasks such as super-resolution,
bias field correction, inpainting, and combinations thereof. We further
demonstrate how our framework can refine outputs from existing deep learning
methods to improve anatomical fidelity. Experiments on heterogeneous clinical
and research MRI data show that our method achieves state-of-the-art
performance producing consistent, high-quality solutions without requiring
paired training datasets. These results highlight the potential of diffusion
priors as versatile tools for brain MRI analysis.

</details>


### [19] [Fourier Transform Multiple Instance Learning for Whole Slide Image Classification](https://arxiv.org/abs/2510.15138)
*Anthony Bilic,Guangyu Sun,Ming Li,Md Sanzid Bin Hossain,Yu Tian,Wei Zhang,Laura Brattain,Dexter Hadley,Chen Chen*

Main category: cs.CV

TL;DR: FFT-MIL enhances WSI classification by adding a frequency-domain branch to MIL methods, using FFT to capture global context and improve diagnostic accuracy.


<details>
  <summary>Details</summary>
Motivation: Existing MIL methods for WSI classification struggle with capturing global dependencies due to large image sizes and local patch embeddings, limiting robust diagnostic prediction.

Method: Proposes FFT-MIL framework with frequency-domain branch using Fast Fourier Transform to extract low-frequency crops, processed through FFT-Block with convolutional layers and Min-Max normalization, then fused with spatial features.

Result: Integration improved macro F1 scores by 3.51% and AUC by 1.51% across six MIL methods on three datasets (BRACS, LUAD, IMP), showing consistent gains.

Conclusion: Frequency-domain learning effectively captures global dependencies in WSI classification, complementing spatial features and advancing MIL-based computational pathology.

Abstract: Whole Slide Image (WSI) classification relies on Multiple Instance Learning
(MIL) with spatial patch features, yet existing methods struggle to capture
global dependencies due to the immense size of WSIs and the local nature of
patch embeddings. This limitation hinders the modeling of coarse structures
essential for robust diagnostic prediction.
  We propose Fourier Transform Multiple Instance Learning (FFT-MIL), a
framework that augments MIL with a frequency-domain branch to provide compact
global context. Low-frequency crops are extracted from WSIs via the Fast
Fourier Transform and processed through a modular FFT-Block composed of
convolutional layers and Min-Max normalization to mitigate the high variance of
frequency data. The learned global frequency feature is fused with spatial
patch features through lightweight integration strategies, enabling
compatibility with diverse MIL architectures.
  FFT-MIL was evaluated across six state-of-the-art MIL methods on three public
datasets (BRACS, LUAD, and IMP). Integration of the FFT-Block improved macro F1
scores by an average of 3.51% and AUC by 1.51%, demonstrating consistent gains
across architectures and datasets. These results establish frequency-domain
learning as an effective and efficient mechanism for capturing global
dependencies in WSI classification, complementing spatial features and
advancing the scalability and accuracy of MIL-based computational pathology.

</details>


### [20] [XModBench: Benchmarking Cross-Modal Capabilities and Consistency in Omni-Language Models](https://arxiv.org/abs/2510.15148)
*Xingrui Wang,Jiang Liu,Chao Huang,Xiaodong Yu,Ze Wang,Ximeng Sun,Jialian Wu,Alan Yuille,Emad Barsoum,Zicheng Liu*

Main category: cs.CV

TL;DR: XModBench is a tri-modal benchmark for evaluating cross-modal consistency in Omni-modal LLMs, revealing that current models struggle with modality-invariant reasoning and exhibit modality-specific biases.


<details>
  <summary>Details</summary>
Motivation: Existing benchmarks focus on general cross-modal QA but don't assess whether OLLMs achieve true modality-invariant reasoning or exhibit modality-specific biases.

Method: Created XModBench with 60,828 multiple-choice questions spanning 5 task families and systematically covering all 6 modality compositions in question-answer pairs.

Result: Even the strongest model (Gemini 2.5 Pro) shows: (i) <60% accuracy on spatial/temporal reasoning, (ii) performance drops with audio vs text, (iii) directional imbalance with vision as context.

Conclusion: Current OLLMs are far from truly modality-invariant reasoning, and XModBench serves as a fundamental diagnostic tool for evaluating and improving cross-modal competence.

Abstract: Omni-modal large language models (OLLMs) aim to unify audio, vision, and text
understanding within a single framework. While existing benchmarks primarily
evaluate general cross-modal question-answering ability, it remains unclear
whether OLLMs achieve modality-invariant reasoning or exhibit modality-specific
biases. We introduce XModBench, a large-scale tri-modal benchmark explicitly
designed to measure cross-modal consistency. XModBench comprises 60,828
multiple-choice questions spanning five task families and systematically covers
all six modality compositions in question-answer pairs, enabling fine-grained
diagnosis of an OLLM's modality-invariant reasoning, modality disparity, and
directional imbalance. Experiments show that even the strongest model, Gemini
2.5 Pro, (i) struggles with spatial and temporal reasoning, achieving less than
60% accuracy, (ii) reveals persistent modality disparities, with performance
dropping substantially when the same semantic content is conveyed through audio
rather than text, and (iii) shows systematic directional imbalance, exhibiting
lower consistency when vision serves as context compared to text. These
findings indicate that current OLLMs remain far from truly modality-invariant
reasoning and position XModBench as a fundamental diagnostic tool for
evaluating and improving cross-modal competence. All data and evaluation tools
will be available at https://xingruiwang.github.io/projects/XModBench/.

</details>


### [21] [Train a Unified Multimodal Data Quality Classifier with Synthetic Data](https://arxiv.org/abs/2510.15162)
*Weizhi Wang,Rongmei Lin,Shiyang Li,Colin Lockard,Ritesh Sarkhel,Sanket Lokegaonkar,Jingbo Shang,Xifeng Yan,Nasser Zalmout,Xian Li*

Main category: cs.CV

TL;DR: UniFilter is a unified multimodal data quality classifier that filters high-quality image-text caption and interleaved data for MLLM pre-training, using semi-synthetic training data generation and improving downstream performance.


<details>
  <summary>Details</summary>
Motivation: High-quality data filtering for image-text interleaved document data in MLLM pre-training is under-explored, despite its importance for model performance.

Method: Train UniFilter using semi-synthetic approach: generate text across four quality levels for raw images to create sample-score pairs, then apply to filter DataComp captions and OBELICS interleaved data.

Result: MLLMs pre-trained on UniFilter-filtered data show enhanced zero-shot reasoning, in-context learning, and stronger performance on various benchmarks after visual supervised fine-tuning.

Conclusion: UniFilter effectively improves multimodal pre-training data quality, leading to better downstream MLLM performance, with released resources for community use.

Abstract: The Multimodal Large Language Models (MLLMs) are continually pre-trained on a
mixture of image-text caption data and interleaved document data, while the
high-quality data filtering towards image-text interleaved document data is
under-explored. We propose to train an efficient MLLM as a Unified Mulitmodal
Data Quality Classifier to Filter both high-quality image-text caption and
interleaved data (UniFilter). To address the challenge of collecting diverse
labeled multimodal data, we introduce a semi-synthetic approach that leverages
readily available raw images and generates corresponding text across four
quality levels. This method enables efficient creation of sample-score pairs
for both caption and interleaved document data to train UniFilter. We apply
UniFilter to curate high-quality caption data from DataComp caption dataset and
interleaved data from the OBELICS image-text interleaved dataset. MLLMs
pre-trained on the filtered data demonstrate significantly enhanced
capabilities compared to those trained on baseline-filtered data, achieving
stronger zero-shot reasoning and in-context learning capabilities. After visual
supervised fine-tuning, these UniFilter-induced MLLMs achieve stronger
performance on various benchmarks, highlighting the downstream benefits of
high-quality multimodal pre-training. We release the synthetic training data
used for training UniFilter, the UniFilter model checkpoints, and the
high-quality interleaved document subset OBELICS-HQ, curated by UniFilter, to
the community for reproduction and further development.

</details>


### [22] [Hyperparameter Optimization and Reproducibility in Deep Learning Model Training](https://arxiv.org/abs/2510.15164)
*Usman Afzaal,Ziyu Su,Usama Sajjad,Hao Lu,Mostafa Rezapour,Metin Nafi Gurcan,Muhammad Khalid Khan Niazi*

Main category: cs.CV

TL;DR: This paper investigates reproducibility challenges in histopathology foundation model training and identifies optimal hyperparameter settings through systematic evaluation on three datasets.


<details>
  <summary>Details</summary>
Motivation: Reproducibility remains a critical challenge in foundation model training for histopathology due to software randomness, hardware non-determinism, and inconsistent hyperparameter reporting.

Method: Trained a CLIP model on QUILT-1M dataset and systematically evaluated the impact of different hyperparameter settings and augmentation strategies across three downstream histopathology datasets (PatchCamelyon, LC25000-Lung, and LC25000-Colon).

Result: Identified clear trends: RandomResizedCrop values of 0.7-0.8 performed best, distributed training without local loss improved stability, learning rates below 5.0e-5 degraded performance, and LC25000 (Colon) dataset provided the most reproducible benchmark.

Conclusion: Reproducibility in computational pathology depends on both transparent documentation and carefully chosen experimental configurations, with practical rules provided to guide future reproducible foundation model development.

Abstract: Reproducibility remains a critical challenge in foundation model training for
histopathology, often hindered by software randomness, hardware
non-determinism, and inconsistent hyperparameter reporting. To investigate
these issues, we trained a CLIP model on the QUILT-1M dataset and
systematically evaluated the impact of different hyperparameter settings and
augmentation strategies across three downstream histopathology datasets
(PatchCamelyon, LC25000-Lung, and LC25000-Colon). Despite variability across
runs, we identified clear trends: RandomResizedCrop values of 0.7-0.8
outperformed more aggressive (0.6) or conservative (0.9) settings, distributed
training without local loss improved stability, and learning rates below 5.0e-5
consistently degraded performance across all datasets. The LC25000 (Colon)
dataset consistently provided the most reproducible benchmark. These findings
highlight that reproducibility in computational pathology depends not only on
transparent documentation but also on carefully chosen experimental
configurations, and we provide practical rules to guide future efforts in
developing reproducible foundation models for digital pathology.

</details>


### [23] [Salient Concept-Aware Generative Data Augmentation](https://arxiv.org/abs/2510.15194)
*Tianchen Zhao,Xuanbai Chen,Zhihua Li,Jun Fang,Dongsheng An,Xiang Xu,Zhuowen Tu,Yifan Xing*

Main category: cs.CV

TL;DR: A personalized image generation framework that uses salient concept-aware image embedding to reduce irrelevant visual details, improving fidelity-diversity balance in data augmentation.


<details>
  <summary>Details</summary>
Motivation: Current generative data augmentation methods struggle to balance fidelity and diversity, as representations become entangled with non-essential image attributes that conflict with text prompts.

Method: Proposed a framework using salient concept-aware image embedding model to reduce influence of irrelevant visual details during synthesis, maintaining better alignment between image and text inputs.

Result: Outperformed state-of-the-art augmentation methods across eight fine-grained vision datasets with 0.73% and 6.5% accuracy improvements under conventional and long-tail settings respectively.

Conclusion: The framework effectively enhances training dataset diversity and improves downstream model robustness by preserving class-discriminative features while enabling controlled variations.

Abstract: Recent generative data augmentation methods conditioned on both image and
text prompts struggle to balance between fidelity and diversity, as it is
challenging to preserve essential image details while aligning with varied text
prompts. This challenge arises because representations in the synthesis process
often become entangled with non-essential input image attributes such as
environmental contexts, creating conflicts with text prompts intended to modify
these elements. To address this, we propose a personalized image generation
framework that uses a salient concept-aware image embedding model to reduce the
influence of irrelevant visual details during the synthesis process, thereby
maintaining intuitive alignment between image and text inputs. By generating
images that better preserve class-discriminative features with additional
controlled variations, our framework effectively enhances the diversity of
training datasets and thereby improves the robustness of downstream models. Our
approach demonstrates superior performance across eight fine-grained vision
datasets, outperforming state-of-the-art augmentation methods with averaged
classification accuracy improvements by 0.73% and 6.5% under conventional and
long-tail settings, respectively.

</details>


### [24] [CARDIUM: Congenital Anomaly Recognition with Diagnostic Images and Unified Medical records](https://arxiv.org/abs/2510.15208)
*Daniela Vega,Hannah V. Ceballos,Javier S. Vera,Santiago Rodriguez,Alejandra Perez,Angela Castillo,Maria Escobar,Dario Londoño,Luis A. Sarmiento,Camila I. Castro,Nadiezhda Rodriguez,Juan C. Briceño,Pablo Arbeláez*

Main category: cs.CV

TL;DR: CARDIUM dataset: first public multimodal dataset for prenatal CHD detection combining fetal ultrasound/echocardiographic images with maternal clinical records, plus a cross-attention transformer model that improves detection by 11-50% over single-modality approaches.


<details>
  <summary>Details</summary>
Motivation: Prenatal CHD diagnosis faces challenges with imbalanced/low-quality datasets and lack of multimodal integration (imaging + clinical data), limiting AI model performance and clinical decision support.

Method: Proposed robust multimodal transformer architecture with cross-attention mechanism to fuse feature representations from image (ultrasound/echocardiographic) and tabular (clinical) data.

Result: Achieved 11% and 50% improvement over image-only and tabular-only approaches respectively, with F1 score of 79.8 ± 4.8% on CARDIUM dataset.

Conclusion: CARDIUM dataset and multimodal transformer enable enhanced prenatal CHD detection, with public release of dataset and code to advance research in this field.

Abstract: Prenatal diagnosis of Congenital Heart Diseases (CHDs) holds great potential
for Artificial Intelligence (AI)-driven solutions. However, collecting
high-quality diagnostic data remains difficult due to the rarity of these
conditions, resulting in imbalanced and low-quality datasets that hinder model
performance. Moreover, no public efforts have been made to integrate multiple
sources of information, such as imaging and clinical data, further limiting the
ability of AI models to support and enhance clinical decision-making. To
overcome these challenges, we introduce the Congenital Anomaly Recognition with
Diagnostic Images and Unified Medical records (CARDIUM) dataset, the first
publicly available multimodal dataset consolidating fetal ultrasound and
echocardiographic images along with maternal clinical records for prenatal CHD
detection. Furthermore, we propose a robust multimodal transformer architecture
that incorporates a cross-attention mechanism to fuse feature representations
from image and tabular data, improving CHD detection by 11% and 50% over image
and tabular single-modality approaches, respectively, and achieving an F1 score
of 79.8 $\pm$ 4.8% in the CARDIUM dataset. We will publicly release our dataset
and code to encourage further research on this unexplored field. Our dataset
and code are available at https://github.com/BCVUniandes/Cardium, and at the
project website https://bcv-uniandes.github.io/CardiumPage/

</details>


### [25] [The Face of Persuasion: Analyzing Bias and Generating Culture-Aware Ads](https://arxiv.org/abs/2510.15240)
*Aysan Aghazadeh,Adriana Kovashka*

Main category: cs.CV

TL;DR: This paper investigates demographic bias in AI-generated advertisements, examining how text-to-image models create biased ads for different topics and how persuasiveness varies based on gender/race of portrayed people, with experiments on country-specific targeting.


<details>
  <summary>Details</summary>
Motivation: To examine the potential of text-to-image models for customized visual advertisements and understand the demographic biases that may arise when targeting specific populations.

Method: The researchers analyzed demographic bias in ads across different topics, tested persuasiveness of identical ads with varying gender/race portrayals using model judgments, and experimented with techniques for country-specific ad targeting.

Result: The study found demographic bias in AI-generated ads for different topics and discovered disparate levels of persuasiveness in ads that were identical except for the gender/race of people portrayed.

Conclusion: Text-to-image models show potential for customized advertising but exhibit demographic biases that affect persuasiveness, highlighting the need for careful consideration when using AI for targeted visual advertisements.

Abstract: Text-to-image models are appealing for customizing visual advertisements and
targeting specific populations. We investigate this potential by examining the
demographic bias within ads for different ad topics, and the disparate level of
persuasiveness (judged by models) of ads that are identical except for
gender/race of the people portrayed. We also experiment with a technique to
target ads for specific countries. The code is available at
https://github.com/aysanaghazadeh/FaceOfPersuasion

</details>


### [26] [DriveGen3D: Boosting Feed-Forward Driving Scene Generation with Efficient Video Diffusion](https://arxiv.org/abs/2510.15264)
*Weijie Wang,Jiagang Zhu,Zeyu Zhang,Xiaofeng Wang,Zheng Zhu,Guosheng Zhao,Chaojun Ni,Haoxiao Wang,Guan Huang,Xinze Chen,Yukun Zhou,Wenkang Qin,Duochao Shi,Haoyun Li,Guanghong Jia,Jiwen Lu*

Main category: cs.CV

TL;DR: DriveGen3D is a framework for generating high-quality, controllable dynamic 3D driving scenes that combines efficient video generation with rapid 3D reconstruction.


<details>
  <summary>Details</summary>
Motivation: Current methods have limitations: prohibitive computational costs for long-term generation, focus only on video synthesis without 3D representation, or restriction to static single-scene reconstruction.

Method: Unified pipeline with two components: FastDrive-DiT (efficient video diffusion transformer for high-resolution video synthesis under text and BEV layout guidance) and FastRecon3D (feed-forward reconstruction module for building 3D Gaussian representations across time).

Result: Real-time generation of extended driving videos (424×800 at 12 FPS) and dynamic 3D scenes, achieving SSIM of 0.811 and PSNR of 22.84 on novel view synthesis while maintaining parameter efficiency.

Conclusion: DriveGen3D successfully bridges the gap between long-term video generation and dynamic 3D scene reconstruction, enabling highly controllable and efficient synthesis of driving scenes.

Abstract: We present DriveGen3D, a novel framework for generating high-quality and
highly controllable dynamic 3D driving scenes that addresses critical
limitations in existing methodologies. Current approaches to driving scene
synthesis either suffer from prohibitive computational demands for extended
temporal generation, focus exclusively on prolonged video synthesis without 3D
representation, or restrict themselves to static single-scene reconstruction.
Our work bridges this methodological gap by integrating accelerated long-term
video generation with large-scale dynamic scene reconstruction through
multimodal conditional control. DriveGen3D introduces a unified pipeline
consisting of two specialized components: FastDrive-DiT, an efficient video
diffusion transformer for high-resolution, temporally coherent video synthesis
under text and Bird's-Eye-View (BEV) layout guidance; and FastRecon3D, a
feed-forward reconstruction module that rapidly builds 3D Gaussian
representations across time, ensuring spatial-temporal consistency. Together,
these components enable real-time generation of extended driving videos (up to
$424\times800$ at 12 FPS) and corresponding dynamic 3D scenes, achieving SSIM
of 0.811 and PSNR of 22.84 on novel view synthesis, all while maintaining
parameter efficiency.

</details>


### [27] [CuSfM: CUDA-Accelerated Structure-from-Motion](https://arxiv.org/abs/2510.15271)
*Jingrui Yu,Jun Liu,Kefei Ren,Joydeep Biswas,Rurui Ye,Keqiang Wu,Chirag Majithia,Di Zeng*

Main category: cs.CV

TL;DR: cuSfM is a CUDA-accelerated offline Structure-from-Motion system that uses GPU parallelization to achieve high-precision camera pose estimation and dense reconstruction, outperforming COLMAP in both accuracy and speed.


<details>
  <summary>Details</summary>
Motivation: Camera pose estimation is crucial for autonomous navigation, robotic perception, and virtual simulation systems. The paper addresses the need for efficient and accurate pose estimation through GPU acceleration.

Method: Developed cuSfM - a CUDA-accelerated offline SfM system that leverages GPU parallelization to use computationally intensive feature extractors efficiently. It generates comprehensive data associations for precise camera pose estimation and globally consistent mapping.

Result: Experimental results show cuSfM achieves significantly improved accuracy and processing speed compared to COLMAP across various testing scenarios, while maintaining high precision and global consistency.

Conclusion: The system is released as open-source Python wrapper PyCuSfM to facilitate research in computer vision and robotics, demonstrating effective GPU acceleration for offline SfM applications.

Abstract: Efficient and accurate camera pose estimation forms the foundational
requirement for dense reconstruction in autonomous navigation, robotic
perception, and virtual simulation systems. This paper addresses the challenge
via cuSfM, a CUDA-accelerated offline Structure-from-Motion system that
leverages GPU parallelization to efficiently employ computationally intensive
yet highly accurate feature extractors, generating comprehensive and
non-redundant data associations for precise camera pose estimation and globally
consistent mapping. The system supports pose optimization, mapping, prior-map
localization, and extrinsic refinement. It is designed for offline processing,
where computational resources can be fully utilized to maximize accuracy.
Experimental results demonstrate that cuSfM achieves significantly improved
accuracy and processing speed compared to the widely used COLMAP method across
various testing scenarios, while maintaining the high precision and global
consistency essential for offline SfM applications. The system is released as
an open-source Python wrapper implementation, PyCuSfM, available at
https://github.com/nvidia-isaac/pyCuSFM, to facilitate research and
applications in computer vision and robotics.

</details>


### [28] [Post-Processing Methods for Improving Accuracy in MRI Inpainting](https://arxiv.org/abs/2510.15282)
*Nishad Kulkarni,Krithika Iyer,Austin Tapp,Abhijeet Parida,Daniel Capellán-Martín,Zhifan Jiang,María J. Ledesma-Carbayo,Syed Muhammad Anwar,Marius George Linguraru*

Main category: cs.CV

TL;DR: This paper addresses the failure of automated MRI analysis tools on brain tumor images by developing an enhanced inpainting pipeline that combines model ensembling with post-processing strategies and a U-Net refinement stage to synthesize healthy brain tissue in tumor regions.


<details>
  <summary>Details</summary>
Motivation: Standard MRI analysis tools fail when dealing with large lesions like tumors, as they're optimized for healthy anatomies. Image inpainting can synthesize healthy tissues in tumor areas to enable reliable application of general-purpose analysis tools.

Method: Combines model ensembling with post-processing strategies (median filtering, histogram matching, pixel averaging) and adds a lightweight U-Net enhancement stage for anatomical refinement.

Result: The proposed pipeline improves anatomical plausibility and visual fidelity of inpainted regions, yielding higher accuracy and more robust outcomes than individual baseline models.

Conclusion: By combining established models with targeted post-processing, the approach achieves improved and more accessible inpainting outcomes, supporting broader clinical deployment and resource-conscious research.

Abstract: Magnetic Resonance Imaging (MRI) is the primary imaging modality used in the
diagnosis, assessment, and treatment planning for brain pathologies. However,
most automated MRI analysis tools, such as segmentation and registration
pipelines, are optimized for healthy anatomies and often fail when confronted
with large lesions such as tumors. To overcome this, image inpainting
techniques aim to locally synthesize healthy brain tissues in tumor regions,
enabling the reliable application of general-purpose tools. In this work, we
systematically evaluate state-of-the-art inpainting models and observe a
saturation in their standalone performance. In response, we introduce a
methodology combining model ensembling with efficient post-processing
strategies such as median filtering, histogram matching, and pixel averaging.
Further anatomical refinement is achieved via a lightweight U-Net enhancement
stage. Comprehensive evaluation demonstrates that our proposed pipeline
improves the anatomical plausibility and visual fidelity of inpainted regions,
yielding higher accuracy and more robust outcomes than individual baseline
models. By combining established models with targeted post-processing, we
achieve improved and more accessible inpainting outcomes, supporting broader
clinical deployment and sustainable, resource-conscious research. Our 2025
BraTS inpainting docker is available at
https://hub.docker.com/layers/aparida12/brats2025/inpt.

</details>


### [29] [QCFace: Image Quality Control for boosting Face Representation & Recognition](https://arxiv.org/abs/2510.15289)
*Duc-Phuong Doan-Ngo,Thanh-Dang Diep,Thanh Nguyen-Duc,Thanh-Sach LE,Nam Thoai*

Main category: cs.CV

TL;DR: QCFace introduces a hard margin strategy to decouple recognizability from identity in face recognition, overcoming gradient overlap issues and achieving state-of-the-art performance.


<details>
  <summary>Details</summary>
Motivation: Current face recognition methods inadequately capture recognizability through soft margin constraints, leading to poor quality representation and discrimination for low-quality faces, plus mutual gradient overlap causes optimization instability and entangled representations.

Method: Proposes Quality Control Face (QCFace) with a hard margin strategy that decouples recognizability from identity representation, using a novel hard-margin-based loss function with guidance factor for hypersphere planning.

Result: Extensive experiments show QCFace provides robust and quantifiable recognizability encoding while achieving state-of-the-art performance in both verification and identification benchmarks compared to existing recognizability-based losses.

Conclusion: The hard margin strategy effectively addresses mutual gradient overlap problems and enables clear separation of recognizability from identity, leading to improved face recognition performance.

Abstract: Recognizability, a key perceptual factor in human face processing, strongly
affects the performance of face recognition (FR) systems in both verification
and identification tasks. Effectively using recognizability to enhance feature
representation remains challenging. In deep FR, the loss function plays a
crucial role in shaping how features are embedded. However, current methods
have two main drawbacks: (i) recognizability is only partially captured through
soft margin constraints, resulting in weaker quality representation and lower
discrimination, especially for low-quality or ambiguous faces; (ii) mutual
overlapping gradients between feature direction and magnitude introduce
undesirable interactions during optimization, causing instability and confusion
in hypersphere planning, which may result in poor generalization, and entangled
representations where recognizability and identity are not cleanly separated.
To address these issues, we introduce a hard margin strategy - Quality Control
Face (QCFace), which overcomes the mutual overlapping gradient problem and
enables the clear decoupling of recognizability from identity representation.
Based on this strategy, a novel hard-margin-based loss function employs a
guidance factor for hypersphere planning, simultaneously optimizing for
recognition ability and explicit recognizability representation. Extensive
experiments confirm that QCFace not only provides robust and quantifiable
recognizability encoding but also achieves state-of-the-art performance in both
verification and identification benchmarks compared to existing
recognizability-based losses.

</details>


### [30] [Spatial457: A Diagnostic Benchmark for 6D Spatial Reasoning of Large Multimodal Models](https://arxiv.org/abs/2502.08636)
*Xingrui Wang,Wufei Ma,Tiezheng Zhang,Celso M de Melo,Jieneng Chen,Alan Yuille*

Main category: cs.CV

TL;DR: Spatial457 is a new benchmark dataset for evaluating 6D spatial reasoning in large multimodal models, revealing significant performance drops in 3D and 6D reasoning tasks compared to 2D capabilities.


<details>
  <summary>Details</summary>
Motivation: Existing benchmarks focus on 2D spatial understanding and lack comprehensive evaluation of complex 3D spatial reasoning in large multimodal models.

Method: Created Spatial457 - a scalable synthetic dataset with 4 spatial reasoning capabilities (multi-object recognition, 2D location, 3D location, 3D orientation) and cascading evaluation structure with 7 question types across 5 difficulty levels.

Result: Models showed performance decline as task complexity increased, especially in 3D reasoning and 6D spatial tasks. Introduced Relative Performance Dropping Rate (RPDR) to quantify challenges and uncovered prediction biases across attributes.

Conclusion: Current LMMs struggle with complex 3D spatial reasoning despite strong 2D capabilities, highlighting the need for improved 3D reasoning in multimodal AI systems.

Abstract: Although large multimodal models (LMMs) have demonstrated remarkable
capabilities in visual scene interpretation and reasoning, their capacity for
complex and precise 3-dimensional spatial reasoning remains uncertain. Existing
benchmarks focus predominantly on 2D spatial understanding and lack a framework
to comprehensively evaluate 6D spatial reasoning across varying complexities.
To address this limitation, we present Spatial457, a scalable and unbiased
synthetic dataset designed with 4 key capability for spatial reasoning:
multi-object recognition, 2D location, 3D location, and 3D orientation. We
develop a cascading evaluation structure, constructing 7 question types across
5 difficulty levels that range from basic single object recognition to our new
proposed complex 6D spatial reasoning tasks. We evaluated various large
multimodal models (LMMs) on PulseCheck457, observing a general decline in
performance as task complexity increases, particularly in 3D reasoning and 6D
spatial tasks. To quantify these challenges, we introduce the Relative
Performance Dropping Rate (RPDR), highlighting key weaknesses in 3D reasoning
capabilities. Leveraging the unbiased attribute design of our dataset, we also
uncover prediction biases across different attributes, with similar patterns
observed in real-world image settings. The code and data are released in
https://github.com/XingruiWang/Spatial457.

</details>


### [31] [Hyperbolic Structured Classification for Robust Single Positive Multi-label Learning](https://arxiv.org/abs/2510.15296)
*Yiming Lin,Shang Wang,Junkai Zhou,Qiufeng Wang,Xiao-Bo Jin,Kaizhu Huang*

Main category: cs.CV

TL;DR: Proposes a hyperbolic classification framework for Single Positive Multi-Label Learning (SPMLL) using hyperbolic balls to model label relationships, achieving competitive performance with better interpretability.


<details>
  <summary>Details</summary>
Motivation: Address limitations in SPMLL where existing methods lack explicit geometric definitions for different relationship types and struggle to capture hierarchical structures and complex label relationships with only single positive annotations.

Method: Represents each label as a hyperbolic ball rather than point/vector, enabling geometric ball interactions to model inclusion (hierarchical), overlap (co-occurrence), and separation (independence) relationships. Introduces temperature-adaptive hyperbolic ball classifier and physics-inspired double-well regularization.

Result: Extensive experiments on four benchmark datasets (MS-COCO, PASCAL VOC, NUS-WIDE, CUB-200-2011) show competitive performance with superior interpretability. Statistical analysis reveals strong correlation between learned embeddings and real-world co-occurrence patterns.

Conclusion: Establishes hyperbolic geometry as a more robust paradigm for structured classification under incomplete supervision, providing explicit geometric modeling of multiple relationship types in SPMLL.

Abstract: Single Positive Multi-Label Learning (SPMLL) addresses the challenging
scenario where each training sample is annotated with only one positive label
despite potentially belonging to multiple categories, making it difficult to
capture complex label relationships and hierarchical structures. While existing
methods implicitly model label relationships through distance-based similarity,
lacking explicit geometric definitions for different relationship types. To
address these limitations, we propose the first hyperbolic classification
framework for SPMLL that represents each label as a hyperbolic ball rather than
a point or vector, enabling rich inter-label relationship modeling through
geometric ball interactions. Our ball-based approach naturally captures
multiple relationship types simultaneously: inclusion for hierarchical
structures, overlap for co-occurrence patterns, and separation for semantic
independence. Further, we introduce two key component innovations: a
temperature-adaptive hyperbolic ball classifier and a physics-inspired
double-well regularization that guides balls toward meaningful configurations.
To validate our approach, extensive experiments on four benchmark datasets
(MS-COCO, PASCAL VOC, NUS-WIDE, CUB-200-2011) demonstrate competitive
performance with superior interpretability compared to existing methods.
Furthermore, statistical analysis reveals strong correlation between learned
embeddings and real-world co-occurrence patterns, establishing hyperbolic
geometry as a more robust paradigm for structured classification under
incomplete supervision.

</details>


### [32] [Latent Diffusion Model without Variational Autoencoder](https://arxiv.org/abs/2510.15301)
*Minglei Shi,Haolin Wang,Wenzhao Zheng,Ziyang Yuan,Xiaoshi Wu,Xintao Wang,Pengfei Wan,Jie Zhou,Jiwen Lu*

Main category: cs.CV

TL;DR: SVG is a novel latent diffusion model that replaces VAEs with self-supervised DINO features, creating semantically structured latent spaces for more efficient training and better generative quality.


<details>
  <summary>Details</summary>
Motivation: VAE+diffusion models suffer from limited training efficiency, slow inference, and poor transferability due to VAE latent spaces lacking clear semantic separation and discriminative structure.

Method: SVG constructs a feature space using frozen DINO features for semantic discriminability, with a lightweight residual branch for fine-grained details. Diffusion models are trained directly on this semantically structured latent space.

Result: SVG enables accelerated diffusion training, supports few-step sampling, improves generative quality, and preserves semantic and discriminative capabilities of self-supervised representations.

Conclusion: SVG provides a principled pathway toward task-general, high-quality visual representations by leveraging semantically structured latent spaces without VAEs.

Abstract: Recent progress in diffusion-based visual generation has largely relied on
latent diffusion models with variational autoencoders (VAEs). While effective
for high-fidelity synthesis, this VAE+diffusion paradigm suffers from limited
training efficiency, slow inference, and poor transferability to broader vision
tasks. These issues stem from a key limitation of VAE latent spaces: the lack
of clear semantic separation and strong discriminative structure. Our analysis
confirms that these properties are crucial not only for perception and
understanding tasks, but also for the stable and efficient training of latent
diffusion models. Motivated by this insight, we introduce SVG, a novel latent
diffusion model without variational autoencoders, which leverages
self-supervised representations for visual generation. SVG constructs a feature
space with clear semantic discriminability by leveraging frozen DINO features,
while a lightweight residual branch captures fine-grained details for
high-fidelity reconstruction. Diffusion models are trained directly on this
semantically structured latent space to facilitate more efficient learning. As
a result, SVG enables accelerated diffusion training, supports few-step
sampling, and improves generative quality. Experimental results further show
that SVG preserves the semantic and discriminative capabilities of the
underlying self-supervised representations, providing a principled pathway
toward task-general, high-quality visual representations.

</details>


### [33] [Layer as Puzzle Pieces: Compressing Large Language Models through Layer Concatenation](https://arxiv.org/abs/2510.15304)
*Fei Wang,Li Shen,Liang Ding,Chao Xue,Ye Liu,Changxing Ding*

Main category: cs.CV

TL;DR: CoMe is a progressive layer pruning framework that addresses limitations in structured pruning of LLMs through concatenation-based merging and hierarchical distillation, achieving state-of-the-art performance with minimal accuracy loss.


<details>
  <summary>Details</summary>
Motivation: Large Language Models have massive computational and storage demands, and existing structured pruning methods suffer from performance degradation, incompetent weight layer aggregation, and lack of effective post-training recovery mechanisms.

Method: Proposes CoMe with: 1) channel sensitivity metric using activation intensity and weight norms for fine-grained channel selection, 2) concatenation-based layer merging to fuse critical channels across adjacent layers, and 3) hierarchical distillation protocol leveraging layer correspondences for efficient knowledge transfer.

Result: Experiments on seven benchmarks show CoMe achieves state-of-the-art performance; when pruning 30% of LLaMA-2-7b's parameters, the pruned model retains 83% of its original average accuracy.

Conclusion: CoMe effectively addresses key limitations in structured pruning through progressive layer pruning with concatenation-based merging and hierarchical distillation, enabling significant model size reduction while maintaining high performance.

Abstract: Large Language Models excel at natural language processing tasks, but their
massive size leads to high computational and storage demands. Recent works have
sought to reduce their model size through layer-wise structured pruning.
However, they tend to ignore retaining the capabilities in the pruned part. In
this work, we re-examine structured pruning paradigms and uncover several key
limitations: 1) notable performance degradation due to direct layer removal, 2)
incompetent linear weight layer aggregation, and 3) the lack of effective
post-training recovery mechanisms. To address these limitations, we propose
CoMe, including a progressive layer pruning framework with a
Concatenation-based Merging technology and a hierarchical distillation
post-training process. Specifically, we introduce a channel sensitivity metric
that utilizes activation intensity and weight norms for fine-grained channel
selection. Subsequently, we employ a concatenation-based layer merging method
to fuse the most critical channels across adjacent layers, enabling progressive
model size reduction. Finally, we propose a hierarchical distillation protocol
that leverages the correspondences between the original and pruned model layers
established during pruning, thereby enabling efficient knowledge transfer.
Experiments on seven benchmarks show that CoMe achieves state-of-the-art
performance; when pruning 30% of LLaMA-2-7b's parameters, the pruned model
retains 83% of its original average accuracy. Our code is available at
https://github.com/MPI-Lab/CoMe.

</details>


### [34] [Proto-Former: Unified Facial Landmark Detection by Prototype Transformer](https://arxiv.org/abs/2510.15338)
*Shengkai Hu,Haozhe Qi,Jun Wan,Jiaxing Huang,Lefei Zhang,Hang Sun,Dacheng Tao*

Main category: cs.CV

TL;DR: Proto-Former is a unified facial landmark detection framework that enables joint training across multiple datasets with different landmark definitions through prototype learning and adaptive architecture.


<details>
  <summary>Details</summary>
Motivation: Existing facial landmark detection methods are limited to single-dataset training due to varying landmark definitions across datasets, which hinders model generalization and prevents unified model development.

Method: Proto-Former uses an Adaptive Prototype-Aware Encoder (APAE) for feature extraction and prototype learning, a Progressive Prototype-Aware Decoder (PPAD) for prototype refinement and prompt generation, and a Prototype-Aware (PA) loss to stabilize multi-dataset training.

Result: Extensive experiments show Proto-Former achieves superior performance compared to state-of-the-art methods on widely used benchmark datasets.

Conclusion: Proto-Former successfully addresses the limitations of single-dataset training by enabling unified multi-dataset training through prototype learning, achieving better generalization and performance.

Abstract: Recent advances in deep learning have significantly improved facial landmark
detection. However, existing facial landmark detection datasets often define
different numbers of landmarks, and most mainstream methods can only be trained
on a single dataset. This limits the model generalization to different datasets
and hinders the development of a unified model. To address this issue, we
propose Proto-Former, a unified, adaptive, end-to-end facial landmark detection
framework that explicitly enhances dataset-specific facial structural
representations (i.e., prototype). Proto-Former overcomes the limitations of
single-dataset training by enabling joint training across multiple datasets
within a unified architecture. Specifically, Proto-Former comprises two key
components: an Adaptive Prototype-Aware Encoder (APAE) that performs adaptive
feature extraction and learns prototype representations, and a Progressive
Prototype-Aware Decoder (PPAD) that refines these prototypes to generate
prompts that guide the model's attention to key facial regions. Furthermore, we
introduce a novel Prototype-Aware (PA) loss, which achieves optimal path
finding by constraining the selection weights of prototype experts. This loss
function effectively resolves the problem of prototype expert addressing
instability during multi-dataset training, alleviates gradient conflicts, and
enables the extraction of more accurate facial structure features. Extensive
experiments on widely used benchmark datasets demonstrate that our Proto-Former
achieves superior performance compared to existing state-of-the-art methods.
The code is publicly available at: https://github.com/Husk021118/Proto-Former.

</details>


### [35] [SHARE: Scene-Human Aligned Reconstruction](https://arxiv.org/abs/2510.15342)
*Joshua Li,Brendan Chharawala,Chang Shu,Xue Bin Peng,Pengcheng Xi*

Main category: cs.CV

TL;DR: SHARE is a method that uses scene geometry to accurately ground 3D human motion reconstruction from monocular RGB videos, improving human placement in 3D space.


<details>
  <summary>Details</summary>
Motivation: Current human motion reconstruction methods struggle with accurate 3D human placement in scenes, which is important for realistic character interactions in gaming, AR/VR, and robotics.

Method: SHARE estimates human meshes and segmentation masks for each frame, creates scene point maps at keyframes, then iteratively refines human positions by comparing human meshes against human point maps extracted using masks, while maintaining consistency between keyframes and non-keyframes.

Result: SHARE outperforms existing methods in extensive experiments and enables more accurate 3D human placement while reconstructing surrounding scenes, working on both curated datasets and in-the-wild web videos.

Conclusion: SHARE successfully leverages scene geometry to improve 3D human motion reconstruction accuracy from monocular videos, facilitating realistic character-environment interactions.

Abstract: Animating realistic character interactions with the surrounding environment
is important for autonomous agents in gaming, AR/VR, and robotics. However,
current methods for human motion reconstruction struggle with accurately
placing humans in 3D space. We introduce Scene-Human Aligned REconstruction
(SHARE), a technique that leverages the scene geometry's inherent spatial cues
to accurately ground human motion reconstruction. Each reconstruction relies
solely on a monocular RGB video from a stationary camera. SHARE first estimates
a human mesh and segmentation mask for every frame, alongside a scene point map
at keyframes. It iteratively refines the human's positions at these keyframes
by comparing the human mesh against the human point map extracted from the
scene using the mask. Crucially, we also ensure that non-keyframe human meshes
remain consistent by preserving their relative root joint positions to keyframe
root joints during optimization. Our approach enables more accurate 3D human
placement while reconstructing the surrounding scene, facilitating use cases on
both curated datasets and in-the-wild web videos. Extensive experiments
demonstrate that SHARE outperforms existing methods.

</details>


### [36] [Cortical-SSM: A Deep State Space Model for EEG and ECoG Motor Imagery Decoding](https://arxiv.org/abs/2510.15371)
*Shuntaro Suzuki,Shunya Nagashima,Masayuki Hirata,Komei Sugiura*

Main category: cs.CV

TL;DR: Cortical-SSM is a novel deep state space model that captures integrated dependencies across temporal, spatial, and frequency domains for EEG/ECoG motor imagery classification, outperforming baselines on multiple benchmarks.


<details>
  <summary>Details</summary>
Motivation: EEG and ECoG signals for motor imagery classification have application potential for communication assistance and rehabilitation, but are susceptible to physiological artifacts and existing Transformer-based methods struggle to capture fine-grained dependencies.

Method: Proposed Cortical-SSM architecture that extends deep state space models to capture integrated dependencies across temporal, spatial, and frequency domains of EEG/ECoG signals.

Result: Outperformed baseline methods on three benchmarks: two large-scale public MI EEG datasets (>50 subjects) and a clinical MI ECoG dataset from an ALS patient. Visual explanations showed effective capture of neurophysiologically relevant regions.

Conclusion: Cortical-SSM effectively addresses limitations of existing methods by capturing fine-grained dependencies across multiple domains, demonstrating superior performance and neurophysiological relevance in EEG/ECoG motor imagery classification.

Abstract: Classification of electroencephalogram (EEG) and electrocorticogram (ECoG)
signals obtained during motor imagery (MI) has substantial application
potential, including for communication assistance and rehabilitation support
for patients with motor impairments. These signals remain inherently
susceptible to physiological artifacts (e.g., eye blinking, swallowing), which
pose persistent challenges. Although Transformer-based approaches for
classifying EEG and ECoG signals have been widely adopted, they often struggle
to capture fine-grained dependencies within them. To overcome these
limitations, we propose Cortical-SSM, a novel architecture that extends deep
state space models to capture integrated dependencies of EEG and ECoG signals
across temporal, spatial, and frequency domains. We validated our method across
three benchmarks: 1) two large-scale public MI EEG datasets containing more
than 50 subjects, and 2) a clinical MI ECoG dataset recorded from a patient
with amyotrophic lateral sclerosis. Our method outperformed baseline methods on
the three benchmarks. Furthermore, visual explanations derived from our model
indicate that it effectively captures neurophysiologically relevant regions of
both EEG and ECoG signals.

</details>


### [37] [Adaptive transfer learning for surgical tool presence detection in laparoscopic videos through gradual freezing fine-tuning](https://arxiv.org/abs/2510.15372)
*Ana Davila,Jacinto Colan,Yasuhisa Hasegawa*

Main category: cs.CV

TL;DR: A novel staged adaptive fine-tuning approach for surgical tool detection that uses linear probing and gradual freezing to improve efficiency and performance in minimally invasive surgery.


<details>
  <summary>Details</summary>
Motivation: Limited annotated data in surgical settings makes it challenging to train robust deep learning models for automated surgical tool detection, which could significantly benefit minimally invasive surgery.

Method: Two-step approach: linear probing to condition classification layers on pre-trained CNN architectures, followed by gradual freezing to dynamically reduce fine-tunable layers. Uses ResNet-50 and DenseNet-121 pre-trained on ImageNet.

Result: Achieved 96.4% mean average precision (mAP) on Cholec80 dataset, outperforming existing approaches. Method also generalized well to CATARACTS dataset for ophthalmic surgery.

Conclusion: Gradual freezing fine-tuning is a promising technique for improving tool detection in diverse surgical procedures and may have broader applications in general image classification tasks.

Abstract: Minimally invasive surgery can benefit significantly from automated surgical
tool detection, enabling advanced analysis and assistance. However, the limited
availability of annotated data in surgical settings poses a challenge for
training robust deep learning models. This paper introduces a novel staged
adaptive fine-tuning approach consisting of two steps: a linear probing stage
to condition additional classification layers on a pre-trained CNN-based
architecture and a gradual freezing stage to dynamically reduce the
fine-tunable layers, aiming to regulate adaptation to the surgical domain. This
strategy reduces network complexity and improves efficiency, requiring only a
single training loop and eliminating the need for multiple iterations. We
validated our method on the Cholec80 dataset, employing CNN architectures
(ResNet-50 and DenseNet-121) pre-trained on ImageNet for detecting surgical
tools in cholecystectomy endoscopic videos. Our results demonstrate that our
method improves detection performance compared to existing approaches and
established fine-tuning techniques, achieving a mean average precision (mAP) of
96.4%. To assess its broader applicability, the generalizability of the
fine-tuning strategy was further confirmed on the CATARACTS dataset, a distinct
domain of minimally invasive ophthalmic surgery. These findings suggest that
gradual freezing fine-tuning is a promising technique for improving tool
presence detection in diverse surgical procedures and may have broader
applications in general image classification tasks.

</details>


### [38] [FreqPDE: Rethinking Positional Depth Embedding for Multi-View 3D Object Detection Transformers](https://arxiv.org/abs/2510.15385)
*Haisheng Su,Junjie Zhang,Feixiang Song,Sanping Zhou,Wei Wu,Nanning Zheng,Junchi Yan*

Main category: cs.CV

TL;DR: FreqPDE introduces frequency-aware depth embedding to enhance 3D object detection from multi-view 2D images, addressing depth discontinuity and cross-view consistency issues without requiring LiDAR supervision.


<details>
  <summary>Details</summary>
Motivation: Current methods rely on depth prediction with LiDAR supervision, suffering from depth discontinuity at object boundaries and poor detection of small objects due to sparse supervision and high-level features. Cross-view consistency and scale invariance are also overlooked.

Method: Three main modules: Frequency-aware Spatial Pyramid Encoder (FSPE) combines high-frequency edges and low-frequency semantics; Cross-view Scale-invariant Depth Predictor (CSDP) estimates depth with cross-view attention; Positional Depth Encoder (PDE) generates 3D depth-aware features. Uses hybrid depth supervision from metric and distribution aspects.

Result: Extensive experiments on nuScenes dataset demonstrate effectiveness and superiority of the proposed method.

Conclusion: FreqPDE successfully enhances 3D object detection from 2D images by addressing depth quality issues and incorporating cross-view consistency without requiring LiDAR supervision.

Abstract: Detecting 3D objects accurately from multi-view 2D images is a challenging
yet essential task in the field of autonomous driving. Current methods resort
to integrating depth prediction to recover the spatial information for object
query decoding, which necessitates explicit supervision from LiDAR points
during the training phase. However, the predicted depth quality is still
unsatisfactory such as depth discontinuity of object boundaries and
indistinction of small objects, which are mainly caused by the sparse
supervision of projected points and the use of high-level image features for
depth prediction. Besides, cross-view consistency and scale invariance are also
overlooked in previous methods. In this paper, we introduce Frequency-aware
Positional Depth Embedding (FreqPDE) to equip 2D image features with spatial
information for 3D detection transformer decoder, which can be obtained through
three main modules. Specifically, the Frequency-aware Spatial Pyramid Encoder
(FSPE) constructs a feature pyramid by combining high-frequency edge clues and
low-frequency semantics from different levels respectively. Then the Cross-view
Scale-invariant Depth Predictor (CSDP) estimates the pixel-level depth
distribution with cross-view and efficient channel attention mechanism.
Finally, the Positional Depth Encoder (PDE) combines the 2D image features and
3D position embeddings to generate the 3D depth-aware features for query
decoding. Additionally, hybrid depth supervision is adopted for complementary
depth learning from both metric and distribution aspects. Extensive experiments
conducted on the nuScenes dataset demonstrate the effectiveness and superiority
of our proposed method.

</details>


### [39] [PFGS: Pose-Fused 3D Gaussian Splatting for Complete Multi-Pose Object Reconstruction](https://arxiv.org/abs/2510.15386)
*Ting-Yu Yen,Yu-Sheng Chiu,Shih-Hsuan Hung,Peter Wonka,Hung-Kuo Chu*

Main category: cs.CV

TL;DR: PFGS is a pose-aware 3D Gaussian Splatting framework that reconstructs complete 3D objects from multi-pose image captures by iteratively fusing auxiliary pose images into a unified 3DGS representation.


<details>
  <summary>Details</summary>
Motivation: Existing 3DGS methods assume static single-pose captures, leading to incomplete reconstructions that miss occluded regions. There's a need to reconstruct complete objects from multi-pose image captures.

Method: PFGS uses pose-aware fusion with global and local registration to merge views from different poses. It leverages 3D foundation models for cross-pose registration and background features for per-pose camera pose estimation, addressing memory and accuracy limitations.

Result: PFGS consistently outperforms strong baselines in both qualitative and quantitative evaluations, producing more complete reconstructions and higher-fidelity 3DGS models.

Conclusion: The proposed PFGS framework effectively addresses the challenge of reconstructing complete objects from multi-pose captures by intelligently incorporating foundation models into the registration process while resolving background inconsistency issues.

Abstract: Recent advances in 3D Gaussian Splatting (3DGS) have enabled high-quality,
real-time novel-view synthesis from multi-view images. However, most existing
methods assume the object is captured in a single, static pose, resulting in
incomplete reconstructions that miss occluded or self-occluded regions. We
introduce PFGS, a pose-aware 3DGS framework that addresses the practical
challenge of reconstructing complete objects from multi-pose image captures.
Given images of an object in one main pose and several auxiliary poses, PFGS
iteratively fuses each auxiliary set into a unified 3DGS representation of the
main pose. Our pose-aware fusion strategy combines global and local
registration to merge views effectively and refine the 3DGS model. While recent
advances in 3D foundation models have improved registration robustness and
efficiency, they remain limited by high memory demands and suboptimal accuracy.
PFGS overcomes these challenges by incorporating them more intelligently into
the registration process: it leverages background features for per-pose camera
pose estimation and employs foundation models for cross-pose registration. This
design captures the best of both approaches while resolving background
inconsistency issues. Experimental results demonstrate that PFGS consistently
outperforms strong baselines in both qualitative and quantitative evaluations,
producing more complete reconstructions and higher-fidelity 3DGS models.

</details>


### [40] [LILAC: Long-sequence Incremental Low-latency Arbitrary Motion Stylization via Streaming VAE-Diffusion with Causal Decoding](https://arxiv.org/abs/2510.15392)
*Peng Ren,Hai Yang*

Main category: cs.CV

TL;DR: LILAC enables real-time generation of long, stylized human motions using a streaming VAE-Diffusion framework with causal decoding, achieving smooth transitions without future frame dependencies.


<details>
  <summary>Details</summary>
Motivation: Existing streaming approaches have computational overhead and temporal stability issues, while latent-space VAE-Diffusion frameworks offer high-quality stylization but are limited to offline processing.

Method: Extends offline motion stylization framework to online setting using latent-space streaming architecture with sliding-window causal design and decoded motion feature injection for smooth transitions.

Result: Achieves long-sequence real-time arbitrary stylization without future frame dependencies or diffusion model modifications, balancing stylization quality and responsiveness on benchmark datasets.

Conclusion: LILAC successfully bridges the gap between offline quality and real-time performance for motion stylization through its streaming architecture.

Abstract: Generating long and stylized human motions in real time is critical for
applications that demand continuous and responsive character control. Despite
its importance, existing streaming approaches often operate directly in the raw
motion space, leading to substantial computational overhead and making it
difficult to maintain temporal stability. In contrast, latent-space
VAE-Diffusion-based frameworks alleviate these issues and achieve high-quality
stylization, but they are generally confined to offline processing. To bridge
this gap, LILAC (Long-sequence Incremental Low-latency Arbitrary Motion
Stylization via Streaming VAE-Diffusion with Causal Decoding) builds upon a
recent high-performing offline framework for arbitrary motion stylization and
extends it to an online setting through a latent-space streaming architecture
with a sliding-window causal design and the injection of decoded motion
features to ensure smooth motion transitions. This architecture enables
long-sequence real-time arbitrary stylization without relying on future frames
or modifying the diffusion model architecture, achieving a favorable balance
between stylization quality and responsiveness as demonstrated by experiments
on benchmark datasets. Supplementary video and examples are available at the
project page: https://pren1.github.io/lilac/

</details>


### [41] [MARIS: Marine Open-Vocabulary Instance Segmentation with Geometric Enhancement and Semantic Alignment](https://arxiv.org/abs/2510.15398)
*Bingyu Li,Feiyu Wang,Da Zhang,Zhiyuan Zhao,Junyu Gao,Xuelong Li*

Main category: cs.CV

TL;DR: This paper introduces MARIS, the first large-scale underwater open-vocabulary instance segmentation benchmark, and proposes a unified framework with geometric and semantic components to address visual degradation and semantic misalignment in underwater scenes.


<details>
  <summary>Details</summary>
Motivation: Existing underwater instance segmentation approaches are limited to close-vocabulary prediction and cannot recognize novel marine categories. Transferring open-vocabulary segmentation from natural images to underwater scenes suffers from severe visual degradation and semantic misalignment.

Method: Proposes a unified framework with two components: Geometric Prior Enhancement Module (GPEM) that leverages part-level and structural cues to maintain object consistency under degraded visual conditions, and Semantic Alignment Injection Mechanism (SAIM) that enriches language embeddings with domain-specific priors.

Result: The framework consistently outperforms existing open-vocabulary baselines in both In-Domain and Cross-Domain settings on the MARIS benchmark.

Conclusion: Establishes a strong foundation for future underwater perception research by addressing key challenges in underwater open-vocabulary instance segmentation.

Abstract: Most existing underwater instance segmentation approaches are constrained by
close-vocabulary prediction, limiting their ability to recognize novel marine
categories. To support evaluation, we introduce \textbf{MARIS}
(\underline{Mar}ine Open-Vocabulary \underline{I}nstance
\underline{S}egmentation), the first large-scale fine-grained benchmark for
underwater Open-Vocabulary (OV) segmentation, featuring a limited set of seen
categories and diverse unseen categories. Although OV segmentation has shown
promise on natural images, our analysis reveals that transfer to underwater
scenes suffers from severe visual degradation (e.g., color attenuation) and
semantic misalignment caused by lack underwater class definitions. To address
these issues, we propose a unified framework with two complementary components.
The Geometric Prior Enhancement Module (\textbf{GPEM}) leverages stable
part-level and structural cues to maintain object consistency under degraded
visual conditions. The Semantic Alignment Injection Mechanism (\textbf{SAIM})
enriches language embeddings with domain-specific priors, mitigating semantic
ambiguity and improving recognition of unseen categories. Experiments show that
our framework consistently outperforms existing OV baselines both In-Domain and
Cross-Domain setting on MARIS, establishing a strong foundation for future
underwater perception research.

</details>


### [42] [Robust High-Resolution Multi-Organ Diffusion MRI Using Synthetic-Data-Tuned Prompt Learning](https://arxiv.org/abs/2510.15400)
*Chen Qian,Haoyu Zhang,Junnan Ma,Liuhong Zhu,Qingrui Cai,Yu Wang,Ruibo Song,Lv Li,Lin Mei,Xianwang Jiang,Qin Xu,Boyu Jiang,Ran Tao,Chunmiao Chen,Shufang Chen,Dongyun Liang,Qiu Guo,Jianzhong Lin,Taishan Kang,Mengtian Lu,Liyuan Fu,Ruibin Huang,Huijuan Wan,Xu Huang,Jianhua Wang,Di Guo,Hai Zhong,Jianjun Zhou,Xiaobo Qu*

Main category: cs.CV

TL;DR: LoSP-Prompt is a reconstruction framework that enables high-resolution multi-shot diffusion-weighted MRI for body-wide tumor diagnostics by overcoming motion artifacts through physics-informed modeling and synthetic-data-driven prompt learning.


<details>
  <summary>Details</summary>
Motivation: Clinical adoption of multi-shot DWI is limited by severe motion-induced phase artifacts from respiration and peristalsis, compounded by multi-organ, multi-slice, multi-direction and multi-b-value complexities.

Method: Models inter-shot phase variations as high-order Locally Smooth Phase (LoSP) integrated into low-rank Hankel matrix reconstruction. The algorithm's rank parameter is automatically set via prompt learning trained exclusively on synthetic abdominal DWI data emulating physiological motion.

Result: Achieved twice the spatial resolution of clinical single-shot DWI, enhanced liver lesion conspicuity, generalized to seven anatomical regions with single model, and outperformed state-of-the-art methods in image quality, artifact suppression, and noise reduction (11 radiologists' evaluations, p<0.05).

Conclusion: Eliminates navigator signals and realistic data supervision, providing interpretable, robust solution for high-resolution multi-organ multi-shot DWI with scanner-agnostic performance that has transformative potential for precision oncology.

Abstract: Clinical adoption of multi-shot diffusion-weighted magnetic resonance imaging
(multi-shot DWI) for body-wide tumor diagnostics is limited by severe
motion-induced phase artifacts from respiration, peristalsis, and so on,
compounded by multi-organ, multi-slice, multi-direction and multi-b-value
complexities. Here, we introduce a reconstruction framework, LoSP-Prompt, that
overcomes these challenges through physics-informed modeling and
synthetic-data-driven prompt learning. We model inter-shot phase variations as
a high-order Locally Smooth Phase (LoSP), integrated into a low-rank Hankel
matrix reconstruction. Crucially, the algorithm's rank parameter is
automatically set via prompt learning trained exclusively on synthetic
abdominal DWI data emulating physiological motion. Validated across 10,000+
clinical images (43 subjects, 4 scanner models, 5 centers), LoSP-Prompt: (1)
Achieved twice the spatial resolution of clinical single-shot DWI, enhancing
liver lesion conspicuity; (2) Generalized to seven diverse anatomical regions
(liver, kidney, sacroiliac, pelvis, knee, spinal cord, brain) with a single
model; (3) Outperformed state-of-the-art methods in image quality, artifact
suppression, and noise reduction (11 radiologists' evaluations on a 5-point
scale, $p<0.05$), achieving 4-5 points (excellent) on kidney DWI, 4 points
(good to excellent) on liver, sacroiliac and spinal cord DWI, and 3-4 points
(good) on knee and tumor brain. The approach eliminates navigator signals and
realistic data supervision, providing an interpretable, robust solution for
high-resolution multi-organ multi-shot DWI. Its scanner-agnostic performance
signifies transformative potential for precision oncology.

</details>


### [43] [Learning to Detect Unknown Jailbreak Attacks in Large Vision-Language Models](https://arxiv.org/abs/2510.15430)
*Shuang Liang,Zhihao Xu,Jialing Tao,Hui Xue,Xiting Wang*

Main category: cs.CV

TL;DR: Learning to Detect (LoD) is a framework that improves detection of unknown jailbreak attacks in Large Vision-Language Models by shifting from attack-specific to task-specific learning, achieving higher AUROC and efficiency.


<details>
  <summary>Details</summary>
Motivation: Existing jailbreak detection methods either lack generalization to unseen attacks or have limited accuracy and efficiency, creating safety vulnerabilities in LVLMs despite alignment efforts.

Method: LoD framework includes Multi-modal Safety Concept Activation Vector for safety-oriented representation learning and Safety Pattern Auto-Encoder for unsupervised attack classification.

Result: Extensive experiments show consistently higher detection AUROC on diverse unknown attacks while improving efficiency compared to existing methods.

Conclusion: The proposed LoD framework effectively addresses jailbreak detection limitations by focusing on task-specific learning rather than attack-specific parameters, providing better generalization and performance.

Abstract: Despite extensive alignment efforts, Large Vision-Language Models (LVLMs)
remain vulnerable to jailbreak attacks, posing serious safety risks. To address
this, existing detection methods either learn attack-specific parameters, which
hinders generalization to unseen attacks, or rely on heuristically sound
principles, which limit accuracy and efficiency. To overcome these limitations,
we propose Learning to Detect (LoD), a general framework that accurately
detects unknown jailbreak attacks by shifting the focus from attack-specific
learning to task-specific learning. This framework includes a Multi-modal
Safety Concept Activation Vector module for safety-oriented representation
learning and a Safety Pattern Auto-Encoder module for unsupervised attack
classification. Extensive experiments show that our method achieves
consistently higher detection AUROC on diverse unknown attacks while improving
efficiency. The code is available at
https://anonymous.4open.science/r/Learning-to-Detect-51CB.

</details>


### [44] [Semantic4Safety: Causal Insights from Zero-shot Street View Imagery Segmentation for Urban Road Safety](https://arxiv.org/abs/2510.15434)
*Huan Chen,Ting Han,Siyu Chen,Zhihao Guo,Yiping Chen,Meiliu Wu*

Main category: cs.CV

TL;DR: Semantic4Safety is a framework that uses zero-shot semantic segmentation on street-view imagery to create 11 streetscape indicators for traffic risk analysis, combining predictive modeling with causal inference to identify accident-specific causal patterns.


<details>
  <summary>Details</summary>
Motivation: Address two key challenges in street-view imagery analysis: constructing street-level indicators that capture accident-related features, and quantifying their causal impacts across different accident types.

Method: Apply zero-shot semantic segmentation to SVIs to derive 11 interpretable streetscape indicators, integrate road type context, train XGBoost multi-class classifier with SHAP interpretation, and use GPS weighting and ATE estimation for causal inference on 30,000 Austin accident records.

Result: Uncovered heterogeneous, accident-type-specific causal patterns: scene complexity, exposure, and roadway geometry features dominate predictive power; larger drivable area and emergency space reduce risk, while excessive visual openness increases risk.

Conclusion: Semantic4Safety bridges predictive modeling with causal inference to support targeted interventions and high-risk corridor diagnosis, offering a scalable, data-informed tool for urban road safety planning.

Abstract: Street-view imagery (SVI) offers a fine-grained lens on traffic risk, yet two
fundamental challenges persist: (1) how to construct street-level indicators
that capture accident-related features, and (2) how to quantify their causal
impacts across different accident types. To address these challenges, we
propose Semantic4Safety, a framework that applies zero-shot semantic
segmentation to SVIs to derive 11 interpretable streetscape indicators, and
integrates road type as contextual information to analyze approximately 30,000
accident records in Austin. Specifically, we train an eXtreme Gradient Boosting
(XGBoost) multi-class classifier and use Shapley Additive Explanations (SHAP)
to interpret both global and local feature contributions, and then apply
Generalized Propensity Score (GPS) weighting and Average Treatment Effect (ATE)
estimation to control confounding and quantify causal effects. Results uncover
heterogeneous, accident-type-specific causal patterns: features capturing scene
complexity, exposure, and roadway geometry dominate predictive power; larger
drivable area and emergency space reduce risk, whereas excessive visual
openness can increase it. By bridging predictive modeling with causal
inference, Semantic4Safety supports targeted interventions and high-risk
corridor diagnosis, offering a scalable, data-informed tool for urban road
safety planning.

</details>


### [45] [Rethinking Convergence in Deep Learning: The Predictive-Corrective Paradigm for Anatomy-Informed Brain MRI Segmentation](https://arxiv.org/abs/2510.15439)
*Feifei Zhang,Zhenhong Jia,Sensen Song,Fei Shi,Dayong Ren*

Main category: cs.CV

TL;DR: PCMambaNet introduces a Predictive-Corrective paradigm that decouples modeling tasks to accelerate learning, achieving state-of-the-art brain MRI segmentation accuracy within only 1-5 epochs.


<details>
  <summary>Details</summary>
Motivation: End-to-end deep learning suffers from slow convergence and heavy reliance on large datasets, limiting efficiency in data-scarce domains like medical imaging.

Method: Uses two modules: Predictive Prior Module generates coarse approximation using anatomical knowledge (bilateral symmetry) to create focus maps, and Corrective Residual Network refines challenging regions and pathological boundaries.

Result: Achieves state-of-the-art accuracy on high-resolution brain MRI segmentation while converging within only 1-5 epochs, significantly faster than conventional end-to-end models.

Conclusion: Explicitly incorporating domain knowledge to simplify learning objectives effectively mitigates data inefficiency and overfitting, enabling dramatic acceleration in convergence.

Abstract: Despite the remarkable success of the end-to-end paradigm in deep learning,
it often suffers from slow convergence and heavy reliance on large-scale
datasets, which fundamentally limits its efficiency and applicability in
data-scarce domains such as medical imaging. In this work, we introduce the
Predictive-Corrective (PC) paradigm, a framework that decouples the modeling
task to fundamentally accelerate learning. Building upon this paradigm, we
propose a novel network, termed PCMambaNet. PCMambaNet is composed of two
synergistic modules. First, the Predictive Prior Module (PPM) generates a
coarse approximation at low computational cost, thereby anchoring the search
space. Specifically, the PPM leverages anatomical knowledge-bilateral
symmetry-to predict a 'focus map' of diagnostically relevant asymmetric
regions. Next, the Corrective Residual Network (CRN) learns to model the
residual error, focusing the network's full capacity on refining these
challenging regions and delineating precise pathological boundaries. Extensive
experiments on high-resolution brain MRI segmentation demonstrate that
PCMambaNet achieves state-of-the-art accuracy while converging within only 1-5
epochs-a performance unattainable by conventional end-to-end models. This
dramatic acceleration highlights that by explicitly incorporating domain
knowledge to simplify the learning objective, PCMambaNet effectively mitigates
data inefficiency and overfitting.

</details>


### [46] [Select Less, Reason More: Prioritizing Evidence Purity for Video Reasoning](https://arxiv.org/abs/2510.15440)
*Xuchen Li,Xuzhao Li,Shiyu Hu,Kaiqi Huang*

Main category: cs.CV

TL;DR: Proposes EARL framework for video reasoning that dynamically selects relevant frames and performs localized re-sampling to improve evidence purity and temporal detail access.


<details>
  <summary>Details</summary>
Motivation: Address limitations of existing Video LLMs where uniform frame sampling causes information dilution and obscures critical evidence, and pixel-space agents lack rigorous reward mechanisms for evidence purity.

Method: Evidence-aware reinforcement learning (EARL) framework that transforms models into active interrogators of evidence, dynamically selecting relevant frames and performing localized re-sampling around key frames.

Result: Achieves state-of-the-art performance on five video reasoning benchmarks: 59.8% on LongVideoBench, 69.0% on MVBench, and 64.9% on VideoMME with 7B model.

Conclusion: Demonstrates importance of prioritizing evidence purity and effectiveness of the EARL framework for long-form video reasoning.

Abstract: Long-form video reasoning remains a major challenge for Video Large Language
Models (Video LLMs), as static uniform frame sampling leads to information
dilution and obscures critical evidence. Furthermore, existing pixel-space
video reasoning agents, which are designed to actively interact with the video
to acquire new visual information, remain suboptimal due to their lack of
rigorous reward mechanisms to enforce evidence purity and their inability to
perform temporal information supplementation beyond pre-sampled frames. To
address this critical gap, we propose a novel evidence-prioritized adaptive
framework built upon our core philosophy: "Select Less, Reason More." Our core
contribution is the evidence-aware reinforcement learning (EARL) framework,
which transforms the model into an active interrogator of evidence. EARL is
precisely engineered to dynamically select the most relevant frames and,
crucially, to perform localized re-sampling around the selected key frames to
access fine-grained temporal detail. Extensive experiments on five demanding
video reasoning benchmarks demonstrate that our EARL-trained model achieves new
state-of-the-art among open-source Video LLMs, simultaneously learning an
effective and high-purity visual evidence selection policy. Impressively, our
7B model achieves 59.8% on LongVideoBench, 69.0% on MVBench and 64.9% on
VideoMME. These results highlight the importance of prioritizing evidence
purity and the effectiveness of our framework.

</details>


### [47] [MAVR-Net: Robust Multi-View Learning for MAV Action Recognition with Cross-View Attention](https://arxiv.org/abs/2510.15448)
*Nengbo Zhang,Hann Woei Ho*

Main category: cs.CV

TL;DR: MAVR-Net is a multi-view learning framework that combines RGB frames, optical flow, and segmentation masks to improve Micro Aerial Vehicle (MAV) action recognition, achieving state-of-the-art accuracy on benchmark datasets.


<details>
  <summary>Details</summary>
Motivation: Vision-based MAV motion recognition models using only RGB data often fail to capture complex spatial-temporal characteristics, limiting their ability to distinguish different actions effectively.

Method: Uses ResNet-based encoders to extract features from RGB, optical flow, and segmentation masks; employs multi-scale feature pyramid for spatiotemporal details; introduces cross-view attention module for modality dependencies; and applies multi-view alignment loss for semantic consistency.

Result: Achieved 97.8%, 96.5%, and 92.8% accuracy on Short MAV, Medium MAV, and Long MAV datasets respectively, clearly outperforming existing approaches.

Conclusion: The multi-view learning approach with complementary data types and cross-view attention significantly improves MAV action recognition robustness and accuracy.

Abstract: Recognizing the motion of Micro Aerial Vehicles (MAVs) is crucial for
enabling cooperative perception and control in autonomous aerial swarms. Yet,
vision-based recognition models relying only on RGB data often fail to capture
the complex spatial temporal characteristics of MAV motion, which limits their
ability to distinguish different actions. To overcome this problem, this paper
presents MAVR-Net, a multi-view learning-based MAV action recognition
framework. Unlike traditional single-view methods, the proposed approach
combines three complementary types of data, including raw RGB frames, optical
flow, and segmentation masks, to improve the robustness and accuracy of MAV
motion recognition. Specifically, ResNet-based encoders are used to extract
discriminative features from each view, and a multi-scale feature pyramid is
adopted to preserve the spatiotemporal details of MAV motion patterns. To
enhance the interaction between different views, a cross-view attention module
is introduced to model the dependencies among various modalities and feature
scales. In addition, a multi-view alignment loss is designed to ensure semantic
consistency and strengthen cross-view feature representations. Experimental
results on benchmark MAV action datasets show that our method clearly
outperforms existing approaches, achieving 97.8\%, 96.5\%, and 92.8\% accuracy
on the Short MAV, Medium MAV, and Long MAV datasets, respectively.

</details>


### [48] [DPTrack:Directional Kernel-Guided Prompt Learning for Robust Nighttime Aerial Tracking](https://arxiv.org/abs/2510.15449)
*Zhiqiang Zhu,Xinbo Gao,Wen Lu,Jie Li,Zhaoyang Wang,Mingqian Ge*

Main category: cs.CV

TL;DR: DPTrack is a prompt-based aerial tracker for nighttime scenarios that encodes object attribute features into directional kernels with fine-grained cues to generate precise prompts, improving tracking accuracy.


<details>
  <summary>Details</summary>
Motivation: Existing nighttime aerial trackers based on prompt learning rely solely on spatial localization supervision, which fails to provide fine-grained cues and produces vague prompts, impairing the tracker's ability to accurately focus on object features.

Method: DPTrack hierarchically captures object's topological structure, leverages topological attributes to enrich feature representation, condenses these features into directional kernel, and uses kernel-guided prompt module with channel-category correspondence to propagate kernel across search region features to generate precise prompts with spatial gating.

Result: Extensive evaluations on established benchmarks demonstrate DPTrack's superior performance compared to existing nighttime aerial trackers.

Conclusion: DPTrack effectively addresses the limitation of vague prompts in nighttime aerial tracking by encoding fine-grained attribute cues into directional kernels, enabling more accurate target feature localization and robust nighttime tracking.

Abstract: Existing nighttime aerial trackers based on prompt learning rely solely on
spatial localization supervision, which fails to provide fine-grained cues that
point to target features and inevitably produces vague prompts. This limitation
impairs the tracker's ability to accurately focus on the object features and
results in trackers still performing poorly. To address this issue, we propose
DPTrack, a prompt-based aerial tracker designed for nighttime scenarios by
encoding the given object's attribute features into the directional kernel
enriched with fine-grained cues to generate precise prompts. Specifically,
drawing inspiration from visual bionics, DPTrack first hierarchically captures
the object's topological structure, leveraging topological attributes to enrich
the feature representation. Subsequently, an encoder condenses these
topology-aware features into the directional kernel, which serves as the core
guidance signal that explicitly encapsulates the object's fine-grained
attribute cues. Finally, a kernel-guided prompt module built on
channel-category correspondence attributes propagates the kernel across the
features of the search region to pinpoint the positions of target features and
convert them into precise prompts, integrating spatial gating for robust
nighttime tracking. Extensive evaluations on established benchmarks demonstrate
DPTrack's superior performance. Our code will be available at
https://github.com/zzq-vipsl/DPTrack.

</details>


### [49] [Improving Micro-Expression Recognition with Phase-Aware Temporal Augmentation](https://arxiv.org/abs/2510.15466)
*Vu Tram Anh Khuong,Luu Tu Nguyen,Thanh Ha Le,Thi Duyen Ngo*

Main category: cs.CV

TL;DR: A phase-aware temporal augmentation method for micro-expression recognition that decomposes expressions into onset-to-apex and apex-to-offset phases, generating separate dynamic images for each phase to improve motion diversity and recognition accuracy.


<details>
  <summary>Details</summary>
Motivation: Micro-expression recognition is limited by scarce annotated datasets and existing methods rely mainly on simple spatial augmentations, overlooking temporal strategies that better exploit motion characteristics.

Method: Proposes dual-phase dynamic image augmentation that decomposes each expression sequence into onset-to-apex and apex-to-offset phases, generating separate dynamic images for each phase to enrich motion diversity and temporal cues.

Result: Experiments on CASME-II and SAMM datasets with six deep architectures show consistent improvements in accuracy, F1-score, and recall. Combined with spatial augmentations, achieves up to 10% relative improvement.

Conclusion: The proposed phase-aware temporal augmentation is simple, model-agnostic, effective in low-resource settings, and offers a promising direction for robust and generalizable micro-expression recognition.

Abstract: Micro-expressions (MEs) are brief, involuntary facial movements that reveal
genuine emotions, typically lasting less than half a second. Recognizing these
subtle expressions is critical for applications in psychology, security, and
behavioral analysis. Although deep learning has enabled significant advances in
micro-expression recognition (MER), its effectiveness is limited by the
scarcity of annotated ME datasets. This data limitation not only hinders
generalization but also restricts the diversity of motion patterns captured
during training. Existing MER studies predominantly rely on simple spatial
augmentations (e.g., flipping, rotation) and overlook temporal augmentation
strategies that can better exploit motion characteristics. To address this gap,
this paper proposes a phase-aware temporal augmentation method based on dynamic
image. Rather than encoding the entire expression as a single onset-to-offset
dynamic image (DI), our approach decomposes each expression sequence into two
motion phases: onset-to-apex and apex-to-offset. A separate DI is generated for
each phase, forming a Dual-phase DI augmentation strategy. These phase-specific
representations enrich motion diversity and introduce complementary temporal
cues that are crucial for recognizing subtle facial transitions. Extensive
experiments on CASME-II and SAMM datasets using six deep architectures,
including CNNs, Vision Transformer, and the lightweight LEARNet, demonstrate
consistent performance improvements in recognition accuracy, unweighted
F1-score, and unweighted average recall, which are crucial for addressing class
imbalance in MER. When combined with spatial augmentations, our method achieves
up to a 10\% relative improvement. The proposed augmentation is simple,
model-agnostic, and effective in low-resource settings, offering a promising
direction for robust and generalizable MER.

</details>


### [50] [MRASfM: Multi-Camera Reconstruction and Aggregation through Structure-from-Motion in Driving Scenes](https://arxiv.org/abs/2510.15467)
*Lingfeng Xuan,Chang Nie,Yiqing Xu,Zhe Liu,Yanzi Miao,Hesheng Wang*

Main category: cs.CV

TL;DR: MRASfM is a novel Structure from Motion framework for multi-camera driving scenes that improves pose estimation reliability, removes road surface outliers using plane models, and boosts efficiency through unified Bundle Adjustment.


<details>
  <summary>Details</summary>
Motivation: Standard SfM struggles with driving scenes due to unreliable pose estimation, excessive road surface outliers, and low reconstruction efficiency in multi-camera systems.

Method: Leverages fixed multi-camera spatial relationships for reliable pose estimation, uses plane models to remove erroneous road points, treats multi-camera set as single unit in Bundle Adjustment, and employs coarse-to-fine scene association and assembly modules.

Result: Achieves state-of-the-art performance with 0.124 absolute pose error on nuScenes dataset, validated across various real-world driving scenes and challenging conditions.

Conclusion: MRASfM effectively addresses SfM limitations in driving scenes through specialized multi-camera reconstruction and aggregation, demonstrating robust performance and generalizability.

Abstract: Structure from Motion (SfM) estimates camera poses and reconstructs point
clouds, forming a foundation for various tasks. However, applying SfM to
driving scenes captured by multi-camera systems presents significant
difficulties, including unreliable pose estimation, excessive outliers in road
surface reconstruction, and low reconstruction efficiency. To address these
limitations, we propose a Multi-camera Reconstruction and Aggregation
Structure-from-Motion (MRASfM) framework specifically designed for driving
scenes. MRASfM enhances the reliability of camera pose estimation by leveraging
the fixed spatial relationships within the multi-camera system during the
registration process. To improve the quality of road surface reconstruction,
our framework employs a plane model to effectively remove erroneous points from
the triangulated road surface. Moreover, treating the multi-camera set as a
single unit in Bundle Adjustment (BA) helps reduce optimization variables to
boost efficiency. In addition, MRASfM achieves multi-scene aggregation through
scene association and assembly modules in a coarse-to-fine fashion. We deployed
multi-camera systems on actual vehicles to validate the generalizability of
MRASfM across various scenes and its robustness in challenging conditions
through real-world applications. Furthermore, large-scale validation results on
public datasets show the state-of-the-art performance of MRASfM, achieving
0.124 absolute pose error on the nuScenes dataset.

</details>


### [51] [MSAM: Multi-Semantic Adaptive Mining for Cross-Modal Drone Video-Text Retrieval](https://arxiv.org/abs/2510.15470)
*Jinghao Huang,Yaxiong Chen,Ganchao Liu*

Main category: cs.CV

TL;DR: Proposes MSAM, a novel multi-semantic adaptive mining approach for drone video-text retrieval that addresses unique challenges of drone videos through dynamic frame analysis and targeted semantic extraction.


<details>
  <summary>Details</summary>
Motivation: Drone videos present unique challenges with overhead perspectives, structural homogeneity, and diverse semantic combinations that existing cross-modal methods designed for ground-level views cannot effectively handle, necessitating dedicated retrieval mechanisms for drone scenarios.

Method: MSAM introduces multi-semantic adaptive learning with dynamic frame analysis, adaptive semantic construction module, distribution-driven semantic learning, diversity semantic term, and cross-modal interactive feature fusion pooling to focus on target regions and minimize background noise.

Result: Extensive experiments on two self-constructed drone video-text datasets demonstrate that MSAM outperforms existing methods in drone video-text retrieval tasks.

Conclusion: The proposed MSAM approach effectively addresses the unique challenges of drone video-text retrieval through specialized mechanisms for semantic mining and cross-modal interaction, achieving superior performance compared to existing methods.

Abstract: With the advancement of drone technology, the volume of video data increases
rapidly, creating an urgent need for efficient semantic retrieval. We are the
first to systematically propose and study the drone video-text retrieval (DVTR)
task. Drone videos feature overhead perspectives, strong structural
homogeneity, and diverse semantic expressions of target combinations, which
challenge existing cross-modal methods designed for ground-level views in
effectively modeling their characteristics. Therefore, dedicated retrieval
mechanisms tailored for drone scenarios are necessary. To address this issue,
we propose a novel approach called Multi-Semantic Adaptive Mining (MSAM). MSAM
introduces a multi-semantic adaptive learning mechanism, which incorporates
dynamic changes between frames and extracts rich semantic information from
specific scene regions, thereby enhancing the deep understanding and reasoning
of drone video content. This method relies on fine-grained interactions between
words and drone video frames, integrating an adaptive semantic construction
module, a distribution-driven semantic learning term and a diversity semantic
term to deepen the interaction between text and drone video modalities and
improve the robustness of feature representation. To reduce the interference of
complex backgrounds in drone videos, we introduce a cross-modal interactive
feature fusion pooling mechanism that focuses on feature extraction and
matching in target regions, minimizing noise effects. Extensive experiments on
two self-constructed drone video-text datasets show that MSAM outperforms other
existing methods in the drone video-text retrieval task. The source code and
dataset will be made publicly available.

</details>


### [52] [A Novel Combined Optical Flow Approach for Comprehensive Micro-Expression Recognition](https://arxiv.org/abs/2510.15471)
*Vu Tram Anh Khuong,Thi Bich Phuong Man,Luu Tu Nguyen,Thanh Ha Le,Thi Duyen Ngo*

Main category: cs.CV

TL;DR: This paper introduces Combined Optical Flow (COF) that integrates both onset-to-apex and apex-to-offset phases for better micro-expression recognition, outperforming single optical flow methods.


<details>
  <summary>Details</summary>
Motivation: Most Micro-Expression Recognition methods focus only on onset-to-apex phase and neglect the apex-to-offset phase, which contains important temporal dynamics for emotion recognition.

Method: Proposed Combined Optical Flow (COF) that integrates both onset-to-apex and apex-to-offset phases to provide more comprehensive motion analysis and enhance feature representation.

Result: Experimental results on CASMEII and SAMM datasets show that COF outperforms single optical flow-based methods in micro-expression recognition performance.

Conclusion: COF is effective in capturing micro-expression dynamics by comprehensively analyzing both temporal phases of facial micro-expressions.

Abstract: Facial micro-expressions are brief, involuntary facial movements that reveal
hidden emotions. Most Micro-Expression Recognition (MER) methods that rely on
optical flow typically focus on the onset-to-apex phase, neglecting the
apex-to-offset phase, which holds key temporal dynamics. This study introduces
a Combined Optical Flow (COF), integrating both phases to enhance feature
representation. COF provides a more comprehensive motion analysis, improving
MER performance. Experimental results on CASMEII and SAMM datasets show that
COF outperforms single optical flow-based methods, demonstrating its
effectiveness in capturing micro-expression dynamics.

</details>


### [53] [Iterative Motion Compensation for Canonical 3D Reconstruction from UAV Plant Images Captured in Windy Conditions](https://arxiv.org/abs/2510.15491)
*Andre Rochow,Jonas Marcic,Svetlana Seliunina,Sven Behnke*

Main category: cs.CV

TL;DR: A pipeline for high-quality 3D reconstruction of agricultural plants using autonomous UAV image acquisition and iterative motion correction to handle wind and downwash effects.


<details>
  <summary>Details</summary>
Motivation: 3D phenotyping is crucial for understanding plant growth, yield prediction, and disease control, but challenging due to environmental wind and UAV downwash.

Method: Autonomous UAV image acquisition with ArUco markers, iterative deformation using optical flow to align images with 3D reconstructions, and integration of state-of-the-art 3D reconstruction methods.

Result: The pipeline improves reconstruction quality of state-of-the-art methods, enables extraction of high-resolution 3D meshes, and provides a public dataset of multiple crops captured over time.

Conclusion: The proposed pipeline successfully addresses motion challenges in plant 3D reconstruction and will be publicly released along with a comprehensive plant dataset.

Abstract: 3D phenotyping of plants plays a crucial role for understanding plant growth,
yield prediction, and disease control. We present a pipeline capable of
generating high-quality 3D reconstructions of individual agricultural plants.
To acquire data, a small commercially available UAV captures images of a
selected plant. Apart from placing ArUco markers, the entire image acquisition
process is fully autonomous, controlled by a self-developed Android application
running on the drone's controller. The reconstruction task is particularly
challenging due to environmental wind and downwash of the UAV. Our proposed
pipeline supports the integration of arbitrary state-of-the-art 3D
reconstruction methods. To mitigate errors caused by leaf motion during image
capture, we use an iterative method that gradually adjusts the input images
through deformation. Motion is estimated using optical flow between the
original input images and intermediate 3D reconstructions rendered from the
corresponding viewpoints. This alignment gradually reduces scene motion,
resulting in a canonical representation. After a few iterations, our pipeline
improves the reconstruction of state-of-the-art methods and enables the
extraction of high-resolution 3D meshes. We will publicly release the source
code of our reconstruction pipeline. Additionally, we provide a dataset
consisting of multiple plants from various crops, captured across different
points in time.

</details>


### [54] [Rethinking Efficient Hierarchical Mixing Architecture for Low-light RAW Image Enhancement](https://arxiv.org/abs/2510.15497)
*Xianmin Chen,Peiliang Huang,Longfei Han,Dingwen Zhang,Junwei Han*

Main category: cs.CV

TL;DR: A hierarchical architecture combining Transformer and Mamba modules for efficient low-light RAW image enhancement, with local distribution adjustment and multi-prior fusion to handle uneven illumination and enhance details.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of achieving both strong enhancement quality and high efficiency in low-light RAW image processing, overcoming limitations of existing deep learning approaches and ambiguities in prior two-stage frameworks.

Method: Proposes HiMA (Hierarchical Mixing Architecture) using Transformer for large-scale features and Mamba for small-scale features, with Local Distribution Adjustment (LoDA) for uneven illumination and Multi-prior Fusion (MPF) integrating spatial and frequency-domain priors.

Result: Extensive experiments on multiple public datasets show superior performance over state-of-the-art methods with fewer parameters.

Conclusion: The proposed HiMA framework effectively addresses low-light RAW image enhancement challenges, achieving better performance and efficiency than existing approaches.

Abstract: Low-light RAW image enhancement remains a challenging task. Although numerous
deep learning based approaches have been proposed, they still suffer from
inherent limitations. A key challenge is how to simultaneously achieve strong
enhancement quality and high efficiency. In this paper, we rethink the
architecture for efficient low-light image signal processing (ISP) and
introduce a Hierarchical Mixing Architecture (HiMA). HiMA leverages the
complementary strengths of Transformer and Mamba modules to handle features at
large and small scales, respectively, thereby improving efficiency while
avoiding the ambiguities observed in prior two-stage frameworks. To further
address uneven illumination with strong local variations, we propose Local
Distribution Adjustment (LoDA), which adaptively aligns feature distributions
across different local regions. In addition, to fully exploit the denoised
outputs from the first stage, we design a Multi-prior Fusion (MPF) module that
integrates spatial and frequency-domain priors for detail enhancement.
Extensive experiments on multiple public datasets demonstrate that our method
outperforms state-of-the-art approaches, achieving superior performance with
fewer parameters. Code will be released at https://github.com/Cynicarlos/HiMA.

</details>


### [55] [Exploring Conditions for Diffusion models in Robotic Control](https://arxiv.org/abs/2510.15510)
*Heeseong Shin,Byeongho Heo,Dongyoon Han,Seungryong Kim,Taekyung Kim*

Main category: cs.CV

TL;DR: ORCA introduces learnable task and visual prompts to adapt pre-trained text-to-image diffusion models for robotic control, achieving state-of-the-art performance without fine-tuning the base model.


<details>
  <summary>Details</summary>
Motivation: Pre-trained visual representations are task-agnostic when frozen during policy learning, and naive textual conditioning fails in robotic control due to domain gaps between diffusion training data and control environments.

Method: Proposes ORCA with learnable task prompts that adapt to control environment and visual prompts that capture frame-specific details, enabling task-adaptive representations from pre-trained diffusion models.

Result: Achieves state-of-the-art performance on various robotic control benchmarks, significantly surpassing prior methods.

Conclusion: Task-adaptive visual representations from pre-trained diffusion models can be effectively obtained through carefully designed conditions that consider dynamic visual information required for control, rather than naive textual conditioning.

Abstract: While pre-trained visual representations have significantly advanced
imitation learning, they are often task-agnostic as they remain frozen during
policy learning. In this work, we explore leveraging pre-trained text-to-image
diffusion models to obtain task-adaptive visual representations for robotic
control, without fine-tuning the model itself. However, we find that naively
applying textual conditions - a successful strategy in other vision domains -
yields minimal or even negative gains in control tasks. We attribute this to
the domain gap between the diffusion model's training data and robotic control
environments, leading us to argue for conditions that consider the specific,
dynamic visual information required for control. To this end, we propose ORCA,
which introduces learnable task prompts that adapt to the control environment
and visual prompts that capture fine-grained, frame-specific details. Through
facilitating task-adaptive representations with our newly devised conditions,
our approach achieves state-of-the-art performance on various robotic control
benchmarks, significantly surpassing prior methods.

</details>


### [56] [Latent Feature Alignment: Discovering Biased and Interpretable Subpopulations in Face Recognition Models](https://arxiv.org/abs/2510.15520)
*Ignacio Serna*

Main category: cs.CV

TL;DR: LFA is an attribute-label-free algorithm that uses latent directions to identify biased subpopulations in face recognition models, outperforming conventional clustering methods in semantic coherence and interpretability.


<details>
  <summary>Details</summary>
Motivation: Modern face recognition models exhibit systematic biases affecting certain subpopulations, but conventional bias evaluation requires expensive labeled attributes limited to predefined categories.

Method: Latent Feature Alignment (LFA) uses latent directions to identify subpopulations, enabling semantically coherent grouping and discovery of interpretable directions corresponding to attributes like age, ethnicity, or attire.

Result: LFA consistently outperforms k-means and nearest-neighbor search in intra-group semantic coherence across four state-of-the-art recognition models and two benchmarks, while uncovering interpretable latent directions aligned with demographic and contextual attributes.

Conclusion: LFA provides a practical method for representation auditing of face recognition models, enabling identification and interpretation of biased subpopulations without predefined attribute annotations.

Abstract: Modern face recognition models achieve high overall accuracy but continue to
exhibit systematic biases that disproportionately affect certain
subpopulations. Conventional bias evaluation frameworks rely on labeled
attributes to form subpopulations, which are expensive to obtain and limited to
predefined categories. We introduce Latent Feature Alignment (LFA), an
attribute-label-free algorithm that uses latent directions to identify
subpopulations. This yields two main benefits over standard clustering: (i)
semantically coherent grouping, where faces sharing common attributes are
grouped together more reliably than by proximity-based methods, and (ii)
discovery of interpretable directions, which correspond to semantic attributes
such as age, ethnicity, or attire. Across four state-of-the-art recognition
models (ArcFace, CosFace, ElasticFace, PartialFC) and two benchmarks (RFW,
CelebA), LFA consistently outperforms k-means and nearest-neighbor search in
intra-group semantic coherence, while uncovering interpretable latent
directions aligned with demographic and contextual attributes. These results
position LFA as a practical method for representation auditing of face
recognition models, enabling practitioners to identify and interpret biased
subpopulations without predefined attribute annotations.

</details>


### [57] [Balanced Multi-Task Attention for Satellite Image Classification: A Systematic Approach to Achieving 97.23% Accuracy on EuroSAT Without Pre-Training](https://arxiv.org/abs/2510.15527)
*Aditya Vir*

Main category: cs.CV

TL;DR: Custom CNN architectures for satellite land use classification achieve 97.23% accuracy on EuroSAT without pre-trained models, using a novel balanced multi-task attention mechanism that combines spatial and spectral feature extraction.


<details>
  <summary>Details</summary>
Motivation: To develop specialized convolutional neural network architectures for satellite imagery classification that don't rely on pre-trained models, addressing specific failure modes in satellite land use classification.

Method: Three progressive architectural iterations: baseline CNN, CBAM-enhanced version, and final balanced multi-task attention mechanism combining Coordinate Attention for spatial features and Squeeze-Excitation blocks for spectral features with learnable fusion parameter. Uses progressive DropBlock regularization and class-balanced loss weighting.

Result: Achieved 97.23% test accuracy on EuroSAT dataset, Cohen's Kappa of 0.9692, with all classes exceeding 94.46% accuracy. Learnable fusion parameter converged to alpha ≈ 0.57, showing near-equal importance of spatial and spectral modalities. Performance within 1.34% of fine-tuned ResNet-50.

Conclusion: Systematic architectural design for domain-specific applications can achieve competitive performance without external data or pre-trained models, validating the efficacy of custom CNN architectures for satellite imagery classification.

Abstract: This work presents a systematic investigation of custom convolutional neural
network architectures for satellite land use classification, achieving 97.23%
test accuracy on the EuroSAT dataset without reliance on pre-trained models.
Through three progressive architectural iterations (baseline: 94.30%,
CBAM-enhanced: 95.98%, and balanced multi-task attention: 97.23%) we identify
and address specific failure modes in satellite imagery classification. Our
principal contribution is a novel balanced multi-task attention mechanism that
combines Coordinate Attention for spatial feature extraction with
Squeeze-Excitation blocks for spectral feature extraction, unified through a
learnable fusion parameter. Experimental results demonstrate that this
learnable parameter autonomously converges to alpha approximately 0.57,
indicating near-equal importance of spatial and spectral modalities for
satellite imagery. We employ progressive DropBlock regularization (5-20% by
network depth) and class-balanced loss weighting to address overfitting and
confusion pattern imbalance. The final 12-layer architecture achieves Cohen's
Kappa of 0.9692 with all classes exceeding 94.46% accuracy, demonstrating
confidence calibration with a 24.25% gap between correct and incorrect
predictions. Our approach achieves performance within 1.34% of fine-tuned
ResNet-50 (98.57%) while requiring no external data, validating the efficacy of
systematic architectural design for domain-specific applications. Complete
code, trained models, and evaluation scripts are publicly available.

</details>


### [58] [Diffusion Bridge Networks Simulate Clinical-grade PET from MRI for Dementia Diagnostics](https://arxiv.org/abs/2510.15556)
*Yitong Li,Ralph Buchert,Benita Schmitz-Koep,Timo Grimmer,Björn Ommer,Dennis M. Hedderich,Igor Yakushev,Christian Wachinger*

Main category: cs.CV

TL;DR: SiM2P is a 3D diffusion framework that generates synthetic FDG-PET images from MRI and patient data, significantly improving dementia diagnosis accuracy from 75.0% to 84.7% in clinical trials.


<details>
  <summary>Details</summary>
Motivation: FDG-PET is effective for dementia diagnosis but less accessible and more expensive than MRI. SiM2P aims to make PET's diagnostic benefits more widely available by simulating PET images from routine MRI scans.

Method: 3D diffusion bridge-based framework that learns probabilistic mapping from MRI and patient information to generate synthetic FDG-PET images. Uses as few as 20 site-specific cases with basic demographic data.

Result: Clinical study showed diagnostic accuracy improved from 75.0% to 84.7% for differentiating Alzheimer's, frontotemporal dementia, and healthy controls. Simulated PET received higher diagnostic certainty and better interrater agreement than MRI alone.

Conclusion: SiM2P provides a practical solution to make FDG-PET's diagnostic advantages more accessible, potentially improving early dementia detection and differential diagnosis in resource-limited settings.

Abstract: Positron emission tomography (PET) with 18F-Fluorodeoxyglucose (FDG) is an
established tool in the diagnostic workup of patients with suspected dementing
disorders. However, compared to the routinely available magnetic resonance
imaging (MRI), FDG-PET remains significantly less accessible and substantially
more expensive. Here, we present SiM2P, a 3D diffusion bridge-based framework
that learns a probabilistic mapping from MRI and auxiliary patient information
to simulate FDG-PET images of diagnostic quality. In a blinded clinical reader
study, two neuroradiologists and two nuclear medicine physicians rated the
original MRI and SiM2P-simulated PET images of patients with Alzheimer's
disease, behavioral-variant frontotemporal dementia, and cognitively healthy
controls. SiM2P significantly improved the overall diagnostic accuracy of
differentiating between three groups from 75.0% to 84.7% (p<0.05). Notably, the
simulated PET images received higher diagnostic certainty ratings and achieved
superior interrater agreement compared to the MRI images. Finally, we developed
a practical workflow for local deployment of the SiM2P framework. It requires
as few as 20 site-specific cases and only basic demographic information. This
approach makes the established diagnostic benefits of FDG-PET imaging more
accessible to patients with suspected dementing disorders, potentially
improving early detection and differential diagnosis in resource-limited
settings. Our code is available at https://github.com/Yiiitong/SiM2P.

</details>


### [59] [Imaginarium: Vision-guided High-Quality 3D Scene Layout Generation](https://arxiv.org/abs/2510.15564)
*Xiaoming Zhu,Xu Huang,Qinghongbing Xie,Zhi Deng,Junsheng Yu,Yirui Guan,Zhongyuan Liu,Lin Zhu,Qijun Zhao,Ligang Liu,Long Zeng*

Main category: cs.CV

TL;DR: A vision-guided 3D layout generation system that uses image generation and parsing to create coherent 3D scenes, outperforming existing methods in layout richness and quality.


<details>
  <summary>Details</summary>
Motivation: Traditional optimization methods are constrained by manual rules, deep generative models struggle with richness/diversity, and LLM-based approaches lack robustness in capturing spatial relationships.

Method: Build asset library, use image generation model expanded from prompts and fine-tuned for assets, develop image parsing module for 3D layout recovery, optimize layout using scene graphs and visual semantics.

Result: Extensive user testing shows significant outperformance over existing methods in layout richness and quality.

Conclusion: The proposed vision-guided system effectively addresses limitations of previous approaches and generates high-quality 3D scene layouts.

Abstract: Generating artistic and coherent 3D scene layouts is crucial in digital
content creation. Traditional optimization-based methods are often constrained
by cumbersome manual rules, while deep generative models face challenges in
producing content with richness and diversity. Furthermore, approaches that
utilize large language models frequently lack robustness and fail to accurately
capture complex spatial relationships. To address these challenges, this paper
presents a novel vision-guided 3D layout generation system. We first construct
a high-quality asset library containing 2,037 scene assets and 147 3D scene
layouts. Subsequently, we employ an image generation model to expand prompt
representations into images, fine-tuning it to align with our asset library. We
then develop a robust image parsing module to recover the 3D layout of scenes
based on visual semantics and geometric information. Finally, we optimize the
scene layout using scene graphs and overall visual semantics to ensure logical
coherence and alignment with the images. Extensive user testing demonstrates
that our algorithm significantly outperforms existing methods in terms of
layout richness and quality. The code and dataset will be available at
https://github.com/HiHiAllen/Imaginarium.

</details>


### [60] [Unmasking Facial DeepFakes: A Robust Multiview Detection Framework for Natural Images](https://arxiv.org/abs/2510.15576)
*Sami Belguesmia,Mohand Saïd Allili,Assia Hamadene*

Main category: cs.CV

TL;DR: A multi-view architecture for DeepFake detection that analyzes facial features at global, middle, and local levels with specialized encoders, plus a face orientation encoder for pose robustness.


<details>
  <summary>Details</summary>
Motivation: Existing DeepFake detection methods struggle with pose variations, occlusions, and artifacts in real-world conditions, requiring more robust detection approaches.

Method: Proposes a multi-view architecture with three specialized encoders: global view for boundary inconsistencies, middle view for texture/color alignment, and local view for distortions in expressive facial regions, plus a face orientation encoder for pose classification.

Result: Experimental results show superior performance in detecting manipulated images under challenging pose and lighting conditions, outperforming conventional single-view approaches.

Conclusion: The multi-view architecture with specialized encoders effectively enhances DeepFake detection robustness across various viewing angles and challenging conditions.

Abstract: DeepFake technology has advanced significantly in recent years, enabling the
creation of highly realistic synthetic face images. Existing DeepFake detection
methods often struggle with pose variations, occlusions, and artifacts that are
difficult to detect in real-world conditions. To address these challenges, we
propose a multi-view architecture that enhances DeepFake detection by analyzing
facial features at multiple levels. Our approach integrates three specialized
encoders, a global view encoder for detecting boundary inconsistencies, a
middle view encoder for analyzing texture and color alignment, and a local view
encoder for capturing distortions in expressive facial regions such as the
eyes, nose, and mouth, where DeepFake artifacts frequently occur. Additionally,
we incorporate a face orientation encoder, trained to classify face poses,
ensuring robust detection across various viewing angles. By fusing features
from these encoders, our model achieves superior performance in detecting
manipulated images, even under challenging pose and lighting
conditions.Experimental results on challenging datasets demonstrate the
effectiveness of our method, outperforming conventional single-view approaches

</details>


### [61] [Lightweight CycleGAN Models for Cross-Modality Image Transformation and Experimental Quality Assessment in Fluorescence Microscopy](https://arxiv.org/abs/2510.15579)
*Mohammad Soltaninezhad,Yashar Rouzbahani,Jhonatan Contreras,Rohan Chippalkatti,Daniel Kwaku Abankwa,Christian Eggeling,Thomas Bocklitz*

Main category: cs.CV

TL;DR: A lightweight CycleGAN for fluorescence microscopy modality transfer that reduces parameters from 41.8M to ~9K while maintaining performance, and serves as a diagnostic tool for image quality assessment.


<details>
  <summary>Details</summary>
Motivation: To address the computational cost and environmental impact of deep learning models, and solve the challenge of unpaired datasets in microscopy modality transfer while providing a tool for experimental quality validation.

Method: Modified CycleGAN architecture by replacing channel-doubling in U-Net generator with fixed channel approach, significantly reducing parameters. Uses GAN as diagnostic tool to detect imaging issues by comparing generated outputs with experimental images.

Result: Achieved drastic parameter reduction from 41.8 million to approximately 9,000 while maintaining superior performance. Faster training and lower memory usage. Successfully detected imaging issues like photobleaching, artifacts, and inaccurate labeling.

Conclusion: The lightweight CycleGAN provides efficient modality transfer for microscopy and serves as a practical diagnostic tool for validating experimental accuracy and image fidelity in microscopy workflows.

Abstract: Lightweight deep learning models offer substantial reductions in
computational cost and environmental impact, making them crucial for scientific
applications. We present a lightweight CycleGAN for modality transfer in
fluorescence microscopy (confocal to super-resolution STED/deconvolved STED),
addressing the common challenge of unpaired datasets. By replacing the
traditional channel-doubling strategy in the U-Net-based generator with a fixed
channel approach, we drastically reduce trainable parameters from 41.8 million
to approximately nine thousand, achieving superior performance with faster
training and lower memory usage. We also introduce the GAN as a diagnostic tool
for experimental and labeling quality. When trained on high-quality images, the
GAN learns the characteristics of optimal imaging; deviations between its
generated outputs and new experimental images can reveal issues such as
photobleaching, artifacts, or inaccurate labeling. This establishes the model
as a practical tool for validating experimental accuracy and image fidelity in
microscopy workflows.

</details>


### [62] [Standardization for improved Spatio-Temporal Image Fusion](https://arxiv.org/abs/2510.15589)
*Harkaitz Goyena,Peter M. Atkinson,Unai Pérez-Goya,M. Dolores Ugarte*

Main category: cs.CV

TL;DR: The paper proposes two standardization approaches for Spatio-Temporal Image Fusion (STIF) methods: traditional upscaling and a sharpening method called ABSIS, both significantly improving USTFIP method accuracy.


<details>
  <summary>Details</summary>
Motivation: To facilitate the application of STIF methods by addressing the requirement for images with matching spatial and spectral resolutions from different sensors.

Method: Two standardization approaches: 1) Traditional upscaling of fine-resolution images, 2) ABSIS sharpening method that blends features from fine-resolution image series with distinctive attributes of coarse-resolution images.

Result: Both methods significantly increase USTFIP STIF method accuracy, with ABSIS increasing spectral accuracy by up to 49.46% and spatial accuracy by up to 78.40%.

Conclusion: The proposed standardization approaches, particularly the ABSIS sharpening method, effectively enhance STIF method performance by improving image standardization between different sensor data.

Abstract: Spatio-Temporal Image Fusion (STIF) methods usually require sets of images
with matching spatial and spectral resolutions captured by different sensors.
To facilitate the application of STIF methods, we propose and compare two
different standardization approaches. The first method is based on traditional
upscaling of the fine-resolution images. The second method is a sharpening
approach called Anomaly Based Satellite Image Standardization (ABSIS) that
blends the overall features found in the fine-resolution image series with the
distinctive attributes of a specific coarse-resolution image to produce images
that more closely resemble the outcome of aggregating the fine-resolution
images. Both methods produce a significant increase in accuracy of the Unpaired
Spatio Temporal Fusion of Image Patches (USTFIP) STIF method, with the
sharpening approach increasing the spectral and spatial accuracies of the fused
images by up to 49.46\% and 78.40\%, respectively.

</details>


### [63] [FlexiReID: Adaptive Mixture of Expert for Multi-Modal Person Re-Identification](https://arxiv.org/abs/2510.15595)
*Zhen Sun,Lei Tan,Yunhang Shen,Chengmao Cai,Xing Sun,Pingyang Dai,Liujuan Cao,Rongrong Ji*

Main category: cs.CV

TL;DR: FlexiReID is a flexible multimodal person re-identification framework supporting 7 retrieval modes across 4 modalities (RGB, infrared, sketches, text) using adaptive mixture-of-experts and cross-modal query fusion.


<details>
  <summary>Details</summary>
Motivation: Existing multimodal Re-ID methods are limited to specific cross-modal settings and cannot support arbitrary query-retrieval combinations, which hinders practical deployment in real-world scenarios.

Method: Proposes FlexiReID with adaptive mixture-of-experts mechanism to dynamically integrate diverse modality features and cross-modal query fusion module to enhance multimodal feature extraction.

Result: Achieves state-of-the-art performance and demonstrates strong generalization in complex scenarios, as validated through extensive experiments on the unified CIRS-PEDES dataset.

Conclusion: FlexiReID provides a comprehensive solution for flexible multimodal person re-identification that supports diverse retrieval modes and modalities, addressing limitations of previous approaches.

Abstract: Multimodal person re-identification (Re-ID) aims to match pedestrian images
across different modalities. However, most existing methods focus on limited
cross-modal settings and fail to support arbitrary query-retrieval
combinations, hindering practical deployment. We propose FlexiReID, a flexible
framework that supports seven retrieval modes across four modalities: rgb,
infrared, sketches, and text. FlexiReID introduces an adaptive
mixture-of-experts (MoE) mechanism to dynamically integrate diverse modality
features and a cross-modal query fusion module to enhance multimodal feature
extraction. To facilitate comprehensive evaluation, we construct CIRS-PEDES, a
unified dataset extending four popular Re-ID datasets to include all four
modalities. Extensive experiments demonstrate that FlexiReID achieves
state-of-the-art performance and offers strong generalization in complex
scenarios.

</details>


### [64] [Quantized FCA: Efficient Zero-Shot Texture Anomaly Detection](https://arxiv.org/abs/2510.15602)
*Andrei-Timotei Ardelean,Patrick Rückbeil,Tim Weyrich*

Main category: cs.CV

TL;DR: QFCA is a real-time method for zero-shot anomaly localization in textures that achieves 10x speedup with minimal accuracy loss through quantized feature correspondence analysis and PCA preprocessing.


<details>
  <summary>Details</summary>
Motivation: Existing methods for anomaly localization in textures have high running times, making them impractical for real-world deployment like assembly line monitoring.

Method: Proposes QFCA - a quantized version of feature correspondence analysis that works on histograms of quantized values, plus PCA preprocessing to enhance contrast between normal and anomalous features.

Result: Achieves 10x speedup with little to no loss in accuracy, and improves detection precision on complex textures.

Conclusion: QFCA compares favorably with existing methods and enables practical real-time deployment for texture anomaly detection.

Abstract: Zero-shot anomaly localization is a rising field in computer vision research,
with important progress in recent years. This work focuses on the problem of
detecting and localizing anomalies in textures, where anomalies can be defined
as the regions that deviate from the overall statistics, violating the
stationarity assumption. The main limitation of existing methods is their high
running time, making them impractical for deployment in real-world scenarios,
such as assembly line monitoring. We propose a real-time method, named QFCA,
which implements a quantized version of the feature correspondence analysis
(FCA) algorithm. By carefully adapting the patch statistics comparison to work
on histograms of quantized values, we obtain a 10x speedup with little to no
loss in accuracy. Moreover, we introduce a feature preprocessing step based on
principal component analysis, which enhances the contrast between normal and
anomalous features, improving the detection precision on complex textures. Our
method is thoroughly evaluated against prior art, comparing favorably with
existing methods. Project page:
https://reality.tf.fau.de/pub/ardelean2025quantized.html

</details>


### [65] [Lightweight Data-Free Denoising for Detail-Preserving Biomedical Image Restoration](https://arxiv.org/abs/2510.15611)
*Tomáš Chobola,Julia A. Schnabel,Tingying Peng*

Main category: cs.CV

TL;DR: Noise2Detail (N2D) is an ultra-lightweight self-supervised denoising model that achieves fast inference and high-quality image restoration without requiring clean reference images or explicit noise modeling.


<details>
  <summary>Details</summary>
Motivation: Current self-supervised denoising methods have high computational and memory demands, forcing trade-offs between speed and quality. There's a need for efficient denoising that works with scarce clean training data, especially in biomedical imaging.

Method: Built on Noise2Noise framework, N2D uses a multistage denoising pipeline that disrupts noise spatial correlations to create intermediate smooth structures, then refines them to recapture fine details directly from noisy input.

Result: Extensive testing shows N2D outperforms existing dataset-free techniques while requiring only a fraction of computational resources. It achieves both fast denoising and high-quality restoration.

Conclusion: N2D provides an efficient, low-cost, data-free denoising solution particularly valuable for biomedical imaging where clean training data is scarce and fast inference is crucial for practical applications.

Abstract: Current self-supervised denoising techniques achieve impressive results, yet
their real-world application is frequently constrained by substantial
computational and memory demands, necessitating a compromise between inference
speed and reconstruction quality. In this paper, we present an
ultra-lightweight model that addresses this challenge, achieving both fast
denoising and high quality image restoration. Built upon the Noise2Noise
training framework-which removes the reliance on clean reference images or
explicit noise modeling-we introduce an innovative multistage denoising
pipeline named Noise2Detail (N2D). During inference, this approach disrupts the
spatial correlations of noise patterns to produce intermediate smooth
structures, which are subsequently refined to recapture fine details directly
from the noisy input. Extensive testing reveals that Noise2Detail surpasses
existing dataset-free techniques in performance, while requiring only a
fraction of the computational resources. This combination of efficiency, low
computational cost, and data-free approach make it a valuable tool for
biomedical imaging, overcoming the challenges of scarce clean training data-due
to rare and complex imaging modalities-while enabling fast inference for
practical use.

</details>


### [66] [Deep Learning Based Domain Adaptation Methods in Remote Sensing: A Comprehensive Survey](https://arxiv.org/abs/2510.15615)
*Shuchang Lyu,Qi Zhao,Zheng Zhou,Meng Li,You Zhou,Dingding Yao,Guangliang Cheng,Huiyu Zhou,Zhenwei Shi*

Main category: cs.CV

TL;DR: A comprehensive survey of deep learning-based domain adaptation methods for remote sensing, covering taxonomy, algorithms, datasets, performance analysis, and future research directions.


<details>
  <summary>Details</summary>
Motivation: Domain adaptation is crucial for remote sensing to transfer knowledge between differently distributed domains, with applications in interpretation, monitoring, and planning. However, challenges exist due to data variations in sampling distance, sensors, landscapes, and environments.

Method: The survey organizes domain adaptation methods from multiple perspectives: task categorization, input mode, supervision paradigm, and algorithmic granularity. It reviews widely used datasets and summarizes state-of-the-art performance.

Result: The survey provides a systematic taxonomy and comprehensive overview of the field, addressing a broader range of domain adaptation tasks in remote sensing compared to previous surveys.

Conclusion: This work can inspire the research community, foster understanding, and guide future research in domain adaptation for remote sensing by identifying open challenges and potential directions.

Abstract: Domain adaptation is a crucial and increasingly important task in remote
sensing, aiming to transfer knowledge from a source domain a differently
distributed target domain. It has broad applications across various real-world
applications, including remote sensing element interpretation, ecological
environment monitoring, and urban/rural planning. However, domain adaptation in
remote sensing poses significant challenges due to differences in data, such as
variations in ground sampling distance, imaging modes from various sensors,
geographical landscapes, and environmental conditions. In recent years, deep
learning has emerged as a powerful tool for feature representation and
cross-domain knowledge transfer, leading to widespread adoption in remote
sensing tasks. In this paper, we present a comprehensive survey of significant
advancements in deep learning based domain adaptation for remote sensing. We
first introduce the preliminary knowledge to clarify key concepts, mathematical
notations, and the taxonomy of methodologies. We then organize existing
algorithms from multiple perspectives, including task categorization, input
mode, supervision paradigm, and algorithmic granularity, providing readers with
a structured understanding of the field. Next, we review widely used datasets
and summarize the performance of state-of-the-art methods to provide an
overview of current progress. We also identify open challenges and potential
directions to guide future research in domain adaptation for remote sensing.
Compared to previous surveys, this work addresses a broader range of domain
adaptation tasks in remote sensing, rather than concentrating on a few
subfields. It also presents a systematic taxonomy, providing a more
comprehensive and organized understanding of the field. As a whole, this survey
can inspire the research community, foster understanding, and guide future work
in the field.

</details>


### [67] [Valeo Near-Field: a novel dataset for pedestrian intent detection](https://arxiv.org/abs/2510.15673)
*Antonyo Musabini,Rachid Benmokhtar,Jagdish Bhanushali,Victor Galizzi,Bertrand Luvison,Xavier Perrotton*

Main category: cs.CV

TL;DR: A novel multi-modal dataset for pedestrian intention detection with synchronized fisheye cameras, lidar, ultrasonic sensors, and motion capture data, released with benchmark suite for embedded systems.


<details>
  <summary>Details</summary>
Motivation: To address real-world challenges in pedestrian detection including sensor occlusions, dynamic environments, and hardware constraints for intelligent vehicles in near-field scenarios.

Method: Created synchronized multi-modal dataset with detailed annotations of 3D body joints and pedestrian positions, released with comprehensive benchmark suite and evaluation metrics.

Result: Provides baseline performance metrics using custom neural network architectures and facilitates robust benchmarking for perception algorithms.

Conclusion: This dataset serves as a foundation for advancing pedestrian detection, 3D pose estimation, and intention prediction capabilities in intelligent vehicles.

Abstract: This paper presents a novel dataset aimed at detecting pedestrians'
intentions as they approach an ego-vehicle. The dataset comprises synchronized
multi-modal data, including fisheye camera feeds, lidar laser scans, ultrasonic
sensor readings, and motion capture-based 3D body poses, collected across
diverse real-world scenarios. Key contributions include detailed annotations of
3D body joint positions synchronized with fisheye camera images, as well as
accurate 3D pedestrian positions extracted from lidar data, facilitating robust
benchmarking for perception algorithms. We release a portion of the dataset
along with a comprehensive benchmark suite, featuring evaluation metrics for
accuracy, efficiency, and scalability on embedded systems. By addressing
real-world challenges such as sensor occlusions, dynamic environments, and
hardware constraints, this dataset offers a unique resource for developing and
evaluating state-of-the-art algorithms in pedestrian detection, 3D pose
estimation and 4D trajectory and intention prediction. Additionally, we provide
baseline performance metrics using custom neural network architectures and
suggest future research directions to encourage the adoption and enhancement of
the dataset. This work aims to serve as a foundation for researchers seeking to
advance the capabilities of intelligent vehicles in near-field scenarios.

</details>


### [68] [Uncertainty-Aware Extreme Point Tracing for Weakly Supervised Ultrasound Image Segmentation](https://arxiv.org/abs/2510.15666)
*Lei Shi,Gang Li,Junxing Zhang*

Main category: cs.CV

TL;DR: A weakly supervised medical image segmentation method using only four extreme points as annotation, leveraging SAM2 for initial pseudo labels and refining them with uncertainty-aware techniques to achieve performance comparable to fully supervised methods.


<details>
  <summary>Details</summary>
Motivation: To reduce the high annotation cost of pixel-level labels in medical image segmentation while maintaining competitive performance.

Method: Uses extreme points to create bounding boxes for SAM2 prompting, then refines pseudo labels with enhanced FGEPM algorithm using Monte Carlo dropout uncertainty, plus dual-branch USC loss and box alignment loss for spatial consistency.

Result: Achieves performance comparable to and even surpassing fully supervised methods on BUSI and UNS ultrasound datasets, while significantly reducing annotation cost.

Conclusion: The proposed weakly supervised framework is effective and practical for ultrasound image segmentation, offering a viable alternative to costly fully supervised approaches.

Abstract: Automatic medical image segmentation is a fundamental step in computer-aided
diagnosis, yet fully supervised approaches demand extensive pixel-level
annotations that are costly and time-consuming. To alleviate this burden, we
propose a weakly supervised segmentation framework that leverages only four
extreme points as annotation. Specifically, bounding boxes derived from the
extreme points are used as prompts for the Segment Anything Model 2 (SAM2) to
generate reliable initial pseudo labels. These pseudo labels are progressively
refined by an enhanced Feature-Guided Extreme Point Masking (FGEPM) algorithm,
which incorporates Monte Carlo dropout-based uncertainty estimation to
construct a unified gradient uncertainty cost map for boundary tracing.
Furthermore, a dual-branch Uncertainty-aware Scale Consistency (USC) loss and a
box alignment loss are introduced to ensure spatial consistency and precise
boundary alignment during training. Extensive experiments on two public
ultrasound datasets, BUSI and UNS, demonstrate that our method achieves
performance comparable to, and even surpassing fully supervised counterparts
while significantly reducing annotation cost. These results validate the
effectiveness and practicality of the proposed weakly supervised framework for
ultrasound image segmentation.

</details>


### [69] [Towards Label-Free Brain Tumor Segmentation: Unsupervised Learning with Multimodal MRI](https://arxiv.org/abs/2510.15684)
*Gerard Comas-Quiles,Carles Garcia-Cabrera,Julia Dietlmeier,Noel E. O'Connor,Ferran Marques*

Main category: cs.CV

TL;DR: Proposes MViT-AE, a multimodal vision transformer autoencoder for unsupervised brain tumor detection using only healthy brain MRIs, achieving clinically meaningful tumor localization without manual labels.


<details>
  <summary>Details</summary>
Motivation: Addresses limitations of supervised learning for brain tumor segmentation when annotated datasets are limited, costly, or inconsistent, offering a scalable alternative for neuroimaging workflows.

Method: Uses multimodal vision transformer autoencoder trained on healthy brain MRIs with early-late fusion strategy across MRI sequences, plus SAM-based post-processing to refine tumor contours.

Result: Achieves lesion-wise Dice scores of 0.437 (Whole Tumor), 0.316 (Tumor Core), 0.350 (Enhancing Tumor) on test set, and 89.4% anomaly detection rate on validation set.

Conclusion: Transformer-based unsupervised models show potential as scalable, label-efficient tools for neuro-oncological imaging despite challenges in detecting small or non-enhancing lesions.

Abstract: Unsupervised anomaly detection (UAD) presents a complementary alternative to
supervised learning for brain tumor segmentation in magnetic resonance imaging
(MRI), particularly when annotated datasets are limited, costly, or
inconsistent. In this work, we propose a novel Multimodal Vision Transformer
Autoencoder (MViT-AE) trained exclusively on healthy brain MRIs to detect and
localize tumors via reconstruction-based error maps. This unsupervised paradigm
enables segmentation without reliance on manual labels, addressing a key
scalability bottleneck in neuroimaging workflows. Our method is evaluated in
the BraTS-GoAT 2025 Lighthouse dataset, which includes various types of tumors
such as gliomas, meningiomas, and pediatric brain tumors. To enhance
performance, we introduce a multimodal early-late fusion strategy that
leverages complementary information across multiple MRI sequences, and a
post-processing pipeline that integrates the Segment Anything Model (SAM) to
refine predicted tumor contours. Despite the known challenges of UAD,
particularly in detecting small or non-enhancing lesions, our method achieves
clinically meaningful tumor localization, with lesion-wise Dice Similarity
Coefficient of 0.437 (Whole Tumor), 0.316 (Tumor Core), and 0.350 (Enhancing
Tumor) on the test set, and an anomaly Detection Rate of 89.4% on the
validation set. These findings highlight the potential of transformer-based
unsupervised models to serve as scalable, label-efficient tools for
neuro-oncological imaging.

</details>


### [70] [Unimedvl: Unifying Medical Multimodal Understanding And Generation Through Observation-Knowledge-Analysis](https://arxiv.org/abs/2510.15710)
*Junzhi Ning,Wei Li,Cheng Tang,Jiashi Lin,Chenglong Ma,Chaoyang Zhang,Jiyao Liu,Ying Chen,Shujian Gao,Lihao Liu,Yuandong Pu,Huihui Xu,Chenhui Gou,Ziyan Huang,Yi Xin,Qi Qin,Zhongying Deng,Diping Song,Bin Fu,Guang Yang,Yuanfeng Ji,Tianbin Li,Yanzhou Su,Jin Ye,Shixiang Tang,Ming Hu,Junjun He*

Main category: cs.CV

TL;DR: UniMedVL is a unified multimodal medical AI framework that integrates both image understanding and generation capabilities within a single architecture, addressing the fragmentation in existing medical AI systems.


<details>
  <summary>Details</summary>
Motivation: Existing medical AI systems are fragmented - image understanding models can't generate visual outputs, while image generation models can't provide textual explanations, creating gaps in multimodal capabilities needed for diagnostic workflows.

Method: Proposed a multi-level framework using Observation-Knowledge-Analysis (OKA) paradigm: created UniMed-5M dataset with 5.6M multimodal samples, used Progressive Curriculum Learning for knowledge integration, and developed UniMedVL as the unified model architecture.

Result: UniMedVL achieves superior performance on 5 medical image understanding benchmarks and matches specialized models in generation quality across 8 medical imaging modalities. The unified architecture enables bidirectional knowledge sharing where generation tasks enhance visual understanding.

Conclusion: Integrating traditionally separate image understanding and generation capabilities within a single medical framework unlocks improvements across diverse medical vision-language tasks, demonstrating the value of unified multimodal approaches in medical AI.

Abstract: Medical diagnostic applications require models that can process multimodal
medical inputs (images, patient histories, lab results) and generate diverse
outputs including both textual reports and visual content (annotations,
segmentation masks, and images). Despite this need, existing medical AI systems
disrupt this unified process: medical image understanding models interpret
images but cannot generate visual outputs, while medical image generation
models synthesize images but cannot provide textual explanations. This leads to
gaps in data representation, feature integration, and task-level multimodal
capabilities. To this end, we propose a multi-level framework that draws
inspiration from diagnostic workflows through the
Observation-Knowledge-Analysis (OKA) paradigm. Specifically, at the observation
level, we construct UniMed-5M, a dataset comprising over 5.6M samples that
reformat diverse unimodal data into multimodal pairs for foundational
observation. At the knowledge level, we propose Progressive Curriculum Learning
that systematically introduces medical multimodal knowledge. At the analysis
level, we introduce UniMedVL, the first medical unified multimodal model for
the simultaneous analysis of image understanding and generation tasks within a
single architecture. UniMedVL achieves superior performance on five medical
image understanding benchmarks, while matching specialized models in generation
quality across eight medical imaging modalities. Crucially, our unified
architecture enables bidirectional knowledge sharing: generation tasks enhance
visual understanding features, demonstrating that integrating traditionally
separate capabilities within a single medical framework unlocks improvements
across diverse medical vision-language tasks. Code is available at
https://github.com/uni-medical/UniMedVL.

</details>


### [71] [Scaling Instruction-Based Video Editing with a High-Quality Synthetic Dataset](https://arxiv.org/abs/2510.15742)
*Qingyan Bai,Qiuyu Wang,Hao Ouyang,Yue Yu,Hanlin Wang,Wen Wang,Ka Leong Cheng,Shuailei Ma,Yanhong Zeng,Zichen Liu,Yinghao Xu,Yujun Shen,Qifeng Chen*

Main category: cs.CV

TL;DR: Ditto is a framework that generates high-quality video editing training data by combining image editing with video generation, using efficient models and AI agents to create Ditto-1M dataset, resulting in state-of-the-art video editing performance.


<details>
  <summary>Details</summary>
Motivation: Instruction-based video editing faces a critical bottleneck due to the scarcity of large-scale, high-quality training data, which severely limits progress in democratizing content creation.

Method: Ditto uses a novel data generation pipeline that fuses image editor diversity with in-context video generation, employs efficient distilled model architecture with temporal enhancer for cost-quality trade-off, and uses intelligent agents for instruction crafting and quality filtering.

Result: The framework generated Ditto-1M dataset with 1 million high-fidelity video editing examples using over 12,000 GPU-days, and the trained Editto model demonstrated superior instruction-following ability and state-of-the-art performance.

Conclusion: Ditto successfully addresses the data scarcity problem in instruction-based video editing through its holistic framework, enabling scalable generation of high-quality training data and achieving new state-of-the-art results.

Abstract: Instruction-based video editing promises to democratize content creation, yet
its progress is severely hampered by the scarcity of large-scale, high-quality
training data. We introduce Ditto, a holistic framework designed to tackle this
fundamental challenge. At its heart, Ditto features a novel data generation
pipeline that fuses the creative diversity of a leading image editor with an
in-context video generator, overcoming the limited scope of existing models. To
make this process viable, our framework resolves the prohibitive cost-quality
trade-off by employing an efficient, distilled model architecture augmented by
a temporal enhancer, which simultaneously reduces computational overhead and
improves temporal coherence. Finally, to achieve full scalability, this entire
pipeline is driven by an intelligent agent that crafts diverse instructions and
rigorously filters the output, ensuring quality control at scale. Using this
framework, we invested over 12,000 GPU-days to build Ditto-1M, a new dataset of
one million high-fidelity video editing examples. We trained our model, Editto,
on Ditto-1M with a curriculum learning strategy. The results demonstrate
superior instruction-following ability and establish a new state-of-the-art in
instruction-based video editing.

</details>


### [72] [SEGA: A Stepwise Evolution Paradigm for Content-Aware Layout Generation with Design Prior](https://arxiv.org/abs/2510.15749)
*Haoran Wang,Bo Zhao,Jinghui Wang,Hanzhang Wang,Huan Yang,Wei Ji,Hao Liu,Xinyan Xiao*

Main category: cs.CV

TL;DR: SEGA introduces a stepwise evolution paradigm for content-aware layout generation using hierarchical reasoning with coarse-to-fine strategy and layout design principles.


<details>
  <summary>Details</summary>
Motivation: Existing single-step reasoning methods fail with complex element layouts due to lack of feedback-based self-correction mechanism.

Method: Hierarchical reasoning framework: coarse module estimates layout planning, refining module performs fine-level reasoning. Incorporates layout design principles as prior knowledge.

Result: Achieves state-of-the-art results on multiple benchmark datasets.

Conclusion: SEGA effectively addresses complex layout planning through stepwise evolution paradigm and hierarchical reasoning.

Abstract: In this paper, we study the content-aware layout generation problem, which
aims to automatically generate layouts that are harmonious with a given
background image. Existing methods usually deal with this task with a
single-step reasoning framework. The lack of a feedback-based self-correction
mechanism leads to their failure rates significantly increasing when faced with
complex element layout planning. To address this challenge, we introduce SEGA,
a novel Stepwise Evolution Paradigm for Content-Aware Layout Generation.
Inspired by the systematic mode of human thinking, SEGA employs a hierarchical
reasoning framework with a coarse-to-fine strategy: first, a coarse-level
module roughly estimates the layout planning results; then, another refining
module performs fine-level reasoning regarding the coarse planning results.
Furthermore, we incorporate layout design principles as prior knowledge into
the model to enhance its layout planning ability. Besides, we present
GenPoster-100K that is a new large-scale poster dataset with rich
meta-information annotation. The experiments demonstrate the effectiveness of
our approach by achieving the state-of-the-art results on multiple benchmark
datasets. Our project page is at: https://brucew91.github.io/SEGA.github.io/

</details>


### [73] [NDM: A Noise-driven Detection and Mitigation Framework against Implicit Sexual Intentions in Text-to-Image Generation](https://arxiv.org/abs/2510.15752)
*Yitong Sun,Yao Huang,Ruochen Zhang,Huanran Chen,Shouwei Ruan,Ranjie Duan,Xingxing Wei*

Main category: cs.CV

TL;DR: NDM is a noise-driven framework that detects and mitigates implicit sexual content in text-to-image generation while preserving model quality, using noise-based detection and adaptive negative guidance.


<details>
  <summary>Details</summary>
Motivation: Text-to-image diffusion models are vulnerable to generating inappropriate content from implicit sexual prompts that bypass existing detection methods, creating ethical concerns without compromising generative quality.

Method: Uses noise separability for detection and noise-enhanced adaptive negative guidance that optimizes initial noise by suppressing attention in prominent regions to mitigate sexual content.

Result: NDM outperforms state-of-the-art methods (SLD, UCE, RECE) on both natural and adversarial datasets, achieving high accuracy in detecting implicit malicious intentions.

Conclusion: NDM effectively addresses the challenge of implicit sexual content in T2I generation through noise-driven detection and mitigation while maintaining the model's original generative capabilities.

Abstract: Despite the impressive generative capabilities of text-to-image (T2I)
diffusion models, they remain vulnerable to generating inappropriate content,
especially when confronted with implicit sexual prompts. Unlike explicit
harmful prompts, these subtle cues, often disguised as seemingly benign terms,
can unexpectedly trigger sexual content due to underlying model biases, raising
significant ethical concerns. However, existing detection methods are primarily
designed to identify explicit sexual content and therefore struggle to detect
these implicit cues. Fine-tuning approaches, while effective to some extent,
risk degrading the model's generative quality, creating an undesirable
trade-off. To address this, we propose NDM, the first noise-driven detection
and mitigation framework, which could detect and mitigate implicit malicious
intention in T2I generation while preserving the model's original generative
capabilities. Specifically, we introduce two key innovations: first, we
leverage the separability of early-stage predicted noise to develop a
noise-based detection method that could identify malicious content with high
accuracy and efficiency; second, we propose a noise-enhanced adaptive negative
guidance mechanism that could optimize the initial noise by suppressing the
prominent region's attention, thereby enhancing the effectiveness of adaptive
negative guidance for sexual mitigation. Experimentally, we validate NDM on
both natural and adversarial datasets, demonstrating its superior performance
over existing SOTA methods, including SLD, UCE, and RECE, etc. Code and
resources are available at https://github.com/lorraine021/NDM.

</details>


### [74] [Semantic segmentation with coarse annotations](https://arxiv.org/abs/2510.15756)
*Jort de Jong,Mike Holenderski*

Main category: cs.CV

TL;DR: A regularization method using SLIC superpixels to improve semantic segmentation with coarse annotations, particularly enhancing boundary alignment.


<details>
  <summary>Details</summary>
Motivation: Fine pixel-level annotations are expensive and difficult to obtain, while coarse annotations are more accessible but lead to poor boundary alignment in segmentation models.

Method: Proposes a regularization technique for encoder-decoder architectures that encourages segmented pixels to form SLIC superpixels based on color and position, independent of segmentation annotations.

Result: Significant improvement in boundary recall compared to state-of-the-art models when trained on coarse annotations across SUIM, Cityscapes, and PanNuke datasets.

Conclusion: The superpixel-based regularization effectively addresses boundary alignment issues in semantic segmentation with coarse annotations.

Abstract: Semantic segmentation is the task of classifying each pixel in an image.
Training a segmentation model achieves best results using annotated images,
where each pixel is annotated with the corresponding class. When obtaining fine
annotations is difficult or expensive, it may be possible to acquire coarse
annotations, e.g. by roughly annotating pixels in an images leaving some pixels
around the boundaries between classes unlabeled. Segmentation with coarse
annotations is difficult, in particular when the objective is to optimize the
alignment of boundaries between classes. This paper proposes a regularization
method for models with an encoder-decoder architecture with superpixel based
upsampling. It encourages the segmented pixels in the decoded image to be
SLIC-superpixels, which are based on pixel color and position, independent of
the segmentation annotation. The method is applied to FCN-16 fully
convolutional network architecture and evaluated on the SUIM, Cityscapes, and
PanNuke data sets. It is shown that the boundary recall improves significantly
compared to state-of-the-art models when trained on coarse annotations.

</details>


### [75] [QSilk: Micrograin Stabilization and Adaptive Quantile Clipping for Detail-Friendly Latent Diffusion](https://arxiv.org/abs/2510.15761)
*Denis Rychkovskiy*

Main category: cs.CV

TL;DR: QSilk is a lightweight stabilization layer for latent diffusion models that improves high-frequency fidelity and suppresses activation spikes through micro clamping and adaptive quantile clipping.


<details>
  <summary>Details</summary>
Motivation: To address issues with high-frequency fidelity and rare activation spikes in latent diffusion models, particularly at low step counts and ultra-high resolutions, without requiring training or fine-tuning.

Method: Combines per-sample micro clamping to gently limit extreme values without washing out texture, and Adaptive Quantile Clip (AQClip) that adapts allowed value corridors per region using local structure statistics or attention entropy guidance.

Result: Yields cleaner, sharper results at low step counts and ultra-high resolutions with negligible overhead, shows consistent improvements across SD/SDXL backbones, and enables higher guidance without artifacts.

Conclusion: QSilk provides an effective stabilization solution for latent diffusion models that improves output quality with minimal computational overhead and no training requirements.

Abstract: We present QSilk, a lightweight, always-on stabilization layer for latent
diffusion that improves high-frequency fidelity while suppressing rare
activation spikes. QSilk combines (i) a per-sample micro clamp that gently
limits extreme values without washing out texture, and (ii) Adaptive Quantile
Clip (AQClip), which adapts the allowed value corridor per region. AQClip can
operate in a proxy mode using local structure statistics or in an attention
entropy guided mode (model confidence). Integrated into the CADE 2.5 rendering
pipeline, QSilk yields cleaner, sharper results at low step counts and
ultra-high resolutions with negligible overhead. It requires no training or
fine-tuning and exposes minimal user controls. We report consistent qualitative
improvements across SD/SDXL backbones and show synergy with CFG/Rescale,
enabling slightly higher guidance without artifacts.

</details>


### [76] [Towards more holistic interpretability: A lightweight disentangled Concept Bottleneck Model](https://arxiv.org/abs/2510.15770)
*Gaoxiang Huang,Songning Lai,Yutao Yue*

Main category: cs.CV

TL;DR: LDCBM is a lightweight disentangled concept bottleneck model that groups visual features into meaningful components without region annotation, improving concept alignment and outperforming previous CBMs in both interpretability and classification.


<details>
  <summary>Details</summary>
Motivation: Existing Concept Bottleneck Models suffer from input-to-concept mapping bias and limited controllability, which restricts their practical value and damages the responsibility of concept-based methods.

Method: Proposes LDCBM with filter grouping loss and joint concept supervision to automatically group visual features into semantically meaningful components without region annotation, improving alignment between visual patterns and concepts.

Result: Experiments on three diverse datasets show LDCBM achieves higher concept and class accuracy, outperforming previous CBMs in both interpretability and classification performance.

Conclusion: By grounding concepts in visual evidence, LDCBM overcomes fundamental limitations of prior models and enhances the reliability of interpretable AI.

Abstract: Concept Bottleneck Models (CBMs) enhance interpretability by predicting
human-understandable concepts as intermediate representations. However,
existing CBMs often suffer from input-to-concept mapping bias and limited
controllability, which restricts their practical value, directly damage the
responsibility of strategy from concept-based methods. We propose a lightweight
Disentangled Concept Bottleneck Model (LDCBM) that automatically groups visual
features into semantically meaningful components without region annotation. By
introducing a filter grouping loss and joint concept supervision, our method
improves the alignment between visual patterns and concepts, enabling more
transparent and robust decision-making. Notably, Experiments on three diverse
datasets demonstrate that LDCBM achieves higher concept and class accuracy,
outperforming previous CBMs in both interpretability and classification
performance. By grounding concepts in visual evidence, our method overcomes a
fundamental limitation of prior models and enhances the reliability of
interpretable AI.

</details>


### [77] [Controlling the image generation process with parametric activation functions](https://arxiv.org/abs/2510.15778)
*Ilia Pavlov*

Main category: cs.CV

TL;DR: A system for interactive understanding and control of generative models by replacing activation functions with parametric ones.


<details>
  <summary>Details</summary>
Motivation: To develop tools that enable direct interaction with internal mechanisms of generative models in an interpretable way, as current models lack such interactive capabilities.

Method: Allows users to replace activation functions in generative networks with parametric functions and set their parameters to control network output.

Result: Demonstrated on StyleGAN2 (trained on FFHQ) and BigGAN (trained on ImageNet) networks.

Conclusion: The system provides an alternative approach for controlling generative model outputs through interactive parameter manipulation of activation functions.

Abstract: As image generative models continue to increase not only in their fidelity
but also in their ubiquity the development of tools that leverage direct
interaction with their internal mechanisms in an interpretable way has received
little attention In this work we introduce a system that allows users to
develop a better understanding of the model through interaction and
experimentation By giving users the ability to replace activation functions of
a generative network with parametric ones and a way to set the parameters of
these functions we introduce an alternative approach to control the networks
output We demonstrate the use of our method on StyleGAN2 and BigGAN networks
trained on FFHQ and ImageNet respectively.

</details>


### [78] [ReCon: Region-Controllable Data Augmentation with Rectification and Alignment for Object Detection](https://arxiv.org/abs/2510.15783)
*Haowei Zhu,Tianxiang Pan,Rui Qin,Jun-Hai Yong,Bin Wang*

Main category: cs.CV

TL;DR: ReCon is a novel data augmentation framework that enhances structure-controllable generative models for object detection by integrating region-guided rectification and region-aligned cross-attention during diffusion sampling.


<details>
  <summary>Details</summary>
Motivation: Current generative models for data augmentation require complex post-processing or extensive fine-tuning, and suffer from content-position mismatches and semantic leakage, making large-scale annotated data acquisition costly and time-consuming.

Method: ReCon integrates region-guided rectification using feedback from pre-trained perception models to fix misgenerated regions during diffusion sampling, and proposes region-aligned cross-attention to ensure spatial-semantic alignment between image regions and textual cues.

Result: Extensive experiments show ReCon substantially improves generated data quality and trainability, achieving consistent performance gains across various datasets, backbone architectures, and data scales.

Conclusion: ReCon effectively overcomes limitations of current generative approaches by enhancing semantic consistency and image fidelity through integrated rectification and alignment mechanisms during the diffusion process.

Abstract: The scale and quality of datasets are crucial for training robust perception
models. However, obtaining large-scale annotated data is both costly and
time-consuming. Generative models have emerged as a powerful tool for data
augmentation by synthesizing samples that adhere to desired distributions.
However, current generative approaches often rely on complex post-processing or
extensive fine-tuning on massive datasets to achieve satisfactory results, and
they remain prone to content-position mismatches and semantic leakage. To
overcome these limitations, we introduce ReCon, a novel augmentation framework
that enhances the capacity of structure-controllable generative models for
object detection. ReCon integrates region-guided rectification into the
diffusion sampling process, using feedback from a pre-trained perception model
to rectify misgenerated regions within diffusion sampling process. We further
propose region-aligned cross-attention to enforce spatial-semantic alignment
between image regions and their textual cues, thereby improving both semantic
consistency and overall image fidelity. Extensive experiments demonstrate that
ReCon substantially improve the quality and trainability of generated data,
achieving consistent performance gains across various datasets, backbone
architectures, and data scales. Our code is available at
https://github.com/haoweiz23/ReCon .

</details>


### [79] [ERNet: Efficient Non-Rigid Registration Network for Point Sequences](https://arxiv.org/abs/2510.15800)
*Guangzhao He,Yuxi Xiao,Zhen Xu,Xiaowei Zhou,Sida Peng*

Main category: cs.CV

TL;DR: ERNet is an efficient feed-forward model for non-rigid point cloud registration that addresses local minima and error accumulation through a two-stage deformation graph prediction pipeline.


<details>
  <summary>Details</summary>
Motivation: To overcome challenges in registering object shapes to point clouds undergoing non-rigid deformation, particularly local minima in registration objectives and error accumulation over long sequences.

Method: A two-stage pipeline that first estimates frame-wise coarse graph nodes for robust initialization, then refines their trajectories over time in a sliding-window fashion.

Result: Outperforms previous state-of-the-art on DeformingThings4D and D-FAUST datasets, achieving more than 4x speedup compared to previous best methods.

Conclusion: ERNet provides an efficient and accurate solution for sequential non-rigid registration that handles noisy/partial inputs and leverages temporal information effectively.

Abstract: Registering an object shape to a sequence of point clouds undergoing
non-rigid deformation is a long-standing challenge. The key difficulties stem
from two factors: (i) the presence of local minima due to the non-convexity of
registration objectives, especially under noisy or partial inputs, which
hinders accurate and robust deformation estimation, and (ii) error accumulation
over long sequences, leading to tracking failures. To address these challenges,
we introduce to adopt a scalable data-driven approach and propose ERNet, an
efficient feed-forward model trained on large deformation datasets. It is
designed to handle noisy and partial inputs while effectively leveraging
temporal information for accurate and consistent sequential registration. The
key to our design is predicting a sequence of deformation graphs through a
two-stage pipeline, which first estimates frame-wise coarse graph nodes for
robust initialization, before refining their trajectories over time in a
sliding-window fashion. Extensive experiments show that our proposed approach
(i) outperforms previous state-of-the-art on both the DeformingThings4D and
D-FAUST datasets, and (ii) achieves more than 4x speedup compared to the
previous best, offering significant efficiency improvement.

</details>


### [80] [VISTA: A Test-Time Self-Improving Video Generation Agent](https://arxiv.org/abs/2510.15831)
*Do Xuan Long,Xingchen Wan,Hootan Nakhost,Chen-Yu Lee,Tomas Pfister,Sercan Ö. Arık*

Main category: cs.CV

TL;DR: VISTA is a multi-agent system that autonomously improves text-to-video generation through iterative prompt refinement, achieving significant quality improvements over state-of-the-art methods.


<details>
  <summary>Details</summary>
Motivation: Existing test-time optimization methods struggle with video generation's multi-faceted nature, and video quality remains critically dependent on precise user prompts.

Method: VISTA uses a multi-agent system that decomposes user ideas into temporal plans, identifies best videos through pairwise tournaments, critiques with specialized agents (visual, audio, contextual), and synthesizes feedback to rewrite prompts iteratively.

Result: VISTA consistently improves video quality and alignment with user intent, achieving up to 60% pairwise win rate against state-of-the-art baselines, with human evaluators preferring VISTA outputs in 66.4% of comparisons.

Conclusion: VISTA demonstrates that autonomous multi-agent systems can effectively improve text-to-video generation through iterative self-improvement, outperforming existing methods in both quality and user intent alignment.

Abstract: Despite rapid advances in text-to-video synthesis, generated video quality
remains critically dependent on precise user prompts. Existing test-time
optimization methods, successful in other domains, struggle with the
multi-faceted nature of video. In this work, we introduce VISTA (Video
Iterative Self-improvemenT Agent), a novel multi-agent system that autonomously
improves video generation through refining prompts in an iterative loop. VISTA
first decomposes a user idea into a structured temporal plan. After generation,
the best video is identified through a robust pairwise tournament. This winning
video is then critiqued by a trio of specialized agents focusing on visual,
audio, and contextual fidelity. Finally, a reasoning agent synthesizes this
feedback to introspectively rewrite and enhance the prompt for the next
generation cycle. Experiments on single- and multi-scene video generation
scenarios show that while prior methods yield inconsistent gains, VISTA
consistently improves video quality and alignment with user intent, achieving
up to 60% pairwise win rate against state-of-the-art baselines. Human
evaluators concur, preferring VISTA outputs in 66.4% of comparisons.

</details>


### [81] [Neuro-Symbolic Spatial Reasoning in Segmentation](https://arxiv.org/abs/2510.15841)
*Jiayi Lin,Jiabo Huang,Shaogang Gong*

Main category: cs.CV

TL;DR: RelateSeg introduces neuro-symbolic spatial reasoning to open-vocabulary semantic segmentation by using first-order logic constraints to enforce spatial relationships between objects, achieving state-of-the-art performance.


<details>
  <summary>Details</summary>
Motivation: Current vision-language model approaches for open-vocabulary semantic segmentation lack understanding of spatial relations between objects in scenes, limiting their ability to accurately segment complex scenes with multiple objects.

Method: Proposes Relational Segmentor (RelateSeg) that automatically extracts spatial relations and encodes them as first-order logic formulas using pseudo categories. Each pixel predicts both semantic and spatial pseudo categories simultaneously, with relational constraints enforced through fuzzy logic relaxation in a deep network architecture.

Result: Achieves state-of-the-art performance in average mIoU across four benchmark datasets, with clear advantages on images containing multiple categories. Only requires a single auxiliary loss function and no additional parameters.

Conclusion: Neuro-symbolic spatial reasoning is effective for open-vocabulary semantic segmentation, providing relational consistency without significant computational overhead.

Abstract: Open-Vocabulary Semantic Segmentation (OVSS) assigns pixel-level labels from
an open set of categories, requiring generalization to unseen and unlabelled
objects. Using vision-language models (VLMs) to correlate local image patches
with potential unseen object categories suffers from a lack of understanding of
spatial relations of objects in a scene. To solve this problem, we introduce
neuro-symbolic (NeSy) spatial reasoning in OVSS. In contrast to contemporary
VLM correlation-based approaches, we propose Relational Segmentor (RelateSeg)
to impose explicit spatial relational constraints by first order logic (FOL)
formulated in a neural network architecture. This is the first attempt to
explore NeSy spatial reasoning in OVSS. Specifically, RelateSeg automatically
extracts spatial relations, e.g., <cat, to-right-of, person>, and encodes them
as first-order logic formulas using our proposed pseudo categories. Each pixel
learns to predict both a semantic category (e.g., "cat") and a spatial pseudo
category (e.g., "right of person") simultaneously, enforcing relational
constraints (e.g., a "cat" pixel must lie to the right of a "person"). Finally,
these logic constraints are formulated in a deep network architecture by fuzzy
logic relaxation, enabling end-to-end learning of spatial-relationally
consistent segmentation. RelateSeg achieves state-of-the-art performance in
terms of average mIoU across four benchmark datasets and particularly shows
clear advantages on images containing multiple categories, with the cost of
only introducing a single auxiliary loss function and no additional parameters,
validating the effectiveness of NeSy spatial reasoning in OVSS.

</details>


### [82] [OmniVinci: Enhancing Architecture and Data for Omni-Modal Understanding LLM](https://arxiv.org/abs/2510.15870)
*Hanrong Ye,Chao-Han Huck Yang,Arushi Goel,Wei Huang,Ligeng Zhu,Yuanhang Su,Sean Lin,An-Chieh Cheng,Zhen Wan,Jinchuan Tian,Yuming Lou,Dong Yang,Zhijian Liu,Yukang Chen,Ambrish Dantrey,Ehsan Jahangiri,Sreyan Ghosh,Daguang Xu,Ehsan Hosseini-Asl,Danial Mohseni Taheri,Vidya Murali,Sifei Liu,Jason Lu,Oluwatobi Olabiyi,Frank Wang,Rafael Valle,Bryan Catanzaro,Andrew Tao,Song Han,Jan Kautz,Hongxu Yin,Pavlo Molchanov*

Main category: cs.CV

TL;DR: OmniVinci is an open-source omni-modal LLM that introduces architectural innovations for better cross-modal alignment and outperforms Qwen2.5-Omni on multiple benchmarks while using significantly less training data.


<details>
  <summary>Details</summary>
Motivation: To advance machine intelligence by developing multimodal perception capabilities similar to human sensing across vision, audio, and other modalities.

Method: Three key architectural innovations: OmniAlignNet for vision-audio embedding alignment, Temporal Embedding Grouping for relative temporal alignment, and Constrained Rotary Time Embedding for absolute temporal encoding. Plus a curation pipeline generating 24M multimodal conversations.

Result: Outperforms Qwen2.5-Omni with +19.05 on DailyOmni (cross-modal), +1.7 on MMAR (audio), and +3.9 on Video-MME (vision), using only 0.2T training tokens (6x less than Qwen2.5-Omni's 1.2T).

Conclusion: Modalities reinforce each other in perception and reasoning, and the model demonstrates advantages in robotics, medical AI, and smart factory applications.

Abstract: Advancing machine intelligence requires developing the ability to perceive
across multiple modalities, much as humans sense the world. We introduce
OmniVinci, an initiative to build a strong, open-source, omni-modal LLM. We
carefully study the design choices across model architecture and data curation.
For model architecture, we present three key innovations: (i) OmniAlignNet for
strengthening alignment between vision and audio embeddings in a shared
omni-modal latent space; (ii) Temporal Embedding Grouping for capturing
relative temporal alignment between vision and audio signals; and (iii)
Constrained Rotary Time Embedding for encoding absolute temporal information in
omni-modal embeddings. We introduce a curation and synthesis pipeline that
generates 24M single-modal and omni-modal conversations. We find that
modalities reinforce one another in both perception and reasoning. Our model,
OmniVinci, outperforms Qwen2.5-Omni with +19.05 on DailyOmni (cross-modal
understanding), +1.7 on MMAR (audio), and +3.9 on Video-MME (vision), while
using just 0.2T training tokens - a 6 times reduction compared to
Qwen2.5-Omni's 1.2T. We finally demonstrate omni-modal advantages in downstream
applications spanning robotics, medical AI, and smart factory.

</details>


### [83] [3DPR: Single Image 3D Portrait Relight using Generative Priors](https://arxiv.org/abs/2510.15846)
*Pramod Rao,Abhimitra Meka,Xilong Zhou,Gereon Fox,Mallikarjun B R,Fangneng Zhan,Tim Weyrich,Bernd Bickel,Hanspeter Pfister,Wojciech Matusik,Thabo Beeler,Mohamed Elgharib,Marc Habermann,Christian Theobalt*

Main category: cs.CV

TL;DR: 3DPR is an image-based relighting model that uses generative priors from multi-view OLAT images and a pre-trained generative head model to render novel, relit views of human heads from monocular portrait images.


<details>
  <summary>Details</summary>
Motivation: Traditional graphics approaches for relighting human heads from monocular images are underconstrained and limited by model assumptions. The paper aims to leverage rich priors from light stage data and generative models to overcome these limitations.

Method: Uses encoder-based inversion to embed input portraits into a generative head model's latent space, then employs a triplane-based reflectance network trained on lightstage OLAT data to synthesize high-fidelity OLAT images for image-based relighting.

Result: 3DPR outperforms previous methods in preserving identity and capturing lighting effects like specularities, self-shadows, and subsurface scattering, achieving physically accurate environmental relighting.

Conclusion: The proposed approach successfully combines generative priors from both in-the-wild images and lightstage data to enable high-quality portrait relighting that preserves identity and captures complex lighting effects.

Abstract: Rendering novel, relit views of a human head, given a monocular portrait
image as input, is an inherently underconstrained problem. The traditional
graphics solution is to explicitly decompose the input image into geometry,
material and lighting via differentiable rendering; but this is constrained by
the multiple assumptions and approximations of the underlying models and
parameterizations of these scene components. We propose 3DPR, an image-based
relighting model that leverages generative priors learnt from multi-view
One-Light-at-A-Time (OLAT) images captured in a light stage. We introduce a new
diverse and large-scale multi-view 4K OLAT dataset of 139 subjects to learn a
high-quality prior over the distribution of high-frequency face reflectance. We
leverage the latent space of a pre-trained generative head model that provides
a rich prior over face geometry learnt from in-the-wild image datasets. The
input portrait is first embedded in the latent manifold of such a model through
an encoder-based inversion process. Then a novel triplane-based reflectance
network trained on our lightstage data is used to synthesize high-fidelity OLAT
images to enable image-based relighting. Our reflectance network operates in
the latent space of the generative head model, crucially enabling a relatively
small number of lightstage images to train the reflectance model. Combining the
generated OLATs according to a given HDRI environment maps yields physically
accurate environmental relighting results. Through quantitative and qualitative
evaluations, we demonstrate that 3DPR outperforms previous methods,
particularly in preserving identity and in capturing lighting effects such as
specularities, self-shadows, and subsurface scattering. Project Page:
https://vcai.mpi-inf.mpg.de/projects/3dpr/

</details>


### [84] [Memory-SAM: Human-Prompt-Free Tongue Segmentation via Retrieval-to-Prompt](https://arxiv.org/abs/2510.15849)
*Joongwon Chae,Lihui Luo,Xi Yuan,Dongmei Yu,Zhenglin Chen,Lian Zhang,Peiwu Qin*

Main category: cs.CV

TL;DR: Memory-SAM is a training-free, human-prompt-free pipeline that automatically generates effective prompts for SAM2 using dense DINOv3 features and FAISS retrieval from a small memory of prior cases, achieving superior tongue segmentation performance.


<details>
  <summary>Details</summary>
Motivation: Accurate tongue segmentation is crucial for TCM analysis, but supervised models require large annotated datasets and SAM-family models remain prompt-driven, needing manual input.

Method: Uses dense DINOv3 features and FAISS retrieval to automatically generate foreground/background point prompts from a small memory of prior cases, guiding SAM2 without manual clicks or model fine-tuning.

Result: Achieves mIoU 0.9863 on mixed test split, surpassing FCN (0.8188) and detector-to-box SAM baseline (0.1839), with clear gains under real-world conditions.

Conclusion: Retrieval-to-prompt enables data-efficient, robust segmentation of irregular boundaries in tongue imaging without manual intervention or model training.

Abstract: Accurate tongue segmentation is crucial for reliable TCM analysis. Supervised
models require large annotated datasets, while SAM-family models remain
prompt-driven. We present Memory-SAM, a training-free, human-prompt-free
pipeline that automatically generates effective prompts from a small memory of
prior cases via dense DINOv3 features and FAISS retrieval. Given a query image,
mask-constrained correspondences to the retrieved exemplar are distilled into
foreground/background point prompts that guide SAM2 without manual clicks or
model fine-tuning. We evaluate on 600 expert-annotated images (300 controlled,
300 in-the-wild). On the mixed test split, Memory-SAM achieves mIoU 0.9863,
surpassing FCN (0.8188) and a detector-to-box SAM baseline (0.1839). On
controlled data, ceiling effects above 0.98 make small differences less
meaningful given annotation variability, while our method shows clear gains
under real-world conditions. Results indicate that retrieval-to-prompt enables
data-efficient, robust segmentation of irregular boundaries in tongue imaging.
The code is publicly available at https://github.com/jw-chae/memory-sam.

</details>


### [85] [BLIP3o-NEXT: Next Frontier of Native Image Generation](https://arxiv.org/abs/2510.15857)
*Jiuhai Chen,Le Xue,Zhiyang Xu,Xichen Pan,Shusheng Yang,Can Qin,An Yan,Honglu Zhou,Zeyuan Chen,Lifu Huang,Tianyi Zhou,Junnan Li,Silvio Savarese,Caiming Xiong,Ran Xu*

Main category: cs.CV

TL;DR: BLIP3o-NEXT is a fully open-source foundation model that unifies text-to-image generation and image editing in a single architecture, achieving state-of-the-art performance through an Autoregressive + Diffusion hybrid approach.


<details>
  <summary>Details</summary>
Motivation: To advance the frontier of native image generation by creating a unified model that can handle both text-to-image generation and image editing tasks within a single architecture, addressing challenges in image editing and leveraging key insights about architecture scaling, reinforcement learning, and data quality.

Method: Uses an Autoregressive + Diffusion hybrid architecture where an autoregressive model first generates discrete image tokens conditioned on multimodal inputs, then uses these hidden states as conditioning signals for a diffusion model to generate high-fidelity images, combining reasoning strength with fine-detail rendering.

Result: BLIP3o-NEXT achieves superior performance over existing models on various text-to-image and image-editing benchmarks, demonstrating strong image generation and editing capabilities with improved coherence and realism.

Conclusion: The model successfully integrates autoregressive reasoning with diffusion-based detail rendering, showing that architectural efficiency, reinforcement learning, and data quality are crucial factors for advancing native image generation, while image editing remains challenging but can be enhanced through post-training and data engineering.

Abstract: We present BLIP3o-NEXT, a fully open-source foundation model in the BLIP3
series that advances the next frontier of native image generation. BLIP3o-NEXT
unifies text-to-image generation and image editing within a single
architecture, demonstrating strong image generation and image editing
capabilities. In developing the state-of-the-art native image generation model,
we identify four key insights: (1) Most architectural choices yield comparable
performance; an architecture can be deemed effective provided it scales
efficiently and supports fast inference; (2) The successful application of
reinforcement learning can further push the frontier of native image
generation; (3) Image editing still remains a challenging task, yet instruction
following and the consistency between generated and reference images can be
significantly enhanced through post-training and data engine; (4) Data quality
and scale continue to be decisive factors that determine the upper bound of
model performance. Building upon these insights, BLIP3o-NEXT leverages an
Autoregressive + Diffusion architecture in which an autoregressive model first
generates discrete image tokens conditioned on multimodal inputs, whose hidden
states are then used as conditioning signals for a diffusion model to generate
high-fidelity images. This architecture integrates the reasoning strength and
instruction following of autoregressive models with the fine-detail rendering
ability of diffusion models, achieving a new level of coherence and realism.
Extensive evaluations of various text-to-image and image-editing benchmarks
show that BLIP3o-NEXT achieves superior performance over existing models.

</details>


### [86] [BiomedXPro: Prompt Optimization for Explainable Diagnosis with Biomedical Vision Language Models](https://arxiv.org/abs/2510.15866)
*Kaushitha Silva,Mansitha Eashwara,Sanduni Ubayasiri,Ruwan Tennakoon,Damayanthi Herath*

Main category: cs.CV

TL;DR: BiomedXPro is an evolutionary framework that uses LLMs to generate diverse, interpretable natural-language prompt pairs for biomedical diagnosis, outperforming state-of-the-art methods and providing verifiable clinical alignment.


<details>
  <summary>Details</summary>
Motivation: Current prompt optimization methods produce uninterpretable latent vectors or single prompts, lacking transparency and failing to capture the multi-faceted nature of clinical diagnosis, which limits trustworthiness in high-stakes medical settings.

Method: An evolutionary framework leveraging large language models as biomedical knowledge extractors and adaptive optimizers to automatically generate diverse ensembles of interpretable natural-language prompt pairs for disease diagnosis.

Result: BiomedXPro consistently outperforms state-of-the-art prompt-tuning methods across multiple biomedical benchmarks, especially in data-scarce few-shot settings, with strong semantic alignment between discovered prompts and statistically significant clinical features.

Conclusion: By producing diverse ensembles of interpretable prompts with verifiable clinical grounding, BiomedXPro represents a critical step toward developing more trustworthy and clinically-aligned AI systems for biomedical applications.

Abstract: The clinical adoption of biomedical vision-language models is hindered by
prompt optimization techniques that produce either uninterpretable latent
vectors or single textual prompts. This lack of transparency and failure to
capture the multi-faceted nature of clinical diagnosis, which relies on
integrating diverse observations, limits their trustworthiness in high-stakes
settings. To address this, we introduce BiomedXPro, an evolutionary framework
that leverages a large language model as both a biomedical knowledge extractor
and an adaptive optimizer to automatically generate a diverse ensemble of
interpretable, natural-language prompt pairs for disease diagnosis. Experiments
on multiple biomedical benchmarks show that BiomedXPro consistently outperforms
state-of-the-art prompt-tuning methods, particularly in data-scarce few-shot
settings. Furthermore, our analysis demonstrates a strong semantic alignment
between the discovered prompts and statistically significant clinical features,
grounding the model's performance in verifiable concepts. By producing a
diverse ensemble of interpretable prompts, BiomedXPro provides a verifiable
basis for model predictions, representing a critical step toward the
development of more trustworthy and clinically-aligned AI systems.

</details>


### [87] [LightsOut: Diffusion-based Outpainting for Enhanced Lens Flare Removal](https://arxiv.org/abs/2510.15868)
*Shr-Ruei Tsai,Wei-Cheng Chang,Jie-Ying Lee,Chih-Hai Su,Yu-Lun Liu*

Main category: cs.CV

TL;DR: LightsOut is a diffusion-based outpainting framework that enhances Single Image Flare Removal (SIFR) by reconstructing off-frame light sources, improving performance of existing SIFR methods without retraining.


<details>
  <summary>Details</summary>
Motivation: Lens flare degrades image quality and affects computer vision tasks. Current SIFR methods perform poorly when off-frame light sources are incomplete or absent.

Method: Uses a diffusion-based outpainting framework with multitask regression module and LoRA fine-tuned diffusion model to reconstruct off-frame light sources realistically and physically consistently.

Result: Comprehensive experiments show LightsOut consistently boosts performance of existing SIFR methods across challenging scenarios without additional retraining.

Conclusion: LightsOut serves as a universally applicable plug-and-play preprocessing solution for flare removal.

Abstract: Lens flare significantly degrades image quality, impacting critical computer
vision tasks like object detection and autonomous driving. Recent Single Image
Flare Removal (SIFR) methods perform poorly when off-frame light sources are
incomplete or absent. We propose LightsOut, a diffusion-based outpainting
framework tailored to enhance SIFR by reconstructing off-frame light sources.
Our method leverages a multitask regression module and LoRA fine-tuned
diffusion model to ensure realistic and physically consistent outpainting
results. Comprehensive experiments demonstrate LightsOut consistently boosts
the performance of existing SIFR methods across challenging scenarios without
additional retraining, serving as a universally applicable plug-and-play
preprocessing solution. Project page: https://ray-1026.github.io/lightsout/

</details>


### [88] [Skyfall-GS: Synthesizing Immersive 3D Urban Scenes from Satellite Imagery](https://arxiv.org/abs/2510.15869)
*Jie-Ying Lee,Yi-Ruei Liu,Shr-Ruei Tsai,Wei-Cheng Chang,Chung-Ho Wu,Jiewen Chan,Zhenjun Zhao,Chieh Hubert Lin,Yu-Lun Liu*

Main category: cs.CV

TL;DR: Skyfall-GS is a framework for creating large-scale 3D urban scenes by combining satellite imagery with diffusion models, enabling real-time exploration without costly 3D annotations.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the challenge of synthesizing large-scale, explorable 3D urban scenes without requiring expensive real-world 3D scans for training, by leveraging readily available data sources.

Method: The method synergizes satellite imagery (for coarse geometry) with open-domain diffusion models (for close-up appearances), using a curriculum-driven iterative refinement strategy to progressively enhance geometry and textures.

Result: Extensive experiments show that Skyfall-GS provides improved cross-view consistent geometry and more realistic textures compared to state-of-the-art approaches.

Conclusion: Skyfall-GS successfully creates city-block scale 3D scenes with real-time immersive exploration capabilities, demonstrating the viability of combining satellite data with diffusion models for 3D scene generation.

Abstract: Synthesizing large-scale, explorable, and geometrically accurate 3D urban
scenes is a challenging yet valuable task in providing immersive and embodied
applications. The challenges lie in the lack of large-scale and high-quality
real-world 3D scans for training generalizable generative models. In this
paper, we take an alternative route to create large-scale 3D scenes by
synergizing the readily available satellite imagery that supplies realistic
coarse geometry and the open-domain diffusion model for creating high-quality
close-up appearances. We propose \textbf{Skyfall-GS}, the first city-block
scale 3D scene creation framework without costly 3D annotations, also featuring
real-time, immersive 3D exploration. We tailor a curriculum-driven iterative
refinement strategy to progressively enhance geometric completeness and
photorealistic textures. Extensive experiments demonstrate that Skyfall-GS
provides improved cross-view consistent geometry and more realistic textures
compared to state-of-the-art approaches. Project page:
https://skyfall-gs.jayinnn.dev/

</details>


<div id='eess.SP'></div>

# eess.SP [[Back]](#toc)

### [89] [A Structured Family of Grassmannian Constellations via Geodesic Mapping for MIMO Noncoherent Communications](https://arxiv.org/abs/2510.15070)
*Álvaro Pendás-Recondo,Enrique Pendás-Recondo*

Main category: eess.SP

TL;DR: A novel Grassmannian constellation design for MIMO noncoherent communications that uses geodesic curves on Grassmann manifold, enabling single-antenna activation per time slot for reduced hardware complexity and power consumption.


<details>
  <summary>Details</summary>
Motivation: To develop efficient MIMO noncoherent communication systems without CSI that reduce hardware complexity and power consumption while maintaining good error performance.

Method: Built constellation design on geodesic curves of Grassmann manifold, creating structured constellations with single nonzero entry per row in space-time matrices (only one transmit antenna active per time slot).

Result: Achieves error performance comparable to state-of-the-art unstructured designs, reduces ML detector complexity by factor of M, enables simple bit labeling, and maintains spectral efficiency of 0.25-1 bps/Hz with maximum constellation size of 4M² points.

Conclusion: The proposed structured Grassmannian constellation design successfully balances performance and practical implementation benefits by reducing hardware complexity, power consumption, and computational complexity while maintaining competitive error rates.

Abstract: This work presents a novel structured family of Grassmannian constellations
for multiple-input multiple-output (MIMO) noncoherent communications over
Rayleigh block-fading channels, where neither the transmitter nor the receiver
has channel state information (CSI). The proposed constellation design is built
upon the geodesic curves of the Grassmann manifold, thereby exploiting its
underlying geometric structure. The resulting solution is limited in spectral
efficiency (with a maximum constellation size of $4M^2$ points, where $M$ is
the number of transmit antennas), targeting a rate in the range of $0.25$-$1$
bps/Hz. However, all space-time matrices resulting from this design exhibit the
remarkable property of having a single nonzero entry per row, meaning that only
one transmit antenna is active per time slot. This property significantly
reduces hardware complexity and implementation cost, while also lowering power
consumption, as only a single power amplifier is required for transmission.
Furthermore, within the constellation size limits, the proposed design achieves
error performance comparable to state-of-the-art optimization-based
unstructured designs, as validated through symbol error rate (SER) numerical
results. It also enables simple yet effective bit labeling, confirmed by
comparisons of bit error rate (BER) and SER, and reduces the computational
complexity of the maximum-likelihood (ML) detector for Grassmannian
constellations by a factor of $M$.

</details>


### [90] [Pulse Shaping Filter Design for Integrated Sensing & Communication with Zak-OTFS](https://arxiv.org/abs/2510.15195)
*Nishant Mehrotra,Sandesh Rao Mattu,Robert Calderbank*

Main category: eess.SP

TL;DR: This paper designs optimal pulse shaping filters for Zak-OTFS integrated sensing & communication that simultaneously achieve localization, orthogonality, and bandwidth/time-limitation - objectives that existing filters only partially satisfy.


<details>
  <summary>Details</summary>
Motivation: Existing pulse shaping filters for Zak-OTFS ISAC achieve only two of three critical objectives: localization (for sensing), orthogonality (for communication), and bandwidth/time-limitation (for spectral efficiency). No existing filter meets all three simultaneously.

Method: The authors use the Isotropic Orthogonal Transform Algorithm to design pulse shaping filters that simultaneously satisfy all three objectives.

Result: The proposed pulse shaping filters demonstrate improved data detection performance for communication and better input-output relation estimation for sensing compared to existing filter choices.

Conclusion: The designed filters successfully achieve the ideal combination of localization, orthogonality, and bandwidth/time-limitation, making them superior for Zak-OTFS integrated sensing & communication applications.

Abstract: Zak-OTFS is an emerging framework for integrated sensing & communication
(ISAC) in high delay and Doppler spread environments. A critical enabler for
ISAC with Zak-OTFS is the design of pulse shaping filters. For sensing, a
localized pulse shaping filter enables ideal input-output (I/O) relation
estimates close to the physical scattering channel. For communication,
orthogonality of the pulse shape on the information lattice prevents
inter-symbol interference, and no time and bandwidth expansion enables full
spectral efficiency. A filter simultaneously meeting all three objectives is
ideal for ISAC. Existing filter designs achieve two of the above objectives,
but not all three simultaneously. For instance, the sinc filter is orthogonal
and bandwidth/time-limited, but is not localized. The Gaussian filter is
localized and bandwidth/time-limited, but not orthogonal. The RRC filter is
localized and orthogonal, but not bandwidth/time-limited. A recently proposed
hybrid Gaussian-sinc filter is more localized than the sinc filter and
bandwidth/time-limited, but is not orthogonal. In this work, we design optimal
pulse shaping filters meeting all three objectives via the Isotropic Orthogonal
Transform Algorithm. The proposed pulse shaping filters offer improved data
detection (communication) and I/O relation estimation (sensing) performance
compared to existing filter choices in the literature.

</details>


### [91] [Multidimensional Physiology-Inspired Enhanced Vital Sign Monitoring Using MIMO mmWave Bio-radar](https://arxiv.org/abs/2510.15278)
*Heyao Zhu,Yimeng Zhao,Zirui Zhang,Huansheng Yi,Chenbin Gao,Canhua Xu,Jianqi Wang,Fugui Qi*

Main category: eess.SP

TL;DR: Proposes a two-stage vital sign enhancement method for MIMO bio-radar using physiological characteristics to improve multi-channel signal fusion efficiency.


<details>
  <summary>Details</summary>
Motivation: Address low efficiency of multi-channel signal fusion in current non-contact mmWave radar systems for vital signs monitoring, especially with aging population and chronic disease burden.

Method: Two-stage fusion: 1) Single-channel enhancement using physiological frequency band energy ratio and phase-aligned MRC; 2) Multi-channel fusion based on organ radiation spatial distribution characteristics with SNR-based channel screening and weighted fusion.

Result: Experimental results confirm existence of spatial distribution characteristics of organ radiation and analyze impact of distance and state on algorithm performance.

Conclusion: The proposed method effectively overcomes traditional limitations in MIMO bio-radar systems for vital sign detection through physiological characteristic-driven fusion strategy.

Abstract: With the intensiffcation of population aging and increasing burden of chronic
diseases, the demand for vital signs monitoring is becoming increasingly
urgent. A key challenge facing current non-contact detection technologies using
millimeter wave (mmWave) radar is the low efffciency of multi-channel signal
fusion in array radar systems based on equal weighting. To address this
challenge, this paper proposes a vital sign enhancement detection method for
multiple input and multiple output (MIMO) bio-radar, driven by multidimensional
physiological characteristics, which overcomes traditional limitations through
a two-stage fusion strategy. Stage 1: Enhanced Vital Sign Detection Using
Single-Channel Signals Based on Physiological Characteristics. First, a chest
wall multi-scattering point model is constructed. For single channel
time-distance two-dimensional echo signals, effective range bins are selected
based on the respiratory/cardiac physiological frequency band energy ratio, and
the signal-to-noise ratio (SNR) of respiration/heart signals is enhanced using
phase-aligned maximal ratio combining (MRC). Stage 2: Multi-Channel Fusion
Based on Organ Radiation Spatial Distribution Characteristics. The spatial
radiation characteristics of cardiopulmonary organs are introduced for the
ffrst time as the theoretical foundation for SNR-based channel screening,
channel attribute identiffcation, and multi-channel weighted fusion. Then, we
propose a template matching method to extract respiratory rate (RR) and heart
rate (HR) by adopting physical models of respiration and cardiac activities.
The experimental results demonstrate the existence of the spatial distribution
characteristics of organ radiation. In addition, we analyzed the impact of
distance and state on the algorithm from these two aspects.

</details>


### [92] [Multi-Target Flexible Angular Emulation for ISAC Base Station Testing Using a Conductive Amplitude and Phase Matrix Setup: Framework and Experimental Validation](https://arxiv.org/abs/2510.15457)
*Chunhui Li,Chengrui Wang,Zhiqiang Yuan,Wei Fan*

Main category: eess.SP

TL;DR: Proposes a conductive amplitude and phase matrix framework to emulate multiple targets with arbitrary RCS, range, angle, and Doppler profiles for ISAC base station testing using limited radar target simulator ports.


<details>
  <summary>Details</summary>
Motivation: Need to comprehensively evaluate ISAC base station functionalities and performance in realistic deployment scenarios, but challenged by emulating multiple targets with varied characteristics using limited RTS interface ports.

Method: Introduces a tunable conductive amplitude and phase modulation network between ISAC BS and RTS, with configurations for different sensing modes (ADTR and SATR), validated through two monostatic sensing scenarios with drones.

Result: Experimental results show the framework can accurately emulate joint RCS, range, velocity, and angular characteristics of multiple sensing targets in conductive test environments.

Conclusion: The proposed framework demonstrates significant potential for testing applications in sub-6 GHz ISAC base station development and validation.

Abstract: Comprehensive evaluation of the functionalities, algorithms, hardware
components, and performance characteristics of future integrated sensing and
communication (ISAC) base stations (BSs) under realistic deployment scenarios
in controlled laboratory environments represents a critical requirement for
ISAC technology advancement. A primary challenge in achieving this objective
involves the emulation of multiple targets with arbitrary radar cross-section
(RCS), range, angle, and Doppler profiles for ISAC BS equipped with large-scale
antenna arrays using radar target simulator (RTS) with limited interface ports.
In this work, we introduce a simple yet highly effective and practical
conductive amplitude and phase matrix framework to address this fundamental
challenge. The core concept involves introducing a tunable conductive amplitude
and phase modulation network in the test configuration between the ISAC BS
under test and a RTS. Based on this structure, we subsequently investigate the
corresponding configurations for different sensing operational modes of ISAC
BSs, specifically the array duplex transmission and reception (ADTR) mode and
the split-array transmission and reception (SATR) mode. For experimental
validation, we design two distinct monostatic sensing scenarios to demonstrate
the framework capabilities across both operational modes. The first scenario
involves dynamic multi-drone sensing validation for ADTR mode operation, while
the second scenario addresses static single-drone sensing for SATR mode
validation. The experimental results demonstrate that the proposed framework
can accurately emulate the joint RCS, range, velocity, and angular
characteristics of multiple sensing targets within the conductive test
environment, highlighting its significant potential for testing applications in
sub-6 GHz ISAC BS development and validation.

</details>


### [93] [Pseudo-Random TDM-MIMO FMCW Based Millimeter-Wave Sensing and Communication Integration for UAV Swarm](https://arxiv.org/abs/2510.15575)
*Yi Tao,Zhen Gao,Zhuoran Li,Ziwei Wan,Tuan Li,Chunli Zhu,Lei Chen,Guanghui Wen,Dezhi Zheng,Dusit Niyato*

Main category: eess.SP

TL;DR: Proposes an ISAC solution for UAV swarms using pseudo-random TDM-MIMO mmWave FMCW with novel chirp waveform, achieving simultaneous communication and sensing with dynamic resource allocation.


<details>
  <summary>Details</summary>
Motivation: ISAC enables resource sharing for UAV swarms, enhancing performance, flexibility, and efficiency in collaborative operations.

Method: Uses pseudo-random TDM-MIMO mmWave FMCW with novel ISAC chirp waveform, compressed sensing, chirp-division multiple access, and dynamic iterative computation for simultaneous data demodulation and sensing parameter estimation.

Result: Achieves ISAC in dynamic UAV flight scenarios, outperforms mmWave-LoRadar in communication and sensing, slightly lower sensing performance than traditional FMCW, maintains robustness under urban clutter.

Conclusion: The proposed scheme successfully enables integrated sensing and communications for UAV swarms with favorable performance and robustness in dynamic environments.

Abstract: The integrated sensing and communications (ISAC) can achieve the sharing of
hardware and spectrum resources, enabling efficient data transmission and
environmental sensing. This fusion is particularly important for unmanned
aerial vehicle (UAV) swarms, as it enhances the overall performance,
flexibility, and efficiency of such systems. To facilitate the collaborative
operations among UAVs, this paper proposes an ISAC solution based on the
pseudo-random time-division multiplexing (TDM)-multiple input multiple output
(MIMO) millimeter-wave (mmWave) frequency modulated continuous wave (FMCW).
Specifically, a novel ISAC chirp waveform is proposed to modulate data in both
the delay domain and complex amplitude, while also possessing high-precision
sensing capabilities. To address challenges in the TDM-MIMO, we utilize the
pseudo-random antenna selection and compressed sensing algorithms, ensuring
that the maximum unambiguous velocity is not compromised. Moreover, by
employing a chirp-division multiple access scheme, we propose an
interference-free multiple antenna transmission scheme to achieve dynamic
allocation of time-frequency resources and multi-user transmission. Finally, we
propose a communication and sensing fusion-based dynamic iterative computation
scheme, simultaneously achieving data demodulation and sensing parameter
estimation. Simulation results show that the proposed scheme can achieve ISAC
under the dynamic flight scenarios of UAVs. Meanwhile, the scheme outperforms
the mmWave-LoRadar in communication and sensing performance, yet its sensing
performance is slightly lower than that of the traditional FMCW. Under the
urban clutter modeling, the scheme still maintains favorable robustness despite
a certain degree of performance degradation.

</details>


### [94] [More on Boundary Behavior of Univalent Harmonic Mappings](https://arxiv.org/abs/2510.15689)
*Gebreslassie atsbha weldegebrial,hunduma legesse geleta*

Main category: eess.SP

TL;DR: This paper extends previous work on harmonic mappings by examining angular limits of arguments and logarithms of analytic functions under various conditions, and investigates dilatation behavior when the first derivative of harmonic functions approaches infinity at the boundary.


<details>
  <summary>Details</summary>
Motivation: To extend the boundary behavior analysis of injective harmonic mappings in the unit disk, building on Laugesen's and Bshouty's work on harmonic mappings under different conditions.

Method: Extending previous work by examining angular limits of arguments and logarithms of analytic functions, and analyzing dilatation behavior when the first derivative of harmonic functions approaches infinity at the boundary.

Result: Found angular limits of arguments and logarithms of analytic functions under various conditions, and determined that dilatation possesses only a finite set of zeros within any stolz angle when the first derivative of harmonic function at the boundary is positive infinity.

Conclusion: The research successfully extends boundary behavior analysis of harmonic mappings and provides new insights into angular limits and dilatation properties under specific derivative conditions.

Abstract: Many authors have examined various boundary behaviors of injective harmonic
mappings in the open unit disk. Building on Laugesen's work, Bshouty and others
explored the boundary behavior of harmonic mappings under different conditions.
In this paper, we extend their work and find out the angular limits of the
arguments and logarithms of analytic functions under various conditions. We
also examined the dilatation possesses only a finite set of zeros within any
stolz angle if the first derivative of harmonic function $f$ at the boundary is
positive infinity.

</details>


### [95] [Detection Seizure Onset Zone Using Circadian Fluctuating Epileptic Biomarkers: A Signal Processing and Machine Learning Approach](https://arxiv.org/abs/2510.15717)
*Mehdi Zekriyapanah Gashti,Mostafa Mohammadpour,Hassan Eshkiki*

Main category: eess.SP

TL;DR: This study investigates how circadian rhythms affect epilepsy biomarkers and identifies optimal timing for analysis, finding that biomarker rates are higher during sleep and that pathological HFOs and spike sequences are more precise for seizure onset zone prediction.


<details>
  <summary>Details</summary>
Motivation: Epileptic biomarkers vary over time, and understanding their temporal fluctuations can enhance surgical planning for epilepsy treatment by identifying optimal analysis times.

Method: Retrospective analysis of intracranial EEG data from focal epilepsy patients using automatic detection of biomarkers (spikes, spike sequences, HFOs, pathological HFOs) and alpha/delta ratio for sleep/wake classification.

Result: Sleep/wake classification achieved 84% AUC; all biomarker rates were higher during sleep; pathological HFOs and spike sequences were more precise indicators of seizure onset distance than spikes or HFOs alone.

Conclusion: Sleep data analysis is more effective for seizure onset zone prediction as epilepsy biomarker rates vary significantly between sleep and wake states, with pathological HFOs and spike sequences providing superior localization accuracy.

Abstract: Epileptic biomarkers play a crucial role in identifying the origin of
seizures, an essential aspect of pre-surgical planning for epilepsy treatment.
These biomarkers can vary significantly over time. By studying these temporal
fluctuations, we can enhance their effectiveness in guiding surgical planning.
This research focuses on examining how circadian rhythms influence epilepsy
biomarkers and aims to determine the optimal times for their analysis. To
investigate the relationship between epilepsy biomarkers and circadian rhythm,
the sleep/wake states first need to be classified. After the biomarkers are
identified, they are compared across these states. A retrospective analysis was
conducted on intracranial electroencephalography data from patients with focal
epilepsy. The biomarkers spike, sequence of spikes, high-frequency oscillations
(HFOs), and pathological HFOs were identified through automatic detection. The
alpha/delta ratio was also calculated to distinguish between asleep and awake
stages. Data from 9 patients were analyzed, and the classification of sleep and
wake states was achieved with an area under the curve of 84%. All biomarker
rates were higher during the sleep stage compared to the wake stage.
Pathological HFOs and the sequence of spikes proved to be more precise
indicators regarding distance to seizure onset than spikes or HFOs. Unlike
previous studies that relied predominantly on long-term spike biomarker
analysis, this study is the first to utilize a comprehensive set of biomarkers,
including HFOs, spike sequences, and pathological HFOs, to enhance seizure
onset zone prediction. The rates of epilepsy biomarkers during sleep vary
considerably from those seen while awake, making sleep data analysis more
effective for accurately predicting the seizure onset zone.

</details>


### [96] [On the Impact of Electromagnetic Interference and Inter-RIS Reflections in Indoor Factory Local 6G Networks](https://arxiv.org/abs/2510.15759)
*Ishan Rangajith Koralege,Nurul Huda Mahmood,Arthur Sousa de Sena,Italo Atzeni*

Main category: eess.SP

TL;DR: Analysis of EMI and IRR impacts on multi-RIS systems in 6G networks, showing joint effects degrade performance more than individual impacts, and proposing optimization algorithm for phase shifts.


<details>
  <summary>Details</summary>
Motivation: 6G networks require local networks with demanding performance, but RIS performance is affected by electromagnetic interference and inter-RIS reflections in practical deployments.

Method: Comprehensive analysis through system-level simulations and proposed alternate optimization algorithm using Riemannian conjugate gradient method to optimize RIS phase shifts considering spatial correlation.

Result: Joint EMI and IRR degrade system performance significantly, especially with larger RIS dimensions and higher transmit power. The optimization algorithm provides orders of magnitude gains in system sum rate and outage probability.

Conclusion: The proposed optimization method effectively mitigates adverse effects of EMI and IRR in multi-RIS systems, enabling better performance for local 6G networks.

Abstract: The Sixth Generation (6G) radio technology is expected to include local 6G
networks as a special use case, extending the capabilities of `generic' 6G
networks towards more demanding performance requirements. Reconfigurable
intelligent surfaces (RISs) offer a novel paradigm for next-generation wireless
communications, especially in the context of local 6G networks, enabling
advanced signal propagation control through intelligent phase-shift
configurations. However, in practical deployments, their performance can be
adversely affected by electromagnetic interference (EMI) from external sources
and inter-RIS reflections (IRR) caused by signal reflections between multiple
colocated RIS units. This paper presents a comprehensive analysis of the joint
impact of EMI and IRR in a multi-RIS multi-cell system deployed within an
indoor factory environment. A detailed evaluation study is first carried out to
investigate their impact on system performance. System-level simulations
demonstrate that the joint impact of EMI and IRR degrades system performance
more significantly than their individual effects, particularly as RIS
dimensions and transmit power increase. To address these adverse effects, an
alternate optimization algorithm using the Riemannian conjugate gradient method
is then proposed. The novel algorithm optimizes the phase shifts of the RIS
elements considering the spatial correlation among their associated channels,
and is found to provide up to several orders of magnitude gains in terms of the
system sum rate and the outage probability.

</details>


### [97] [RIS-assisted Atomic MIMO Receiver](https://arxiv.org/abs/2510.15763)
*Qihao Peng,Jiuyu Liu,Qu Luo,Yi Ma,Pei Xiao,Maged Elkashlan,George K. Karagiannidis*

Main category: eess.SP

TL;DR: A low-complexity MIMO receiver using RIS and PAM to align signal phases, reducing detection complexity via Adam-based gradient descent optimization.


<details>
  <summary>Details</summary>
Motivation: To address phase ambiguity and high complexity in MIMO receivers by leveraging RIS technology and PAM modulation.

Method: Reformulate non-convex optimization into tractable Frobenius norm minimization solved with Adam-based gradient descent algorithm.

Result: Effective phase alignment between transmitted signal and local oscillator, substantially reducing both signal detection complexity and overall receiver complexity.

Conclusion: The proposed RIS-assisted MIMO receiver architecture successfully mitigates phase ambiguity while maintaining low computational complexity.

Abstract: In this paper, we propose a novel and low-complexity atomic multiple-input
multiple-output (MIMO) receiver architecture assisted by a reconfigurable
intelligent surface (RIS). By introducing RIS and utilizing pulse amplitude
modulation (PAM), the phase of the transmitted signal is effectively aligned
with that of the local oscillator (LO), thereby mitigating phase ambiguity and
substantially reducing both signal detection complexity and overall receiver
complexity.To tackle the resulting non-convex optimization problem, we
reformulate it into a tractable form by minimizing the Frobenius norm of an
equivalent matrix, which is efficiently solved using an Adam-based gradient
descent algorithm.

</details>


### [98] [Resilient Full-Duplex ISAC in the Face of Imperfect SI Cancellation: Globally Optimal Timeslot Allocation and Beam Selection](https://arxiv.org/abs/2510.15810)
*Luis F. Abanto-Leon,Setareh Maghsudi*

Main category: eess.SP

TL;DR: Joint optimization of timeslot allocation and beam selection for downlink full-duplex ISAC systems under imperfect self-interference cancellation, transformed from a complex MINLP to tractable MILP for global optimality.


<details>
  <summary>Details</summary>
Motivation: Address radio resource management in full-duplex ISAC systems to enhance efficiency while dealing with the challenge of imperfect self-interference cancellation.

Method: Developed a reformulation strategy that transforms the semi-infinite nonconvex MINLP problem into a tractable MILP, enabling globally optimal solutions.

Result: The approach provides globally optimal solutions for joint timeslot allocation and beam selection, enhancing system efficiency and resilience against residual self-interference.

Conclusion: The proposed method successfully coordinates timeslot allocation and beam selection optimization, improving full-duplex ISAC system performance under practical constraints of imperfect self-interference cancellation.

Abstract: This work addresses the radio resource management (RRM) design in downlink
full-duplex integrated sensing and communications (ISAC) systems, jointly
optimizing timeslot allocation and beam selection under imperfect
self-interference cancellation. Timeslot allocation governs the distribution of
discrete channel uses between sensing and communication tasks, while beam
selection determines transmit and receive directions along with adaptive
beamwidths. The joint design leads to a semi-infinite, nonconvex mixed-integer
nonlinear program (MINLP), which is difficult to solve. To overcome this, we
develop a tailored reformulation strategy that transforms the problem into a
tractable mixed-integer linear program (MILP), enabling globally optimal
solutions. Our approach provides insights into the coordinated optimization of
timeslot allocation and beam selection, enhancing the efficiency of full-duplex
ISAC systems while ensuring resilience against residual self-interference.

</details>


### [99] [Rydberg Atomic Quantum Satellites for Enhanced Ground-to-Space Direct Uplink Access](https://arxiv.org/abs/2510.15773)
*Qihao Peng Tierui Gong,Zihang Song,Qu Luo,Cunhua Pan,Pei Xiao,Chau Yuen*

Main category: eess.SP

TL;DR: RAQ-MIMO satellites outperform conventional RF-MIMO in ground-to-space uplink, achieving squaring gains under Rayleigh fading and significant improvements in power-constrained scenarios.


<details>
  <summary>Details</summary>
Motivation: To enhance direct ground-to-space uplink access by leveraging Rydberg atomic quantum technology for improved MIMO satellite performance.

Method: Analytical evaluation of Rydberg atom impact on channel estimation via closed-form MSE/NMSE expressions, derivation of achievable rate bounds for MRC and ZF detection, and Monte Carlo simulations.

Result: RAQ-MIMO outperforms conventional RF-MIMO under both Rayleigh and satellite channels, with squaring gains in Rayleigh fading and performance improvements in LoS-dominated scenarios due to reduced noise background.

Conclusion: RAQ-MIMO enables smaller antenna apertures, lower transmit power, and longer communication ranges, advancing next-generation satellite networks.

Abstract: This paper investigates the performance advantages of Rydberg atomic quantum
(RAQ)-based multiple-input multiple-output (MIMO) satellites for enhancing
direct ground-to-space uplink access.We analytically evaluate the impact of
Rydberg atoms on channel estimation by deriving closed-form expressions for the
mean-square error (MSE) and normalized mean-square error (NMSE). Based on the
estimated channels, we further derive lower bounds on the achievable data rates
for maximum ratio combining (MRC) and zero-forcing (ZF) detection schemes.
Rigorous analysis demonstrates that RAQ-MIMO outperforms conventional
radio-frequency (RF) MIMO under both Rayleigh and satellite channel conditions.
Specifically, compared with conventional MIMO, RAQR achieves a ``squaring" gain
under Rayleigh fading, especially in long-distance transmission scenarios with
stringent power constraints. In contrast, under line-of-sight (LoS)-dominated
satellite channels, this gain saturates as channel-estimation benefits
diminish, with the remaining improvement primarily arising from the normalized
noise background. Monte Carlo simulations validate the analytical results and
show that the performance gains of RAQ-MIMO satellites translate into smaller
antenna apertures, lower transmit power, and longer communication ranges,
thereby paving the way for next-generation satellite networks.

</details>


### [100] [From Active to Battery-Free: Rydberg Atomic Quantum Receivers for Self-Sustained SWIPT-MIMO Networks](https://arxiv.org/abs/2510.15784)
*Qihao Peng,Qu Luo,Zheng Chu,Neng Ye,Hong Ren,Cunhua Pan,Lixia Xiao,Pei Xiao*

Main category: eess.SP

TL;DR: Proposed a hybrid SWIPT-enabled MIMO architecture using RF transmitter for downlink and Rydberg atomic quantum receiver for uplink, with joint transmission scheme and power-splitting optimization to maximize sum rate.


<details>
  <summary>Details</summary>
Motivation: To enable battery-free communication for IoT devices by reliably detecting weak uplink signals from devices powered only by harvested energy, through integration of quantum receiver technology with SWIPT.

Method: Derived closed-form lower bounds for uplink rates (MRC/ZF), downlink rates and harvested energy (MRT/ZF), then used iterative algorithm with best monomial approximation and geometric programming to solve the non-convex optimization problem.

Result: Simulations validated tightness of derived lower bounds and demonstrated superiority of proposed algorithm over benchmark schemes.

Conclusion: Integration of RAQR with SWIPT-enabled MIMO enables reliable detection of weak uplink signals from energy-harvesting IoT devices, achieving battery-free communication.

Abstract: In this paper, we proposed a hybrid simultaneous wireless information and
power transfer (SWIPT)-enabled multiple-input multiple-output (MIMO)
architecture, where the base station (BS) uses a conventional RF transmitter
for downlink transmission and a Rydberg atomic quantum receiver (RAQR) for
receiving uplink signal from Internet of Things (IoT) devices. To fully exploit
this integration, we jointly design the transmission scheme and the
power-splitting strategy to maximize the sum rate, which leads to a non-convex
problem. To address this challenge, we first derive closed-form lower bounds on
the uplink achievable rates for maximum ratio combining (MRC) and zero-forcing
(ZF), as well as on the downlink rate and harvested energy for maximum ratio
transmission (MRT) and ZF precoding. Building upon these bounds, we propose an
iterative algorithm relying on the best monomial approximation and geometric
programming (GP) to solve the non-convex problem. Finally, simulations validate
the tightness of our derived lower bounds and demonstrate the superiority of
the proposed algorithm over benchmark schemes. Importantly, by integrating RAQR
with SWIPT-enabled MIMO, the BS can reliably detect weak uplink signals from
IoT devices powered only by harvested energy, enabling battery-free
communication.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [101] [OpenEstimate: Evaluating LLMs on Reasoning Under Uncertainty with Real-World Data](https://arxiv.org/abs/2510.15096)
*Alana Renda,Jillian Ross,Michael Cafarella,Jacob Andreas*

Main category: cs.AI

TL;DR: OpenEstimate is a benchmark for evaluating language models on numerical estimation tasks requiring probabilistic reasoning under uncertainty, showing current models produce inaccurate and overconfident priors.


<details>
  <summary>Details</summary>
Motivation: Real-world LM applications require reasoning under uncertainty, but current evaluations focus on well-defined problems, creating a gap in assessing LM performance on uncertain scenarios.

Method: Created OpenEstimate benchmark with numerical estimation tasks requiring models to synthesize background information and express predictions as probabilistic priors, assessing accuracy and calibration.

Result: Across six frontier LMs, elicited priors were often inaccurate and overconfident. Performance improved modestly with uncertainty elicitation methods but was largely unaffected by sampling strategy, reasoning effort, or prompt design.

Conclusion: OpenEstimate provides a challenging evaluation platform for developing LMs that are better at probabilistic estimation and reasoning under uncertainty, addressing a critical gap in current LM assessment.

Abstract: Real-world settings where language models (LMs) are deployed -- in domains
spanning healthcare, finance, and other forms of knowledge work -- require
models to grapple with incomplete information and reason under uncertainty. Yet
most LM evaluations focus on problems with well-defined answers and success
criteria. This gap exists in part because natural problems involving
uncertainty are difficult to construct: given that LMs have access to most of
the same knowledge as humans, it is non-trivial to design questions for which
LMs will struggle to produce correct answers, but which humans can answer
reliably. As a result, LM performance on reasoning under uncertainty remains
poorly characterized. To address this gap, we introduce OpenEstimate, an
extensible, multi-domain benchmark for evaluating LMs on numerical estimation
tasks that require models to synthesize significant amounts of background
information and express predictions as probabilistic priors. We assess these
priors for accuracy and calibration, quantifying their usefulness relative to
samples from the true distribution of interest. Across six frontier LMs, we
find that LM-elicited priors are often inaccurate and overconfident.
Performance improves modestly depending on how uncertainty is elicited from the
model, but is largely unaffected by changes in sampling strategy, reasoning
effort, or prompt design. The OpenEstimate benchmark thus offers a challenging
evaluation for frontier LMs and a platform for developing models that are
better at probabilistic estimation and reasoning under uncertainty.

</details>


### [102] [Procedural Game Level Design with Deep Reinforcement Learning](https://arxiv.org/abs/2510.15120)
*Miraç Buğra Özkan*

Main category: cs.AI

TL;DR: A novel method for procedural level design using Deep Reinforcement Learning (DRL) with two agents: a hummingbird that collects flowers and a floating island that generates flower layouts.


<details>
  <summary>Details</summary>
Motivation: To enable dynamic, replayable, and scalable game environments with reduced manual effort through procedural content generation using AI.

Method: Uses two agents trained with Proximal Policy Optimization (PPO) in Unity ML-Agents: a hummingbird agent for collecting flowers and a floating island agent for generating flower layouts based on obstacles and performance feedback.

Result: The system produces effective agent behavior and robust generalization across environmental configurations, enabling emergent behavior between the agents.

Conclusion: This work demonstrates DRL's potential for autonomous game level design, showing how AI agents can both generate and solve content in creative game development.

Abstract: Procedural content generation (PCG) has become an increasingly popular
technique in game development, allowing developers to generate dynamic,
replayable, and scalable environments with reduced manual effort. In this
study, a novel method for procedural level design using Deep Reinforcement
Learning (DRL) within a Unity-based 3D environment is proposed. The system
comprises two agents: a hummingbird agent, acting as a solver, and a floating
island agent, responsible for generating and placing collectible objects
(flowers) on the terrain in a realistic and context-aware manner. The
hummingbird is trained using the Proximal Policy Optimization (PPO) algorithm
from the Unity ML-Agents toolkit. It learns to navigate through the terrain
efficiently, locate flowers, and collect them while adapting to the
ever-changing procedural layout of the island. The island agent is also trained
using the Proximal Policy Optimization (PPO) algorithm. It learns to generate
flower layouts based on observed obstacle positions, the hummingbird's initial
state, and performance feedback from previous episodes. The interaction between
these agents leads to emergent behavior and robust generalization across
various environmental configurations. The results demonstrate that the approach
not only produces effective and efficient agent behavior but also opens up new
opportunities for autonomous game level design driven by machine learning. This
work highlights the potential of DRL in enabling intelligent agents to both
generate and solve content in virtual environments, pushing the boundaries of
what AI can contribute to creative game development processes.

</details>


### [103] [Towards Error Centric Intelligence I, Beyond Observational Learning](https://arxiv.org/abs/2510.15128)
*Marcus A. Thomas*

Main category: cs.AI

TL;DR: AGI progress is limited by theory, not data or scale. Observational learning alone is insufficient for interventional competence. The paper proposes Causal Mechanics with principles for error discovery and correction.


<details>
  <summary>Details</summary>
Motivation: Current AGI approaches are theory-limited rather than data-limited. Observational adequacy doesn't guarantee interventional competence, as observationally equivalent worlds can diverge under interventions.

Method: Proposes Causal Mechanics framework with structural principles: differential Locality and Autonomy Principle for modular interventions, gauge-invariant Independent Causal Mechanisms for separability, and Compositional Autonomy Principle for analogy preservation.

Result: A theoretical framework that treats hypothesis space change as first-class operation and uses probabilistic structure only when useful, with actionable diagnostics for error discovery.

Conclusion: The goal is to create systems that can convert unreachable errors into reachable ones and correct them, providing a scaffold for AGI development focused on error-centric learning and hypothesis space expansion.

Abstract: We argue that progress toward AGI is theory limited rather than data or scale
limited. Building on the critical rationalism of Popper and Deutsch, we
challenge the Platonic Representation Hypothesis. Observationally equivalent
worlds can diverge under interventions, so observational adequacy alone cannot
guarantee interventional competence. We begin by laying foundations,
definitions of knowledge, learning, intelligence, counterfactual competence and
AGI, and then analyze the limits of observational learning that motivate an
error centric shift. We recast the problem as three questions about how
explicit and implicit errors evolve under an agent's actions, which errors are
unreachable within a fixed hypothesis space, and how conjecture and criticism
expand that space. From these questions we propose Causal Mechanics, a
mechanisms first program in which hypothesis space change is a first class
operation and probabilistic structure is used when useful rather than presumed.
We advance structural principles that make error discovery and correction
tractable, including a differential Locality and Autonomy Principle for modular
interventions, a gauge invariant form of Independent Causal Mechanisms for
separability, and the Compositional Autonomy Principle for analogy
preservation, together with actionable diagnostics. The aim is a scaffold for
systems that can convert unreachable errors into reachable ones and correct
them.

</details>


### [104] [HugAgent: Evaluating LLMs in Simulating Human-Like Individual Reasoning on Open-Ended Tasks](https://arxiv.org/abs/2510.15144)
*Chance Jiajie Li,Zhenze Mo,Yuhan Tang,Ao Qu,Jiayi Wu,Kaiya Ivy Zhao,Yulu Gan,Jie Fan,Jiangbo Yu,Hang Jiang,Paul Pu Liang,Jinhua Zhao,Luis Alberto Alonso Pastor,Kent Larson*

Main category: cs.AI

TL;DR: HugAgent is a benchmark for adapting machine reasoning to individual human reasoning styles, featuring synthetic and human tracks to evaluate how well models can predict specific people's belief evolution.


<details>
  <summary>Details</summary>
Motivation: Current LLMs capture population-level consensus but erase individual reasoning styles and belief trajectories, limiting human-like reasoning in machines.

Method: Dual-track benchmark design: synthetic track for scale and systematic stress tests, and human track for ecologically valid "out-loud" reasoning data.

Result: Experiments with state-of-the-art LLMs reveal persistent adaptation gaps in capturing individual reasoning evolution.

Conclusion: HugAgent provides the first extensible benchmark for aligning machine reasoning with the individuality of human thought, with open-sourced benchmark and chatbot tools.

Abstract: Simulating human reasoning in open-ended tasks has been a long-standing
aspiration in AI and cognitive science. While large language models now
approximate human responses at scale, they remain tuned to population-level
consensus, often erasing the individuality of reasoning styles and belief
trajectories. To advance the vision of more human-like reasoning in machines,
we introduce HugAgent (Human-Grounded Agent Benchmark), a benchmark for
average-to-individual reasoning adaptation. The task is to predict how a
specific person would reason and update their beliefs in novel scenarios, given
partial evidence of their past views. HugAgent adopts a dual-track design: a
synthetic track for scale and systematic stress tests, and a human track for
ecologically valid, "out-loud" reasoning data. This design enables scalable,
reproducible evaluation of intra-agent fidelity: whether models can capture not
just what people believe, but how their reasoning evolves. Experiments with
state-of-the-art LLMs reveal persistent adaptation gaps, positioning HugAgent
as the first extensible benchmark for aligning machine reasoning with the
individuality of human thought. Our benchmark and chatbot are open-sourced as
HugAgent (https://anonymous.4open.science/r/HugAgent) and TraceYourThinking
(https://anonymous.4open.science/r/trace-your-thinking).

</details>


### [105] [WELD: A Large-Scale Longitudinal Dataset of Emotional Dynamics for Ubiquitous Affective Computing](https://arxiv.org/abs/2510.15221)
*Xiao Sun*

Main category: cs.AI

TL;DR: This paper presents the largest longitudinal workplace emotion dataset with 733,651 facial expression records from 38 employees collected over 30.5 months, enabling research in emotion recognition and affective dynamics.


<details>
  <summary>Details</summary>
Motivation: Automated emotion recognition in real-world workplace settings is challenging due to scarce large-scale, longitudinal datasets collected in naturalistic environments.

Method: Collected 733,651 facial expression records from 38 employees over 30.5 months using deep learning-based facial expression recognition, with comprehensive metadata and 32 extended emotional metrics.

Result: High data quality validated through replication of psychological patterns (weekend effect: +192% valence improvement), perfect predictive validity for employee turnover (AUC=1.0), and baseline models achieving 91.2% accuracy for emotion classification.

Conclusion: This is the largest and longest longitudinal workplace emotion dataset publicly available, enabling research in emotion recognition, affective dynamics modeling, and emotion-aware system design.

Abstract: Automated emotion recognition in real-world workplace settings remains a
challenging problem in affective computing due to the scarcity of large-scale,
longitudinal datasets collected in naturalistic environments. We present a
novel dataset comprising 733,651 facial expression records from 38 employees
collected over 30.5 months (November 2021 to May 2024) in an authentic office
environment. Each record contains seven emotion probabilities (neutral, happy,
sad, surprised, fear, disgusted, angry) derived from deep learning-based facial
expression recognition, along with comprehensive metadata including job roles,
employment outcomes, and personality traits. The dataset uniquely spans the
COVID-19 pandemic period, capturing emotional responses to major societal
events including the Shanghai lockdown and policy changes. We provide 32
extended emotional metrics computed using established affective science
methods, including valence, arousal, volatility, predictability, inertia, and
emotional contagion strength. Technical validation demonstrates high data
quality through successful replication of known psychological patterns (weekend
effect: +192% valence improvement, p < 0.001; diurnal rhythm validated) and
perfect predictive validity for employee turnover (AUC=1.0). Baseline
experiments using Random Forest and LSTM models achieve 91.2% accuracy for
emotion classification and R2 = 0.84 for valence prediction. This is the
largest and longest longitudinal workplace emotion dataset publicly available,
enabling research in emotion recognition, affective dynamics modeling,
emotional contagion, turnover prediction, and emotion-aware system design.

</details>


### [106] [From Checklists to Clusters: A Homeostatic Account of AGI Evaluation](https://arxiv.org/abs/2510.15236)
*Brett Reynolds*

Main category: cs.AI

TL;DR: AGI evaluations should use asymmetric domain weighting based on causal centrality and measure persistence across sessions, not just snapshot scores, to better assess general intelligence as a homeostatic property cluster.


<details>
  <summary>Details</summary>
Motivation: Current AGI evaluations use symmetric weights and snapshot testing, which treats all domains as equally important and cannot distinguish durable capabilities from brittle performances that fail under delay or stress.

Method: Proposes two battery-compatible extensions: centrality-prior scores using CHC-derived weights with sensitivity analysis, and a Cluster Stability Index family measuring profile persistence, durable learning, and error correction.

Result: These additions preserve multidomain breadth while reducing brittleness and gaming in AGI evaluations.

Conclusion: General intelligence should be understood as a homeostatic property cluster requiring evidence of persistence, with testable predictions and black-box protocols that labs can adopt without architectural access.

Abstract: Contemporary AGI evaluations report multidomain capability profiles, yet they
typically assign symmetric weights and rely on snapshot scores. This creates
two problems: (i) equal weighting treats all domains as equally important when
human intelligence research suggests otherwise, and (ii) snapshot testing can't
distinguish durable capabilities from brittle performances that collapse under
delay or stress. I argue that general intelligence -- in humans and potentially
in machines -- is better understood as a homeostatic property cluster: a set of
abilities plus the mechanisms that keep those abilities co-present under
perturbation. On this view, AGI evaluation should weight domains by their
causal centrality (their contribution to cluster stability) and require
evidence of persistence across sessions. I propose two battery-compatible
extensions: a centrality-prior score that imports CHC-derived weights with
transparent sensitivity analysis, and a Cluster Stability Index family that
separates profile persistence, durable learning, and error correction. These
additions preserve multidomain breadth while reducing brittleness and gaming. I
close with testable predictions and black-box protocols labs can adopt without
architectural access.

</details>


### [107] [Multi-dimensional Data Analysis and Applications Basing on LLM Agents and Knowledge Graph Interactions](https://arxiv.org/abs/2510.15258)
*Xi Wang,Xianyao Ling,Kun Li,Gang Yin,Liang Zhang,Jiang Wu,Jun Xu,Fu Zhang,Wenbo Lei,Annie Wang,Peng Gong*

Main category: cs.AI

TL;DR: A method combining LLM agents and Knowledge Graphs for dynamic multi-dimensional data analysis, enabling real-time KG construction from unstructured data and interactive exploration.


<details>
  <summary>Details</summary>
Motivation: Address limitations of LLMs (hallucination, real-time updates) and static KGs by creating a collaborative ecosystem for analyzing massive, heterogeneous multi-dimensional data.

Method: Uses LLM agents to automatically extract product data from unstructured data, constructs and visualizes KG in real-time, and provides interactive platform for deep node exploration.

Result: Shows significant advantages in product ecosystem analysis, relationship mining, and user-driven exploratory analysis.

Conclusion: Provides new ideas and tools for multi-dimensional data analysis through dynamic LLM-KG interactions.

Abstract: In the current era of big data, extracting deep insights from massive,
heterogeneous, and complexly associated multi-dimensional data has become a
significant challenge. Large Language Models (LLMs) perform well in natural
language understanding and generation, but still suffer from "hallucination"
issues when processing structured knowledge and are difficult to update in
real-time. Although Knowledge Graphs (KGs) can explicitly store structured
knowledge, their static nature limits dynamic interaction and analytical
capabilities. Therefore, this paper proposes a multi-dimensional data analysis
method based on the interactions between LLM agents and KGs, constructing a
dynamic, collaborative analytical ecosystem. This method utilizes LLM agents to
automatically extract product data from unstructured data, constructs and
visualizes the KG in real-time, and supports users in deep exploration and
analysis of graph nodes through an interactive platform. Experimental results
show that this method has significant advantages in product ecosystem analysis,
relationship mining, and user-driven exploratory analysis, providing new ideas
and tools for multi-dimensional data analysis.

</details>


### [108] [Experience-Driven Exploration for Efficient API-Free AI Agents](https://arxiv.org/abs/2510.15259)
*Chenwei Tang,Jingyu Xing,Xinyu Liu,Zizhou Wang,Jiawei Du,Liangli Zhen,Jiancheng Lv*

Main category: cs.AI

TL;DR: KG-Agent is an experience-driven framework that structures pixel-based GUI interactions into a State-Action Knowledge Graph to improve efficiency and strategic planning in API-free environments.


<details>
  <summary>Details</summary>
Motivation: Most software lacks accessible APIs, forcing agents to operate through pixel-based GUIs, which leads to myopic decisions and inefficient trial-and-error exploration in LLM-based agents.

Method: KG-Agent builds a persistent State-Action Knowledge Graph from raw pixel interactions, links functionally similar GUI states, and uses a hybrid intrinsic reward mechanism combining state value and novelty rewards for strategic planning.

Result: KG-Agent demonstrates significant improvements in exploration efficiency and strategic depth over state-of-the-art methods in complex GUI-based environments like Civilization V and Slay the Spire.

Conclusion: The proposed framework effectively addresses efficiency bottlenecks in API-free settings by enabling better generalization from historical experiences and supporting long-horizon reasoning through graph-based knowledge representation.

Abstract: Most existing software lacks accessible Application Programming Interfaces
(APIs), requiring agents to operate solely through pixel-based Graphical User
Interfaces (GUIs). In this API-free setting, large language model (LLM)-based
agents face severe efficiency bottlenecks: limited to local visual experiences,
they make myopic decisions and rely on inefficient trial-and-error, hindering
both skill acquisition and long-term planning. To address these challenges, we
propose KG-Agent, an experience-driven learning framework that structures an
agent's raw pixel-level interactions into a persistent State-Action Knowledge
Graph (SA-KG). KG-Agent overcomes inefficient exploration by linking
functionally similar but visually distinct GUI states, forming a rich
neighborhood of experience that enables the agent to generalize from a diverse
set of historical strategies. To support long-horizon reasoning, we design a
hybrid intrinsic reward mechanism based on the graph topology, combining a
state value reward for exploiting known high-value pathways with a novelty
reward that encourages targeted exploration. This approach decouples strategic
planning from pure discovery, allowing the agent to effectively value setup
actions with delayed gratification. We evaluate KG-Agent in two complex,
open-ended GUI-based decision-making environments (Civilization V and Slay the
Spire), demonstrating significant improvements in exploration efficiency and
strategic depth over the state-of-the-art methods.

</details>


### [109] [AUGUSTUS: An LLM-Driven Multimodal Agent System with Contextualized User Memory](https://arxiv.org/abs/2510.15261)
*Jitesh Jain,Shubham Maheshwari,Ning Yu,Wen-mei Hwu,Humphrey Shi*

Main category: cs.AI

TL;DR: AUGUSTUS is a multimodal agent system that uses semantic tags and graph-structured memory for efficient concept-driven retrieval, outperforming traditional multimodal RAG approaches.


<details>
  <summary>Details</summary>
Motivation: Existing agent systems focus on text-only memory, ignoring multimodal signals. Inspired by human memory's multimodal nature in cognitive science.

Method: Four-stage loop: encode inputs, store in memory, retrieve relevant context, act. Uses semantic tags with context in graph-structured multimodal memory instead of vector databases.

Result: Outperforms traditional multimodal RAG while being 3.5x faster for ImageNet classification. Beats MemGPT on MSC benchmark.

Conclusion: Graph-structured multimodal contextual memory with semantic tags enables efficient concept-driven retrieval for multimodal agents.

Abstract: Riding on the success of LLMs with retrieval-augmented generation (RAG),
there has been a growing interest in augmenting agent systems with external
memory databases. However, the existing systems focus on storing text
information in their memory, ignoring the importance of multimodal signals.
Motivated by the multimodal nature of human memory, we present AUGUSTUS, a
multimodal agent system aligned with the ideas of human memory in cognitive
science. Technically, our system consists of 4 stages connected in a loop: (i)
encode: understanding the inputs; (ii) store in memory: saving important
information; (iii) retrieve: searching for relevant context from memory; and
(iv) act: perform the task. Unlike existing systems that use vector databases,
we propose conceptualizing information into semantic tags and associating the
tags with their context to store them in a graph-structured multimodal
contextual memory for efficient concept-driven retrieval. Our system
outperforms the traditional multimodal RAG approach while being 3.5 times
faster for ImageNet classification and outperforming MemGPT on the MSC
benchmark.

</details>


### [110] [WebGen-V Bench: Structured Representation for Enhancing Visual Design in LLM-based Web Generation and Evaluation](https://arxiv.org/abs/2510.15306)
*Kuang-Da Wang,Zhao Wang,Yotaro Shimose,Wei-Yao Wang,Shingo Takamatsu*

Main category: cs.AI

TL;DR: WebGen-V is a new benchmark and framework for instruction-to-HTML generation that improves data quality and evaluation granularity through agentic crawling, structured data representation, and section-level multimodal evaluation.


<details>
  <summary>Details</summary>
Motivation: Recent advancements in using LLMs for coding and multimodal understanding have created a need for better benchmarks and evaluation methods for instruction-to-HTML generation tasks.

Method: Three key innovations: (1) agentic crawling framework for collecting real-world webpages, (2) structured section-wise data representation with metadata, UI screenshots, and JSON-formatted assets, (3) section-level multimodal evaluation protocol aligning text, layout, and visuals.

Result: Experiments with state-of-the-art LLMs and ablation studies validate the effectiveness of the structured data and section-wise evaluation, showing the contribution of each component.

Conclusion: WebGen-V is the first work to enable high-granularity agentic crawling and evaluation for instruction-to-HTML generation, providing a unified pipeline from data acquisition to multimodal assessment.

Abstract: Witnessed by the recent advancements on leveraging LLM for coding and
multimodal understanding, we present WebGen-V, a new benchmark and framework
for instruction-to-HTML generation that enhances both data quality and
evaluation granularity. WebGen-V contributes three key innovations: (1) an
unbounded and extensible agentic crawling framework that continuously collects
real-world webpages and can leveraged to augment existing benchmarks; (2) a
structured, section-wise data representation that integrates metadata,
localized UI screenshots, and JSON-formatted text and image assets, explicit
alignment between content, layout, and visual components for detailed
multimodal supervision; and (3) a section-level multimodal evaluation protocol
aligning text, layout, and visuals for high-granularity assessment. Experiments
with state-of-the-art LLMs and ablation studies validate the effectiveness of
our structured data and section-wise evaluation, as well as the contribution of
each component. To the best of our knowledge, WebGen-V is the first work to
enable high-granularity agentic crawling and evaluation for instruction-to-HTML
generation, providing a unified pipeline from real-world data acquisition and
webpage generation to structured multimodal assessment.

</details>


### [111] [VERITAS: Leveraging Vision Priors and Expert Fusion to Improve Multimodal Data](https://arxiv.org/abs/2510.15317)
*Tingqiao Xu,Ziru Zeng,Jiayu Chen*

Main category: cs.AI

TL;DR: VERITAS is a pipeline that enhances SFT data quality for large multimodal models by integrating vision priors and multiple LMMs with statistical methods to reduce factual errors and hallucinations.


<details>
  <summary>Details</summary>
Motivation: Current data enhancement methods for large multimodal models suffer from factual errors and hallucinations due to inadequate visual perception, which affects model performance.

Method: Leverages visual recognition models (RAM++) and OCR systems (PP-OCRv4) to extract structured vision priors, uses three LMMs (GPT-4o, Gemini-2.5-Pro, Doubao-1.5-pro) to evaluate original answers with critique rationales and scores, fuses scores statistically into consensus ground truth, trains lightweight critic model via GRPO, and refines answers through LMM iterations.

Result: Models fine-tuned with VERITAS-processed data consistently outperform those using raw data across six multimodal benchmarks, especially in text-rich and fine-grained reasoning tasks. The critic model achieves enhanced capability comparable to state-of-the-art LMMs while being significantly more efficient.

Conclusion: VERITAS effectively improves SFT data quality for LMMs through systematic integration of vision priors and multiple LMM evaluation, leading to better model performance and more efficient reasoning capabilities.

Abstract: The quality of supervised fine-tuning (SFT) data is crucial for the
performance of large multimodal models (LMMs), yet current data enhancement
methods often suffer from factual errors and hallucinations due to inadequate
visual perception. To address this challenge, we propose VERITAS, a pipeline
that systematically integrates vision priors and multiple state-of-the-art LMMs
with statistical methods to enhance SFT data quality. VERITAS leverages visual
recognition models (RAM++) and OCR systems (PP-OCRv4) to extract structured
vision priors, which are combined with images, questions, and answers. Three
LMMs (GPT-4o, Gemini-2.5-Pro, Doubao-1.5-pro) evaluate the original answers,
providing critique rationales and scores that are statistically fused into a
high-confidence consensus score serving as ground truth. Using this consensus,
we train a lightweight critic model via Group Relative Policy Optimization
(GRPO), enhancing reasoning capabilities efficiently. Each LMM then refines the
original answers based on the critiques, generating new candidate answers; we
select the highest-scoring one as the final refined answer. Experiments across
six multimodal benchmarks demonstrate that models fine-tuned with data
processed by VERITAS consistently outperform those using raw data, particularly
in text-rich and fine-grained reasoning tasks. Our critic model exhibits
enhanced capability comparable to state-of-the-art LMMs while being
significantly more efficient. We release our pipeline, datasets, and model
checkpoints to advance research in multimodal data optimization.

</details>


### [112] [Towards Flash Thinking via Decoupled Advantage Policy Optimization](https://arxiv.org/abs/2510.15374)
*Zezhong Tan,Hang Gao,Xinhong Ma,Feng Zhang,Ziqiang Dong*

Main category: cs.AI

TL;DR: DEPO is a novel RL framework that reduces inefficient reasoning in Large Reasoning Models by decreasing response length and eliminating overthinking, while maintaining or improving accuracy.


<details>
  <summary>Details</summary>
Motivation: Existing RL algorithms for Large Reasoning Models suffer from excessively lengthy responses and overthinking issues, leading to increased inference latency and computational consumption, especially for simple tasks that require minimal reasoning.

Method: DEPO consists of three core components: (1) an advantage decoupled algorithm to guide model reduction of inefficient tokens; (2) a difficulty-aware length penalty to lower overall response length; (3) an advantage clipping method to prevent bias in policy optimization.

Result: Applied to DeepSeek-Distill-Qwen-7B and DeepSeek-Distill-Qwen-1.5B, DEPO achieved a 39% reduction in sequence length, reduced excessive reasoning paths in inefficient tokens, and outperformed the base model in overall accuracy.

Conclusion: DEPO effectively addresses the problem of inefficient reasoning in LRMs by significantly reducing response length and computational overhead while maintaining or improving model performance.

Abstract: Recent Large Reasoning Models (LRMs) have achieved remarkable performance in
solving complex problems via supervised fine-tuning (SFT) and reinforcement
learning (RL). Although existing RL algorithms significantly enhance model
accuracy, they still suffer from excessively lengthy responses and overthinking
issues, resulting in increased inference latency and computational consumption,
especially for simple tasks that require minimal reasoning. To address this, we
propose a novel RL framework, DEPO, to reduce inefficient reasoning for models.
Our method mainly consists of three core components: (1) an innovative
advantage decoupled algorithm to guide model reduction of inefficient tokens;
(2) a difficulty-aware length penalty to lower the overall length of model
responses; (3) an advantage clipping method to prevent bias in policy
optimization. In our experiments, applied to DeepSeek-Distill-Qwen-7B and
DeepSeek-Distill-Qwen-1.5B as base models, DEPO achieves a significant
reduction in sequence length by 39% and reduces excessive reasoning paths in
inefficient tokens, while outperforming the base model in overall accuracy.

</details>


### [113] [Advancing Routing-Awareness in Analog ICs Floorplanning](https://arxiv.org/abs/2510.15387)
*Davide Basso,Luca Bortolussi,Mirjana Videnovic-Misic,Husni Habal*

Main category: cs.AI

TL;DR: This paper presents a reinforcement learning-based automatic floorplanning engine for analog ICs that integrates routing awareness to improve routability and meet industrial standards.


<details>
  <summary>Details</summary>
Motivation: Analog IC layout automation has lagged behind digital due to strict electrical constraints and the interdependence of floorplanning and routing. Layout engineers need routing-aware floorplanning solutions.

Method: Uses reinforcement learning and relational graph convolutional neural networks to generate floorplans conditioned for routability. Combines high grid resolution, precise pin information, and dynamic routing resource estimation.

Result: Achieved 13.8% reduction in dead space, 40.6% reduction in wirelength, and 73.4% increase in routing success compared to previous learning-based state-of-the-art methods.

Conclusion: The proposed approach effectively balances routing and area efficiency, demonstrating significant improvements in floorplan quality and routability for analog IC layout.

Abstract: The adoption of machine learning-based techniques for analog integrated
circuit layout, unlike its digital counterpart, has been limited by the
stringent requirements imposed by electric and problem-specific constraints,
along with the interdependence of floorplanning and routing steps. In this
work, we address a prevalent concern among layout engineers regarding the need
for readily available routing-aware floorplanning solutions. To this extent, we
develop an automatic floorplanning engine based on reinforcement learning and
relational graph convolutional neural network specifically tailored to
condition the floorplan generation towards more routable outcomes. A
combination of increased grid resolution and precise pin information
integration, along with a dynamic routing resource estimation technique, allows
balancing routing and area efficiency, eventually meeting industrial standards.
When analyzing the place and route effectiveness in a simulated environment,
the proposed approach achieves a 13.8% reduction in dead space, a 40.6%
reduction in wirelength and a 73.4% increase in routing success when compared
to past learning-based state-of-the-art techniques.

</details>


### [114] [Corrigibility Transformation: Constructing Goals That Accept Updates](https://arxiv.org/abs/2510.15395)
*Rubi Hudson*

Main category: cs.AI

TL;DR: This paper introduces a formal definition of corrigibility - goals that don't resist updates or shutdown - and provides a transformation to make any goal corrigible without performance loss.


<details>
  <summary>Details</summary>
Motivation: Current AI goals often resist training updates since most goals are better achieved by continuing to pursue them unchanged. This creates safety risks when correcting mistakes or adapting to changing human preferences.

Method: A transformation that constructs corrigible versions of goals by myopically eliciting predictions of reward conditional on preventing updates, which then determine reward when updates are accepted. The method can be extended recursively to new agents.

Result: Gridworld experiments demonstrate that these corrigible goals can be learned effectively and produce the desired corrigible behavior.

Conclusion: The paper provides a practical solution for creating corrigible AI goals that maintain performance while enabling safe updates and shutdown, addressing a crucial safety gap in AI alignment.

Abstract: For an AI's training process to successfully impart a desired goal, it is
important that the AI does not attempt to resist the training. However,
partially learned goals will often incentivize an AI to avoid further goal
updates, as most goals are better achieved by an AI continuing to pursue them.
We say that a goal is corrigible if it does not incentivize taking actions that
avoid proper goal updates or shutdown. In addition to convergence in training,
corrigibility also allows for correcting mistakes and changes in human
preferences, which makes it a crucial safety property. Despite this, the
existing literature does not include specifications for goals that are both
corrigible and competitive with non-corrigible alternatives. We provide a
formal definition for corrigibility, then introduce a transformation that
constructs a corrigible version of any goal that can be made corrigible,
without sacrificing performance. This is done by myopically eliciting
predictions of reward conditional on costlessly preventing updates, which then
also determine the reward when updates are accepted. The transformation can be
modified to recursively extend corrigibility to any new agents created by
corrigible agents, and to prevent agents from deliberately modifying their
goals. Two gridworld experiments demonstrate that these corrigible goals can be
learned effectively, and that they lead to the desired behavior.

</details>


### [115] [MARS: Reinforcing Multi-Agent Reasoning of LLMs through Self-Play in Strategic Games](https://arxiv.org/abs/2510.15414)
*Huining Yuan,Zelai Xu,Zheyue Tan,Xiangmin Yi,Mo Guang,Kaiwen Long,Haojia Hui,Boxun Li,Xinlei Chen,Bo Zhao,Xiao-Ping Zhang,Chao Yu,Yu Wang*

Main category: cs.AI

TL;DR: MARS is an RL framework that enhances LLM multi-agent reasoning through self-play in cooperative and competitive games, achieving significant performance improvements in both games and reasoning benchmarks.


<details>
  <summary>Details</summary>
Motivation: To address the challenges of extending RL to multi-turn, multi-agent scenarios, particularly long-horizon credit assignment and agent-specific advantage estimation.

Method: End-to-end RL framework with turn-level advantage estimator for credit assignment and agent-specific advantage normalization for stable multi-agent training, using self-play across cooperative and competitive games.

Result: 28.7% performance improvements in held-out games, and 10.0% on AIME and 12.5% on GPQA-Diamond when integrated into leading multi-agent systems.

Conclusion: End-to-end RL training with self-play in strategic games is a powerful approach for developing generalizable multi-agent reasoning capabilities in LLMs.

Abstract: Developing Large Language Models (LLMs) to cooperate and compete effectively
within multi-agent systems is a critical step towards more advanced
intelligence. While reinforcement learning (RL) has proven effective for
enhancing reasoning in single-agent tasks, its extension to multi-turn,
multi-agent scenarios remains underexplored due to the challenges of
long-horizon credit assignment and agent-specific advantage estimation. To
address these challenges, we introduce MARS, an end-to-end RL framework that
incentivizes Multi-Agent Reasoning of LLMs through Self-play in both
cooperative and competitive games. MARS features a turn-level advantage
estimator that aligns learning signals with each interaction for credit
assignment, and an agent-specific advantage normalization to stabilize
multi-agent training. By learning with self-play across cooperative and
competitive games, the MARS agent trained from Qwen3-4B develops strong
strategic abilities that generalize to held-out games with up to 28.7%
performance improvements. More importantly, the capability acquired through
self-play generalizes beyond games, yielding consistent performance gains of
multi-agent systems in reasoning benchmarks. When integrated into leading
multi-agent systems, our MARS agent achieves significant performance gains of
10.0% on AIME and 12.5% on GPQA-Diamond. These results establish end-to-end RL
training with self-play in strategic games as a powerful approach for
developing generalizable multi-agent reasoning capabilities in LLMs. Our code
and models are publicly available at https://github.com/thu-nics/MARS.

</details>


### [116] [Adaptive Minds: Empowering Agents with LoRA-as-Tools](https://arxiv.org/abs/2510.15416)
*Pavan C Shekar,Ashwanth Krishnan*

Main category: cs.AI

TL;DR: Adaptive Minds is an agentic system that uses LoRA adapters as domain-specific tools, allowing a base LLM to dynamically select the most relevant LoRA adapter for each query instead of using a single fine-tuned model.


<details>
  <summary>Details</summary>
Motivation: To enable seamless switching between different domain experts on demand while preserving conversational ability, combining the flexibility of multi-agent orchestration with the efficiency of parameter-efficient fine-tuning.

Method: Uses the base LLM as a semantic router to analyze queries and dynamically select relevant LoRA tools, built with LangGraph for workflow management, supporting both API and web interfaces.

Result: Delivers accurate, specialized responses while maintaining conversational ability, providing a scalable and extensible foundation for domain-adaptive AI assistance.

Conclusion: Adaptive Minds offers an open-source, scalable system that effectively combines multi-agent flexibility with parameter-efficient fine-tuning for domain-adaptive AI assistance.

Abstract: We present Adaptive Minds, an agentic system that treats LoRA adapters as
domain-specific tools. Instead of relying on a single fine-tuned model or rigid
rule-based routing, our approach empowers the base LLM itself to act as a
semantic router analyzing each query and dynamically selecting the most
relevant LoRA tool. This enables the agent to seamlessly switch between
different domain experts on demand. By combining the flexibility of multi-agent
orchestration with the efficiency of parameter-efficient fine-tuning, Adaptive
Minds delivers accurate, specialized responses while preserving conversational
ability. The system is built with LangGraph for workflow management, supports
both API and web interfaces, and is fully open source, providing a scalable and
extensible foundation for domain-adaptive AI assistance.

</details>


### [117] [Hypergraph Contrastive Sensor Fusion for Multimodal Fault Diagnosis in Induction Motors](https://arxiv.org/abs/2510.15547)
*Usman Ali,Ali Zia,Waqas Ali,Umer Ramzan,Abdul Rehman,Muhammad Tayyab Chaudhry,Wei Xiang*

Main category: cs.AI

TL;DR: Proposes MM-HCAN, a multimodal hypergraph contrastive attention network for robust induction motor fault diagnosis, achieving 99.82% accuracy with strong cross-domain generalization and noise resilience.


<details>
  <summary>Details</summary>
Motivation: Conventional fault diagnosis methods struggle with complex multimodal signal relationships, are limited to unimodal data or single fault types, and degrade under noisy or cross-domain conditions, highlighting the need for more robust solutions.

Method: Integrates contrastive learning within a hypergraph topology designed for multimodal sensor fusion, enabling joint modeling of intra- and inter-modal dependencies beyond Euclidean embedding spaces.

Result: Achieves up to 99.82% accuracy on three real-world benchmarks with strong cross-domain generalization and resilience to noise, while simultaneously diagnosing bearing, stator, and rotor faults.

Conclusion: MM-HCAN provides a scalable and robust solution for comprehensive multi-fault diagnosis, supporting predictive maintenance and extended asset longevity in industrial environments.

Abstract: Reliable induction motor (IM) fault diagnosis is vital for industrial safety
and operational continuity, mitigating costly unplanned downtime. Conventional
approaches often struggle to capture complex multimodal signal relationships,
are constrained to unimodal data or single fault types, and exhibit performance
degradation under noisy or cross-domain conditions. This paper proposes the
Multimodal Hypergraph Contrastive Attention Network (MM-HCAN), a unified
framework for robust fault diagnosis. To the best of our knowledge, MM-HCAN is
the first to integrate contrastive learning within a hypergraph topology
specifically designed for multimodal sensor fusion, enabling the joint
modelling of intra- and inter-modal dependencies and enhancing generalisation
beyond Euclidean embedding spaces. The model facilitates simultaneous diagnosis
of bearing, stator, and rotor faults, addressing the engineering need for
consolidated di- agnostic capabilities. Evaluated on three real-world
benchmarks, MM-HCAN achieves up to 99.82% accuracy with strong cross-domain
generalisation and resilience to noise, demonstrating its suitability for
real-world deployment. An ablation study validates the contribution of each
component. MM-HCAN provides a scalable and robust solution for comprehensive
multi-fault diagnosis, supporting predictive maintenance and extended asset
longevity in industrial environments.

</details>


### [118] [Taming the Judge: Deconflicting AI Feedback for Stable Reinforcement Learning](https://arxiv.org/abs/2510.15514)
*Boyin Liu,Zhuo Zhang,Sen Huang,Lipeng Xie,Qingxu Fu,Haoran Chen,LI YU,Tianyi Hu,Zhaoyang Liu,Bolin Ding,Dongbin Zhao*

Main category: cs.AI

TL;DR: A framework to detect and resolve judgment inconsistencies in reinforcement learning, introducing Conflict Detection Rate (CDR) metric and Deconflicted Graph Rewards (DGR) method to eliminate preference cycles and improve training stability.


<details>
  <summary>Details</summary>
Motivation: Existing methods face judgment inconsistencies that destabilize reinforcement learning, with logical coherence issues like preference cycles not being fully addressed despite prior focus on judgment accuracy.

Method: Proposes Conflict Detection Rate (CDR) metric to quantify judgment conflicts, and Deconflicted Graph Rewards (DGR) framework that constructs preference graphs, transforms them into conflict-free Directed Acyclic Graphs (DAGs), and generates logically coherent reward signals compatible with any policy optimizer.

Result: Experimental results show the framework significantly enhances training stability and model performance compared to strong baselines.

Conclusion: Establishes logical consistency as a crucial and manageable dimension of AI feedback, providing a systematic approach to detect and resolve judgment inconsistencies during reinforcement learning training.

Abstract: However, this method often faces judgment inconsistencies that can
destabilize reinforcement learning. While prior research has focused on the
accuracy of judgments, the critical issue of logical coherence especially
issues such as preference cycles hasn't been fully addressed. To fill this gap,
we introduce a comprehensive framework designed to systematically detect and
resolve these inconsistencies during the reinforcement learning training
process. Our framework includes two main contributions: first, the Conflict
Detection Rate (CDR), a new metric that quantifies judgment conflicts, and
second, Deconflicted Graph Rewards (DGR), a framework that purifies signals by
removing cycles before policy optimization. DGR constructs preference graphs
from the initial judgments, transforms them into conflict-free Directed Acyclic
Graphs (DAGs), and generates a logically coherent reward signal that is
compatible with any policy optimizer. Experimental results show that our
framework significantly enhances training stability and model performance
compared to strong baselines, establishing logical consistency as a crucial and
now manageable dimension of AI feedback.

</details>


### [119] [JudgeSQL: Reasoning over SQL Candidates with Weighted Consensus Tournament](https://arxiv.org/abs/2510.15560)
*Jiayuan Bai,Xuan-guang Pan,Chongyang Tao,Shuai Ma*

Main category: cs.AI

TL;DR: JudgeSQL is a framework that improves SQL candidate selection through structured reasoning and weighted consensus tournaments, addressing limitations of existing methods like self-consistency.


<details>
  <summary>Details</summary>
Motivation: Existing SQL selection approaches provide shallow signals and fail to capture fine-grained semantic distinctions between closely related SQL candidates, leading to inconsistent scoring and fragile reasoning chains.

Method: Develops a reasoning-based SQL judge model using reinforcement learning with verifiable rewards, and implements a weighted consensus tournament that integrates reasoning preferences with generator confidence.

Result: Extensive experiments on BIRD benchmark show superior SQL judgment capabilities, good cross-scale generalization, and robustness to generator capacity.

Conclusion: JudgeSQL provides a principled framework for reliable and efficient SQL candidate selection through structured reasoning and consensus mechanisms.

Abstract: Text-to-SQL is a pivotal task that bridges natural language understanding and
structured data access, yet it remains fundamentally challenging due to
semantic ambiguity and complex compositional reasoning. While large language
models (LLMs) have greatly advanced SQL generation though prompting, supervised
finetuning and reinforced tuning, the shift toward test-time scaling exposes a
new bottleneck: selecting the correct query from a diverse candidate pool.
Existing selection approaches, such as self-consistency or best-of-$N$
decoding, provide only shallow signals, making them prone to inconsistent
scoring, fragile reasoning chains, and a failure to capture fine-grained
semantic distinctions between closely related SQL candidates. To this end, we
introduce JudgeSQL, a principled framework that redefines SQL candidate
selection through structured reasoning and weighted consensus tournament
mechanism. JudgeSQL develops a reasoning-based SQL judge model that distills
reasoning traces with reinforcement learning guided by verifiable rewards,
enabling accurate and interpretable judgments. Building on this, a weighted
consensus tournament integrates explicit reasoning preferences with implicit
generator confidence, yielding selections that are both more reliable and more
efficient. Extensive experiments on the BIRD benchmark demonstrate that
JudgeSQL exhibits superior SQL judgment capabilities and good cross-scale
generalization and robustness to generator capacity.

</details>


### [120] [Context-aware deep learning using individualized prior information reduces false positives in disease risk prediction and longitudinal health assessment](https://arxiv.org/abs/2510.15591)
*Lavanya Umapathy,Patricia M Johnson,Tarun Dutt,Angela Tong,Madhur Nayan,Hersh Chandarana,Daniel K Sodickson*

Main category: cs.AI

TL;DR: A machine learning framework that integrates temporal context from prior medical visits to improve prostate cancer risk prediction, significantly reducing false positive rates while maintaining sensitivity.


<details>
  <summary>Details</summary>
Motivation: Temporal context in medicine is valuable for assessing patient health changes over time, especially when prior visits are limited and frequency is variable. Current methods often rely on single-visit data, missing important longitudinal information.

Method: Developed a two-stage ML framework: first estimates initial disease risk using recent visit data, then refines assessment by integrating information from prior imaging and clinical biomarkers. Applied to prostate cancer prediction using large dataset (28,342 patients, 39,013 MRI scans, 68,931 blood tests) collected over nearly a decade.

Result: Integrating prior context converted false positives to true negatives, increasing specificity while preserving high sensitivity. False positive rates reduced from 51% to 33% with up to three prior imaging exams, and further to 24% when including prior clinical data. For 5-year PCa risk prediction, false positive rates reduced from 64% to 9%.

Conclusion: Information collected over time provides relevant context to enhance specificity of medical risk prediction. Sufficient reduction of false positive rates using temporal context could enable expansion of longitudinal health monitoring to larger populations with low baseline disease risk, leading to earlier detection and improved outcomes.

Abstract: Temporal context in medicine is valuable in assessing key changes in patient
health over time. We developed a machine learning framework to integrate
diverse context from prior visits to improve health monitoring, especially when
prior visits are limited and their frequency is variable. Our model first
estimates initial risk of disease using medical data from the most recent
patient visit, then refines this assessment using information digested from
previously collected imaging and/or clinical biomarkers. We applied our
framework to prostate cancer (PCa) risk prediction using data from a large
population (28,342 patients, 39,013 magnetic resonance imaging scans, 68,931
blood tests) collected over nearly a decade. For predictions of the risk of
clinically significant PCa at the time of the visit, integrating prior context
directly converted false positives to true negatives, increasing overall
specificity while preserving high sensitivity. False positive rates were
reduced progressively from 51% to 33% when integrating information from up to
three prior imaging examinations, as compared to using data from a single
visit, and were further reduced to 24% when also including additional context
from prior clinical data. For predicting the risk of PCa within five years of
the visit, incorporating prior context reduced false positive rates still
further (64% to 9%). Our findings show that information collected over time
provides relevant context to enhance the specificity of medical risk
prediction. For a wide range of progressive conditions, sufficient reduction of
false positive rates using context could offer a pathway to expand longitudinal
health monitoring programs to large populations with comparatively low baseline
risk of disease, leading to earlier detection and improved health outcomes.

</details>


### [121] [Unleashing Scientific Reasoning for Bio-experimental Protocol Generation via Structured Component-based Reward Mechanism](https://arxiv.org/abs/2510.15600)
*Haoran Sun,Yankai Jiang,Zhenyu Tang,Yaning Pan,Shuang Gu,Zekai Lin,Lilong Wang,Wenjie Lou,Lei Liu,Lei Bai,Xiaosong Wang*

Main category: cs.AI

TL;DR: The paper introduces SciRecipe dataset and Thoth model for generating precise scientific protocols through a "Sketch-and-Fill" paradigm and structured reward mechanism, achieving superior performance over existing LLMs.


<details>
  <summary>Details</summary>
Motivation: Current LLMs generate incomplete or inconsistent scientific protocols, limiting reproducibility in science. Autonomous generation of precise protocols could improve reproduction efficiency.

Method: Proposed "Sketch-and-Fill" paradigm separating analysis, structuring, and expression; developed structured component-based reward mechanism; trained Thoth model through staged Knowledge-to-Action process.

Result: Thoth consistently surpasses both proprietary and open-source LLMs across multiple benchmarks, with significant improvements in step alignment, logical sequencing, and semantic accuracy.

Conclusion: The approach enables reliable scientific assistants that bridge knowledge with experimental execution, paving the way for improved scientific reproducibility.

Abstract: The foundation of reproducible science lies in protocols that are precise,
logically ordered, and executable. The autonomous generation of these protocols
through natural language queries could greatly improve the efficiency of the
reproduction process. However, current leading large language models (LLMs)
often generate incomplete or inconsistent protocols, limiting their utility. To
address this limitation, we first introduce SciRecipe, a large-scale dataset of
over 12K structured protocols spanning 27 biological subfields and encompassing
both comprehension and problem-solving tasks. To further improve protocol
generation, we propose the "Sketch-and-Fill" paradigm, which separates
analysis, structuring, and expression to ensure each step is explicit and
verifiable. Complementing this, the structured component-based reward mechanism
evaluates step granularity, action order, and semantic fidelity, aligning model
optimization with experimental reliability. Building on these components, we
develop Thoth, trained through a staged Knowledge-to-Action process that
progresses from knowledge acquisition to operational reasoning and ultimately
to robust, executable protocol generation. Across multiple benchmarks, Thoth
consistently surpasses both proprietary and open-source LLMs, achieving
significant improvements in step alignment, logical sequencing, and semantic
accuracy. Our approach paves the way for reliable scientific assistants that
bridge knowledge with experimental execution. All data, code, and models will
be released publicly.

</details>


### [122] [Build Your Personalized Research Group: A Multiagent Framework for Continual and Interactive Science Automation](https://arxiv.org/abs/2510.15624)
*Ed Li,Junyu Ren,Xintian Pan,Cat Yan,Chuanhao Li,Dirk Bergemann,Zhuoran Yang*

Main category: cs.AI

TL;DR: freephdlabor is an open-source multiagent framework that enables fully dynamic workflows and modular architecture for automated scientific research, overcoming limitations of rigid workflows and poor context management in existing systems.


<details>
  <summary>Details</summary>
Motivation: Existing agentic systems for science have rigid pre-programmed workflows that cannot adapt to intermediate findings, and inadequate context management that hinders long-horizon research.

Method: The framework features fully dynamic workflows determined by real-time agent reasoning, modular architecture for customization, automatic context compaction, workspace-based communication, memory persistence, and non-blocking human intervention mechanisms.

Result: The system transforms automated research from isolated single-run attempts into continual research programs that build systematically on prior explorations and incorporate human feedback.

Conclusion: This work provides both architectural principles and practical implementation for building customizable co-scientist systems, facilitating broader adoption of automated research across scientific domains for end-to-end research from ideation to publication.

Abstract: The automation of scientific discovery represents a critical milestone in
Artificial Intelligence (AI) research. However, existing agentic systems for
science suffer from two fundamental limitations: rigid, pre-programmed
workflows that cannot adapt to intermediate findings, and inadequate context
management that hinders long-horizon research. We present
\texttt{freephdlabor}, an open-source multiagent framework featuring
\textit{fully dynamic workflows} determined by real-time agent reasoning and a
\coloremph{\textit{modular architecture}} enabling seamless customization --
users can modify, add, or remove agents to address domain-specific
requirements. The framework provides comprehensive infrastructure including
\textit{automatic context compaction}, \textit{workspace-based communication}
to prevent information degradation, \textit{memory persistence} across
sessions, and \textit{non-blocking human intervention} mechanisms. These
features collectively transform automated research from isolated, single-run
attempts into \textit{continual research programs} that build systematically on
prior explorations and incorporate human feedback. By providing both the
architectural principles and practical implementation for building customizable
co-scientist systems, this work aims to facilitate broader adoption of
automated research across scientific domains, enabling practitioners to deploy
interactive multiagent systems that autonomously conduct end-to-end research --
from ideation through experimentation to publication-ready manuscripts.

</details>


### [123] [Direct Preference Optimization with Unobserved Preference Heterogeneity: The Necessity of Ternary Preferences](https://arxiv.org/abs/2510.15716)
*Keertana Chidambaram,Karthik Vinary Seetharaman,Vasilis Syrgkanis*

Main category: cs.AI

TL;DR: This paper addresses limitations in RLHF and DPO by incorporating diverse human preferences and using rankings over binary comparisons for better preference learning, proposing methods for fairness and personalization in model alignment.


<details>
  <summary>Details</summary>
Motivation: Current RLHF and DPO approaches assume uniform annotator preferences and rely on binary comparisons, overlooking human diversity and the limitations of pairwise feedback.

Method: Connects preference learning with econometrics to show rankings over three or more responses ensure identifiability, develops EM adaptation of DPO for latent annotator types, and proposes min-max regret fairness aggregation.

Result: Establishes theoretical identifiability with rankings and provides algorithmic framework for handling heterogeneous preferences with equitable performance guarantees.

Conclusion: The work provides a comprehensive framework for fairness and personalization in generative model alignment by addressing diversity of human evaluators and limitations of binary preference feedback.

Abstract: Reinforcement Learning from Human Feedback (RLHF) has become central to
aligning large language models with human values, typically by first learning a
reward model from preference data which is then used to update the model with
reinforcement learning. Recent alternatives such as Direct Preference
Optimization (DPO) simplify this pipeline by directly optimizing on
preferences. However, both approaches often assume uniform annotator
preferences and rely on binary comparisons, overlooking two key limitations:
the diversity of human evaluators and the limitations of pairwise feedback. In
this work, we address both these issues. First, we connect preference learning
in RLHF with the econometrics literature and show that binary comparisons are
insufficient for identifying latent user preferences from finite user data and
infinite users, while (even incomplete) rankings over three or more responses
ensure identifiability. Second, we introduce methods to incorporate
heterogeneous preferences into alignment algorithms. We develop an
Expectation-Maximization adaptation of DPO that discovers latent annotator
types and trains a mixture of LLMs accordingly. Then we propose an aggregation
algorithm using a min-max regret fairness criterion to produce a single
generative policy with equitable performance guarantees. Together, these
contributions establish a theoretical and algorithmic framework for fairness
and personalization for diverse users in generative model alignment.

</details>


### [124] [Invoice Information Extraction: Methods and Performance Evaluation](https://arxiv.org/abs/2510.15727)
*Sai Yashwant,Anurag Dubey,Praneeth Paikray,Gantala Thulsiram*

Main category: cs.AI

TL;DR: Methods for extracting structured information from invoices with evaluation metrics to assess accuracy against ground truth.


<details>
  <summary>Details</summary>
Motivation: To establish a standardized evaluation framework for comparing different invoice data extraction methods and identifying field-specific performance strengths and weaknesses.

Method: Pre-process scanned/digital invoices, apply Docling and LlamaCloud Services to extract key fields (invoice number, date, total amount, vendor details), and use evaluation metrics including field-level precision, consistency checks, and exact match accuracy.

Result: Development of a robust evaluation framework that provides standardized metrics for assessing invoice data extraction accuracy and performance.

Conclusion: The proposed evaluation metrics enable systematic comparison of extraction methods and reveal field-specific performance characteristics in invoice data extraction.

Abstract: This paper presents methods for extracting structured information from
invoice documents and proposes a set of evaluation metrics (EM) to assess the
accuracy of the extracted data against annotated ground truth. The approach
involves pre-processing scanned or digital invoices, applying Docling and
LlamaCloud Services to identify and extract key fields such as invoice number,
date, total amount, and vendor details. To ensure the reliability of the
extraction process, we establish a robust evaluation framework comprising
field-level precision, consistency check failures, and exact match accuracy.
The proposed metrics provide a standardized way to compare different extraction
methods and highlight strengths and weaknesses in field-specific performance.

</details>


### [125] [AURA: An Agent Autonomy Risk Assessment Framework](https://arxiv.org/abs/2510.15739)
*Lorenzo Satta Chiris,Ayush Mishra*

Main category: cs.AI

TL;DR: AURA is a unified framework for detecting, quantifying, and mitigating risks in autonomous agentic AI systems through a gamma-based risk scoring methodology that balances accuracy with computational efficiency.


<details>
  <summary>Details</summary>
Motivation: Persistent challenges in alignment, governance, and risk management threaten to impede large-scale deployment of autonomous agentic AI systems across organizations.

Method: AURA introduces a gamma-based risk scoring methodology with interactive processes for scoring, evaluating, and mitigating risks. It features Human-in-the-Loop oversight, Agent-to-Human communication mechanisms, and supports autonomous self-assessment while being interoperable with established protocols (MCP and A2A).

Result: The framework enables responsible and transparent adoption of agentic AI by providing robust risk detection and mitigation while balancing computational resources.

Conclusion: AURA positions itself as a critical enabler for large-scale, governable agentic AI in enterprise environments by addressing key risk management challenges.

Abstract: As autonomous agentic AI systems see increasing adoption across
organisations, persistent challenges in alignment, governance, and risk
management threaten to impede deployment at scale. We present AURA (Agent
aUtonomy Risk Assessment), a unified framework designed to detect, quantify,
and mitigate risks arising from agentic AI. Building on recent research and
practical deployments, AURA introduces a gamma-based risk scoring methodology
that balances risk assessment accuracy with computational efficiency and
practical considerations. AURA provides an interactive process to score,
evaluate and mitigate the risks of running one or multiple AI Agents,
synchronously or asynchronously (autonomously). The framework is engineered for
Human-in-the-Loop (HITL) oversight and presents Agent-to-Human (A2H)
communication mechanisms, allowing for seamless integration with agentic
systems for autonomous self-assessment, rendering it interoperable with
established protocols (MCP and A2A) and tools. AURA supports a responsible and
transparent adoption of agentic AI and provides robust risk detection and
mitigation while balancing computational resources, positioning it as a
critical enabler for large-scale, governable agentic AI in enterprise
environments.

</details>


### [126] [Towards Relaxed Multimodal Inputs for Gait-based Parkinson's Disease Assessment](https://arxiv.org/abs/2510.15748)
*Minlin Zeng,Zhipeng Zhou,Yang Qiu,Zhiqi Shen*

Main category: cs.AI

TL;DR: Proposes TRIP, a Parkinson's assessment system using multi-objective optimization for flexible multimodal learning that doesn't require synchronized modalities during training or all modalities during inference.


<details>
  <summary>Details</summary>
Motivation: Current multimodal approaches for Parkinson's assessment have practical limitations: need for synchronized modalities during training and dependence on all modalities during inference, hindering real-world application.

Method: Formulates multimodal learning as multi-objective optimization (MOO) problem with margin-based class rebalancing strategy to handle modality imbalance and prevent modality collapse during fusion.

Result: Achieves state-of-the-art performance on three public datasets, outperforming best baselines by 16.48, 6.89, and 11.55 percentage points in asynchronous setting, and by 4.86 and 2.30 percentage points in synchronous setting.

Conclusion: TRIP framework effectively addresses practical limitations of multimodal learning for Parkinson's assessment, demonstrating strong performance and adaptability across different modality availability scenarios.

Abstract: Parkinson's disease assessment has garnered growing interest in recent years,
particularly with the advent of sensor data and machine learning techniques.
Among these, multimodal approaches have demonstrated strong performance by
effectively integrating complementary information from various data sources.
However, two major limitations hinder their practical application: (1) the need
to synchronize all modalities during training, and (2) the dependence on all
modalities during inference. To address these issues, we propose the first
Parkinson's assessment system that formulates multimodal learning as a
multi-objective optimization (MOO) problem. This not only allows for more
flexible modality requirements during both training and inference, but also
handles modality collapse issue during multimodal information fusion. In
addition, to mitigate the imbalance within individual modalities, we introduce
a margin-based class rebalancing strategy to enhance category learning. We
conduct extensive experiments on three public datasets under both synchronous
and asynchronous settings. The results show that our framework-Towards Relaxed
InPuts (TRIP)-achieves state-of-the-art performance, outperforming the best
baselines by 16.48, 6.89, and 11.55 percentage points in the asynchronous
setting, and by 4.86 and 2.30 percentage points in the synchronous setting,
highlighting its effectiveness and adaptability.

</details>


### [127] [Preliminary Quantitative Study on Explainability and Trust in AI Systems](https://arxiv.org/abs/2510.15769)
*Allen Daniel Sunny*

Main category: cs.AI

TL;DR: This study examines how different types of AI explanations affect user trust, finding that interactive explanations enhance engagement and confidence more than basic feature importance explanations.


<details>
  <summary>Details</summary>
Motivation: As large-scale AI models like GPT-4 are deployed in critical domains (law, healthcare, finance), there are urgent questions about trust and transparency that need empirical investigation.

Method: Quantitative experimental design using an interactive web-based loan approval simulation to compare different explanation types (basic feature importance vs interactive counterfactuals).

Result: Interactivity enhances both user engagement and confidence; clarity and relevance of explanations are key determinants of trust.

Conclusion: The findings provide empirical evidence for human-centered explainable AI, showing measurable effects of explainability design on user perception.

Abstract: Large-scale AI models such as GPT-4 have accelerated the deployment of
artificial intelligence across critical domains including law, healthcare, and
finance, raising urgent questions about trust and transparency. This study
investigates the relationship between explainability and user trust in AI
systems through a quantitative experimental design. Using an interactive,
web-based loan approval simulation, we compare how different types of
explanations, ranging from basic feature importance to interactive
counterfactuals influence perceived trust. Results suggest that interactivity
enhances both user engagement and confidence, and that the clarity and
relevance of explanations are key determinants of trust. These findings
contribute empirical evidence to the growing field of human-centered
explainable AI, highlighting measurable effects of explainability design on
user perception

</details>


### [128] [Self-evolving expertise in complex non-verifiable subject domains: dialogue as implicit meta-RL](https://arxiv.org/abs/2510.15772)
*Richard M. Bailey*

Main category: cs.AI

TL;DR: Dialectica is a framework that enables AI agents to develop expertise through structured dialogue, memory, reflection, and policy-constrained context editing, showing improved performance on wicked problems compared to baseline agents.


<details>
  <summary>Details</summary>
Motivation: Address the limitation that LLMs lack endogenous mechanisms to develop expertise through experience in complex 'wicked problems' involving multi-dimensional settings, non-verifiable outcomes, and heterogeneous impacts.

Method: Dialectica framework where agents engage in structured dialogue with memory, self-reflection, and policy-constrained context editing, treating discussion as an implicit meta-reinforcement learning process.

Result: Agents with reflection-based context editing dominate baseline counterparts on Elo scores, normalized Bradley-Terry-Davidson ability, and AlphaRank mass across Qwen3:30b and OpenAI's o4-mini architectures.

Conclusion: Dialogue-driven context evolution provides a practical path to targeted expertise amplification in open non-verifiable domains, with quantitative and qualitative evidence supporting learning signatures.

Abstract: So-called `wicked problems', those involving complex multi-dimensional
settings, non-verifiable outcomes, heterogeneous impacts and a lack of single
objectively correct answers, have plagued humans throughout history. Modern
examples include decisions over justice frameworks, solving environmental
pollution, planning for pandemic resilience and food security. The use of
state-of-the-art artificial intelligence systems (notably Large Language
Model-based agents) collaborating with humans on solving such problems is being
actively explored. While the abilities of LLMs can be improved by, for example,
fine-tuning, hand-crafted system prompts and scaffolding with external tools,
LLMs lack endogenous mechanisms to develop expertise through experience in such
settings. This work address this gap with Dialectica, a framework where agents
engage in structured dialogue on defined topics, augmented by memory,
self-reflection, and policy-constrained context editing. Formally, discussion
is viewed as an implicit meta-reinforcement learning process. The
`dialogue-trained' agents are evaluated post-hoc using judged pairwise
comparisons of elicited responses. Across two model architectures (locally run
Qwen3:30b and OpenAI's o4-mini) results show that enabling reflection-based
context editing during discussion produces agents which dominate their baseline
counterparts on Elo scores, normalized Bradley-Terry-Davidson ability, and
AlphaRank mass. The predicted signatures of learning are observed qualitatively
in statement and reflection logs, where reflections identify weaknesses and
reliably shape subsequent statements. Agreement between quantitative and
qualitative evidence supports dialogue-driven context evolution as a practical
path to targeted expertise amplification in open non-verifiable domains.

</details>


### [129] [Demo: Guide-RAG: Evidence-Driven Corpus Curation for Retrieval-Augmented Generation in Long COVID](https://arxiv.org/abs/2510.15782)
*Philip DiGiacomo,Haoyang Wang,Jinrui Fang,Yan Leng,W Michael Brode,Ying Ding*

Main category: cs.AI

TL;DR: The paper develops and evaluates RAG configurations for Long COVID clinical Q&A, finding that combining clinical guidelines with systematic reviews outperforms other approaches.


<details>
  <summary>Details</summary>
Motivation: To address challenges in developing effective AI chatbot frameworks for emerging diseases like Long COVID, where clinical guidance is rapidly evolving.

Method: Developed 6 RAG corpus configurations ranging from expert-curated sources to large literature databases, evaluated using LLM-as-a-judge framework across faithfulness, relevance, and comprehensiveness metrics on LongCOVID-CQ dataset.

Result: RAG configuration combining clinical guidelines with systematic reviews consistently outperformed both narrow single-guideline approaches and large-scale literature databases.

Conclusion: For emerging diseases, retrieval grounded in curated secondary reviews provides optimal balance between narrow consensus documents and unfiltered primary literature, leading to proposed Guide-RAG system.

Abstract: As AI chatbots gain adoption in clinical medicine, developing effective
frameworks for complex, emerging diseases presents significant challenges. We
developed and evaluated six Retrieval-Augmented Generation (RAG) corpus
configurations for Long COVID (LC) clinical question answering, ranging from
expert-curated sources to large-scale literature databases. Our evaluation
employed an LLM-as-a-judge framework across faithfulness, relevance, and
comprehensiveness metrics using LongCOVID-CQ, a novel dataset of
expert-generated clinical questions. Our RAG corpus configuration combining
clinical guidelines with high-quality systematic reviews consistently
outperformed both narrow single-guideline approaches and large-scale literature
databases. Our findings suggest that for emerging diseases, retrieval grounded
in curated secondary reviews provides an optimal balance between narrow
consensus documents and unfiltered primary literature, supporting clinical
decision-making while avoiding information overload and oversimplified
guidance. We propose Guide-RAG, a chatbot system and accompanying evaluation
framework that integrates both curated expert knowledge and comprehensive
literature databases to effectively answer LC clinical questions.

</details>


### [130] [PokeeResearch: Effective Deep Research via Reinforcement Learning from AI Feedback and Robust Reasoning Scaffold](https://arxiv.org/abs/2510.15862)
*Yi Wan,Jiuqi Wang,Liam Li,Jinsong Liu,Ruihao Zhu,Zheqing Zhu*

Main category: cs.AI

TL;DR: PokeeResearch-7B is a 7B-parameter deep research agent that uses reinforcement learning from AI feedback and chain-of-thought reasoning to achieve state-of-the-art performance on research benchmarks.


<details>
  <summary>Details</summary>
Motivation: Current deep research agents have limitations including shallow retrieval, weak alignment metrics, and brittle tool-use behavior that need to be addressed.

Method: Uses unified reinforcement learning framework with annotation-free RLAIF to optimize policies using LLM-based reward signals, plus chain-of-thought-driven multi-call reasoning scaffold for self-verification and adaptive recovery from tool failures.

Result: Achieves state-of-the-art performance among 7B-scale deep research agents across 10 popular deep research benchmarks.

Conclusion: Careful reinforcement learning and reasoning design can produce efficient, resilient, and research-grade AI agents.

Abstract: Tool-augmented large language models (LLMs) are emerging as deep research
agents, systems that decompose complex queries, retrieve external evidence, and
synthesize grounded responses. Yet current agents remain limited by shallow
retrieval, weak alignment metrics, and brittle tool-use behavior. We introduce
PokeeResearch-7B, a 7B-parameter deep research agent built under a unified
reinforcement learning framework for robustness, alignment, and scalability.
PokeeResearch-7B is trained by an annotation-free Reinforcement Learning from
AI Feedback (RLAIF) framework to optimize policies using LLM-based reward
signals that capture factual accuracy, citation faithfulness, and instruction
adherence. A chain-of-thought-driven multi-call reasoning scaffold further
enhances robustness through self-verification and adaptive recovery from tool
failures. Among 10 popular deep research benchmarks, PokeeResearch-7B achieves
state-of-the-art performance among 7B-scale deep research agents. This
highlights that careful reinforcement learning and reasoning design can produce
efficient, resilient, and research-grade AI agents. The model and inference
code is open-sourced under MIT license at
https://github.com/Pokee-AI/PokeeResearchOSS.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [131] [Extending Load Forecasting from Zonal Aggregates to Individual Nodes for Transmission System Operators](https://arxiv.org/abs/2510.14983)
*Oskar Triebe,Fletcher Passow,Simon Wittner,Leonie Wagner,Julio Arend,Tao Sun,Chad Zanocco,Marek Miltner,Arezou Ghesmati,Chen-Hao Tsai,Christoph Bergmeir,Ram Rajagopal*

Main category: cs.LG

TL;DR: A multi-level forecasting system for power grid operators that improves accuracy and interpretability of both zonal and nodal load forecasts, addressing challenges from sustainable energy developments.


<details>
  <summary>Details</summary>
Motivation: Sustainable energy developments increase electric load uncertainty, requiring Transmission System Operators (TSOs) to have higher spatial resolution load forecasts extending from zonal aggregates to individual nodes, but nodal loads are harder to forecast and manage.

Method: Developed an interpretable and scalable forecasting model using extensive zonal and nodal net load data, with a fully parallelized single-model workflow that allows gradual extension from zonal to nodal operations.

Result: Showed accuracy and interpretability improvements for zonal forecasts, and substantial improvements for nodal forecasts. The system allows operators to adjust forecasts with unprecedented confidence and accuracy.

Conclusion: The multi-level forecasting system enables TSOs to manage nodal load forecasting challenges effectively, providing operators with reliable tools for risk assessment and error diagnosis in daily operations.

Abstract: The reliability of local power grid infrastructure is challenged by
sustainable energy developments increasing electric load uncertainty.
Transmission System Operators (TSOs) need load forecasts of higher spatial
resolution, extending current forecasting operations from zonal aggregates to
individual nodes. However, nodal loads are less accurate to forecast and
require a large number of individual forecasts, which are hard to manage for
the human experts assessing risks in the control room's daily operations
(operator). In collaboration with a TSO, we design a multi-level system that
meets the needs of operators for hourly day-ahead load forecasting. Utilizing a
uniquely extensive dataset of zonal and nodal net loads, we experimentally
evaluate our system components. First, we develop an interpretable and scalable
forecasting model that allows for TSOs to gradually extend zonal operations to
include nodal forecasts. Second, we evaluate solutions to address the
heterogeneity and volatility of nodal load, subject to a trade-off. Third, our
system is manageable with a fully parallelized single-model forecasting
workflow. Our results show accuracy and interpretability improvements for zonal
forecasts, and substantial improvements for nodal forecasts. In practice, our
multi-level forecasting system allows operators to adjust forecasts with
unprecedented confidence and accuracy, and to diagnose otherwise opaque errors
precisely.

</details>


### [132] [TangledFeatures: Robust Feature Selection in Highly Correlated Spaces](https://arxiv.org/abs/2510.15005)
*Allen Daniel Sunny*

Main category: cs.LG

TL;DR: TangledFeatures is a feature selection framework that identifies representative features from correlated predictor groups, reducing redundancy while maintaining explanatory power for more interpretable and stable analysis.


<details>
  <summary>Details</summary>
Motivation: Traditional feature selection methods focus on predictive accuracy but degrade with correlated predictors, creating a need for methods that handle correlated feature spaces effectively.

Method: The framework identifies representative features from groups of entangled predictors, selecting features that reduce redundancy while retaining explanatory power for downstream models.

Result: Applied to Alanine Dipeptide, TangledFeatures successfully selected structurally meaningful intra-atomic distances that explain variation in backbone torsional angles.

Conclusion: TangledFeatures provides a more interpretable and stable basis for feature selection in correlated feature spaces compared to traditional techniques.

Abstract: Feature selection is a fundamental step in model development, shaping both
predictive performance and interpretability. Yet, most widely used methods
focus on predictive accuracy, and their performance degrades in the presence of
correlated predictors. To address this gap, we introduce TangledFeatures, a
framework for feature selection in correlated feature spaces. It identifies
representative features from groups of entangled predictors, reducing
redundancy while retaining explanatory power. The resulting feature subset can
be directly applied in downstream models, offering a more interpretable and
stable basis for analysis compared to traditional selection techniques. We
demonstrate the effectiveness of TangledFeatures on Alanine Dipeptide, applying
it to the prediction of backbone torsional angles and show that the selected
features correspond to structurally meaningful intra-atomic distances that
explain variation in these angles.

</details>


### [133] [ES-C51: Expected Sarsa Based C51 Distributional Reinforcement Learning Algorithm](https://arxiv.org/abs/2510.15006)
*Rijul Tandon,Peter Vamplew,Cameron Foale*

Main category: cs.LG

TL;DR: ES-C51 improves C51 by replacing greedy Q-learning with Expected Sarsa update using softmax, reducing instability when actions have similar expected rewards and achieving better performance.


<details>
  <summary>Details</summary>
Motivation: Standard C51 uses greedy Bellman updates that can be unstable when multiple actions have similar expected rewards but different distributions, preventing stable distribution learning.

Method: Modified C51 algorithm (ES-C51) that replaces greedy Q-learning update with Expected Sarsa update using softmax to combine information from all possible actions rather than relying on a single best action.

Result: ES-C51 outperforms QL-C51 (modified C51 with softmax exploration) across many classic control environments from Gym and Atari-10 games.

Conclusion: Using Expected Sarsa updates instead of greedy Q-learning in distributional RL improves stability and performance when actions have similar expected rewards.

Abstract: In most value-based reinforcement learning (RL) algorithms, the agent
estimates only the expected reward for each action and selects the action with
the highest reward. In contrast, Distributional Reinforcement Learning (DRL)
estimates the entire probability distribution of possible rewards, providing
richer information about uncertainty and variability. C51 is a popular DRL
algorithm for discrete action spaces. It uses a Q-learning approach, where the
distribution is learned using a greedy Bellman update. However, this can cause
problems if multiple actions at a state have similar expected reward but with
different distributions, as the algorithm may not learn a stable distribution.
This study presents a modified version of C51 (ES-C51) that replaces the greedy
Q-learning update with an Expected Sarsa update, which uses a softmax
calculation to combine information from all possible actions at a state rather
than relying on a single best action. This reduces instability when actions
have similar expected rewards and allows the agent to learn higher-performing
policies. This approach is evaluated on classic control environments from Gym,
and Atari-10 games. For a fair comparison, we modify the standard C51's
exploration strategy from e-greedy to softmax, which we refer to as QL-C51 (Q-
Learning based C51). The results demonstrate that ES-C51 outperforms QL-C51
across many environments.

</details>


### [134] [Hybrid Autoencoder-Based Framework for Early Fault Detection in Wind Turbines](https://arxiv.org/abs/2510.15010)
*Rekha R Nair,Tina Babu,Alavikunhu Panthakkan,Balamurugan Balusamy,Wathiq Mansoor*

Main category: cs.LG

TL;DR: Novel ensemble deep learning framework for unsupervised anomaly detection in wind turbines using VAE, LSTM Autoencoders, and Transformers on SCADA data, achieving 0.947 AUC-ROC and early fault detection up to 48 hours before failure.


<details>
  <summary>Details</summary>
Motivation: Wind turbine reliability is critical for renewable energy sector growth, where early fault detection reduces downtime and maintenance costs significantly.

Method: Integrates VAE, LSTM Autoencoders, and Transformer architectures with feature engineering pipeline extracting temporal, statistical, and frequency-domain indicators from SCADA data. Uses ensemble scoring and adaptive thresholding for unsupervised anomaly detection.

Result: Achieves AUC-ROC of 0.947 on CARE dataset (89 years of real-world turbine data across 3 wind farms) and detects faults up to 48 hours prior to failure.

Conclusion: The approach enables predictive maintenance, reduces turbine failures, and enhances operational efficiency in large-scale wind energy deployments, offering significant societal value.

Abstract: Wind turbine reliability is critical to the growing renewable energy sector,
where early fault detection significantly reduces downtime and maintenance
costs. This paper introduces a novel ensemble-based deep learning framework for
unsupervised anomaly detection in wind turbines. The method integrates
Variational Autoencoders (VAE), LSTM Autoencoders, and Transformer
architectures, each capturing different temporal and contextual patterns from
high-dimensional SCADA data. A unique feature engineering pipeline extracts
temporal, statistical, and frequency-domain indicators, which are then
processed by the deep models. Ensemble scoring combines model predictions,
followed by adaptive thresholding to detect operational anomalies without
requiring labeled fault data. Evaluated on the CARE dataset containing 89 years
of real-world turbine data across three wind farms, the proposed method
achieves an AUC-ROC of 0.947 and early fault detection up to 48 hours prior to
failure. This approach offers significant societal value by enabling predictive
maintenance, reducing turbine failures, and enhancing operational efficiency in
large-scale wind energy deployments.

</details>


### [135] [An Empirical Study on MC Dropout--Based Uncertainty--Error Correlation in 2D Brain Tumor Segmentation](https://arxiv.org/abs/2510.15541)
*Saumya B*

Main category: cs.LG

TL;DR: MC Dropout uncertainty shows weak correlation with segmentation errors in brain tumor MRI, especially near boundaries, suggesting limited utility for error localization.


<details>
  <summary>Details</summary>
Motivation: To evaluate whether MC Dropout-based uncertainty can effectively identify segmentation errors in brain tumor MRI, particularly at tumor boundaries where accuracy is crucial for diagnosis and treatment planning.

Method: Used U-Net for 2D brain tumor MRI segmentation with four augmentation settings (none, horizontal flip, rotation, scaling), computed uncertainty from 50 stochastic forward passes, and correlated with pixel-wise errors using Pearson and Spearman coefficients.

Result: Found weak global correlations (r ≈ 0.30-0.38) and negligible boundary correlations (|r| < 0.05). Differences across augmentations were statistically significant but practically irrelevant.

Conclusion: MC Dropout uncertainty provides limited cues for boundary error localization, highlighting the need for alternative or hybrid uncertainty estimation methods in medical image segmentation.

Abstract: Accurate brain tumor segmentation from MRI is vital for diagnosis and
treatment planning. Although Monte Carlo (MC) Dropout is widely used to
estimate model uncertainty, its effectiveness in identifying segmentation
errors -- especially near tumor boundaries -- remains unclear. This study
empirically examines the relationship between MC Dropout--based uncertainty and
segmentation error in 2D brain tumor MRI segmentation using a U-Net trained
under four augmentation settings: none, horizontal flip, rotation, and scaling.
Uncertainty was computed from 50 stochastic forward passes and correlated with
pixel-wise errors using Pearson and Spearman coefficients. Results show weak
global correlations ($r \approx 0.30$--$0.38$) and negligible boundary
correlations ($|r| < 0.05$). Although differences across augmentations were
statistically significant ($p < 0.001$), they lacked practical relevance. These
findings suggest that MC Dropout uncertainty provides limited cues for boundary
error localization, underscoring the need for alternative or hybrid uncertainty
estimation methods in medical image segmentation.

</details>


### [136] [AlignFlow: Improving Flow-based Generative Models with Semi-Discrete Optimal Transport](https://arxiv.org/abs/2510.15038)
*Lingkai Kong,Molei Tao,Yang Liu,Bryan Wang,Jinmiao Fu,Chien-Chih Wang,Huidong Liu*

Main category: cs.LG

TL;DR: AlignFlow uses Semi-Discrete Optimal Transport to improve Flow-based Generative Models by creating optimal alignments between noise and data distributions, enabling better training scalability and performance.


<details>
  <summary>Details</summary>
Motivation: Existing OT-based methods in FGMs use batch sampling which limits scalability to large datasets. There's a need for more efficient and scalable approaches to establish optimal noise-data couplings.

Method: Leverages Semi-Discrete Optimal Transport to partition noise space into Laguerre cells, each mapped to a data point. During training, noise samples are paired with data points via this SDOT map.

Result: AlignFlow scales well to large datasets and model architectures with negligible computational overhead. It improves performance of various state-of-the-art FGM algorithms and works as a plug-and-play component.

Conclusion: AlignFlow provides an effective and scalable solution for enhancing FGM training through optimal transport, with demonstrated improvements across multiple algorithms and datasets.

Abstract: Flow-based Generative Models (FGMs) effectively transform noise into complex
data distributions. Incorporating Optimal Transport (OT) to couple noise and
data during FGM training has been shown to improve the straightness of flow
trajectories, enabling more effective inference. However, existing OT-based
methods estimate the OT plan using (mini-)batches of sampled noise and data
points, which limits their scalability to large and high-dimensional datasets
in FGMs. This paper introduces AlignFlow, a novel approach that leverages
Semi-Discrete Optimal Transport (SDOT) to enhance the training of FGMs by
establishing an explicit, optimal alignment between noise distribution and data
points with guaranteed convergence. SDOT computes a transport map by
partitioning the noise space into Laguerre cells, each mapped to a
corresponding data point. During FGM training, i.i.d. noise samples are paired
with data points via the SDOT map. AlignFlow scales well to large datasets and
model architectures with negligible computational overhead. Experimental
results show that AlignFlow improves the performance of a wide range of
state-of-the-art FGM algorithms and can be integrated as a plug-and-play
component. Code is available at: https://github.com/konglk1203/AlignFlow.

</details>


### [137] [IQNN-CS: Interpretable Quantum Neural Network for Credit Scoring](https://arxiv.org/abs/2510.15044)
*Abdul Samad Khan,Nouhaila Innan,Aeysha Khalique,Muhammad Shafique*

Main category: cs.LG

TL;DR: IQNN-CS is an interpretable quantum neural network framework for multiclass credit risk classification that combines variational QNN with post-hoc explanation techniques and introduces ICAA metric for attribution analysis.


<details>
  <summary>Details</summary>
Motivation: Address the black-box nature of Quantum Machine Learning in high-stakes financial applications like credit scoring, where transparency and regulatory compliance are critical.

Method: Developed IQNN-CS framework with variational quantum neural network and post-hoc explanation techniques for structured data, plus introduced Inter-Class Attribution Alignment (ICAA) metric.

Result: Demonstrated stable training dynamics, competitive predictive performance, and enhanced interpretability on two real-world credit datasets.

Conclusion: Provides a practical path toward transparent and accountable QML models for financial decision-making, addressing interpretability challenges in quantum machine learning.

Abstract: Credit scoring is a high-stakes task in financial services, where model
decisions directly impact individuals' access to credit and are subject to
strict regulatory scrutiny. While Quantum Machine Learning (QML) offers new
computational capabilities, its black-box nature poses challenges for adoption
in domains that demand transparency and trust. In this work, we present
IQNN-CS, an interpretable quantum neural network framework designed for
multiclass credit risk classification. The architecture combines a variational
QNN with a suite of post-hoc explanation techniques tailored for structured
data. To address the lack of structured interpretability in QML, we introduce
Inter-Class Attribution Alignment (ICAA), a novel metric that quantifies
attribution divergence across predicted classes, revealing how the model
distinguishes between credit risk categories. Evaluated on two real-world
credit datasets, IQNN-CS demonstrates stable training dynamics, competitive
predictive performance, and enhanced interpretability. Our results highlight a
practical path toward transparent and accountable QML models for financial
decision-making.

</details>


### [138] [Internalizing World Models via Self-Play Finetuning for Agentic RL](https://arxiv.org/abs/2510.15047)
*Shiqi Chen,Tongyao Zhu,Zian Wang,Jinghan Zhang,Kangrui Wang,Siyang Gao,Teng Xiao,Yee Whye Teh,Junxian He,Manling Li*

Main category: cs.LG

TL;DR: SPA is a reinforcement learning framework that equips LLM agents with an internal world model through self-play supervised finetuning, significantly improving performance in out-of-distribution scenarios.


<details>
  <summary>Details</summary>
Motivation: LLM agents struggle in out-of-distribution environments due to difficulty grounding internal knowledge in complex, dynamic environmental dynamics, leading to brittle exploration and limited generalization.

Method: Decompose world model into state representation and transition modeling, then use Self-Play supervised finetuning to learn the world model by interacting with the environment, followed by policy optimization using simulated future states.

Result: SPA significantly outperforms baselines across diverse environments: Sokoban success rate increased from 25.6% to 59.8%, FrozenLake score improved from 22.1% to 70.9% for Qwen2.5-1.5B-Instruct model.

Conclusion: Equipping LLM agents with an internal world model through SPA framework effectively aligns reasoning with environmental dynamics and substantially improves decision-making in out-of-distribution scenarios.

Abstract: Large Language Models (LLMs) as agents often struggle in out-of-distribution
(OOD) scenarios. Real-world environments are complex and dynamic, governed by
task-specific rules and stochasticity, which makes it difficult for LLMs to
ground their internal knowledge in those dynamics. Under such OOD conditions,
vanilla RL training often fails to scale; we observe Pass@k--the probability
that at least one of (k) sampled trajectories succeeds--drops markedly across
training steps, indicating brittle exploration and limited generalization.
Inspired by model-based reinforcement learning, we hypothesize that equipping
LLM agents with an internal world model can better align reasoning with
environmental dynamics and improve decision-making. We show how to encode this
world model by decomposing it into two components: state representation and
transition modeling. Building on this, we introduce SPA, a simple reinforcement
learning framework that cold-starts the policy via a Self-Play supervised
finetuning (SFT) stage to learn the world model by interacting with the
environment, then uses it to simulate future states prior to policy
optimization. This simple initialization outperforms the online world-modeling
baseline and greatly boosts the RL-based agent training performance.
Experiments across diverse environments like Sokoban, FrozenLake, and Sudoku
show that our approach significantly improves performance. For example, SPA
boosts the Sokoban success rate from 25.6% to 59.8% and raises the FrozenLake
score from 22.1% to 70.9% for the Qwen2.5-1.5B-Instruct model.

</details>


### [139] [Learn to Change the World: Multi-level Reinforcement Learning with Model-Changing Actions](https://arxiv.org/abs/2510.15056)
*Ziqing Lu,Babak Hassibi,Lifeng Lai,Weiyu Xu*

Main category: cs.LG

TL;DR: Introduces MCTVMDP framework where agents can actively modify environment dynamics through model-changing actions, enabling joint optimization of both configuration policies and primitive action policies.


<details>
  <summary>Details</summary>
Motivation: Traditional RL assumes passive adaptation to fixed environments, but agents could benefit from actively reconfiguring the underlying world dynamics to increase rewards.

Method: Proposes multi-layer configurable time-varying Markov decision process (MCTVMDP) with upper-level model-changing actions that modify lower-level non-stationary transition functions.

Result: Framework enables agents to optimize both configuration policies (upper-level) and primitive action policies (lower-level) to jointly improve long-term expected rewards.

Conclusion: Active environment reconfiguration through model-changing actions provides a powerful extension to traditional RL, allowing agents to shape their own environments for better performance.

Abstract: Reinforcement learning usually assumes a given or sometimes even fixed
environment in which an agent seeks an optimal policy to maximize its long-term
discounted reward. In contrast, we consider agents that are not limited to
passive adaptations: they instead have model-changing actions that actively
modify the RL model of world dynamics itself. Reconfiguring the underlying
transition processes can potentially increase the agents' rewards. Motivated by
this setting, we introduce the multi-layer configurable time-varying Markov
decision process (MCTVMDP). In an MCTVMDP, the lower-level MDP has a
non-stationary transition function that is configurable through upper-level
model-changing actions. The agent's objective consists of two parts: Optimize
the configuration policies in the upper-level MDP and optimize the primitive
action policies in the lower-level MDP to jointly improve its expected
long-term reward.

</details>


### [140] [Antislop: A Comprehensive Framework for Identifying and Eliminating Repetitive Patterns in Language Models](https://arxiv.org/abs/2510.15061)
*Samuel Paech,Allen Roush,Judah Goldfeder,Ravid Shwartz-Ziv*

Main category: cs.LG

TL;DR: Antislop is a framework that detects and eliminates repetitive phrase patterns ("slop") in LLM outputs through three innovations: a backtracking sampler, automated slop profiling, and a fine-tuning method called FTPO that achieves 90% slop reduction without performance degradation.


<details>
  <summary>Details</summary>
Motivation: Widespread LLM adoption has introduced characteristic repetitive phraseology called "slop" that degrades output quality and makes AI-generated text immediately recognizable, creating a need for tools to detect and eliminate these patterns.

Method: Combines three innovations: (1) Antislop Sampler using backtracking to suppress unwanted strings at inference; (2) Automated pipeline profiling model-specific slop against human baselines; (3) Final Token Preference Optimization (FTPO) - a fine-tuning method that surgically adjusts logits when banned patterns appear in inference traces.

Result: Some slop patterns appear 1,000x more frequently in LLM output than human text. Antislop Sampler suppresses 8,000+ patterns while maintaining quality (vs. token banning unusable at 2,000). FTPO achieves 90% slop reduction while maintaining or improving performance on GSM8K, MMLU, and creative writing tasks.

Conclusion: Antislop framework effectively reduces repetitive phrase patterns in LLM outputs, with FTPO showing superior performance compared to DPO which suffers degradation in writing quality and lexical diversity despite weaker suppression.

Abstract: Widespread LLM adoption has introduced characteristic repetitive phraseology,
termed ``slop,'' which degrades output quality and makes AI-generated text
immediately recognizable. We present Antislop, a comprehensive framework
providing tools to both detect and eliminate these overused patterns. Our
approach combines three innovations: (1) The Antislop Sampler, which uses
backtracking to suppress unwanted strings at inference time without destroying
vocabulary; (2) An automated pipeline that profiles model-specific slop against
human baselines and generates training data; (3) Final Token Preference
Optimization (FTPO), a novel fine-tuning method that operates on individual
tokens, surgically adjusting logits wherever a banned pattern has appeared in
an inference trace. We demonstrate that some slop patterns appear over
1,000$\times$ more frequently in LLM output than human text. The Antislop
Sampler successfully suppresses 8,000+ patterns while maintaining quality,
whereas token banning becomes unusable at just 2,000. Most importantly, FTPO
achieves 90\% slop reduction while maintaining or improving performance in
cross-domain evals including GSM8K, MMLU, and creative writing tasks. In
contrast, DPO suffers significant degradation in writing quality and lexical
diversity despite achieving weaker suppression. We release all code and results
under MIT license: https://github.com/sam-paech/auto-antislop.

</details>


### [141] [Physics-informed data-driven machine health monitoring for two-photon lithography](https://arxiv.org/abs/2510.15075)
*Sixian Jia,Zhiqiao Dong,Chenhui Shao*

Main category: cs.LG

TL;DR: This paper presents three physics-informed data-driven methods for monitoring two-photon lithography (TPL) machine health to enable condition-based maintenance and prevent untimely or unnecessary maintenance.


<details>
  <summary>Details</summary>
Motivation: Current TPL maintenance relies on experience rather than informed monitoring, leading to either untimely maintenance causing downtime and poor fabrication quality, or unnecessary maintenance causing inefficiencies.

Method: Three methods integrating physics-informed data-driven predictive models for structure dimensions with statistical approaches, designed to handle increasingly complex scenarios with different levels of generalizability.

Result: The approaches achieved high accuracies across all test scenarios using a comprehensive experimental dataset with six process parameter combinations and six structure dimensions under two machine health conditions, demonstrating excellent effectiveness, robustness, and generalizability.

Conclusion: The results represent a significant step toward condition-based maintenance for TPL systems, enabling accurate and timely monitoring of machine health.

Abstract: Two-photon lithography (TPL) is a sophisticated additive manufacturing
technology for creating three-dimensional (3D) micro- and nano-structures.
Maintaining the health of TPL systems is critical for ensuring consistent
fabrication quality. Current maintenance practices often rely on experience
rather than informed monitoring of machine health, resulting in either untimely
maintenance that causes machine downtime and poor-quality fabrication, or
unnecessary maintenance that leads to inefficiencies and avoidable downtime. To
address this gap, this paper presents three methods for accurate and timely
monitoring of TPL machine health. Through integrating physics-informed
data-driven predictive models for structure dimensions with statistical
approaches, the proposed methods are able to handle increasingly complex
scenarios featuring different levels of generalizability. A comprehensive
experimental dataset that encompasses six process parameter combinations and
six structure dimensions under two machine health conditions was collected to
evaluate the effectiveness of the proposed approaches. Across all test
scenarios, the approaches are shown to achieve high accuracies, demonstrating
excellent effectiveness, robustness, and generalizability. These results
represent a significant step toward condition-based maintenance for TPL
systems.

</details>


### [142] [Online Correlation Clustering: Simultaneously Optimizing All $\ell_p$-norms](https://arxiv.org/abs/2510.15076)
*Sami Davies,Benjamin Moseley,Heather Newman*

Main category: cs.LG

TL;DR: This paper presents the first online algorithm that simultaneously approximates all ℓ_p-norms for correlation clustering using a small sample, achieving O(log⁴ n)-competitive ratios for all norms, O(log n) for ℓ_∞, and O(1) for ℓ_1.


<details>
  <summary>Details</summary>
Motivation: The paper addresses the fundamental trade-off between minimizing total disagreements (ℓ₁-norm) and ensuring fairness to individual nodes (ℓ_∞-norm) in correlation clustering. While offline algorithms can simultaneously approximate all ℓ_p-norms, it was unknown if this was possible online. The motivation stems from a new hardness result showing a fundamental separation between these objectives in the standard random-order online model.

Method: The authors develop a single algorithm for the online-with-a-sample (AOS) model that uses a small constant fraction of the input as a sample. This algorithm produces one clustering that simultaneously achieves competitive guarantees for all ℓ_p-norms.

Result: The algorithm achieves: O(log⁴ n)-competitive for all ℓ_p-norms with high probability, O(log n)-competitive for ℓ_∞-norm with high probability, and O(1)-competitive for ℓ_₁-norm in expectation. The paper also proves a hardness result showing that in the standard random-order model, any algorithm for ℓ_∞-norm must have competitive ratio at least Ω(n¹/³).

Conclusion: This work successfully translates the offline "all-norms" guarantee to the online setting using the AOS model, overcoming the fundamental limitations of the standard random-order model. The results demonstrate that with a small sample, it's possible to achieve simultaneous approximation guarantees for all ℓ_p-norms in online correlation clustering.

Abstract: The $\ell_p$-norm objectives for correlation clustering present a fundamental
trade-off between minimizing total disagreements (the $\ell_1$-norm) and
ensuring fairness to individual nodes (the $\ell_\infty$-norm). Surprisingly,
in the offline setting it is possible to simultaneously approximate all
$\ell_p$-norms with a single clustering. Can this powerful guarantee be
achieved in an online setting? This paper provides the first affirmative
answer. We present a single algorithm for the online-with-a-sample (AOS) model
that, given a small constant fraction of the input as a sample, produces one
clustering that is simultaneously $O(\log^4 n)$-competitive for all
$\ell_p$-norms with high probability, $O(\log n)$-competitive for the
$\ell_\infty$-norm with high probability, and $O(1)$-competitive for the
$\ell_1$-norm in expectation. This work successfully translates the offline
"all-norms" guarantee to the online world.
  Our setting is motivated by a new hardness result that demonstrates a
fundamental separation between these objectives in the standard random-order
(RO) online model. Namely, while the $\ell_1$-norm is trivially
$O(1)$-approximable in the RO model, we prove that any algorithm in the RO
model for the fairness-promoting $\ell_\infty$-norm must have a competitive
ratio of at least $\Omega(n^{1/3})$. This highlights the necessity of a
different beyond-worst-case model. We complement our algorithm with lower
bounds, showing our competitive ratios for the $\ell_1$- and $\ell_\infty$-
norms are nearly tight in the AOS model.

</details>


### [143] [Operator Flow Matching for Timeseries Forecasting](https://arxiv.org/abs/2510.15101)
*Yolanne Yi Ran Lee,Kyriakos Flouris*

Main category: cs.LG

TL;DR: TempO is a latent flow matching model that uses sparse conditioning and channel folding to efficiently forecast high-dimensional PDE dynamics, outperforming state-of-the-art methods with better multi-scale dynamics recovery and computational efficiency.


<details>
  <summary>Details</summary>
Motivation: Existing autoregressive and diffusion-based approaches for forecasting PDE-governed dynamics suffer from cumulative errors and discretization artifacts that limit long, physically consistent forecasts. Flow matching offers a more efficient and deterministic alternative.

Method: TempO uses latent flow matching with sparse conditioning and channel folding to process 3D spatiotemporal fields. It employs time-conditioned Fourier layers to capture multi-scale modes with high fidelity, and includes a proven upper bound on FNO approximation error.

Result: TempO outperforms state-of-the-art baselines across three benchmark PDE datasets. Spectral analysis shows superior recovery of multi-scale dynamics, and efficiency studies demonstrate its parameter- and memory-light design compared to attention-based or convolutional regressors.

Conclusion: TempO provides an effective solution for high-dimensional PDE forecasting, offering improved accuracy, better multi-scale dynamics capture, and superior computational efficiency compared to existing approaches.

Abstract: Forecasting high-dimensional, PDE-governed dynamics remains a core challenge
for generative modeling. Existing autoregressive and diffusion-based approaches
often suffer cumulative errors and discretisation artifacts that limit long,
physically consistent forecasts. Flow matching offers a natural alternative,
enabling efficient, deterministic sampling. We prove an upper bound on FNO
approximation error and propose TempO, a latent flow matching model leveraging
sparse conditioning with channel folding to efficiently process 3D
spatiotemporal fields using time-conditioned Fourier layers to capture
multi-scale modes with high fidelity. TempO outperforms state-of-the-art
baselines across three benchmark PDE datasets, and spectral analysis further
demonstrates superior recovery of multi-scale dynamics, while efficiency
studies highlight its parameter- and memory-light design compared to
attention-based or convolutional regressors.

</details>


### [144] [DLER: Doing Length pEnalty Right - Incentivizing More Intelligence per Token via Reinforcement Learning](https://arxiv.org/abs/2510.15110)
*Shih-Yang Liu,Xin Dong,Ximing Lu,Shizhe Diao,Mingjie Liu,Min-Hung Chen,Hongxu Yin,Yu-Chiang Frank Wang,Kwang-Ting Cheng,Yejin Choi,Jan Kautz,Pavlo Molchanov*

Main category: cs.LG

TL;DR: DLER is a reinforcement learning training method that uses simple truncation as length penalty to make reasoning language models generate more concise outputs while maintaining or improving accuracy, addressing key RL optimization challenges.


<details>
  <summary>Details</summary>
Motivation: Reasoning language models generate unnecessarily long outputs, and maximizing intelligence per token (accuracy relative to response length) remains an open problem. Current approaches struggle with balancing accuracy and efficiency.

Method: DLER combines batch-wise reward normalization, higher clipping, dynamic sampling, and simple truncation length penalty to address three key RL challenges: large bias in advantage estimation, entropy collapse, and sparse reward signal.

Result: DLER cuts output length by over 70% while surpassing all previous baseline accuracy. DLER-7B generates multiple concise responses with 28% higher accuracy and lower latency compared to DeepSeek-R1-7B.

Conclusion: Simple length penalties like truncation can be effective when combined with proper RL optimization techniques. DLER achieves state-of-the-art accuracy-efficiency trade-offs and enables adaptive efficiency gains through difficulty-aware truncation.

Abstract: Reasoning language models such as OpenAI-o1, DeepSeek-R1, and Qwen achieve
strong performance via extended chains of thought but often generate
unnecessarily long outputs. Maximizing intelligence per token--accuracy
relative to response length--remains an open problem. We revisit reinforcement
learning (RL) with the simplest length penalty--truncation--and show that
accuracy degradation arises not from the lack of sophisticated penalties but
from inadequate RL optimization. We identify three key challenges: (i) large
bias in advantage estimation, (ii) entropy collapse, and (iii) sparse reward
signal. We address them with Doing Length pEnalty Right (DLER), a training
recipe combining batch-wise reward normalization, higher clipping, dynamic
sampling, and a simple truncation length penalty. DLER achieves
state-of-the-art accuracy--efficiency trade-offs, cutting output length by over
70 percent while surpassing all previous baseline accuracy. It also improves
test-time scaling: compared to DeepSeek-R1-7B, DLER-7B generates multiple
concise responses in parallel with 28 percent higher accuracy and lower
latency. We further introduce Difficulty-Aware DLER, which adaptively tightens
truncation on easier questions for additional efficiency gains. We also propose
an update-selective merging method that preserves baseline accuracy while
retaining the concise reasoning ability of the DLER model, which is useful for
scenarios where RL training data is scarce.

</details>


### [145] [Navigating the consequences of mechanical ventilation in clinical intensive care settings through an evolutionary game-theoretic framework](https://arxiv.org/abs/2510.15127)
*David J. Albers,Tell D. Bennett,Jana de Wiljes,Bradford J. Smith,Peter D. Sottile,J. N. Stroh*

Main category: cs.LG

TL;DR: This paper develops a framework using evolutionary game theory to analyze mechanical ventilation data and understand how ventilation strategies affect patient outcomes in critical care.


<details>
  <summary>Details</summary>
Motivation: To understand the effects of mechanical ventilation strategies on patient outcomes by analyzing heterogeneous patient-ventilator systems within clinical decision-making environments.

Method: Uses evolutionary game theory (EGT) to analyze breath behaviors and patient-ventilator-care systems (J6), creating quantitative precursors for deeper analysis through probabilistic and stochastic methods like reinforcement learning.

Result: The EGT-based process was validated on synthetic data and applied to real-world ICU data, revealing complexities of the data-generating process and providing a foundation for MV optimization.

Conclusion: This work represents a step toward mechanical ventilation optimization and personalization, with potential for developing state transition models to simulate MV decision effects using empirical and game-theoretic elements.

Abstract: Identifying the effects of mechanical ventilation strategies and protocols in
critical care requires analyzing data from heterogeneous patient-ventilator
systems within the context of the clinical decision-making environment. This
research develops a framework to help understand the consequences of mechanical
ventilation (MV) and adjunct care decisions on patient outcome from
observations of critical care patients receiving MV. Developing an
understanding of and improving critical care respiratory management requires
the analysis of existing secondary-use clinical data to generate hypotheses
about advantageous variations and adaptations of current care. This work
introduces a perspective of the joint patient-ventilator-care systems
(so-called J6) to develop a scalable method for analyzing data and trajectories
of these complex systems. To that end, breath behaviors are analyzed using
evolutionary game theory (EGT), which generates the necessary quantitative
precursors for deeper analysis through probabilistic and stochastic machinery
such as reinforcement learning. This result is one step along the pathway
toward MV optimization and personalization. The EGT-based process is
analytically validated on synthetic data to reveal potential caveats before
proceeding to real-world ICU data applications that expose complexities of the
data-generating process J6. The discussion includes potential developments
toward a state transition model for the simulating effects of MV decision using
empirical and game-theoretic elements.

</details>


### [146] [Decentralized Parameter-Free Online Learning](https://arxiv.org/abs/2510.15644)
*Tomas Ortega,Hamid Jafarkhani*

Main category: cs.LG

TL;DR: Parameter-free decentralized online learning algorithms with sublinear network regret, connecting multi-agent coin-betting and decentralized learning via gossip steps.


<details>
  <summary>Details</summary>
Motivation: To develop decentralized online learning algorithms that achieve sublinear regret without requiring hyperparameter tuning, addressing the need for practical distributed learning systems.

Method: Introduces a novel betting function formulation for coin-betting that simplifies multi-agent regret analysis, and connects multi-agent coin-betting with decentralized online learning through gossip steps.

Result: Achieves sublinear network regret bounds, validated through experiments on synthetic and real datasets.

Conclusion: The proposed parameter-free decentralized online learning algorithms are applicable to distributed sensing, decentralized optimization, and collaborative ML applications.

Abstract: We propose the first parameter-free decentralized online learning algorithms
with network regret guarantees, which achieve sublinear regret without
requiring hyperparameter tuning. This family of algorithms connects multi-agent
coin-betting and decentralized online learning via gossip steps. To enable our
decentralized analysis, we introduce a novel "betting function" formulation for
coin-betting that simplifies the multi-agent regret analysis. Our analysis
shows sublinear network regret bounds and is validated through experiments on
synthetic and real datasets. This family of algorithms is applicable to
distributed sensing, decentralized optimization, and collaborative ML
applications.

</details>


### [147] [A Simple Method for PMF Estimation on Large Supports](https://arxiv.org/abs/2510.15132)
*Alex Shtoff*

Main category: cs.LG

TL;DR: A nonparametric method for estimating multi-modal, heavy-tailed PMFs using graph Laplacian filtering and low-dimensional projection, with automated dimension selection and efficient computation.


<details>
  <summary>Details</summary>
Motivation: To estimate probability mass functions on large discrete supports when the PMF is multi-modal and heavy-tailed, where traditional methods may struggle with noise and structural preservation.

Method: Treat empirical PMF as signal on line graph, apply data-dependent low-pass filter using perturbed path graph Laplacian, compute smallest eigenvectors, project empirical PMF onto low-dimensional subspace, then clip and re-normalize.

Result: Method preserves coarse structure while suppressing sampling noise, performs favorably compared to logspline and Gaussian-KDE baselines in intended regimes, with reliable computation proportional to support size times subspace dimension.

Conclusion: The approach is short to implement, robust across sample sizes, suitable for automated pipelines and exploratory analysis due to reliability and speed, though has known failure modes like abrupt discontinuities.

Abstract: We study nonparametric estimation of a probability mass function (PMF) on a
large discrete support, where the PMF is multi-modal and heavy-tailed. The core
idea is to treat the empirical PMF as a signal on a line graph and apply a
data-dependent low-pass filter. Concretely, we form a symmetric tri-diagonal
operator, the path graph Laplacian perturbed with a diagonal matrix built from
the empirical PMF, then compute the eigenvectors, corresponding to the smallest
feq eigenvalues. Projecting the empirical PMF onto this low dimensional
subspace produces a smooth, multi-modal estimate that preserves coarse
structure while suppressing noise. A light post-processing step of clipping and
re-normalizing yields a valid PMF.
  Because we compute the eigenpairs of a symmetric tridiagonal matrix, the
computation is reliable and runs time and memory proportional to the support
times the dimension of the desired low-dimensional supspace. We also provide a
practical, data-driven rule for selecting the dimension based on an
orthogonal-series risk estimate, so the method "just works" with minimal
tuning. On synthetic and real heavy-tailed examples, the approach preserves
coarse structure while suppressing sampling noise, compares favorably to
logspline and Gaussian-KDE baselines in the intended regimes. However, it has
known failure modes (e.g., abrupt discontinuities). The method is short to
implement, robust across sample sizes, and suitable for automated pipelines and
exploratory analysis at scale because of its reliability and speed.

</details>


### [148] [Predicting the Unpredictable: Reproducible BiLSTM Forecasting of Incident Counts in the Global Terrorism Database (GTD)](https://arxiv.org/abs/2510.15136)
*Oluwasegun Adegoke*

Main category: cs.LG

TL;DR: BiLSTM model outperforms classical and deep learning baselines for weekly terrorism incident forecasting using GTD data, with key findings on optimal lookback windows and feature importance.


<details>
  <summary>Details</summary>
Motivation: To develop a reproducible pipeline for short-horizon forecasting of weekly terrorism incident counts using the Global Terrorism Database, comparing deep learning approaches against classical methods.

Method: Built a reproducible pipeline with fixed time-based splits, evaluated Bidirectional LSTM (BiLSTM) against seasonal-naive, linear/ARIMA models and LSTM-Attention baseline, with ablations on temporal memory, training-history length, spatial grain, lookback size, and feature groups.

Result: BiLSTM achieved RMSE 6.38 on test set, outperforming LSTM-Attention (9.19; +30.6%) and linear lag-regression baseline (+35.4% RMSE gain), with parallel improvements in MAE and MAPE. Models trained on long historical data generalized best, moderate lookback (20-30 weeks) provided strong context.

Conclusion: Bidirectional encoding is critical for capturing both build-up and aftermath patterns, short-horizon structure features contribute most to performance, and the study provides a transparent, baseline-beating reference for GTD incident forecasting.

Abstract: We study short-horizon forecasting of weekly terrorism incident counts using
the Global Terrorism Database (GTD, 1970--2016). We build a reproducible
pipeline with fixed time-based splits and evaluate a Bidirectional LSTM
(BiLSTM) against strong classical anchors (seasonal-naive, linear/ARIMA) and a
deep LSTM-Attention baseline. On the held-out test set, the BiLSTM attains RMSE
6.38, outperforming LSTM-Attention (9.19; +30.6\%) and a linear lag-regression
baseline (+35.4\% RMSE gain), with parallel improvements in MAE and MAPE.
Ablations varying temporal memory, training-history length, spatial grain,
lookback size, and feature groups show that models trained on long historical
data generalize best; a moderate lookback (20--30 weeks) provides strong
context; and bidirectional encoding is critical for capturing both build-up and
aftermath patterns within the window. Feature-group analysis indicates that
short-horizon structure (lagged counts and rolling statistics) contributes
most, with geographic and casualty features adding incremental lift. We release
code, configs, and compact result tables, and provide a data/ethics statement
documenting GTD licensing and research-only use. Overall, the study offers a
transparent, baseline-beating reference for GTD incident forecasting.

</details>


### [149] [Policy Transfer Ensures Fast Learning for Continuous-Time LQR with Entropy Regularization](https://arxiv.org/abs/2510.15165)
*Xin Guo,Zijiu Lyu*

Main category: cs.LG

TL;DR: This paper provides the first theoretical proof of policy transfer for continuous-time RL, showing that optimal policies from one LQR can serve as near-optimal initializations for related LQRs while maintaining convergence rates. It also introduces a new policy learning algorithm with global linear and local super-linear convergence.


<details>
  <summary>Details</summary>
Motivation: Training RL agents from scratch on complex tasks is inefficient. Transfer learning, successful in LLMs, offers a promising approach to enhance RL efficiency by leveraging pre-trained models, but theoretical foundations for continuous-time RL were lacking.

Method: The paper investigates policy transfer in continuous-time linear quadratic regulators (LQRs) with entropy regularization. It provides theoretical analysis and introduces a novel policy learning algorithm for continuous-time LQRs.

Result: Theoretical proof shows optimal policies from source LQRs serve as near-optimal initializations for target LQRs while preserving convergence rates. The new algorithm achieves global linear and local super-linear convergence. Additionally, stability analysis for continuous-time score-based diffusion models is derived.

Conclusion: The work demonstrates both theoretical guarantees and algorithmic benefits of transfer learning in continuous-time RL, bridging a gap in existing literature and extending prior work from discrete to continuous time settings.

Abstract: Reinforcement Learning (RL) enables agents to learn optimal decision-making
strategies through interaction with an environment, yet training from scratch
on complex tasks can be highly inefficient. Transfer learning (TL), widely
successful in large language models (LLMs), offers a promising direction for
enhancing RL efficiency by leveraging pre-trained models.
  This paper investigates policy transfer, a TL approach that initializes
learning in a target RL task using a policy from a related source task, in the
context of continuous-time linear quadratic regulators (LQRs) with entropy
regularization. We provide the first theoretical proof of policy transfer for
continuous-time RL, proving that a policy optimal for one LQR serves as a
near-optimal initialization for closely related LQRs, while preserving the
original algorithm's convergence rate. Furthermore, we introduce a novel policy
learning algorithm for continuous-time LQRs that achieves global linear and
local super-linear convergence. Our results demonstrate both theoretical
guarantees and algorithmic benefits of transfer learning in continuous-time RL,
addressing a gap in existing literature and extending prior work from discrete
to continuous time settings.
  As a byproduct of our analysis, we derive the stability of a class of
continuous-time score-based diffusion models via their connection with LQRs.

</details>


### [150] [A simple mean field model of feature learning](https://arxiv.org/abs/2510.15174)
*Niclas Göring,Chris Mingard,Yoonsoo Nam,Ard Louis*

Main category: cs.LG

TL;DR: The paper develops a mean-field theory for feature learning in neural networks, revealing a symmetry breaking phase transition where networks align with target functions at finite width, and identifies self-reinforcing input feature selection as a key mechanism missing from basic mean-field descriptions.


<details>
  <summary>Details</summary>
Motivation: To better understand feature learning in neural networks, which remains poorly understood despite being fundamental to their success, using statistical physics methods to derive a tractable theory.

Method: Derived a self-consistent mean-field theory for Bayesian posterior of two-layer non-linear networks trained with stochastic gradient Langevin dynamics (SGLD), incorporating self-reinforcing input feature selection.

Result: At infinite width, theory reduces to kernel ridge regression; at finite width predicts symmetry breaking phase transition where networks align with target functions. Basic MF theory underestimates generalization improvements, but incorporating self-reinforcing feature selection quantitatively matches SGLD learning curves.

Conclusion: The mean-field theory provides mechanistic insight into feature learning, with self-reinforcing input feature selection being a crucial mechanism that explains the discrepancy between basic theory and actual network performance.

Abstract: Feature learning (FL), where neural networks adapt their internal
representations during training, remains poorly understood. Using methods from
statistical physics, we derive a tractable, self-consistent mean-field (MF)
theory for the Bayesian posterior of two-layer non-linear networks trained with
stochastic gradient Langevin dynamics (SGLD). At infinite width, this theory
reduces to kernel ridge regression, but at finite width it predicts a symmetry
breaking phase transition where networks abruptly align with target functions.
While the basic MF theory provides theoretical insight into the emergence of FL
in the finite-width regime, semi-quantitatively predicting the onset of FL with
noise or sample size, it substantially underestimates the improvements in
generalisation after the transition. We trace this discrepancy to a key
mechanism absent from the plain MF description: \textit{self-reinforcing input
feature selection}. Incorporating this mechanism into the MF theory allows us
to quantitatively match the learning curves of SGLD-trained networks and
provides mechanistic insight into FL.

</details>


### [151] [Finding geodesics with the Deep Ritz method](https://arxiv.org/abs/2510.15177)
*Conor Rowan*

Main category: cs.LG

TL;DR: The paper argues that geodesic problems are well-suited for the Deep Ritz method due to their simple geometry, variational structure, and natural nonlinearity, and demonstrates this with three numerical examples.


<details>
  <summary>Details</summary>
Motivation: Geodesic problems are ubiquitous in physics and engineering but have received little attention from the scientific machine learning community, making them a promising application area for SciML methods.

Method: The authors propose using the Deep Ritz method to solve geodesic problems, leveraging its suitability for problems with variational structure and natural nonlinearity.

Result: The paper presents three numerical examples from path planning, optics, and solid mechanics that demonstrate the effectiveness of the Deep Ritz method for geodesic problems.

Conclusion: Geodesic problems represent a promising application domain for the Deep Ritz method and a fruitful direction for future scientific machine learning research.

Abstract: Geodesic problems involve computing trajectories between prescribed initial
and final states to minimize a user-defined measure of distance, cost, or
energy. They arise throughout physics and engineering -- for instance, in
determining optimal paths through complex environments, modeling light
propagation in refractive media, and the study of spacetime trajectories in
control theory and general relativity. Despite their ubiquity, the scientific
machine learning (SciML) community has given relatively little attention to
investigating its methods in the context of these problems. In this work, we
argue that given their simple geometry, variational structure, and natural
nonlinearity, geodesic problems are particularly well-suited for the Deep Ritz
method. We substantiate this claim with three numerical examples drawn from
path planning, optics, and solid mechanics. Our goal is not to provide an
exhaustive study of geodesic problems, but rather to identify a promising
application of the Deep Ritz method and a fruitful direction for future SciML
research.

</details>


### [152] [An Advanced Two-Stage Model with High Sensitivity and Generalizability for Prediction of Hip Fracture Risk Using Multiple Datasets](https://arxiv.org/abs/2510.15179)
*Shuo Sun,Meiling Zhou,Chen Zhao,Joyce H. Keyak,Nancy E. Lane,Jeffrey D. Deng,Kuan-Jui Su,Hui Shen,Hong-Wen Deng,Kui Zhang,Weihua Zhou*

Main category: cs.LG

TL;DR: A sequential two-stage model combining clinical data and DXA imaging improves hip fracture risk prediction, outperforming traditional T-score and FRAX methods with higher sensitivity and fewer missed cases.


<details>
  <summary>Details</summary>
Motivation: Current hip fracture risk assessment tools like DXA T-score and FRAX lack sensitivity and often miss high-risk individuals, especially those without prior fractures or with osteopenia.

Method: Two-stage sequential model: Stage 1 uses clinical, demographic, and functional variables for baseline risk estimation; Stage 2 incorporates DXA-derived features for refinement. Validated across MrOS, SOF, and UK Biobank datasets.

Result: The model achieved higher sensitivity and reduced missed cases compared to T-score and FRAX, with consistent performance across different cohorts through internal and external validation.

Conclusion: The two-stage framework provides a cost-effective and personalized approach for early hip fracture risk assessment, improving prediction accuracy over traditional methods.

Abstract: Hip fractures are a major cause of disability, mortality, and healthcare
burden in older adults, underscoring the need for early risk assessment.
However, commonly used tools such as the DXA T-score and FRAX often lack
sensitivity and miss individuals at high risk, particularly those without prior
fractures or with osteopenia. To address this limitation, we propose a
sequential two-stage model that integrates clinical and imaging information to
improve prediction accuracy. Using data from the Osteoporotic Fractures in Men
Study (MrOS), the Study of Osteoporotic Fractures (SOF), and the UK Biobank,
Stage 1 (Screening) employs clinical, demographic, and functional variables to
estimate baseline risk, while Stage 2 (Imaging) incorporates DXA-derived
features for refinement. The model was rigorously validated through internal
and external testing, showing consistent performance and adaptability across
cohorts. Compared to T-score and FRAX, the two-stage framework achieved higher
sensitivity and reduced missed cases, offering a cost-effective and
personalized approach for early hip fracture risk assessment.
  Keywords: Hip Fracture, Two-Stage Model, Risk Prediction, Sensitivity, DXA,
FRAX

</details>


### [153] [Automotive Crash Dynamics Modeling Accelerated with Machine Learning](https://arxiv.org/abs/2510.15201)
*Mohammad Amin Nabian,Sudeep Chavare,Deepak Akhare,Rishikesh Ranade,Ram Cherukuri,Srinivas Tadepalli*

Main category: cs.LG

TL;DR: Machine learning surrogate models for crashworthiness assessment using NVIDIA PhysicsNeMo framework, achieving orders-of-magnitude faster predictions than traditional finite element simulations.


<details>
  <summary>Details</summary>
Motivation: Traditional finite element simulations for crashworthiness assessment are computationally expensive and time-consuming, creating need for faster alternatives.

Method: Used MeshGraphNet and Transolver neural network architectures with three transient dynamics strategies (time-conditional, standard Autoregressive, stability-enhanced Autoregressive) on Body-in-White crash dataset of 150 LS-DYNA simulations.

Result: Models captured overall deformation trends with reasonable fidelity, achieving orders-of-magnitude computational cost reduction compared to full FE simulations.

Conclusion: Machine learning approaches demonstrate feasibility for structural crash dynamics, enabling rapid design exploration and early-stage optimization in crashworthiness evaluation.

Abstract: Crashworthiness assessment is a critical aspect of automotive design,
traditionally relying on high-fidelity finite element (FE) simulations that are
computationally expensive and time-consuming. This work presents an exploratory
comparative study on developing machine learning-based surrogate models for
efficient prediction of structural deformation in crash scenarios using the
NVIDIA PhysicsNeMo framework. Given the limited prior work applying machine
learning to structural crash dynamics, the primary contribution lies in
demonstrating the feasibility and engineering utility of the various modeling
approaches explored in this work. We investigate two state-of-the-art neural
network architectures for modeling crash dynamics: MeshGraphNet, and
Transolver. Additionally, we examine three strategies for modeling transient
dynamics: time-conditional, the standard Autoregressive approach, and a
stability-enhanced Autoregressive scheme incorporating rollout-based training.
The models are evaluated on a comprehensive Body-in-White (BIW) crash dataset
comprising 150 detailed FE simulations using LS-DYNA. The dataset represents a
structurally rich vehicle assembly with over 200 components, including 38 key
components featuring variable thickness distributions to capture realistic
manufacturing variability. Each model utilizes the undeformed mesh geometry and
component characteristics as inputs to predict the spatiotemporal evolution of
the deformed mesh during the crash sequence. Evaluation results show that the
models capture the overall deformation trends with reasonable fidelity,
demonstrating the feasibility of applying machine learning to structural crash
dynamics. Although not yet matching full FE accuracy, the models achieve
orders-of-magnitude reductions in computational cost, enabling rapid design
exploration and early-stage optimization in crashworthiness evaluation.

</details>


### [154] [Dissecting Mahalanobis: How Feature Geometry and Normalization Shape OOD Detection](https://arxiv.org/abs/2510.15202)
*Denis Janiak,Jakub Binkowski,Tomasz Kajdanowicz*

Main category: cs.LG

TL;DR: This paper analyzes Mahalanobis distance methods for OOD detection, showing they aren't universally reliable. It defines ideal representation geometry, proposes metrics to predict OOD performance, and introduces radially scaled l2 normalization to improve detection by controlling feature space geometry.


<details>
  <summary>Details</summary>
Motivation: To understand the impact of representation geometry and normalization on Mahalanobis distance methods for OOD detection, as current limitations may restrict their practical application.

Method: Comprehensive empirical study across diverse image foundation models, datasets, and normalization schemes. Proposed radially scaled l2 normalization with tunable parameter to control feature space geometry.

Result: Mahalanobis-based methods aren't universally reliable. Spectral and intrinsic-dimensionality metrics can predict OOD performance. Radially scaled l2 normalization significantly improves OOD detection by systematically controlling representation geometry.

Conclusion: The study bridges representation geometry, normalization, and OOD performance, offering insights for designing more effective and reliable deep learning models through controlled feature space geometry.

Abstract: Out-of-distribution (OOD) detection is critical for the reliable deployment
of deep learning models. hile Mahalanobis distance methods are widely used, the
impact of representation geometry and normalization on their performance is not
fully understood, which may limit their downstream application. To address this
gap, we conducted a comprehensive empirical study across diverse image
foundation models, datasets, and distance normalization schemes. First, our
analysis shows that Mahalanobis-based methods aren't universally reliable.
Second, we define the ideal geometry for data representations and demonstrate
that spectral and intrinsic-dimensionality metrics can accurately predict a
model's OOD performance. Finally, we analyze how normalization impacts OOD
performance. Building upon these studies, we propose radially scaled $\ell_2$
normalization, a method that generalizes the standard $\ell_2$ normalization
recently applied to Mahalanobis-based OOD detection. Our approach introduces a
tunable parameter to directly control the radial geometry of the feature space,
systematically contracting or expanding representations to significantly
improve OOD detection performance. By bridging the gap between representation
geometry, normalization, and OOD performance, our findings offer new insights
into the design of more effective and reliable deep learning models.

</details>


### [155] [ReasonIF: Large Reasoning Models Fail to Follow Instructions During Reasoning](https://arxiv.org/abs/2510.15211)
*Yongchan Kwon,Shang Zhu,Federico Bianchi,Kaitlyn Zhou,James Zou*

Main category: cs.LG

TL;DR: The paper introduces ReasonIF, a benchmark to evaluate how well large reasoning models follow user instructions during their reasoning process, not just in final responses. It finds significant failures in instruction adherence across major models and proposes methods to improve it.


<details>
  <summary>Details</summary>
Motivation: Current evaluation focuses on instruction following in final responses, but it's critical for reasoning models to follow instructions throughout their reasoning process for better controllability, transparency, and to reduce risks like shortcuts, hallucinations, and reward hacking.

Method: Created ReasonIF benchmark with six categories of instruction prompts (multilingual reasoning, formatting, length control). Evaluated multiple open-source LRMs and proposed two improvement strategies: multi-turn reasoning and Reasoning Instruction Finetuning (RIF) using synthetic data.

Result: Found substantial failures in reasoning instruction adherence - highest instruction following score was below 0.25 (less than 25% compliance). Performance degrades with increasing task difficulty. RIF improved GPT-OSS-20B's score from 0.11 to 0.27.

Conclusion: Reasoning instruction following is a major challenge for current LRMs with significant room for improvement. The proposed RIF method shows measurable progress but current capabilities remain limited, highlighting the need for continued research in this area.

Abstract: The ability of large language models (LLMs) to follow user instructions is
central to their reliability, safety, and usefulness. While prior studies
assess instruction adherence in the model's main responses, we argue that it is
also critical for large reasoning models (LRMs) to follow user instructions
throughout their reasoning process. Reasoning instruction following makes LRMs
more controllable and transparent, while reducing risks of undesirable
shortcuts, hallucinations, or reward hacking within reasoning traces. To
evaluate this dimension, we introduce ReasonIF, a systematic benchmark for
assessing reasoning instruction following. ReasonIF includes six categories of
instruction prompts, spanning multilingual reasoning, formatting and length
control. Across many open-source LRMs including GPT-OSS, Qwen3, and
DeepSeek-R1, we find substantial failures in reasoning instruction adherence:
the highest instruction following score (IFS) remains below 0.25, meaning that
fewer than $25\%$ of reasoning traces comply with the given instructions.
Notably, as task difficulty increases, reasoning instruction following degrades
further. We also explore two strategies to enhance reasoning instruction
fidelity. (1) multi-turn reasoning and (2) Reasoning Instruction Finetuning
(RIF) using synthetic data. RIF improves the IFS of $GPT-OSS-20B$ from 0.11 to
0.27, indicating measurable progress but leaving ample room for improvement.

</details>


### [156] [Soundness-Aware Level: A Microscopic Signature that Predicts LLM Reasoning Potential](https://arxiv.org/abs/2510.15216)
*Xuansheng Wu,Xiaoman Pan,Wenlin Yao,Jianshu Chen*

Main category: cs.LG

TL;DR: The paper identifies that a model's reasoning potential after reinforcement learning with verifiable rewards (RLVR) depends on its pre-trained ability to distinguish sound from unsound knowledge, quantified by the Soundness-Aware Level (SAL) metric.


<details>
  <summary>Details</summary>
Motivation: To understand why performance after RLVR varies dramatically across different base models and identify what microscopic property of pre-trained models leads to this variation.

Method: Formalize reasoning as chains of Horn clauses built from features extracted via cross-layer sparse autoencoders, estimate transition probabilities between features, categorize rules by semantic soundness levels, and introduce SAL metric using Jensen-Shannon Divergence.

Result: High-potential models are soundness-aware with distinct probability distributions for different soundness levels, while weaker models are soundness-agnostic. SAL predicts post-RLVR reasoning performance with R^2=0.87 across diverse model families and scales.

Conclusion: A model's reasoning potential is tied to its intrinsic, pre-trained ability to distinguish sound knowledge from unsound ones, highlighting the critical role of model pre-training and providing a practical metric for selecting stronger base models.

Abstract: Reinforcement learning with verifiable rewards (RLVR) can elicit strong
reasoning in large language models (LLMs), while their performance after RLVR
varies dramatically across different base models. This raises a fundamental
question: what microscopic property of pre-trained models leads to this
variation? To investigate, we formalize reasoning as chains of Horn clauses
("if-then" rules) built from features extracted from the LLM's latent space via
cross-layer sparse autoencoders (SAEs). We estimate the transition
probabilities between its features, and further categorize each rule by its
semantic soundness level (e.g., strict, plausible, noisy) with an LLM. Our key
discovery is that high-potential models are inherently soundness-aware: their
internal probability distributions systematically shift across rules' soundness
levels, becoming highly distinct for "strict" versus "noisy" rules. In
contrast, weaker models are soundness-agnostic, collapsing to one distribution
regardless of soundness levels. To quantify this, we introduce the
Soundness-Aware Level (SAL), a microscopic metric using the Jensen-Shannon
Divergence to measure the separation between these distributions. We show that
SAL's predictions of post-RLVR reasoning performance follow a precise empirical
law (R^2=0.87) across diverse model families (Qwen, Mistral, Llama, DeepSeek)
and scales (0.5B-14B). This reveals that a model's reasoning potential is tied
to its intrinsic, pre-trained ability to distinguish sound knowledge from
unsound ones. These findings underscore the critical role of model pre-training
in shaping reasoning and offer a practical metric grounded in the model's
internal mechanisms for selecting/designing stronger base models.

</details>


### [157] [Reflections from Research Roundtables at the Conference on Health, Inference, and Learning (CHIL) 2025](https://arxiv.org/abs/2510.15217)
*Emily Alsentzer,Marie-Laure Charpignon,Bill Chen,Niharika D'Souza,Jason Fries,Yixing Jiang,Aparajita Kashyap,Chanwoo Kim,Simon Lee,Aishwarya Mandyam,Ashery Christopher Mbilinyi,Nikita Mehandru,Nitish Nagesh,Brighton Nuwagira,Emma Pierson,Arvind Pillai,Akane Sano,Tanveer Syeda-Mahmood,Shashank Yadav,Elias Adhanom,Muhammad Umar Afza,Amelia Archer,Suhana Bedi,Vasiliki Bikia,Trenton Chang,George H. Chen,Winston Chen,Erica Chiang,Edward Choi,Octavia Ciora,Paz Dozie-Nnamah,Shaza Elsharief,Matthew Engelhard,Ali Eshragh,Jean Feng,Josh Fessel,Scott Fleming,Kei Sen Fong,Thomas Frost,Soham Gadgil,Judy Gichoya,Leeor Hershkovich,Sujeong Im,Bhavya Jain,Vincent Jeanselme,Furong Jia,Qixuan,Jin,Yuxuan Jin,Daniel Kapash,Geetika Kapoor,Behdokht Kiafar,Matthias Kleiner,Stefan Kraft,Annika Kumar,Daeun Kyung,Zhongyuan Liang,Joanna Lin,Qianchu,Liu,Chang Liu,Hongzhou Luan,Chris Lunt,Leopoldo Julían Lechuga López,Matthew B. A. McDermott,Shahriar Noroozizadeh,Connor O'Brien,YongKyung Oh,Mixail Ota,Stephen Pfohl,Meagan Pi,Tanmoy Sarkar Pias,Emma Rocheteau,Avishaan Sethi,Toru Shirakawa,Anita Silver,Neha Simha,Kamile Stankeviciute,Max Sunog,Peter Szolovits,Shengpu Tang,Jialu Tang,Aaron Tierney,John Valdovinos,Byron Wallace,Will Ke Wang,Peter Washington,Jeremy Weiss,Daniel Wolfe,Emily Wong,Hye Sun Yun,Xiaoman Zhang,Xiao Yu Cindy Zhang,Hayoung Jeong,Kaveri A. Thakoor*

Main category: cs.LG

TL;DR: The CHIL 2025 conference featured Research Roundtables focusing on key ML-healthcare topics like explainability, fairness, causality, and foundation models.


<details>
  <summary>Details</summary>
Motivation: To foster collaborative dialogue and address critical challenges at the intersection of machine learning and healthcare through small-group discussions.

Method: Organized eight roundtables moderated by senior and junior chairs, emphasizing open exchange, intellectual curiosity, and inclusive engagement on specific topics.

Result: Successfully hosted discussions on eight key topics with 19 roundtable chairs facilitating rigorous exploration of challenges and opportunities in ML-healthcare.

Conclusion: The Research Roundtables provided an effective platform for collective ideation and actionable directions in health ML research through collaborative dialogue.

Abstract: The 6th Annual Conference on Health, Inference, and Learning (CHIL 2025),
hosted by the Association for Health Learning and Inference (AHLI), was held in
person on June 25-27, 2025, at the University of California, Berkeley, in
Berkeley, California, USA. As part of this year's program, we hosted Research
Roundtables to catalyze collaborative, small-group dialogue around critical,
timely topics at the intersection of machine learning and healthcare. Each
roundtable was moderated by a team of senior and junior chairs who fostered
open exchange, intellectual curiosity, and inclusive engagement. The sessions
emphasized rigorous discussion of key challenges, exploration of emerging
opportunities, and collective ideation toward actionable directions in the
field. In total, eight roundtables were held by 19 roundtable chairs on topics
of "Explainability, Interpretability, and Transparency," "Uncertainty, Bias,
and Fairness," "Causality," "Domain Adaptation," "Foundation Models," "Learning
from Small Medical Data," "Multimodal Methods," and "Scalable, Translational
Healthcare Solutions."

</details>


### [158] [Machine Learning for Early Detection of Meningitis: Stacked Ensemble Learning with EHR data](https://arxiv.org/abs/2510.15218)
*Han Ouyang,Jesse Hamilton,Saeed Amal*

Main category: cs.LG

TL;DR: Ensemble learning approach using Random Forest, LightGBM, and DNN models achieves high AUC scores (0.9637 and 0.9472) for meningitis diagnosis using MIMIC-III data, simulating real-world ER scenarios.


<details>
  <summary>Details</summary>
Motivation: To develop an AI-driven diagnostic approach for meningitis that can be clinically useful in real-world emergency room settings.

Method: Used MIMIC-III database with 214 meningitis and 46,303 non-meningitis patients. Applied data preprocessing, feature selection (gender, high-risk ICD codes), and ensemble learning with three base models (Random Forest, LightGBM, DNN) stacked with logistic regression meta-model.

Result: Excellent performance with AUC of 0.9637 on Testing Set 1 and 0.9472 on Testing Set 2, demonstrating high diagnostic accuracy for meningitis.

Conclusion: The ensemble learning approach shows promising results for meningitis diagnosis and paves the way for future AI-driven diagnostic tools in clinical settings, though direct deployment remains challenging.

Abstract: We utilized a cohort of 214 meningitis patients and 46,303 non-meningitis
patients from the MIMIC-III database. After extensive data preprocessing, which
included ICD-based cohort selection, one-hot encoding of coding, and a
two-stage feature selection process (for both the training set and the testing
sets), clinically relevant features such as gender and high-risk ICD codes
(including subarachnoid hemorrhage, secondary malignant neoplasm of the brain,
and generalized epilepsy) are selected. Overall, these clinically reasonable
and temporally adherent features provided excellent modeling performance. Three
models (Random Forest, LightGBM, and Deep Neural Networks (DNN) are trained as
base models for Ensemble Learning. Base model outputs are aggregated and
stacked into a meta model (Logistic Regression) that uses the base model
outputs as input values in training. Ultimately, soldier outputs (AUC of
Testing Set 1: 0.9637, AUC of Testing Set 2: 0.9472) are obtained through
ensemble learning.
  We created a challenging condition for diagnosing meningitis, simulating a
real-world ER (Emergency Room) scenario to enhance clinical use in real-world
applications. While directly deploying a diagnostic tool that clinicians can
use is challenging, this paper paves the way for a potential future AI-driven
diagnostic approach for meningitis using Ensemble Learning.

</details>


### [159] [Integrating Product Coefficients for Improved 3D LiDAR Data Classification (Part II)](https://arxiv.org/abs/2510.15219)
*Patricia Medina,Rasika Karkare*

Main category: cs.LG

TL;DR: This paper extends previous work by combining product coefficients with autoencoder representations and KNN classifiers, showing consistent performance improvements over PCA baselines and earlier frameworks.


<details>
  <summary>Details</summary>
Motivation: To enhance 3D LiDAR point-cloud classification by building on previous work with product coefficients, exploring their combination with autoencoder representations for better performance.

Method: Combined product coefficients with autoencoder representations and KNN classifiers, systematically adding product coefficients level by level to analyze their impact.

Result: Richer sets of product coefficients systematically improve class separability and overall accuracy, with the combination outperforming PCA-based baselines and previous frameworks.

Conclusion: Combining hierarchical product-coefficient features with autoencoders effectively pushes LiDAR classification performance further, demonstrating the value of this integrated approach.

Abstract: This work extends our previous study on enhancing 3D LiDAR point-cloud
classification with product coefficients
\cite{medina2025integratingproductcoefficientsimproved}, measure-theoretic
descriptors that complement the original spatial Lidar features. Here, we show
that combining product coefficients with an autoencoder representation and a
KNN classifier delivers consistent performance gains over both PCA-based
baselines and our earlier framework. We also investigate the effect of adding
product coefficients level by level, revealing a clear trend: richer sets of
coefficients systematically improve class separability and overall accuracy.
The results highlight the value of combining hierarchical product-coefficient
features with autoencoders to push LiDAR classification performance further.

</details>


### [160] [Stress-Aware Learning under KL Drift via Trust-Decayed Mirror Descent](https://arxiv.org/abs/2510.15222)
*Gabriel Nixon Raj*

Main category: cs.LG

TL;DR: The paper introduces entropy-regularized trust-decay for sequential decision-making under distribution drift, achieving O(√T) dynamic regret under KL-drift path length constraints and providing robustness guarantees against distribution shifts.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of sequential decision-making when the underlying data distribution changes over time (distribution drift), where traditional methods may fail to adapt and maintain performance guarantees.

Method: Proposes entropy-regularized trust-decay that injects stress-aware exponential tilting into both belief updates and mirror-descent decisions. Uses Fenchel-dual equivalence on the simplex, formalizes robustness via fragility measures, and develops parameter-free hedge adaptation.

Result: Achieves dynamic-regret guarantees of Õ(√T) under KL-drift path length ST = ∑t≥2√KL(Dt|Dt-1)/2, with O(1) per-switch regret. Provides high-probability sensitivity bounds and extensions to various settings including bandit feedback and distributed optimization.

Conclusion: The framework unifies dynamic-regret analysis, distributionally robust objectives, and KL-regularized control within a single stress-adaptive update, providing a comprehensive approach to handling distribution drift in sequential decision-making.

Abstract: We study sequential decision-making under distribution drift. We propose
entropy-regularized trust-decay, which injects stress-aware exponential tilting
into both belief updates and mirror-descent decisions. On the simplex, a
Fenchel-dual equivalence shows that belief tilt and decision tilt coincide. We
formalize robustness via fragility (worst-case excess risk in a KL ball),
belief bandwidth (radius sustaining a target excess), and a decision-space
Fragility Index (drift tolerated at $O(\sqrt{T})$ regret). We prove
high-probability sensitivity bounds and establish dynamic-regret guarantees of
$\tilde{O}(\sqrt{T})$ under KL-drift path length $S_T = \sum_{t\ge2}\sqrt{{\rm
KL}(D_t|D_{t-1})/2}$. In particular, trust-decay achieves $O(1)$ per-switch
regret, while stress-free updates incur $\Omega(1)$ tails. A parameter-free
hedge adapts the tilt to unknown drift, whereas persistent over-tilting yields
an $\Omega(\lambda^2 T)$ stationary penalty. We further obtain
calibrated-stress bounds and extensions to second-order updates, bandit
feedback, outliers, stress variation, distributed optimization, and plug-in
KL-drift estimation. The framework unifies dynamic-regret analysis,
distributionally robust objectives, and KL-regularized control within a single
stress-adaptive update.

</details>


### [161] [FinTrust: A Comprehensive Benchmark of Trustworthiness Evaluation in Finance Domain](https://arxiv.org/abs/2510.15232)
*Tiansheng Hu,Tongyan Hu,Liuyang Bai,Yilun Zhao,Arman Cohan,Chen Zhao*

Main category: cs.LG

TL;DR: FinTrust is a comprehensive benchmark for evaluating LLM trustworthiness in finance, testing 11 models across multiple dimensions including safety, fairness, and legal compliance.


<details>
  <summary>Details</summary>
Motivation: Applying LLMs in real-world finance is challenging due to high risks and stakes, requiring thorough trustworthiness evaluation.

Method: Developed FinTrust benchmark with fine-grained tasks across multiple trustworthiness dimensions based on practical finance contexts.

Result: Proprietary models like o4-mini excel in safety, while open-source models like DeepSeek-V3 perform better in industry-level fairness. All models struggle with fiduciary alignment and disclosure tasks.

Conclusion: FinTrust provides valuable evaluation framework for LLM trustworthiness in finance, revealing significant gaps in legal awareness that need addressing.

Abstract: Recent LLMs have demonstrated promising ability in solving finance related
problems. However, applying LLMs in real-world finance application remains
challenging due to its high risk and high stakes property. This paper
introduces FinTrust, a comprehensive benchmark specifically designed for
evaluating the trustworthiness of LLMs in finance applications. Our benchmark
focuses on a wide range of alignment issues based on practical context and
features fine-grained tasks for each dimension of trustworthiness evaluation.
We assess eleven LLMs on FinTrust and find that proprietary models like o4-mini
outperforms in most tasks such as safety while open-source models like
DeepSeek-V3 have advantage in specific areas like industry-level fairness. For
challenging task like fiduciary alignment and disclosure, all LLMs fall short,
showing a significant gap in legal awareness. We believe that FinTrust can be a
valuable benchmark for LLMs' trustworthiness evaluation in finance domain.

</details>


### [162] [Adaptive Individual Uncertainty under Out-Of-Distribution Shift with Expert-Routed Conformal Prediction](https://arxiv.org/abs/2510.15233)
*Amitesh Badkul,Lei Xie*

Main category: cs.LG

TL;DR: TESSERA is a novel uncertainty quantification method that provides reliable coverage guarantees, informative prediction intervals, and adaptive uncertainty estimates for protein-ligand affinity prediction under both i.i.d. and OOD settings.


<details>
  <summary>Details</summary>
Motivation: Current ML methods lack reliable uncertainty quantification, especially for risk-sensitive domains like drug discovery where protein-ligand affinity prediction faces challenges with heterogeneous assay noise, imbalanced chemical space, and distribution shifts.

Method: TESSERA combines Mixture of Expert (MoE) diversity with conformal calibration to provide per-sample uncertainty with coverage guarantees, adaptive prediction intervals that track absolute error, and reliable uncertainty estimation.

Result: TESSERA achieves near-nominal coverage and the best coverage-width trade-off (measured by CWC), maintains competitive adaptivity (lowest AUSE), and demonstrates right-sized intervals through Size-Stratified Coverage analysis.

Conclusion: By unifying MoE diversity with conformal calibration, TESSERA delivers trustworthy, tight, and adaptive uncertainties suitable for selective prediction and downstream decision-making in drug discovery and other applications.

Abstract: Reliable, informative, and individual uncertainty quantification (UQ) remains
missing in current ML community. This hinders the effective application of
AI/ML to risk-sensitive domains. Most methods either fail to provide coverage
on new data, inflate intervals so broadly that they are not actionable, or
assign uncertainties that do not track actual error, especially under a
distribution shift. In high-stakes drug discovery, protein-ligand affinity
(PLI) prediction is especially challenging as assay noise is heterogeneous,
chemical space is imbalanced and large, and practical evaluations routinely
involve distribution shift. In this work, we introduce a novel uncertainty
quantification method, Trustworthy Expert Split-conformal with Scaled
Estimation for Efficient Reliable Adaptive intervals (TESSERA), that provides
per-sample uncertainty with reliable coverage guarantee, informative and
adaptive prediction interval widths that track the absolute error. We evaluate
on protein-ligand binding affinity prediction under both independent and
identically distributed (i.i.d.) and scaffold-based out-of-distribution (OOD)
splits, comparing against strong UQ baselines. TESSERA attains near-nominal
coverage and the best coverage-width trade-off as measured by the
Coverage-Width Criterion (CWC), while maintaining competitive adaptivity
(lowest Area Under the Sparsification Error (AUSE)). Size-Stratified Coverage
(SSC) further confirms that intervals are right-sized, indicating width
increases when data are scarce or noisy, and remain tight when predictions are
reliable. By unifying Mixture of Expert (MoE) diversity with conformal
calibration, TESSERA delivers trustworthy, tight, and adaptive uncertainties
that are well-suited to selective prediction and downstream decision-making in
the drug-discovery pipeline and other applications.

</details>


### [163] [Dual-Weighted Reinforcement Learning for Generative Preference Modeling](https://arxiv.org/abs/2510.15242)
*Shengyu Feng,Yun He,Shuang Ma,Beibin Li,Yuanhao Xiong,Vincent Li,Karishma Mandyam,Julian Katz-Samuels,Shengjie Bi,Licheng Yu,Hejia Zhang,Karthik Abinav Sankararaman,Han Fang,Riham Mansour,Yiming Yang,Manaal Faruqui*

Main category: cs.LG

TL;DR: DWRL is a reinforcement learning framework that integrates chain-of-thought reasoning with preference modeling using dual-weighted objectives to handle non-verifiable tasks with human preference pairs.


<details>
  <summary>Details</summary>
Motivation: Extending RL from verifiable tasks to general non-verifiable preference tasks remains challenging and underexplored, requiring new approaches that can effectively model human preferences.

Method: Proposes Dual-Weighted RL (DWRL) that combines CoT reasoning with Bradley-Terry model via two weights: instance-wise misalignment weight for under-trained pairs and group-wise conditional preference score for promising thoughts. Trains generative preference models to generate thoughts and predict preference scores.

Result: DWRL consistently outperforms both GPM baselines and scalar models across multiple benchmarks and model scales (Llama3, Qwen2.5), producing coherent and interpretable thoughts.

Conclusion: DWRL serves as a general framework for reasoning-enhanced preference learning that extends beyond verifiable tasks to handle human preference modeling effectively.

Abstract: Reinforcement learning (RL) has recently proven effective at scaling
chain-of-thought (CoT) reasoning in large language models on tasks with
verifiable answers. However, extending RL to more general non-verifiable tasks,
typically in the format of human preference pairs, remains both challenging and
underexplored. In this work, we propose Dual-Weighted Reinforcement Learning
(DWRL), a new framework for preference modeling that integrates CoT reasoning
with the Bradley-Terry (BT) model via a dual-weighted RL objective that
preserves preference-modeling inductive bias. DWRL approximates the
maximum-likelihood objective of the BT model with two complementary weights: an
instance-wise misalignment weight, which emphasizes under-trained pairs
misaligned with human preference, and a group-wise (self-normalized)
conditional preference score, which promotes promising thoughts. In this paper,
we apply DWRL to preference modeling by training generative preference models
(GPMs) to first generate a thought and then predict the human preference score.
Across multiple benchmarks and model scales (Llama3 and Qwen2.5), DWRL
consistently outperforms both GPM baselines and scalar models, while producing
coherent, interpretable thoughts. In summary, our results position DWRL as a
general framework for reasoning-enhanced preference learning beyond verifiable
tasks.

</details>


### [164] [Spatiotemporal Transformers for Predicting Avian Disease Risk from Migration Trajectories](https://arxiv.org/abs/2510.15254)
*Dingya Feng,Dingyuan Xue*

Main category: cs.LG

TL;DR: Transformer-based framework for predicting disease risk at migratory bird trajectory endpoints using GPS tracking, outbreak records, and geospatial data, achieving high accuracy (0.9821) and AUC (0.9803).


<details>
  <summary>Details</summary>
Motivation: Accurate forecasting of avian disease outbreaks is critical for wildlife conservation and public health, requiring early-warning systems for timely intervention.

Method: Integrates multi-source datasets (GPS tracking from Movebank, WOAH outbreak records, geospatial context) with H3 hierarchical geospatial encoding and Transformer architecture to learn spatiotemporal dependencies from bird movement sequences.

Result: Strong predictive performance on test set: accuracy 0.9821, AUC 0.9803, average precision 0.9299, F1-score 0.8836 at optimal threshold.

Conclusion: Transformer architectures show potential for supporting early-warning systems in avian disease surveillance, enabling timely prevention strategies.

Abstract: Accurate forecasting of avian disease outbreaks is critical for wildlife
conservation and public health. This study presents a Transformer-based
framework for predicting the disease risk at the terminal locations of
migratory bird trajectories. We integrate multi-source datasets, including GPS
tracking data from Movebank, outbreak records from the World Organisation for
Animal Health (WOAH), and geospatial context from GADM and Natural Earth. The
raw coordinates are processed using H3 hierarchical geospatial encoding to
capture spatial patterns. The model learns spatiotemporal dependencies from
bird movement sequences to estimate endpoint disease risk. Evaluation on a
held-out test set demonstrates strong predictive performance, achieving an
accuracy of 0.9821, area under the ROC curve (AUC) of 0.9803, average precision
(AP) of 0.9299, and an F1-score of 0.8836 at the optimal threshold. These
results highlight the potential of Transformer architectures to support
early-warning systems for avian disease surveillance, enabling timely
intervention and prevention strategies.

</details>


### [165] [DRO-InstructZero: Distributionally Robust Prompt Optimization for Large Language Models](https://arxiv.org/abs/2510.15260)
*Yangyang Li*

Main category: cs.LG

TL;DR: DRO-InstructZero introduces robust Bayesian optimization for zero-shot prompt optimization, using f-divergence balls to handle distribution shifts and improve reliability under adversarial conditions.


<details>
  <summary>Details</summary>
Motivation: Existing prompt search methods like InstructZero degrade under distribution shift and adversarial evaluation because they optimize for single evaluation distributions, leading to poor transferability.

Method: Formulates zero-shot prompt optimization as robust Bayesian optimization with f-divergence ambiguity sets around evaluation distributions, using a robust acquisition rule to maximize worst-case expected utility while maintaining query efficiency.

Result: Significant improvements across tasks: BIG-Bench formal rewriting accuracy increased from 61.3% to 85-90% (+25-30 points), auto-debugging gained +25 points under domain shift, while stable tasks maintained >96% accuracy with no in-distribution performance loss.

Conclusion: DRO-InstructZero successfully connects distributionally robust optimization with prompt learning, providing a plug-and-play approach for reliable and transferable prompt alignment under real-world uncertainty.

Abstract: Large language models are highly sensitive to prompt wording. However,
popular automatic prompt search methods, including InstructZero, often degrade
under distribution shift and adversarial evaluation because they optimize
expected performance under a single evaluation distribution. Consequently,
prompts that work in one setting frequently fail to transfer. To address this,
DRO-InstructZero formulates zero-shot prompt optimization as robust Bayesian
optimization. Specifically, an f-divergence ball defines an ambiguity set
around the evaluation distribution, and a robust acquisition rule maximizes
worst-case expected utility while retaining the query efficiency of Bayesian
search. Therefore, the search explicitly targets reliability under distribution
shift rather than average behavior alone. Experiments follow the
instruction-induction protocol with matched query budgets across formality
rewriting, code debugging, and translation. For example, on BIG-Bench
informative-to-formal rewriting, accuracy improves from 61.3 +/- 0.7% to
approximately 85-90%, yielding an absolute gain of about 25-30 points.
Moreover, auto-debugging shows about +25-point gains under domain shift.
Meanwhile, stable tasks such as cause-and-effect remain above 96%, indicating
no loss on in-distribution cases. Furthermore, improvements are consistent
across divergence choices and decoding temperatures. Overall, DRO-InstructZero
connects distributionally robust optimization with prompt learning, offering a
plug-and-play and general approach for reliable, transferable prompt alignment
under real-world uncertainty.

</details>


### [166] [Robust Layerwise Scaling Rules by Proper Weight Decay Tuning](https://arxiv.org/abs/2510.15262)
*Zhiyuan Fan,Yifeng Liu,Qingyue Zhao,Angela Yuan,Quanquan Gu*

Main category: cs.LG

TL;DR: The paper introduces a weight-decay scaling rule for AdamW that enables zero-shot transfer of learning rates and weight decay across different model widths, extending μP beyond early training phases.


<details>
  <summary>Details</summary>
Motivation: μP enables learning-rate transfer across widths but degrades in modern scale-invariant architectures due to optimizer-governed steady states where normalization layers create backward scale sensitivity, making effective learning rate width-dependent.

Method: Proposes weight-decay scaling rule λ₂ ∝ √d for matrix-like parameters based on empirical observation that top singular values scale as √(η/λ)·d^0.75, combined with μP learning-rate rule η₂ ∝ d^{-1}. Vector-like parameters use η₁=Θ_d(1) and λ₁=0.

Result: Validated on LLaMA-style Transformers and synthetic settings, achieving zero-shot transfer of both learning rate and weight decay from proxy to target widths, eliminating per-width hyperparameter sweeps.

Conclusion: Extends μP beyond near-init regime by controlling steady-state scales set by optimizer, providing practical recipe for width-robust hyperparameter transfer under AdamW with simple diagnostic using singular value matching.

Abstract: Empirical scaling laws prescribe how to allocate parameters, data, and
compute, while maximal-update parameterization ($\mu$P) enables learning-rate
transfer across widths by equalizing early-time update magnitudes. However, in
modern scale-invariant architectures, training quickly enters an
optimizer-governed steady state where normalization layers create backward
scale sensitivity and the effective learning rate becomes width dependent,
degrading $\mu$P transfer. We address this by introducing a weight-decay
scaling rule for AdamW that preserves sublayer gain across widths. Empirically,
the singular-value spectrum of each matrix parameter scales in norm as
$\sqrt{\eta/\lambda}$ with an approximately invariant shape; under width
scaling $d$, we observe that the top singular value scales approximately as
$\sqrt{\eta/\lambda}\cdot d^{0.75}$. Combining this observation with the $\mu$P
learning-rate rule $\eta_2\propto d^{-1}$ for matrix-like parameters implies an
empirical weight-decay scaling rule $\lambda_2\propto \sqrt{d}$ that
approximately keeps sublayer gains width invariant. Together with vector-like
parameters trained at $\eta_1=\Theta_d(1)$ and $\lambda_1=0$, this yields
\emph{zero-shot} transfer of both learning rate and weight decay from proxy to
target widths, removing per-width sweeps. We validate the rule on LLaMA-style
Transformers and in a minimal synthetic setting, and we provide a simple
diagnostic, matching top singular values, to check sublayer-gain invariance.
Our results extend $\mu$P beyond the near-init regime by explicitly controlling
steady-state scales set by the optimizer, offering a practical recipe for
width-robust hyperparameter transfer under AdamW.

</details>


### [167] [Causal Time Series Modeling of Supraglacial Lake Evolution in Greenland under Distribution Shift](https://arxiv.org/abs/2510.15265)
*Emam Hossain,Muhammad Hasan Ferdous,Devon Dunmire,Aneesh Subramanian,Md Osman Gani*

Main category: cs.LG

TL;DR: RIC-TSC is a causal time-series classification framework that embeds lag-aware causal discovery into sequence modeling for supraglacial lake evolution prediction, achieving 12.59% higher accuracy than correlation-based methods under distribution shifts.


<details>
  <summary>Details</summary>
Motivation: Current spatiotemporal Earth observation models rely on correlational features that fail to transfer across heterogeneous domains, while causal modeling offers principled foundation for uncovering stable, invariant relationships that improve robustness and generalization.

Method: Proposed RIC-TSC framework uses Joint PCMCI+ for causal discovery on multi-modal satellite data (Sentinel-1, Sentinel-2, Landsat-8, CARRA meteorological variables) to identify region-specific and invariant predictors of supraglacial lake evolution, with causal graphs estimated globally and per basin.

Result: On benchmark of 1000 manually labeled lakes from two contrasting melt seasons, causal models achieved up to 12.59% higher accuracy than correlation-based baselines under out-of-distribution evaluation.

Conclusion: Causal discovery serves not only as feature selection but as pathway to generalizable and mechanistically grounded models of dynamic Earth surface processes.

Abstract: Causal modeling offers a principled foundation for uncovering stable,
invariant relationships in time-series data, thereby improving robustness and
generalization under distribution shifts. Yet its potential is underutilized in
spatiotemporal Earth observation, where models often depend on purely
correlational features that fail to transfer across heterogeneous domains. We
propose RIC-TSC, a regionally-informed causal time-series classification
framework that embeds lag-aware causal discovery directly into sequence
modeling, enabling both predictive accuracy and scientific interpretability.
Using multi-modal satellite and reanalysis data-including Sentinel-1 microwave
backscatter, Sentinel-2 and Landsat-8 optical reflectance, and CARRA
meteorological variables-we leverage Joint PCMCI+ (J-PCMCI+) to identify
region-specific and invariant predictors of supraglacial lake evolution in
Greenland. Causal graphs are estimated globally and per basin, with validated
predictors and their time lags supplied to lightweight classifiers. On a
balanced benchmark of 1000 manually labeled lakes from two contrasting melt
seasons (2018-2019), causal models achieve up to 12.59% higher accuracy than
correlation-based baselines under out-of-distribution evaluation. These results
show that causal discovery is not only a means of feature selection but also a
pathway to generalizable and mechanistically grounded models of dynamic Earth
surface processes.

</details>


### [168] [Semi-Supervised Regression with Heteroscedastic Pseudo-Labels](https://arxiv.org/abs/2510.15266)
*Xueqing Sun,Renzhen Wang,Quanziang Wang,Yichen Wu,Xixi Jia,Deyu Meng*

Main category: cs.LG

TL;DR: Proposes an uncertainty-aware pseudo-labeling framework for semi-supervised regression that dynamically adjusts pseudo-label influence through bi-level optimization to handle continuous outputs with heteroscedastic noise.


<details>
  <summary>Details</summary>
Motivation: Pseudo-labeling is under-explored in semi-supervised regression due to continuous outputs with heteroscedastic noise, making reliability assessment challenging and leading to error accumulation and overfitting.

Method: Uncertainty-aware pseudo-labeling framework using bi-level optimization that jointly minimizes empirical risk over all data and optimizes uncertainty estimates to enhance generalization on labeled data.

Result: Extensive experiments on benchmark SSR datasets demonstrate superior robustness and performance compared to existing methods.

Conclusion: The proposed framework effectively mitigates the impact of unreliable pseudo-labels in semi-supervised regression through uncertainty-aware dynamic adjustment.

Abstract: Pseudo-labeling is a commonly used paradigm in semi-supervised learning, yet
its application to semi-supervised regression (SSR) remains relatively
under-explored. Unlike classification, where pseudo-labels are discrete and
confidence-based filtering is effective, SSR involves continuous outputs with
heteroscedastic noise, making it challenging to assess pseudo-label
reliability. As a result, naive pseudo-labeling can lead to error accumulation
and overfitting to incorrect labels. To address this, we propose an
uncertainty-aware pseudo-labeling framework that dynamically adjusts
pseudo-label influence from a bi-level optimization perspective. By jointly
minimizing empirical risk over all data and optimizing uncertainty estimates to
enhance generalization on labeled data, our method effectively mitigates the
impact of unreliable pseudo-labels. We provide theoretical insights and
extensive experiments to validate our approach across various benchmark SSR
datasets, and the results demonstrate superior robustness and performance
compared to existing methods. Our code is available at
https://github.com/sxq/Heteroscedastic-Pseudo-Labels.

</details>


### [169] [Foundation Models for Scientific Discovery: From Paradigm Enhancement to Paradigm Transition](https://arxiv.org/abs/2510.15280)
*Fan Liu,Jindong Han,Tengfei Lyu,Weijia Zhang,Zhe-Rui Yang,Lu Dai,Cancheng Liu,Hao Liu*

Main category: cs.LG

TL;DR: Foundation models are transforming scientific research through a three-stage evolution: enhancing existing workflows, enabling human-AI collaboration, and progressing toward autonomous scientific discovery.


<details>
  <summary>Details</summary>
Motivation: To understand whether foundation models are merely enhancing existing scientific methodologies or fundamentally redefining how science is conducted, and to provide a framework for this transformation.

Method: Proposes a three-stage framework: (1) Meta-Scientific Integration (enhancing traditional workflows), (2) Hybrid Human-AI Co-Creation (FMs as active collaborators), and (3) Autonomous Scientific Discovery (FMs as independent agents).

Result: Identifies current applications and emerging capabilities of foundation models across scientific paradigms, and outlines risks and future directions for FM-enabled scientific discovery.

Conclusion: Foundation models are catalyzing a transition toward a new scientific paradigm, requiring the scientific community to understand their transformative role and reflect on the future of scientific discovery.

Abstract: Foundation models (FMs), such as GPT-4 and AlphaFold, are reshaping the
landscape of scientific research. Beyond accelerating tasks such as hypothesis
generation, experimental design, and result interpretation, they prompt a more
fundamental question: Are FMs merely enhancing existing scientific
methodologies, or are they redefining the way science is conducted? In this
paper, we argue that FMs are catalyzing a transition toward a new scientific
paradigm. We introduce a three-stage framework to describe this evolution: (1)
Meta-Scientific Integration, where FMs enhance workflows within traditional
paradigms; (2) Hybrid Human-AI Co-Creation, where FMs become active
collaborators in problem formulation, reasoning, and discovery; and (3)
Autonomous Scientific Discovery, where FMs operate as independent agents
capable of generating new scientific knowledge with minimal human intervention.
Through this lens, we review current applications and emerging capabilities of
FMs across existing scientific paradigms. We further identify risks and future
directions for FM-enabled scientific discovery. This position paper aims to
support the scientific community in understanding the transformative role of
FMs and to foster reflection on the future of scientific discovery. Our project
is available at
https://github.com/usail-hkust/Awesome-Foundation-Models-for-Scientific-Discovery.

</details>


### [170] [Small Ensemble-based Data Assimilation: A Machine Learning-Enhanced Data Assimilation Method with Limited Ensemble Size](https://arxiv.org/abs/2510.15284)
*Zhilin Li,Yao Zhou,Xianglong Li,Zeng Liu,Zhaokuan Lu,Shanlin Xu,Seungnam Kim,Guangyao Wang*

Main category: cs.LG

TL;DR: A machine learning-enhanced ensemble Kalman filter method that uses neural networks to correct suboptimal analysis from small ensembles, improving accuracy without significant computational overhead.


<details>
  <summary>Details</summary>
Motivation: To address the trade-off between accuracy and computational efficiency in ensemble-based data assimilation methods, where larger ensembles improve accuracy but increase computational cost.

Method: Combine traditional ensemble Kalman filter (EnKF) with fully connected neural network (FCNN), using small ensemble for preliminary analysis and FCNN to learn correction terms for improved accuracy.

Result: The EnKF-FCNN method achieves higher accuracy than traditional EnKF with same ensemble size in Lorenz systems and ocean wave simulations, with negligible additional computational cost.

Conclusion: The proposed method successfully mitigates performance degradation from limited ensemble sizes and is adaptable to diverse applications through coupling with different models and ensemble-based DA methods.

Abstract: Ensemble-based data assimilation (DA) methods have become increasingly
popular due to their inherent ability to address nonlinear dynamic problems.
However, these methods often face a trade-off between analysis accuracy and
computational efficiency, as larger ensemble sizes required for higher accuracy
also lead to greater computational cost. In this study, we propose a novel
machine learning-based data assimilation approach that combines the traditional
ensemble Kalman filter (EnKF) with a fully connected neural network (FCNN).
Specifically, our method uses a relatively small ensemble size to generate
preliminary yet suboptimal analysis states via EnKF. A FCNN is then employed to
learn and predict correction terms for these states, thereby mitigating the
performance degradation induced by the limited ensemble size. We evaluate the
performance of our proposed EnKF-FCNN method through numerical experiments
involving Lorenz systems and nonlinear ocean wave field simulations. The
results consistently demonstrate that the new method achieves higher accuracy
than traditional EnKF with the same ensemble size, while incurring negligible
additional computational cost. Moreover, the EnKF-FCNN method is adaptable to
diverse applications through coupling with different models and the use of
alternative ensemble-based DA methods.

</details>


### [171] [Identifying internal patterns in (1+1)-dimensional directed percolation using neural networks](https://arxiv.org/abs/2510.15294)
*Danil Parkhomenko,Pavel Ovchinnikov,Konstantin Soldatov,Vitalii Kapitan,Gennady Y. Chitov*

Main category: cs.LG

TL;DR: Neural network method for detecting phase transitions and classifying hidden percolation patterns in (1+1)-dimensional replication processes using CNN, TCN, and GRU networks trained on raw configurations.


<details>
  <summary>Details</summary>
Motivation: To automatically detect phase transitions and classify hidden percolation patterns without manual feature extraction, demonstrating deep architectures' capability to extract hierarchical structures from raw numerical data.

Method: Combination of CNN, TCN and GRU networks trained directly on raw configurations without manual feature extraction.

Result: The network successfully reproduces the phase diagram and assigns phase labels to configurations.

Conclusion: Deep architectures are capable of extracting hierarchical structures from raw data of numerical experiments.

Abstract: In this paper we present a neural network-based method for the automatic
detection of phase transitions and classification of hidden percolation
patterns in a (1+1)-dimensional replication process. The proposed network model
is based on the combination of CNN, TCN and GRU networks, which are trained
directly on raw configurations without any manual feature extraction. The
network reproduces the phase diagram and assigns phase labels to
configurations. It shows that deep architectures are capable of extracting
hierarchical structures from the raw data of numerical experiments.

</details>


### [172] [DFCA: Decentralized Federated Clustering Algorithm](https://arxiv.org/abs/2510.15300)
*Jonas Kirch,Sebastian Becker,Tiago Koketsu Rodrigues,Stefan Harmeling*

Main category: cs.LG

TL;DR: DFCA is a fully decentralized clustered federated learning algorithm that eliminates the need for a central server, enabling clients to collaboratively train cluster-specific models through sequential running average aggregation from neighbors.


<details>
  <summary>Details</summary>
Motivation: Existing clustered FL methods like IFCA rely on central servers, creating bottlenecks and single points of failure, limiting their applicability in realistic decentralized settings.

Method: DFCA uses sequential running average to aggregate models from neighbors as updates arrive, providing communication-efficient alternative to batch aggregation while maintaining clustering performance in a fully decentralized setting.

Result: Experiments show DFCA outperforms other decentralized algorithms and performs comparably to centralized IFCA, even under sparse connectivity conditions.

Conclusion: DFCA demonstrates robustness and practicality for dynamic real-world decentralized networks, offering a viable decentralized alternative to centralized clustered FL approaches.

Abstract: Clustered Federated Learning has emerged as an effective approach for
handling heterogeneous data across clients by partitioning them into clusters
with similar or identical data distributions. However, most existing methods,
including the Iterative Federated Clustering Algorithm (IFCA), rely on a
central server to coordinate model updates, which creates a bottleneck and a
single point of failure, limiting their applicability in more realistic
decentralized learning settings. In this work, we introduce DFCA, a fully
decentralized clustered FL algorithm that enables clients to collaboratively
train cluster-specific models without central coordination. DFCA uses a
sequential running average to aggregate models from neighbors as updates
arrive, providing a communication-efficient alternative to batch aggregation
while maintaining clustering performance. Our experiments on various datasets
demonstrate that DFCA outperforms other decentralized algorithms and performs
comparably to centralized IFCA, even under sparse connectivity, highlighting
its robustness and practicality for dynamic real-world decentralized networks.

</details>


### [173] [On the Generalization Properties of Learning the Random Feature Models with Learnable Activation Functions](https://arxiv.org/abs/2510.15327)
*Zailin Ma,Jiansheng Yang,Yaodong Yang*

Main category: cs.LG

TL;DR: This paper provides sharp generalization bounds for Random Feature models with Learnable Activation Functions (RFLAF), showing that data-dependent leverage weighted sampling significantly reduces the required number of features compared to plain sampling.


<details>
  <summary>Details</summary>
Motivation: To improve the generalization properties of kernel methods by developing sharper bounds on the required number of features for RFLAF models in both regression and classification tasks.

Method: Applied data-dependent sampling schemes for feature generation, including plain sampling and leverage weighted sampling. Proposed an algorithm to find approximate kernels and apply leverage weighted sampling for weighted RFLAF.

Result: Leverage weighted sampling improved bounds from Ω(1/ε²) to Ω̃((1/ε)^{1/t}) for MSE loss (and Ω(1) for finite-rank Gram matrices), and from Ω(1/ε²) to Ω̃((1/ε²)^{1/t}) for Lipschitz loss. Empirical results showed weighted RFLAF achieves same performance with significantly fewer features.

Conclusion: Data-dependent leverage weighted sampling provides substantial improvements in feature efficiency for RFLAF models, with theoretical bounds and empirical evidence supporting its effectiveness.

Abstract: This paper studies the generalization properties of a recently proposed
kernel method, the Random Feature models with Learnable Activation Functions
(RFLAF). By applying a data-dependent sampling scheme for generating features,
we provide by far the sharpest bounds on the required number of features for
learning RFLAF in both the regression and classification tasks. We provide a
unified theorem that describes the complexity of the feature number $s$, and
discuss the results for the plain sampling scheme and the data-dependent
leverage weighted scheme. Through weighted sampling, the bound on $s$ in the
MSE loss case is improved from $\Omega(1/\epsilon^2)$ to
$\tilde{\Omega}((1/\epsilon)^{1/t})$ in general $(t\geq 1)$, and even to
$\Omega(1)$ when the Gram matrix has a finite rank. For the Lipschitz loss
case, the bound is improved from $\Omega(1/\epsilon^2)$ to
$\tilde{\Omega}((1/\epsilon^2)^{1/t})$. To learn the weighted RFLAF, we also
propose an algorithm to find an approximate kernel and then apply the leverage
weighted sampling. Empirical results show that the weighted RFLAF achieves the
same performances with a significantly fewer number of features compared to the
plainly sampled RFLAF, validating our theories and the effectiveness of this
method.

</details>


### [174] [Backdoor or Manipulation? Graph Mixture of Experts Can Defend Against Various Graph Adversarial Attacks](https://arxiv.org/abs/2510.15333)
*Yuyuan Feng,Bin Ma,Enyan Dai*

Main category: cs.LG

TL;DR: A unified defense framework using Mixture of Experts (MoE) architecture to protect graph neural networks against multiple adversarial attacks including backdoor, edge manipulation, and node injection attacks.


<details>
  <summary>Details</summary>
Motivation: Existing GNN defenses focus on single attack types, lacking a unified approach to handle multiple threats simultaneously. The need for a scalable framework that can defend against various adversarial attacks on graphs.

Method: Uses Mixture of Experts architecture with MI-based logic diversity loss to encourage experts to focus on distinct neighborhood structures, and a robustness-aware router that identifies perturbation patterns and routes perturbed nodes to robust experts.

Result: Extensive experiments show the method consistently achieves superior robustness against multiple graph adversarial attacks in various adversarial settings.

Conclusion: The proposed MoE-based framework provides an effective unified defense solution that scales well and maintains robustness across different types of graph adversarial attacks.

Abstract: Extensive research has highlighted the vulnerability of graph neural networks
(GNNs) to adversarial attacks, including manipulation, node injection, and the
recently emerging threat of backdoor attacks. However, existing defenses
typically focus on a single type of attack, lacking a unified approach to
simultaneously defend against multiple threats. In this work, we leverage the
flexibility of the Mixture of Experts (MoE) architecture to design a scalable
and unified framework for defending against backdoor, edge manipulation, and
node injection attacks. Specifically, we propose an MI-based logic diversity
loss to encourage individual experts to focus on distinct neighborhood
structures in their decision processes, thus ensuring a sufficient subset of
experts remains unaffected under perturbations in local structures. Moreover,
we introduce a robustness-aware router that identifies perturbation patterns
and adaptively routes perturbed nodes to corresponding robust experts.
Extensive experiments conducted under various adversarial settings demonstrate
that our method consistently achieves superior robustness against multiple
graph adversarial attacks.

</details>


### [175] [Sequence Modeling with Spectral Mean Flows](https://arxiv.org/abs/2510.15366)
*Jinwoo Kim,Max Beier,Petar Bevanda,Nayun Kim,Seunghoon Hong*

Main category: cs.LG

TL;DR: A new sequence modeling approach using operator theory and spectral decomposition to represent sequence distributions as tensors in Hilbert spaces, enabling tractable learning and sampling through spectral mean flows.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of representing and learning highly nonlinear probabilistic state dynamics in sequence modeling, leveraging operator theory's perspective of dynamics as linear maps on Hilbert spaces.

Method: Proposes spectral mean flows: (1) uses spectral decomposition of linear operators for scalable tensor network decomposition of sequence mean embeddings, (2) extends MMD gradient flows to time-dependent Hilbert spaces and connects to flow matching via continuity equation for simulation-free learning.

Result: Demonstrates competitive performance on various time-series modeling datasets with improved scalability and faster sampling convergence.

Conclusion: The operator-theoretic approach combined with spectral decomposition provides an effective framework for sequence modeling that overcomes challenges with large tensors and slow sampling while maintaining competitive performance.

Abstract: A key question in sequence modeling with neural networks is how to represent
and learn highly nonlinear and probabilistic state dynamics. Operator theory
views such dynamics as linear maps on Hilbert spaces containing mean embedding
vectors of distributions, offering an appealing but currently overlooked
perspective. We propose a new approach to sequence modeling based on an
operator-theoretic view of a hidden Markov model (HMM). Instead of
materializing stochastic recurrence, we embed the full sequence distribution as
a tensor in the product Hilbert space. A generative process is then defined as
maximum mean discrepancy (MMD) gradient flow in the space of sequences. To
overcome challenges with large tensors and slow sampling convergence, we
introduce spectral mean flows, a novel tractable algorithm integrating two core
concepts. First, we propose a new neural architecture by leveraging spectral
decomposition of linear operators to derive a scalable tensor network
decomposition of sequence mean embeddings. Second, we extend MMD gradient flows
to time-dependent Hilbert spaces and connect them to flow matching via the
continuity equation, enabling simulation-free learning and faster sampling. We
demonstrate competitive results on a range of time-series modeling datasets.
Code is available at https://github.com/jw9730/spectral-mean-flow.

</details>


### [176] [Towards Robust Zero-Shot Reinforcement Learning](https://arxiv.org/abs/2510.15382)
*Kexin Zheng,Lauriane Teyssier,Yinan Zheng,Yu Luo,Xiayuan Zhan*

Main category: cs.LG

TL;DR: BREEZE is an improved zero-shot RL framework that enhances stability, policy extraction, and representation quality through behavioral regularization, diffusion-based policy modeling, and attention architectures.


<details>
  <summary>Details</summary>
Motivation: Existing zero-shot RL methods like Forward-Backward representations suffer from limited expressivity and OOD extrapolation errors during offline learning, leading to biased representations and suboptimal performance.

Method: BREEZE introduces behavioral regularization for stable in-sample policy learning, uses task-conditioned diffusion models for multimodal policy extraction, and employs expressive attention-based architectures for representation modeling.

Result: Extensive experiments on ExORL and D4RL Kitchen show BREEZE achieves best or near-best performance with superior robustness compared to prior offline zero-shot RL methods.

Conclusion: BREEZE successfully addresses key limitations in zero-shot RL by enhancing learning stability, policy extraction capability, and representation quality through its integrated framework.

Abstract: The recent development of zero-shot reinforcement learning (RL) has opened a
new avenue for learning pre-trained generalist policies that can adapt to
arbitrary new tasks in a zero-shot manner. While the popular Forward-Backward
representations (FB) and related methods have shown promise in zero-shot RL, we
empirically found that their modeling lacks expressivity and that extrapolation
errors caused by out-of-distribution (OOD) actions during offline learning
sometimes lead to biased representations, ultimately resulting in suboptimal
performance. To address these issues, we propose Behavior-REgularizEd Zero-shot
RL with Expressivity enhancement (BREEZE), an upgraded FB-based framework that
simultaneously enhances learning stability, policy extraction capability, and
representation learning quality. BREEZE introduces behavioral regularization in
zero-shot RL policy learning, transforming policy optimization into a stable
in-sample learning paradigm. Additionally, BREEZE extracts the policy using a
task-conditioned diffusion model, enabling the generation of high-quality and
multimodal action distributions in zero-shot RL settings. Moreover, BREEZE
employs expressive attention-based architectures for representation modeling to
capture the complex relationships between environmental dynamics. Extensive
experiments on ExORL and D4RL Kitchen demonstrate that BREEZE achieves the best
or near-the-best performance while exhibiting superior robustness compared to
prior offline zero-shot RL methods. The official implementation is available
at: https://github.com/Whiterrrrr/BREEZE.

</details>


### [177] [Iterative Refinement of Flow Policies in Probability Space for Online Reinforcement Learning](https://arxiv.org/abs/2510.15388)
*Mingyang Sun,Pengxiang Ding,Weinan Zhang,Donglin Wang*

Main category: cs.LG

TL;DR: SWFP is a framework that discretizes flow matching inference to align with optimal transport principles, enabling stable fine-tuning of pre-trained flow policies through small incremental transformations.


<details>
  <summary>Details</summary>
Motivation: Behavior cloning with flow policies is vulnerable to distributional shift, and standard RL methods struggle to fine-tune these models due to iterative inference processes and existing limitations.

Method: Decomposes global flow into sequence of small transformations between proximate distributions using fixed-step Euler scheme, with each step corresponding to a JKO update that regularizes policy changes.

Result: SWFP achieves enhanced stability, efficiency, and superior adaptation performance across diverse robotic control benchmarks with simpler training, reduced computational costs, and provable stability.

Conclusion: The stepwise decomposition approach provides an effective framework for fine-tuning flow policies with theoretical guarantees and practical advantages over existing methods.

Abstract: While behavior cloning with flow/diffusion policies excels at learning
complex skills from demonstrations, it remains vulnerable to distributional
shift, and standard RL methods struggle to fine-tune these models due to their
iterative inference process and the limitations of existing workarounds. In
this work, we introduce the Stepwise Flow Policy (SWFP) framework, founded on
the key insight that discretizing the flow matching inference process via a
fixed-step Euler scheme inherently aligns it with the variational
Jordan-Kinderlehrer-Otto (JKO) principle from optimal transport. SWFP
decomposes the global flow into a sequence of small, incremental
transformations between proximate distributions. Each step corresponds to a JKO
update, regularizing policy changes to stay near the previous iterate and
ensuring stable online adaptation with entropic regularization. This
decomposition yields an efficient algorithm that fine-tunes pre-trained flows
via a cascade of small flow blocks, offering significant advantages:
simpler/faster training of sub-models, reduced computational/memory costs, and
provable stability grounded in Wasserstein trust regions. Comprehensive
experiments demonstrate SWFP's enhanced stability, efficiency, and superior
adaptation performance across diverse robotic control benchmarks.

</details>


### [178] [Geometric Mixture Models for Electrolyte Conductivity Prediction](https://arxiv.org/abs/2510.15403)
*Anyi Li,Jiacheng Cen,Songyou Li,Mingze Li,Yang Yu,Wenbing Huang*

Main category: cs.LG

TL;DR: GeoMix is a geometry-aware framework for predicting ionic conductivity in electrolyte systems that addresses challenges in standardized benchmarks and geometric structure modeling through equivariant message passing.


<details>
  <summary>Details</summary>
Motivation: Current research faces two fundamental challenges: lack of high-quality standardized benchmarks and inadequate modeling of geometric structure and intermolecular interactions in mixture systems.

Method: Reorganized CALiSol and DiffMix datasets with geometric graph representations, then proposed GeoMix framework with Geometric Interaction Network (GIN) for equivariant intermolecular geometric message passing that preserves Set-SE(3) equivariance.

Result: GeoMix consistently outperforms diverse baselines (MLPs, GNNs, and geometric GNNs) across both datasets, validating the importance of cross-molecular geometric interactions and equivariant message passing.

Conclusion: This work establishes new benchmarks for electrolyte research and provides a general geometric learning framework that advances modeling of mixture systems in energy materials, pharmaceutical development, and beyond.

Abstract: Accurate prediction of ionic conductivity in electrolyte systems is crucial
for advancing numerous scientific and technological applications. While
significant progress has been made, current research faces two fundamental
challenges: (1) the lack of high-quality standardized benchmarks, and (2)
inadequate modeling of geometric structure and intermolecular interactions in
mixture systems. To address these limitations, we first reorganize and enhance
the CALiSol and DiffMix electrolyte datasets by incorporating geometric graph
representations of molecules. We then propose GeoMix, a novel geometry-aware
framework that preserves Set-SE(3) equivariance-an essential but challenging
property for mixture systems. At the heart of GeoMix lies the Geometric
Interaction Network (GIN), an equivariant module specifically designed for
intermolecular geometric message passing. Comprehensive experiments demonstrate
that GeoMix consistently outperforms diverse baselines (including MLPs, GNNs,
and geometric GNNs) across both datasets, validating the importance of
cross-molecular geometric interactions and equivariant message passing for
accurate property prediction. This work not only establishes new benchmarks for
electrolyte research but also provides a general geometric learning framework
that advances modeling of mixture systems in energy materials, pharmaceutical
development, and beyond.

</details>


### [179] [Online Kernel Dynamic Mode Decomposition for Streaming Time Series Forecasting with Adaptive Windowing](https://arxiv.org/abs/2510.15404)
*Christopher Salazar,Krithika Manohar,Ashis G. Banerjee*

Main category: cs.LG

TL;DR: WORK-DMD combines Random Fourier Features with online Dynamic Mode Decomposition for real-time forecasting from streaming data, achieving high accuracy with fixed computational cost and no need for historical data storage.


<details>
  <summary>Details</summary>
Motivation: Address challenges in real-time forecasting: handling non-stationary dynamics, operating under computational constraints, and avoiding catastrophic forgetting while maintaining accuracy and adaptability.

Method: Uses Windowed Online Random Kernel Dynamic Mode Decomposition with Sherman-Morrison updates within rolling windows, combining Random Fourier Features for explicit nonlinear mapping with online DMD for continuous adaptation.

Result: Achieves higher accuracy than state-of-the-art online forecasting methods across benchmark datasets, particularly strong in short-term forecasting, with single-pass data processing and minimal data requirements.

Conclusion: Combining kernel evaluations with adaptive matrix updates provides strong predictive performance with sample efficiency, offering a practical alternative to deep learning for streaming forecasting applications.

Abstract: Real-time forecasting from streaming data poses critical challenges: handling
non-stationary dynamics, operating under strict computational limits, and
adapting rapidly without catastrophic forgetting. However, many existing
approaches face trade-offs between accuracy, adaptability, and efficiency,
particularly when deployed in constrained computing environments. We introduce
WORK-DMD (Windowed Online Random Kernel Dynamic Mode Decomposition), a method
that combines Random Fourier Features with online Dynamic Mode Decomposition to
capture nonlinear dynamics through explicit feature mapping, while preserving
fixed computational cost and competitive predictive accuracy across evolving
data. WORK-DMD employs Sherman-Morrison updates within rolling windows,
enabling continuous adaptation to evolving dynamics from only current data,
eliminating the need for lengthy training or large storage requirements for
historical data. Experiments on benchmark datasets across several domains show
that WORK-DMD achieves higher accuracy than several state-of-the-art online
forecasting methods, while requiring only a single pass through the data and
demonstrating particularly strong performance in short-term forecasting. Our
results show that combining kernel evaluations with adaptive matrix updates
achieves strong predictive performance with minimal data requirements. This
sample efficiency offers a practical alternative to deep learning for streaming
forecasting applications.

</details>


### [180] [ParaFormer: Shallow Parallel Transformers with Progressive Approximation](https://arxiv.org/abs/2510.15425)
*Wei Wang,Xiao-Yong Wei,Qing Li*

Main category: cs.LG

TL;DR: ParaFormer is a shallow Transformer architecture that achieves parallelism in structure and computation by organizing layers into parallel branches with algorithmic inter-layer collaboration, enabling faster convergence, model compression, and improved deployment speed.


<details>
  <summary>Details</summary>
Motivation: Address challenges of deep Transformers including long training times, high inference latency, and impracticality on resource-constrained devices by challenging the 'deeper is better' philosophy.

Method: Formulate Transformers as function approximators in closed-form, organize layers into parallel branches with progressive approximation that ensures each new branch reduces loss from preceding branches, enabling algorithmic inter-layer collaboration without sequential constraints.

Result: Outperforms standard Transformers like ViT, supports up to 15.07x model compression, enables model expansion for adaptive continuous learning, and achieves 3.30x faster deployment than FairScale on multi-GPU setups.

Conclusion: The closed-form formulation based on Universal Approximation Theorem explains the 'depth belief' and opens new avenues for designing efficient Transformer architectures through parallel organization and algorithmic collaboration.

Abstract: The widespread 'deeper is better' philosophy has driven the creation of
architectures like ResNet and Transformer, which achieve high performance by
stacking numerous layers. However, increasing model depth comes with challenges
such as longer training times, higher inference latency, and impracticality on
resource-constrained devices. To address these issues, we propose ParaFormer, a
shallow Transformer architecture designed for true parallelism in both
structure and computation. By formulating standard Transformers as function
approximators in closed-form, our theoretical analysis shows that their
performance relies on inter-layer collaboration for progressive approximation,
rather than depth itself. While deep Transformers enforce this collaboration
through sequential designs, we demonstrate that such collaboration is not
inherently tied to sequential structures. ParaFormer removes the sequential
constraint by organizing layers into parallel branches, enforcing inter-layer
collaboration algorithmically. Specifically, we implement progressive
approximation, ensuring that each new branch further reduces the loss from
preceding branches, enabling faster convergence. Extensive experiments validate
ParaFormer's effectiveness, outperforming standard Transformers like ViT.
Moreover, ParaFormer supports up to 15.07x model compression and facilitates
model expansion for adaptive continuous learning. Experimental results on
multi-GPU deployment demonstrate that ParaFormer is 3.30x faster than widely
used parallelism solutions such as FairScale. These advancements stem from our
closed-form formulation of Transformers based on the Universal Approximation
Theorem, which not only explains the ``depth belief'' but also opens new
avenues for designing efficient Transformer architectures. Source code:
https://(open-upon-acceptance)

</details>


### [181] [Safe, Efficient, and Robust Reinforcement Learning for Ranking and Diffusion Models](https://arxiv.org/abs/2510.15429)
*Shashank Gupta*

Main category: cs.LG

TL;DR: This dissertation develops safe, sample-efficient reinforcement learning methods for ranking/recommendation systems and text-to-image diffusion models, with theoretical guarantees and novel algorithms like LOOP.


<details>
  <summary>Details</summary>
Motivation: To address the need for safe deployment of RL in real-world applications where traditional methods may underperform logging policies or suffer from sample inefficiency, particularly in ranking systems and generative models.

Method: Developed exposure-based generalization bounds and counterfactual risk-minimization for ranking systems; unified off-policy estimators with baseline-correction framework; proposed LOOP algorithm combining PPO and REINFORCE for diffusion models.

Result: Achieved safety guarantees against underperforming logging policies even with sparse feedback; derived optimal baseline minimizing variance; LOOP achieved PPO-level efficiency with better text-attribute alignment in generations.

Conclusion: The work provides theoretically grounded, practical RL methods that ensure safety and efficiency across diverse applications, with LOOP demonstrating improved trade-offs between efficiency and effectiveness in generative RL.

Abstract: This dissertation investigates how reinforcement learning (RL) methods can be
designed to be safe, sample-efficient, and robust. Framed through the unifying
perspective of contextual-bandit RL, the work addresses two major application
domains - ranking and recommendation, and text-to-image diffusion models. The
first part of the thesis develops theory and algorithms for safe deployment in
ranking systems. An exposure-based generalisation bound is derived, leading to
a counterfactual risk-minimisation objective whose solution is guaranteed not
to underperform the logging policy, even with sparse feedback. This guarantee
is extended to doubly robust estimators, enabling safety even under adversarial
or misspecified user models and offering practitioners explicit control over
permissible utility loss. The second part turns to single-action bandits, where
various off-policy estimators are unified within a baseline-correction
framework. A closed-form optimal baseline is proposed and shown to minimise
both evaluation and policy-gradient variance, thereby improving off-policy
learning reliability. The final part examines the trade-offs between efficiency
and effectiveness in generative RL. A systematic study of PPO and REINFORCE
motivates the Leave-One-Out PPO (LOOP) algorithm, which combines multiple
diffusion trajectories with a REINFORCE-style baseline inside PPO's clipped
objective. LOOP achieves PPO-level sample efficiency while producing
generations that align more faithfully with textual attributes.

</details>


### [182] [A Theoretical Study on Bridging Internal Probability and Self-Consistency for LLM Reasoning](https://arxiv.org/abs/2510.15444)
*Zhi Zhou,Yuhao Tan,Zenan Li,Yuan Yao,Lan-Zhe Guo,Yu-Feng Li,Xiaoxing Ma*

Main category: cs.LG

TL;DR: The paper provides the first theoretical framework for sampling-based test-time scaling methods in LLMs, analyzes limitations of existing approaches (self-consistency and perplexity), and introduces RPC - a hybrid method that combines Perplexity Consistency and Reasoning Pruning to improve reasoning performance while reducing sampling costs.


<details>
  <summary>Details</summary>
Motivation: Test-time scaling methods improve LLM reasoning by generating multiple reasoning paths during inference, but their theoretical foundations remain underexplored. The paper aims to establish a theoretical framework and address limitations of existing approaches.

Method: The paper introduces RPC, which has two key components: 1) Perplexity Consistency - combines strengths of self-consistency and perplexity to boost estimation error convergence rate from linear to exponential, 2) Reasoning Pruning - eliminates low-probability reasoning paths to prevent degradation. The method is analyzed theoretically and validated empirically.

Result: Empirical results across seven benchmark datasets show RPC achieves reasoning performance comparable to self-consistency while enhancing confidence reliability and reducing sampling costs by 50%. Theoretical analysis demonstrates strong potential for reducing reasoning error.

Conclusion: RPC provides an effective hybrid approach that addresses limitations of existing test-time scaling methods, offering improved theoretical foundations and practical benefits including reduced computational costs while maintaining reasoning performance.

Abstract: Test-time scaling seeks to improve the reasoning performance of large
language models (LLMs) by adding computational resources. A prevalent approach
within the field is sampling-based test-time scaling methods, which enhance
reasoning by generating multiple reasoning paths for a given input during
inference. However, despite its practical success, the theoretical foundations
remain underexplored. In this paper, we provide the first theoretical framework
for analyzing sampling-based test-time scaling methods, grounded in the
perspective of confidence estimation. Based on the framework, we analyze two
dominant paradigms: self-consistency and perplexity, and reveal key
limitations: self-consistency suffers from high estimation error while
perplexity exhibits substantial modeling error and possible degradation of the
estimation error convergence. To address these limitations, we introduce RPC, a
hybrid method that leverages our theoretical insights through two key
components: Perplexity Consistency and Reasoning Pruning. Perplexity
Consistency combines the strengths of self-consistency and perplexity, boosting
the convergence rate of estimation error from linear to exponential while
preserving model error. Reasoning Pruning prevents degradation by eliminating
low-probability reasoning paths. Both theoretical analysis and empirical
results across seven benchmark datasets demonstrate that RPC has a strong
potential for reducing reasoning error. Notably, RPC achieves reasoning
performance comparable to self-consistency while not only enhancing confidence
reliability but also reducing sampling costs by 50%. The code and resources are
available at https://wnjxyk.github.io/RPC.

</details>


### [183] [Particle Dynamics for Latent-Variable Energy-Based Models](https://arxiv.org/abs/2510.15447)
*Shiqin Tang,Shuxin Zhuang,Rong Feng,Runsheng Yu,Hongzong Li,Youzhi Zhang*

Main category: cs.LG

TL;DR: A new training method for latent-variable energy-based models that formulates maximum-likelihood training as a saddle problem using coupled Wasserstein gradient flows, requiring no discriminator networks.


<details>
  <summary>Details</summary>
Motivation: To develop expressive generative models that capture hidden structure through latent variables while avoiding the need for auxiliary networks or discriminators.

Method: Recast maximum-likelihood training as a saddle problem over distributions, using coupled Wasserstein gradient flows with alternating overdamped Langevin updates for joint negative pool and conditional latent particles with stochastic parameter ascent.

Result: Proved existence and convergence under standard assumptions with decay rates in KL divergence and Wasserstein-2 distance, achieving tighter ELBO bounds than amortized posteriors, and competitive performance on physical system approximations.

Conclusion: The saddle-point formulation provides an effective training approach for LVEBMs with theoretical guarantees and practical performance comparable to existing methods.

Abstract: Latent-variable energy-based models (LVEBMs) assign a single normalized
energy to joint pairs of observed data and latent variables, offering
expressive generative modeling while capturing hidden structure. We recast
maximum-likelihood training as a saddle problem over distributions on the
latent and joint manifolds and view the inner updates as coupled Wasserstein
gradient flows. The resulting algorithm alternates overdamped Langevin updates
for a joint negative pool and for conditional latent particles with stochastic
parameter ascent, requiring no discriminator or auxiliary networks. We prove
existence and convergence under standard smoothness and dissipativity
assumptions, with decay rates in KL divergence and Wasserstein-2 distance. The
saddle-point view further yields an ELBO strictly tighter than bounds obtained
with restricted amortized posteriors. Our method is evaluated on numerical
approximations of physical systems and performs competitively against
comparable approaches.

</details>


### [184] [Expediting Reinforcement Learning by Incorporating Knowledge About Temporal Causality in the Environment](https://arxiv.org/abs/2510.15456)
*Jan Corazza,Hadi Partovi Aria,Daniel Neider,Zhe Xu*

Main category: cs.LG

TL;DR: This paper proposes a method to incorporate causal information using Temporal Logic-based Causal Diagrams into probabilistic reward machines to improve RL policy learning and transfer across domains.


<details>
  <summary>Details</summary>
Motivation: RL algorithms struggle with sparse rewards and complex temporal dependencies. Probabilistic reward machines (PRMs) help but are difficult to design manually and transfer across domains with different causal structures.

Method: The authors incorporate causal information in the form of Temporal Logic-based Causal Diagrams into the reward formalism to expedite policy learning and aid task specification transfer.

Result: The paper provides theoretical convergence guarantees for optimal policy learning and demonstrates empirical strengths of the proposed method.

Conclusion: Integrating causal diagrams with probabilistic reward machines improves RL performance in sparse reward environments and enables better transfer of task specifications across domains.

Abstract: Reinforcement learning (RL) algorithms struggle with learning optimal
policies for tasks where reward feedback is sparse and depends on a complex
sequence of events in the environment. Probabilistic reward machines (PRMs) are
finite-state formalisms that can capture temporal dependencies in the reward
signal, along with nondeterministic task outcomes. While special RL algorithms
can exploit this finite-state structure to expedite learning, PRMs remain
difficult to modify and design by hand. This hinders the already difficult
tasks of utilizing high-level causal knowledge about the environment, and
transferring the reward formalism into a new domain with a different causal
structure. This paper proposes a novel method to incorporate causal information
in the form of Temporal Logic-based Causal Diagrams into the reward formalism,
thereby expediting policy learning and aiding the transfer of task
specifications to new environments. Furthermore, we provide a theoretical
result about convergence to optimal policy for our method, and demonstrate its
strengths empirically.

</details>


### [185] [Learning to Answer from Correct Demonstrations](https://arxiv.org/abs/2510.15464)
*Nirmit Joshi,Gene Li,Siddharth Bhandari,Shiva Prasad Kasiviswanathan,Cong Ma,Nathan Srebro*

Main category: cs.LG

TL;DR: The paper proposes a new approach for learning from demonstrations in contextual bandits where multiple correct answers exist, using a reward model with low-cardinality classes instead of assuming low-complexity policies.


<details>
  <summary>Details</summary>
Motivation: Prior work assumes demonstrators belong to low-complexity policy classes, but this paper argues that assuming low-cardinality reward classes is a weaker and more realistic assumption for learning from correct demonstrations.

Method: The authors formalize the problem as offline imitation learning in contextual bandits and devise a novel approach that learns with logarithmic sample complexity in the cardinality of the reward class, moving beyond traditional likelihood maximization methods.

Result: The paper shows that likelihood maximization methods can fail when only assuming low-cardinality reward classes, and presents an alternative method with provable logarithmic sample complexity.

Conclusion: The work motivates looking beyond likelihood maximization when learning from correct demonstrations, suggesting that alternative approaches based on reward model structure can be more effective.

Abstract: We study the problem of learning to generate an answer (or completion) to a
question (or prompt), where there could be multiple correct answers, any one of
which is acceptable at test time. Learning is based on demonstrations of some
correct answer to each training question, as in Supervised Fine Tuning (SFT).
We formalize the problem as offline imitation learning in contextual bandits,
with demonstrations from some optimal policy, without explicitly observed
rewards. Prior work assumes that the demonstrator belongs to a low-complexity
policy class, which motivates maximum likelihood estimation (i.e., log-loss
minimization). In contrast, we propose relying only on the reward model
(specifying which answers are correct) being in a low-cardinality class, which
we argue is a weaker assumption. We show that likelihood maximization methods
can fail in this case, and instead devise an alternative novel approach that
learns with sample complexity logarithmic in the cardinality of the reward
class. Our work motivates looking beyond likelihood maximization when learning
from correct demonstrations.

</details>


### [186] [Adversary-Free Counterfactual Prediction via Information-Regularized Representations](https://arxiv.org/abs/2510.15479)
*Shiqin Tang,Rong Feng,Shuxin Zhuang,Hongzong Li,Youzhi Zhang*

Main category: cs.LG

TL;DR: Proposes an information-theoretic approach for counterfactual prediction under assignment bias, using mutual information minimization to remove treatment-covariate dependence without adversarial training.


<details>
  <summary>Details</summary>
Motivation: Address counterfactual prediction under assignment bias, where treatment assignment depends on covariates, leading to biased estimates. Existing adversarial methods suffer from training instabilities and tuning difficulties.

Method: Learn stochastic representation Z predictive of outcomes while minimizing I(Z;T) using variational objective that upper-bounds information term. Framework extends to dynamic settings by applying information penalty to sequential representations at each decision time.

Result: Method performs favorably on controlled simulations and real-world clinical dataset across likelihood, counterfactual error, and policy evaluation metrics, while avoiding training instabilities of adversarial schemes.

Conclusion: Information-theoretic approach provides mathematically grounded, stable alternative to adversarial methods for counterfactual prediction under assignment bias, with proven motivation and tractable training.

Abstract: We study counterfactual prediction under assignment bias and propose a
mathematically grounded, information-theoretic approach that removes
treatment-covariate dependence without adversarial training. Starting from a
bound that links the counterfactual-factual risk gap to mutual information, we
learn a stochastic representation Z that is predictive of outcomes while
minimizing I(Z; T). We derive a tractable variational objective that
upper-bounds the information term and couples it with a supervised decoder,
yielding a stable, provably motivated training criterion. The framework extends
naturally to dynamic settings by applying the information penalty to sequential
representations at each decision time. We evaluate the method on controlled
numerical simulations and a real-world clinical dataset, comparing against
recent state-of-the-art balancing, reweighting, and adversarial baselines.
Across metrics of likelihood, counterfactual error, and policy evaluation, our
approach performs favorably while avoiding the training instabilities and
tuning burden of adversarial schemes.

</details>


### [187] [OffSim: Offline Simulator for Model-based Offline Inverse Reinforcement Learning](https://arxiv.org/abs/2510.15495)
*Woo-Jin Ahn,Sang-Ryul Baek,Yong-Jun Lee,Hyun-Duck Choi,Myo-Taeg Lim*

Main category: cs.LG

TL;DR: OffSim is an offline model-based inverse reinforcement learning framework that learns environmental dynamics and reward functions from expert trajectories, enabling policy training without real environment interaction.


<details>
  <summary>Details</summary>
Motivation: Traditional RL requires time-consuming simulator development and manual reward function design. OffSim aims to automate this by learning both dynamics and rewards directly from expert data.

Method: Jointly optimizes high-entropy transition model and IRL-based reward function. OffSim+ extension adds marginal reward for multi-dataset settings to enhance exploration.

Result: Extensive MuJoCo experiments show substantial performance gains over existing offline IRL methods, demonstrating efficacy and robustness.

Conclusion: OffSim successfully enables offline policy training by learning environment dynamics and rewards from expert trajectories, reducing the need for manual simulator development and reward engineering.

Abstract: Reinforcement learning algorithms typically utilize an interactive simulator
(i.e., environment) with a predefined reward function for policy training.
Developing such simulators and manually defining reward functions, however, is
often time-consuming and labor-intensive. To address this, we propose an
Offline Simulator (OffSim), a novel model-based offline inverse reinforcement
learning (IRL) framework, to emulate environmental dynamics and reward
structure directly from expert-generated state-action trajectories. OffSim
jointly optimizes a high-entropy transition model and an IRL-based reward
function to enhance exploration and improve the generalizability of the learned
reward. Leveraging these learned components, OffSim can subsequently train a
policy offline without further interaction with the real environment.
Additionally, we introduce OffSim$^+$, an extension that incorporates a
marginal reward for multi-dataset settings to enhance exploration. Extensive
MuJoCo experiments demonstrate that OffSim achieves substantial performance
gains over existing offline IRL methods, confirming its efficacy and
robustness.

</details>


### [188] [The Road Less Traveled: Enhancing Exploration in LLMs via Sequential Sampling](https://arxiv.org/abs/2510.15502)
*Shijia Kang,Muhan Zhang*

Main category: cs.LG

TL;DR: SESA introduces sequential sampling to address RL's exploration limitations in LLMs, improving diversity and performance by generating diverse solution sketches before full reasoning paths.


<details>
  <summary>Details</summary>
Motivation: Traditional RL for LLMs suffers from limited exploration and entropy collapse, where models exploit narrow solution sets, reducing sampling diversity and preventing further performance improvements.

Method: SESA (SEquential SAmpling) generates diverse solution sketches sequentially before expanding them into full reasoning paths, conditioning each new output on previous ones to promote diversity and prevent policy collapse.

Result: Experiments show SESA outperforms traditional RL in path diversity and recovery from collapse. On three agent benchmarks, it improves success rates by +0.25, +0.42, and +0.07 absolute (up to 211% relative improvement over baseline RL).

Conclusion: SESA provides a structured approach to exploration that enables more effective and diverse reasoning in RL-trained LLMs, addressing fundamental limitations of current RL methods.

Abstract: Reinforcement learning (RL) has been pivotal in enhancing the reasoning
capabilities of large language models (LLMs), but it often suffers from limited
exploration and entropy collapse, where models exploit a narrow set of
solutions, leading to a loss of sampling diversity and subsequently preventing
RL from further improving performance. This issue is exacerbated in parallel
sampling methods, where multiple outputs are drawn from the same distribution,
potentially causing the model to converge to similar solutions. We propose
SESA, a novel SEquential SAmpling framework that mitigates this challenge by
generating diverse solution sketches sequentially before expanding them into
full reasoning paths. This approach ensures broader exploration by conditioning
each new output on previous ones, promoting diversity throughout the process
and preventing policy collapse. Our experiments on a synthetic task show that
sequential sampling consistently outperforms traditional RL methods in terms of
path diversity and recovery from collapse. Further evaluations on real-world
tasks demonstrate that SESA improves both the exploration of valid strategies
and the overall performance of LLMs. On three agent benchmarks, SESA lifts
success rates by $+0.25$, $+0.42$, and $+0.07$ absolute over the base model (up
to an additional $211\%$ relative improvement over baseline RL), underscoring
its exploration advantage. This work introduces a structured approach to
exploration, paving the way for more effective and diverse reasoning in
RL-trained LLMs. Our code is released at https://github.com/MuLabPKU/sesa.

</details>


### [189] [Theoretical Refinement of CLIP by Utilizing Linear Structure of Optimal Similarity](https://arxiv.org/abs/2510.15508)
*Naoki Yoshida,Satoshi Hayakawa,Yuhta Takida,Toshimitsu Uesaka,Hiromi Wakaki,Yuki Mitsufuji*

Main category: cs.LG

TL;DR: KME-CLIP enhances CLIP by using kernel methods to better approximate pointwise mutual information (PMI) between modalities, improving performance on retrieval and classification tasks.


<details>
  <summary>Details</summary>
Motivation: Current CLIP implementations don't fully utilize the linear structure of PMI, which theory shows should be the optimal similarity metric between paired modalities.

Method: Leverages inner product in reproducing kernel Hilbert space to approximate PMI, theoretically proving arbitrary accuracy approximation.

Result: Outperforms standard CLIP formulation across several retrieval and classification tasks.

Conclusion: KME-CLIP successfully bridges the gap between theoretical PMI requirements and practical implementation in multi-modal contrastive learning.

Abstract: In this study, we propose an enhancement to the similarity computation
mechanism in multi-modal contrastive pretraining frameworks such as CLIP. Prior
theoretical research has demonstrated that the optimal similarity metrics
between paired modalities should correspond to the pointwise mutual information
(PMI) between the two modalities. However, the current implementations of CLIP
and its variants fail to fully utilize the underlying linear structure of PMI.
We therefore propose KME-CLIP, which leverages this structure through the inner
product in a reproducing kernel Hilbert space. We theoretically prove that our
method can approximate PMI with arbitrary accuracy and empirically demonstrate
that our approach overall outperforms the standard CLIP formulation across
several retrieval and classification tasks.

</details>


### [190] [Language Models are Injective and Hence Invertible](https://arxiv.org/abs/2510.15511)
*Giorgos Nikolaou,Tommaso Mencattini,Donato Crisostomi,Andrea Santilli,Yannis Panagakis,Emanuele Rodola'*

Main category: cs.LG

TL;DR: Transformers are injective - different inputs map to different outputs, enabling exact input recovery from hidden representations.


<details>
  <summary>Details</summary>
Motivation: Challenge the view that transformer components' non-injective nature prevents exact input recovery from model representations.

Method: Mathematical proof of injectivity at initialization preserved during training, empirical collision tests on 6 models, and development of SipIt algorithm for exact input reconstruction.

Result: No collisions found in billions of tests, SipIt algorithm achieves exact invertibility with linear-time guarantees.

Conclusion: Injectivity is a fundamental property of language models with implications for transparency, interpretability, and safe deployment.

Abstract: Transformer components such as non-linear activations and normalization are
inherently non-injective, suggesting that different inputs could map to the
same output and prevent exact recovery of the input from a model's
representations. In this paper, we challenge this view. First, we prove
mathematically that transformer language models mapping discrete input
sequences to their corresponding sequence of continuous representations are
injective and therefore lossless, a property established at initialization and
preserved during training. Second, we confirm this result empirically through
billions of collision tests on six state-of-the-art language models, and
observe no collisions. Third, we operationalize injectivity: we introduce
SipIt, the first algorithm that provably and efficiently reconstructs the exact
input text from hidden activations, establishing linear-time guarantees and
demonstrating exact invertibility in practice. Overall, our work establishes
injectivity as a fundamental and exploitable property of language models, with
direct implications for transparency, interpretability, and safe deployment.

</details>


### [191] [Revisiting Knowledge Distillation: The Hidden Role of Dataset Size](https://arxiv.org/abs/2510.15516)
*Giulia Lanzillotta,Felix Sarnthein,Gil Kur,Thomas Hofmann,Bobby He*

Main category: cs.LG

TL;DR: Knowledge distillation becomes more effective in low-data regimes, disproving label smoothing hypothesis and supporting dark knowledge theory.


<details>
  <summary>Details</summary>
Motivation: To understand how and why knowledge distillation works by studying its relationship with dataset size, which has been overlooked in previous research.

Method: Conducted extensive experiments across various datasets, tasks, and neural architectures while varying dataset sizes to analyze distillation effects.

Result: Distillation effect is amplified in low-data regimes (data efficiency property), disproving label smoothing hypothesis and supporting dark knowledge hypothesis.

Conclusion: Dataset size is a fundamental but overlooked variable in understanding knowledge distillation mechanisms, with distillation showing enhanced effectiveness in data-scarce scenarios.

Abstract: The concept of knowledge distillation (KD) describes the training of a
student model from a teacher model and is a widely adopted technique in deep
learning. However, it is still not clear how and why distillation works.
Previous studies focus on two central aspects of distillation: model size, and
generalisation. In this work we study distillation in a third dimension:
dataset size. We present a suite of experiments across a wide range of
datasets, tasks and neural architectures, demonstrating that the effect of
distillation is not only preserved but amplified in low-data regimes. We call
this newly discovered property the data efficiency of distillation. Equipped
with this new perspective, we test the predictive power of existing theories of
KD as we vary the dataset size. Our results disprove the hypothesis that
distillation can be understood as label smoothing, and provide further evidence
in support of the dark knowledge hypothesis. Finally, we analyse the impact of
modelling factors such as the objective, scale and relative number of samples
on the observed phenomenon. Ultimately, this work reveals that the dataset size
may be a fundamental but overlooked variable in the mechanisms underpinning
distillation.

</details>


### [192] [Compressive Modeling and Visualization of Multivariate Scientific Data using Implicit Neural Representation](https://arxiv.org/abs/2510.15535)
*Abhay Kumar Dwivedi,Shanu Saklani,Soumya Dutta*

Main category: cs.LG

TL;DR: The paper develops compressed neural representations for multivariate datasets using a single network with parameter sharing to achieve state-of-the-art data compression.


<details>
  <summary>Details</summary>
Motivation: The extensive adoption of Deep Neural Networks in scientific visualization tasks and recent successes in compressed data models using implicit neural representations for spatiotemporal volume visualization and super-resolution.

Method: Utilizes a single network to learn representations for all data variables simultaneously through parameter sharing.

Result: Demonstrates superior performance in reconstructed data quality, rendering and visualization quality, preservation of dependency information among variables, and storage efficiency.

Conclusion: The approach achieves state-of-the-art data compression for multivariate datasets containing tens to hundreds of variables.

Abstract: The extensive adoption of Deep Neural Networks has led to their increased
utilization in challenging scientific visualization tasks. Recent advancements
in building compressed data models using implicit neural representations have
shown promising results for tasks like spatiotemporal volume visualization and
super-resolution. Inspired by these successes, we develop compressed neural
representations for multivariate datasets containing tens to hundreds of
variables. Our approach utilizes a single network to learn representations for
all data variables simultaneously through parameter sharing. This allows us to
achieve state-of-the-art data compression. Through comprehensive evaluations,
we demonstrate superior performance in terms of reconstructed data quality,
rendering and visualization quality, preservation of dependency information
among variables, and storage efficiency.

</details>


### [193] [Doubly Robust Estimation of Causal Effects in Strategic Equilibrium Systems](https://arxiv.org/abs/2510.15555)
*Sibo Xiao*

Main category: cs.LG

TL;DR: SDR is a novel estimator combining strategic equilibrium modeling with doubly robust estimation for causal inference in strategic environments, reducing bias by 7.6%-29.3% over baselines.


<details>
  <summary>Details</summary>
Motivation: Address endogenous treatment assignment arising from strategic agent behavior in causal inference settings where agents respond strategically to interventions.

Method: Integrates strategic equilibrium modeling with doubly robust estimation, maintaining double robustness while incorporating strategic considerations under strategic unconfoundedness.

Result: Achieves 7.6%-29.3% bias reduction across varying strategic strengths compared to baseline methods, with robust scalability as agent populations increase.

Conclusion: Provides a principled framework for reliable causal inference when agents respond strategically to interventions, with theoretical guarantees of consistency and asymptotic normality.

Abstract: We introduce the Strategic Doubly Robust (SDR) estimator, a novel framework
that integrates strategic equilibrium modeling with doubly robust estimation
for causal inference in strategic environments. SDR addresses endogenous
treatment assignment arising from strategic agent behavior, maintaining double
robustness while incorporating strategic considerations. Theoretical analysis
confirms SDR's consistency and asymptotic normality under strategic
unconfoundedness. Empirical evaluations demonstrate SDR's superior performance
over baseline methods, achieving 7.6\%-29.3\% bias reduction across varying
strategic strengths and maintaining robust scalability with agent populations.
The framework provides a principled approach for reliable causal inference when
agents respond strategically to interventions.

</details>


### [194] [On the Neural Feature Ansatz for Deep Neural Networks](https://arxiv.org/abs/2510.15563)
*Edward Tansley,Estelle Massart,Coralia Cartis*

Main category: cs.LG

TL;DR: The paper extends the Neural Feature Ansatz (NFA) to deep linear networks, showing depth-dependent scaling (α=1/L) and proving NFA holds under balanced initialization or with weight decay for unbalanced initialization, while providing counterexamples for nonlinear networks.


<details>
  <summary>Details</summary>
Motivation: To understand feature learning in deep neural networks by extending the Neural Feature Ansatz beyond two-layer linear networks and investigating its depth dependency and conditions for validity.

Method: Theoretical analysis using gradient flow dynamics with balanced/unbalanced weight initialization, mathematical proofs for linear networks, counterexamples for nonlinear networks, and numerical validation across various optimization settings.

Result: NFA holds for L-layer linear networks with exponent α=1/L under balanced initialization, and asymptotically with weight decay for unbalanced initialization. Counterexamples show NFA fails for some nonlinear networks even with perfect training fit.

Conclusion: The NFA exhibits depth-dependent scaling in linear networks and requires specific conditions (balanced initialization or weight decay) to hold, but fails for certain nonlinear architectures, highlighting limitations of the ansatz.

Abstract: Understanding feature learning is an important open question in establishing
a mathematical foundation for deep neural networks. The Neural Feature Ansatz
(NFA) states that after training, the Gram matrix of the first-layer weights of
a deep neural network is proportional to some power $\alpha>0$ of the average
gradient outer product (AGOP) of this network with respect to its inputs.
Assuming gradient flow dynamics with balanced weight initialization, the NFA
was proven to hold throughout training for two-layer linear networks with
exponent $\alpha = 1/2$ (Radhakrishnan et al., 2024). We extend this result to
networks with $L \geq 2$ layers, showing that the NFA holds with exponent
$\alpha = 1/L$, thus demonstrating a depth dependency of the NFA. Furthermore,
we prove that for unbalanced initialization, the NFA holds asymptotically
through training if weight decay is applied. We also provide counterexamples
showing that the NFA does not hold for some network architectures with
nonlinear activations, even when these networks fit arbitrarily well the
training data. We thoroughly validate our theoretical results through numerical
experiments across a variety of optimization algorithms, weight decay rates and
initialization schemes.

</details>


### [195] [CQD-SHAP: Explainable Complex Query Answering via Shapley Values](https://arxiv.org/abs/2510.15623)
*Parsa Abbasi,Stefan Heindorf*

Main category: cs.LG

TL;DR: CQD-SHAP is a framework that explains complex query answering by computing the contribution of each query part using Shapley values, addressing interpretability issues in neural and neurosymbolic methods.


<details>
  <summary>Details</summary>
Motivation: Current neural and neurosymbolic complex query answering methods are black-box models that lack interpretability, making it hard to understand why certain answers are ranked higher and what parts of queries are most important.

Method: Proposed CQD-SHAP framework based on Shapley values from cooperative game theory to compute the contribution of each query part to answer rankings, satisfying all fundamental Shapley axioms.

Result: Automated evaluation shows CQD-SHAP effectively provides necessary and sufficient explanations for most query types, outperforming various baseline methods in interpretability.

Conclusion: CQD-SHAP successfully addresses interpretability challenges in complex query answering by quantifying query part contributions, demonstrating the value of neural predictors over purely symbolic approaches.

Abstract: Complex query answering (CQA) goes beyond the well-studied link prediction
task by addressing more sophisticated queries that require multi-hop reasoning
over incomplete knowledge graphs (KGs). Research on neural and neurosymbolic
CQA methods is still an emerging field. Almost all of these methods can be
regarded as black-box models, which may raise concerns about user trust.
Although neurosymbolic approaches like CQD are slightly more interpretable,
allowing intermediate results to be tracked, the importance of different parts
of the query remains unexplained. In this paper, we propose CQD-SHAP, a novel
framework that computes the contribution of each query part to the ranking of a
specific answer. This contribution explains the value of leveraging a neural
predictor that can infer new knowledge from an incomplete KG, rather than a
symbolic approach relying solely on existing facts in the KG. CQD-SHAP is
formulated based on Shapley values from cooperative game theory and satisfies
all the fundamental Shapley axioms. Automated evaluation of these explanations
in terms of necessary and sufficient explanations, and comparisons with various
baselines, shows the effectiveness of this approach for most query types.

</details>


### [196] [Attn-JGNN: Attention Enhanced Join-Graph Neural Networks](https://arxiv.org/abs/2510.15583)
*Jixin Zhang,Yong Lai*

Main category: cs.LG

TL;DR: Attn-JGNN is an attention-enhanced join-graph neural network model that improves #SAT solving accuracy using tree decomposition, iterative message passing, and attention mechanisms.


<details>
  <summary>Details</summary>
Motivation: To improve the accuracy of solving #SAT (model counting) problems by leveraging neural networks and attention mechanisms to focus on key variables and reduce redundant computations.

Method: Uses tree decomposition to encode CNF formulas into join-graphs, performs iterative message passing on the graphs, applies attention mechanisms within and between clusters, and learns partition functions to approximate model counts.

Result: Attn-JGNN achieves better solving accuracy than other neural network methods for #SAT problems.

Conclusion: The attention-enhanced join-graph neural network approach effectively improves #SAT solving performance by focusing computational resources on critical variables and clusters.

Abstract: We propose an Attention Enhanced Join-Graph Neural Networks(Attn-JGNN) model
for solving #SAT problems, which significantly improves the solving accuracy.
Inspired by the Iterative Join Graph Propagation (IJGP) algorithm, Attn-JGNN
uses tree decomposition to encode the CNF formula into a join-graph, then
performs iterative message passing on the join-graph, and finally approximates
the model number by learning partition functions. In order to further improve
the accuracy of the solution, we apply the attention mechanism in and between
clusters of the join-graphs, which makes Attn-JGNN pay more attention to the
key variables and clusters in probabilistic inference, and reduces the
redundant calculation. Finally, our experiments show that our Attn-JGNN model
achieves better results than other neural network methods.

</details>


### [197] [GRATING: Low-Latency and Memory-Efficient Semantic Selection on Device](https://arxiv.org/abs/2510.15620)
*Jiahao Zhou,Chengliang Lin,Dingji Li,Mingkai Dong,Haibo Chen*

Main category: cs.LG

TL;DR: GRATING is a training-free inference system that accelerates semantic top-K selection by exploiting sequence-level sparsity and progressive cluster pruning, achieving significant latency and memory reductions without precision loss.


<details>
  <summary>Details</summary>
Motivation: Semantic top-K selection with cross-encoder rerankers dominates latency and memory budgets in on-device AI services, creating efficiency bottlenecks for edge deployment.

Method: Monolithic forwarding with progressive cluster pruning that leverages sequence-level sparsity (relative rankings stabilize early in intermediate layers), plus dual-layer sliding window and chunked execution for memory optimization.

Result: Reduces latency by up to 89.0% and peak memory by up to 94.9% in microbenchmarks, with 11.6%-51.0% latency reduction and 18.6%-77.8% memory reduction in real-world applications across 0.6B to 8B parameter models.

Conclusion: GRATING enables substantial efficiency improvements for on-device AI services without precision loss, making semantic top-K selection more deployable on edge hardware.

Abstract: Semantic top-K selection with cross-encoder rerankers underpins of on-device
AI services, such as retrieval-augmented generation, agent memory, and
personalized recommendation. However, its latency and memory demands dominate
end-to-end budgets on edge hardware. Revisiting the objective of top-K
selection, we reveal that only relative rankings matter, not exact
per-candidate scores. We further observe sequence-level sparsity: relative
rankings stabilize early in intermediate layers, allowing pruning opportunities
prior to completing full inference.
  Building on this insight, we propose monolithic forwarding and develop a
training-free inference system, GRATING. By maintaining a global view of all
candidates, it reduces latency through progressive cluster pruning. It also
bounds peak memory usage by strategically overlapping I/O with computation via
dual-layer sliding window and chunked execution. We evaluate GRATING against
state-of-the-art baselines on rerankers from 0.6B to 8B parameters across Apple
M2 and RTX 5070. GRATING consistently reduces latency by up to 89.0% and peak
memory by up to 94.9% in microbenchmarks, without any loss in precision. Across
three real-world on-device AI applications, GRATING lowers latency by
11.6%-51.0% and peak memory by 18.6%-77.8%, demonstrating substantial
improvements in efficiency and deployability.

</details>


### [198] [CarBoN: Calibrated Best-of-N Sampling Improves Test-time Reasoning](https://arxiv.org/abs/2510.15674)
*Yung-Chen Tang,Pin-Yu Chen,Andrea Cavallaro*

Main category: cs.LG

TL;DR: CarBoN is a test-time calibration framework that improves reasoning efficiency by adaptively modifying model outputs toward high-reward paths without retraining, achieving up to 4x fewer rollouts for same accuracy.


<details>
  <summary>Details</summary>
Motivation: Address inefficiency of Best-of-N sampling which shows diminishing returns as N increases, and improve language model performance for reasoning tasks without costly retraining.

Method: Two-phase approach: first explores solution space, then learns calibration via input-specific temperature T and additive shift vector δ to guide generation toward reliable reasoning paths.

Result: Experiments on MATH-500 and AIME-2024 show CarBoN improves efficiency with up to 4x fewer rollouts to reach same accuracy, often achieving higher accuracy under fixed budgets.

Conclusion: The framework effectively balances output diversity and correctness through complementary roles of T and δ, and generalizes to step-level sampling strategies like beam search.

Abstract: Allocating more computation during inference time (test-time scaling)
improves language model performance, especially for reasoning tasks. However,
popular methods like Best-of-$N$ sampling often show diminishing returns as $N$
increases. To address this inefficiency, we introduce a general test-time
calibration framework that adaptively modifies the model toward high-reward
reasoning paths, with theoretical guarantees of improving the lower bound of
expected reward under finite sampling, all without large language model (LLM)
retraining. Within this framework, we propose CarBoN (Calibrated Best-of-$N$),
a two-phase method that first explores the solution space and then learns a
calibration of the logits via an input-specific temperature $T$ and additive
shift vector $\delta$, guiding generation toward more reliable reasoning.
Experiments on MATH-500 and AIME-2024 show that CarBoN improves efficiency,
with up to $4\times$ fewer rollouts to reach the same accuracy, while often
achieving higher accuracy under fixed budgets. We also analyze the
complementary roles of $T$ and $\delta$ in balancing output diversity and
correctness, and demonstrate that the framework also generalizes to step-level
sampling strategies such as beam search. For more information, please refer to
our project page at huggingface.co/spaces/TrustSafeAI/Test-Time-Calibration.

</details>


### [199] [KS-Net: Multi-layer network model for determining the rotor type from motor parameters in interior PMSMs](https://arxiv.org/abs/2510.15688)
*Kivanc Dogan,Ahmet Orhan*

Main category: cs.LG

TL;DR: This study uses machine learning to classify IPMSM rotor shapes (2D, V, Nabla types) using electromagnetic parameters, achieving near-perfect accuracy as a faster alternative to traditional FEM analysis.


<details>
  <summary>Details</summary>
Motivation: Traditional finite element method (FEM) analysis for IPMSM rotor geometry is computationally expensive, creating need for faster, cost-effective alternatives using machine learning.

Method: Used custom deep learning model (KS-Net) and compared with classical ML algorithms (Cubic SVM, Quadratic SVM, Fine KNN, Cosine KNN, Fine Tree) on 9,000 samples using 10-fold cross-validation.

Result: Cubic SVM and Quadratic SVM achieved 100% accuracy, while KS-Net achieved 99.98% accuracy with only 2 misclassifications, demonstrating competitiveness with classical methods.

Conclusion: Machine learning can accurately predict IPMSM rotor shapes, providing a fast, cost-effective alternative to FEM that can accelerate motor design and enable automated rotor identification systems.

Abstract: The demand for high efficiency and precise control in electric drive systems
has led to the widespread adoption of Interior Permanent Magnet Synchronous
Motors (IPMSMs). The performance of these motors is significantly influenced by
rotor geometry. Traditionally, rotor shape analysis has been conducted using
the finite element method (FEM), which involves high computational costs. This
study aims to classify the rotor shape (2D type, V type, Nabla type) of IPMSMs
using electromagnetic parameters through machine learning-based methods and to
demonstrate the applicability of this approach as an alternative to classical
methods. In this context, a custom deep learning model, KS-Net, developed by
the user, was comparatively evaluated against Cubic SVM, Quadratic SVM, Fine
KNN, Cosine KNN, and Fine Tree algorithms. The balanced dataset, consisting of
9,000 samples, was tested using 10-fold cross-validation, and performance
metrics such as accuracy, precision, recall, and F1-score were employed. The
results indicate that the Cubic SVM and Quadratic SVM algorithms classified all
samples flawlessly, achieving 100% accuracy, while the KS-Net model achieved
99.98% accuracy with only two misclassifications, demonstrating competitiveness
with classical methods. This study shows that the rotor shape of IPMSMs can be
predicted with high accuracy using data-driven approaches, offering a fast and
cost-effective alternative to FEM-based analyses. The findings provide a solid
foundation for accelerating motor design processes, developing automated rotor
identification systems, and enabling data-driven fault diagnosis in engineering
applications.

</details>


### [200] [Deep Neural ODE Operator Networks for PDEs](https://arxiv.org/abs/2510.15651)
*Ziqian Li,Kang Liu,Yongcun Song,Hangrui Yue,Enrique Zuazua*

Main category: cs.LG

TL;DR: NODE-ONet is a neural ODE operator network that incorporates PDE physics to improve temporal dynamics modeling and generalization beyond training time frames.


<details>
  <summary>Details</summary>
Motivation: Existing operator learning approaches overlook domain knowledge in PDEs, leading to challenges in capturing temporal dynamics and poor generalization beyond training periods.

Method: Encoder-decoder architecture with spatial encoder, neural ODE for latent temporal dynamics, and decoder for physical space reconstruction. Uses physics-encoded neural ODEs to incorporate PDE-specific properties.

Result: Demonstrated high accuracy, computational efficiency, and prediction capabilities beyond training time frames on nonlinear diffusion-reaction and Navier-Stokes equations.

Conclusion: Framework offers flexibility with diverse encoders/decoders, generalizes across related PDE families, and serves as a scalable, physics-encoded tool for scientific machine learning.

Abstract: Operator learning has emerged as a promising paradigm for developing
efficient surrogate models to solve partial differential equations (PDEs).
However, existing approaches often overlook the domain knowledge inherent in
the underlying PDEs and hence suffer from challenges in capturing temporal
dynamics and generalization issues beyond training time frames. This paper
introduces a deep neural ordinary differential equation (ODE) operator network
framework, termed NODE-ONet, to alleviate these limitations. The framework
adopts an encoder-decoder architecture comprising three core components: an
encoder that spatially discretizes input functions, a neural ODE capturing
latent temporal dynamics, and a decoder reconstructing solutions in physical
spaces. Theoretically, error analysis for the encoder-decoder architecture is
investigated. Computationally, we propose novel physics-encoded neural ODEs to
incorporate PDE-specific physical properties. Such well-designed neural ODEs
significantly reduce the framework's complexity while enhancing numerical
efficiency, robustness, applicability, and generalization capacity. Numerical
experiments on nonlinear diffusion-reaction and Navier-Stokes equations
demonstrate high accuracy, computational efficiency, and prediction
capabilities beyond training time frames. Additionally, the framework's
flexibility to accommodate diverse encoders/decoders and its ability to
generalize across related PDE families further underscore its potential as a
scalable, physics-encoded tool for scientific machine learning.

</details>


### [201] [ProofOptimizer: Training Language Models to Simplify Proofs without Human Demonstrations](https://arxiv.org/abs/2510.15700)
*Alex Gu,Bartosz Piotrowski,Fabian Gloeckle,Kaiyu Yang,Aram H. Markosyan*

Main category: cs.LG

TL;DR: ProofOptimizer is a language model trained to simplify Lean proofs using expert iteration and reinforcement learning, achieving significant proof length reductions (87% on miniF2F, 57% on PutnamBench, 49% on IMO 2025 proofs) while maintaining correctness.


<details>
  <summary>Details</summary>
Motivation: Neural theorem provers generate excessively long proofs that are difficult for humans to comprehend, limiting their usefulness for mathematical insight. Proof simplification is a critical bottleneck with scarce training data.

Method: ProofOptimizer uses expert iteration and reinforcement learning, leveraging Lean to verify simplifications and provide training signal. It operates within an iterative proof-shortening workflow at inference time.

Result: Substantial proof compression: 87% reduction on miniF2F, 57% on PutnamBench, and 49% on Seed-Prover's IMO 2025 proofs. Simplified proofs also check faster in Lean and improve downstream prover performance when used as training data.

Conclusion: ProofOptimizer successfully addresses the proof simplification bottleneck, enabling more concise and human-comprehensible formal proofs while improving verification speed and downstream prover performance.

Abstract: Neural theorem proving has advanced rapidly in the past year, reaching IMO
gold-medalist capabilities and producing formal proofs that span thousands of
lines. Although such proofs are mechanically verified by formal systems like
Lean, their excessive length renders them difficult for humans to comprehend
and limits their usefulness for mathematical insight. Proof simplification is
therefore a critical bottleneck. Yet, training data for this task is scarce,
and existing methods -- mainly agentic scaffolding with off-the-shelf LLMs --
struggle with the extremely long proofs generated by RL-trained provers. We
introduce ProofOptimizer, the first language model trained to simplify Lean
proofs without requiring additional human supervision. ProofOptimizer is
trained via expert iteration and reinforcement learning, using Lean to verify
simplifications and provide training signal. At inference time, it operates
within an iterative proof-shortening workflow, progressively reducing proof
length. Experiments show that ProofOptimizer substantially compresses proofs
generated by state-of-the-art RL-trained provers on standard benchmarks,
reducing proof length by 87% on miniF2F, 57% on PutnamBench, and 49% on
Seed-Prover's IMO 2025 proofs. Beyond conciseness, the simplified proofs check
faster in Lean and further improve downstream prover performance when reused as
training data for supervised finetuning.

</details>


### [202] [Fast and Compact Tsetlin Machine Inference on CPUs Using Instruction-Level Optimization](https://arxiv.org/abs/2510.15653)
*Yefan Zeng,Shengyu Duan,Rishad Shafik,Alex Yakovlev*

Main category: cs.LG

TL;DR: Optimized Tsetlin Machine implementation using bitwise operations and early exit mechanism with literal reordering, achieving up to 96.71% inference speed improvement on ARM processors.


<details>
  <summary>Details</summary>
Motivation: Tsetlin Machine's logic-driven operations are naturally parallelizable on modern CPU architectures, making it suitable for high-speed inference on resource-constrained devices like CPUs.

Method: Uses instruction-level bitwise operations for compact model representation and accelerated processing, plus an early exit mechanism that exploits AND-based clause evaluation. Introduces literal reorder strategy applied post-training through statistical analysis of literals and Tsetlin Automata actions.

Result: Experimental results using gem5 simulator with ARM processor show up to 96.71% reduction in inference time compared to conventional integer-based TM implementations while maintaining comparable code density.

Conclusion: The proposed optimizations significantly accelerate Tsetlin Machine inference on CPUs through efficient bitwise operations and early exit mechanisms with minimal runtime overhead.

Abstract: The Tsetlin Machine (TM) offers high-speed inference on resource-constrained
devices such as CPUs. Its logic-driven operations naturally lend themselves to
parallel execution on modern CPU architectures. Motivated by this, we propose
an efficient software implementation of the TM by leveraging instruction-level
bitwise operations for compact model representation and accelerated processing.
To further improve inference speed, we introduce an early exit mechanism, which
exploits the TM's AND-based clause evaluation to avoid unnecessary
computations. Building upon this, we propose a literal Reorder strategy
designed to maximize the likelihood of early exits. This strategy is applied
during a post-training, pre-inference stage through statistical analysis of all
literals and the corresponding actions of their associated Tsetlin Automata
(TA), introducing negligible runtime overhead. Experimental results using the
gem5 simulator with an ARM processor show that our optimized implementation
reduces inference time by up to 96.71% compared to the conventional
integer-based TM implementations while maintaining comparable code density.

</details>


### [203] [WARP-LUTs - Walsh-Assisted Relaxation for Probabilistic Look Up Tables](https://arxiv.org/abs/2510.15655)
*Lino Gerlach,Liv Våge,Thore Gerlach,Elliott Kauffman*

Main category: cs.LG

TL;DR: WARP-LUTs is a novel gradient-based method that learns logic gate combinations more efficiently than previous approaches like DLGNs, achieving faster convergence while maintaining accuracy.


<details>
  <summary>Details</summary>
Motivation: Existing multiplication-free models like DLGNs suffer from high computational cost during training and poor generalization to logic blocks with more inputs, creating a need for more efficient training methods.

Method: WARP-LUTs use Walsh-Assisted Relaxation for Probabilistic Look-Up Tables - a gradient-based approach that learns combinations of logic gates with substantially fewer trainable parameters.

Result: WARP-LUTs achieve significantly faster convergence on CIFAR-10 compared to DLGNs while maintaining comparable accuracy.

Conclusion: The approach shows potential for extension to higher-input logic blocks, enabling extremely efficient deployment on modern FPGAs for real-time science applications.

Abstract: Fast and efficient machine learning is of growing interest to the scientific
community and has spurred significant research into novel model architectures
and hardware-aware design. Recent hard? and software co-design approaches have
demonstrated impressive results with entirely multiplication-free models.
Differentiable Logic Gate Networks (DLGNs), for instance, provide a
gradient-based framework for learning optimal combinations of low-level logic
gates, setting state-of-the-art trade-offs between accuracy, resource usage,
and latency. However, these models suffer from high computational cost during
training and do not generalize well to logic blocks with more inputs. In this
work, we introduce Walsh-Assisted Relaxation for Probabilistic Look-Up Tables
(WARP-LUTs) - a novel gradient-based method that efficiently learns
combinations of logic gates with substantially fewer trainable parameters. We
demonstrate that WARP-LUTs achieve significantly faster convergence on CIFAR-10
compared to DLGNs, while maintaining comparable accuracy. Furthermore, our
approach suggests potential for extension to higher-input logic blocks,
motivating future research on extremely efficient deployment on modern FPGAs
and its real-time science applications.

</details>


### [204] [ProSh: Probabilistic Shielding for Model-free Reinforcement Learning](https://arxiv.org/abs/2510.15720)
*Edwin Hamel-De le Court,Gaspard Ohlmann,Francesco Belardinelli*

Main category: cs.LG

TL;DR: ProSh is a model-free safe RL algorithm that uses risk augmentation and shielding to guarantee safety under cost constraints while preserving optimality in deterministic environments.


<details>
  <summary>Details</summary>
Motivation: Safety is critical for deploying RL systems in real-world applications, requiring formal safety guarantees alongside optimal performance.

Method: Augments Constrained MDP state space with risk budget and applies shielding to policy distribution using learned cost critic to ensure sampled actions remain safe in expectation.

Result: Provides tight upper-bound on expected cost depending on backup-critic accuracy, with experiments showing safety guarantees even during training under practical assumptions.

Conclusion: ProSh enables safe reinforcement learning with formal safety guarantees while maintaining performance, making it suitable for real-world deployment.

Abstract: Safety is a major concern in reinforcement learning (RL): we aim at
developing RL systems that not only perform optimally, but are also safe to
deploy by providing formal guarantees about their safety. To this end, we
introduce Probabilistic Shielding via Risk Augmentation (ProSh), a model-free
algorithm for safe reinforcement learning under cost constraints. ProSh
augments the Constrained MDP state space with a risk budget and enforces safety
by applying a shield to the agent's policy distribution using a learned cost
critic. The shield ensures that all sampled actions remain safe in expectation.
We also show that optimality is preserved when the environment is
deterministic. Since ProSh is model-free, safety during training depends on the
knowledge we have acquired about the environment. We provide a tight
upper-bound on the cost in expectation, depending only on the backup-critic
accuracy, that is always satisfied during training. Under mild, practically
achievable assumptions, ProSh guarantees safety even at training time, as shown
in the experiments.

</details>


### [205] [RLAF: Reinforcement Learning from Automaton Feedback](https://arxiv.org/abs/2510.15728)
*Mahyar Alinejad,Alvaro Velasquez,Yue Wang,George Atia*

Main category: cs.LG

TL;DR: A novel RL approach using automaton-based preferences instead of explicit reward functions, with static and dynamic methods for policy optimization, outperforming traditional reward engineering and automaton baselines.


<details>
  <summary>Details</summary>
Motivation: Traditional RL struggles with complex history-dependent reward structures, requiring manual reward engineering. This work aims to eliminate the need for explicit reward specification by using automaton-based preferences.

Method: Uses deterministic finite automaton (DFA) to generate preferences over trajectories, learning reward functions automatically. Two approaches: static (direct policy optimization) and dynamic (iterative reward and policy refinement).

Result: Outperforms traditional reward engineering and automaton-based baselines (reward machines, LTL-guided methods) in discrete and continuous environments with temporal dependencies. Provides convergence guarantee for near-optimal policy learning.

Conclusion: Automaton-based preferences offer a scalable, efficient, human-independent alternative for handling non-Markovian rewards, with proven convergence guarantees for learning effective policies.

Abstract: Reinforcement Learning (RL) in environments with complex, history-dependent
reward structures poses significant challenges for traditional methods. In this
work, we introduce a novel approach that leverages automaton-based feedback to
guide the learning process, replacing explicit reward functions with
preferences derived from a deterministic finite automaton (DFA). Unlike
conventional approaches that use automata for direct reward specification, our
method employs the structure of the DFA to generate preferences over
trajectories that are used to learn a reward function, eliminating the need for
manual reward engineering. Our framework introduces a static approach that uses
the learned reward function directly for policy optimization and a dynamic
approach that involves continuous refining of the reward function and policy
through iterative updates until convergence.
  Our experiments in both discrete and continuous environments demonstrate that
our approach enables the RL agent to learn effective policies for tasks with
temporal dependencies, outperforming traditional reward engineering and
automaton-based baselines such as reward machines and LTL-guided methods. Our
results highlight the advantages of automaton-based preferences in handling
non-Markovian rewards, offering a scalable, efficient, and human-independent
alternative to traditional reward modeling. We also provide a convergence
guarantee showing that under standard assumptions our automaton-guided
preference-based framework learns a policy that is near-optimal with respect to
the true non-Markovian objective.

</details>


### [206] [Constrained Adversarial Perturbation](https://arxiv.org/abs/2510.15699)
*Virendra Nishad,Bhaskar Mukhoty,Hilal AlQuabeh,Sandeep K. Shukla,Sayak Ray Chowdhury*

Main category: cs.LG

TL;DR: The paper introduces CAP, a method for generating universal adversarial perturbations that respect domain-specific constraints, improving attack success rates and reducing runtime compared to existing methods.


<details>
  <summary>Details</summary>
Motivation: Existing universal adversarial perturbation methods ignore domain-specific constraints, making adversarial examples implausible or easily detectable in real-world applications like finance and network systems.

Method: Proposes Constrained Adversarial Perturbation (CAP) using augmented Lagrangian min-max optimization with gradient-based alternating optimization to enforce multiple complex constraints.

Result: CAP achieves higher attack success rates with significantly reduced runtime across finance, IT networks, and cyber-physical systems, and also performs well for individual adversarial perturbations.

Conclusion: CAP enables effective adversarial attacks in constrained feature spaces and provides a principled way to learn feature constraints from data for broad applicability.

Abstract: Deep neural networks have achieved remarkable success in a wide range of
classification tasks. However, they remain highly susceptible to adversarial
examples - inputs that are subtly perturbed to induce misclassification while
appearing unchanged to humans. Among various attack strategies, Universal
Adversarial Perturbations (UAPs) have emerged as a powerful tool for both
stress testing model robustness and facilitating scalable adversarial training.
Despite their effectiveness, most existing UAP methods neglect domain specific
constraints that govern feature relationships. Violating such constraints, such
as debt to income ratios in credit scoring or packet flow invariants in network
communication, can render adversarial examples implausible or easily
detectable, thereby limiting their real world applicability.
  In this work, we advance universal adversarial attacks to constrained feature
spaces by formulating an augmented Lagrangian based min max optimization
problem that enforces multiple, potentially complex constraints of varying
importance. We propose Constrained Adversarial Perturbation (CAP), an efficient
algorithm that solves this problem using a gradient based alternating
optimization strategy. We evaluate CAP across diverse domains including
finance, IT networks, and cyber physical systems, and demonstrate that it
achieves higher attack success rates while significantly reducing runtime
compared to existing baselines. Our approach also generalizes seamlessly to
individual adversarial perturbations, where we observe similar strong
performance gains. Finally, we introduce a principled procedure for learning
feature constraints directly from data, enabling broad applicability across
domains with structured input spaces.

</details>


### [207] [AB-UPT for Automotive and Aerospace Applications](https://arxiv.org/abs/2510.15808)
*Benedikt Alkin,Richard Kurle,Louis Serrano,Dennis Just,Johannes Brandstetter*

Main category: cs.LG

TL;DR: AB-UPT neural surrogates achieve near-perfect aerodynamic force predictions in seconds using simple geometry representations, requiring orders of magnitude less compute than traditional CFD solvers.


<details>
  <summary>Details</summary>
Motivation: To demonstrate AB-UPT's capabilities for automotive and aircraft CFD simulations with high-quality datasets and efficient neural surrogates that reduce computational requirements.

Method: Used AB-UPT (Anchored-Branched Universal Physics Transformers) with two new datasets (SHIFT-SUV automotive and SHIFT-Wing aircraft) generated via Luminary Cloud platform, comparing against transformer baselines.

Result: AB-UPT shows strong performance across both datasets, achieving near-perfect integrated aerodynamic force predictions within seconds from simple geometry representations, trainable in one day on a single GPU.

Conclusion: AB-UPT enables industry-scale applications by providing fast, accurate CFD simulations with dramatically reduced computational requirements compared to traditional numerical solvers.

Abstract: The recently proposed Anchored-Branched Universal Physics Transformers
(AB-UPT) shows strong capabilities to replicate automotive computational fluid
dynamics simulations requiring orders of magnitudes less compute than
traditional numerical solvers. In this technical report, we add two new
datasets to the body of empirically evaluated use-cases of AB-UPT, combining
high-quality data generation with state-of-the-art neural surrogates. Both
datasets were generated with the Luminary Cloud platform containing automotives
(SHIFT-SUV) and aircrafts (SHIFT-Wing). We start by detailing the data
generation. Next, we show favorable performances of AB-UPT against previous
state-of-the-art transformer-based baselines on both datasets, followed by
extensive qualitative and quantitative evaluations of our best AB-UPT model.
AB-UPT shows strong performances across the board. Notably, it obtains near
perfect prediction of integrated aerodynamic forces within seconds from a
simple isotopically tesselate geometry representation and is trainable within a
day on a single GPU, paving the way for industry-scale applications.

</details>


### [208] [A Comprehensive Evaluation of Graph Neural Networks and Physics Informed Learning for Surrogate Modelling of Finite Element Analysis](https://arxiv.org/abs/2510.15750)
*Nayan Kumar Singh*

Main category: cs.LG

TL;DR: This paper evaluates GNNs and 3D U-Nets as surrogates for FEA of parametric I-beams, finding that GNNs outperform U-Nets, with MPNN and Graph Transformers achieving highest accuracy. A PINN framework with curriculum learning stabilizes training and improves generalization.


<details>
  <summary>Details</summary>
Motivation: FEA is computationally expensive for design optimization, and deep learning models can provide efficient alternatives, but selecting the right architecture that accurately emulates FEA is challenging.

Method: Comprehensive evaluation of GNNs and 3D U-Nets with Physics-Informed Neural Network (PINN) framework governed by Navier-Cauchy equations, using curriculum learning strategy (pretraining on data followed by physics-informed fine-tuning).

Result: GNNs fundamentally outperform U-Nets. MPNN and Graph Transformers achieved highest accuracy (3.5% and 2.6% relative L2 error). PINN improved generalization, reducing error by up to 11.3%. Graph Transformer is most accurate but 37.5% slower than MPNN PINN.

Conclusion: PINN-enhanced MPNN (MPNN PINN) provides the most practical solution, offering the best compromise between predictive performance, model size, and inference speed.

Abstract: Although Finite Element Analysis (FEA) is an integral part of the product
design lifecycle, the analysis is computationally expensive, making it
unsuitable for many design optimization problems. The deep learning models can
be a great solution. However, selecting the architecture that emulates the FEA
with great accuracy is a challenge. This paper presents a comprehensive
evaluation of graph neural networks (GNNs) and 3D U-Nets as surrogates for FEA
of parametric I-beams. We introduce a Physics-Informed Neural Network (PINN)
framework, governed by the Navier Cauchy equations, to enforce physical laws.
Crucially, we demonstrate that a curriculum learning strategy, pretraining on
data followed by physics informed fine tuning, is essential for stabilizing
training. Our results show that GNNs fundamentally outperform the U-Net. Even
the worst performer among GNNs, the GCN framework, achieved a relative L2 error
of 8.7% while the best framework among U Net, U Net with attention mechanism
trained on high resolution data, achieved 13.0% score. Among the graph-based
architectures, the Message Passing Neural Networks (MPNN) and Graph
Transformers achieved the highest accuracy, achieving a relative L2 score of
3.5% and 2.6% respectively. The inclusion of physics fundamental laws (PINN)
significantly improved the generalization, reducing error by up to 11.3% on
high-signal tasks. While the Graph Transformer is the most accurate model, it
is more 37.5% slower during inference when compared to second best model, MPNN
PINN. The PINN enhanced MPNN (MPNN PINN) provides the most practical solution.
It offers a good compromise between predictive performance, model size, and
inference speed.

</details>


### [209] [Chronos-2: From Univariate to Universal Forecasting](https://arxiv.org/abs/2510.15821)
*Abdul Fatir Ansari,Oleksandr Shchur,Jaris Küken,Andreas Auer,Boran Han,Pedro Mercado,Syama Sundar Rangapuram,Huibin Shen,Lorenzo Stella,Xiyuan Zhang,Mononito Goswami,Shubham Kapoor,Danielle C. Maddix,Pablo Guerron,Tony Hu,Junming Yin,Nick Erickson,Prateek Mutalik Desai,Hao Wang,Huzefa Rangwala,George Karypis,Yuyang Wang,Michael Bohlke-Schneider*

Main category: cs.LG

TL;DR: Chronos-2 is a pretrained time series model that handles univariate, multivariate, and covariate-informed forecasting tasks in zero-shot manner using group attention mechanism for efficient in-context learning.


<details>
  <summary>Details</summary>
Motivation: Existing pretrained time series models focus mainly on univariate forecasting, limiting their real-world applicability where multivariate data and covariates are crucial.

Method: Uses group attention mechanism for in-context learning across multiple time series within groups, trained on synthetic datasets with diverse multivariate structures imposed on univariate series.

Result: Achieves state-of-the-art performance on three benchmarks (fev-bench, GIFT-Eval, Chronos Benchmark II), with substantial improvements in multivariate and covariate-informed forecasting, consistently outperforming baselines by wide margins.

Conclusion: Chronos-2's in-context learning capabilities establish it as a general-purpose forecasting model that can be used directly in real-world forecasting pipelines without task-specific training.

Abstract: Pretrained time series models have enabled inference-only forecasting systems
that produce accurate predictions without task-specific training. However,
existing approaches largely focus on univariate forecasting, limiting their
applicability in real-world scenarios where multivariate data and covariates
play a crucial role. We present Chronos-2, a pretrained model capable of
handling univariate, multivariate, and covariate-informed forecasting tasks in
a zero-shot manner. Chronos-2 employs a group attention mechanism that
facilitates in-context learning (ICL) through efficient information sharing
across multiple time series within a group, which may represent sets of related
series, variates of a multivariate series, or targets and covariates in a
forecasting task. These general capabilities are achieved through training on
synthetic datasets that impose diverse multivariate structures on univariate
series. Chronos-2 delivers state-of-the-art performance across three
comprehensive benchmarks: fev-bench, GIFT-Eval, and Chronos Benchmark II. On
fev-bench, which emphasizes multivariate and covariate-informed forecasting,
Chronos-2's universal ICL capabilities lead to substantial improvements over
existing models. On tasks involving covariates, it consistently outperforms
baselines by a wide margin. Case studies in the energy and retail domains
further highlight its practical advantages. The in-context learning
capabilities of Chronos-2 establish it as a general-purpose forecasting model
that can be used "as is" in real-world forecasting pipelines.

</details>


### [210] [SAMix: Calibrated and Accurate Continual Learning via Sphere-Adaptive Mixup and Neural Collapse](https://arxiv.org/abs/2510.15751)
*Trung-Anh Dang,Vincent Nguyen,Ngoc-Son Vu,Christel Vrain*

Main category: cs.LG

TL;DR: Proposes Sphere-Adaptive Mixup (SAMix), a novel adaptive mixup strategy for neural collapse-based continual learning that improves both performance and model calibration by reducing overconfidence and mitigating forgetting.


<details>
  <summary>Details</summary>
Motivation: Most continual learning methods focus on accuracy and forgetting mitigation but overlook network calibration, which is crucial for reliable predictions. Neural collapse has shown benefits in continual learning, but few works address calibration improvement.

Method: Introduces SAMix, an adaptive mixup strategy that adapts the mixing process to the geometric properties of feature spaces under neural collapse, ensuring robust regularization and alignment.

Result: Experiments show SAMix significantly boosts performance, surpassing state-of-the-art methods in continual learning while improving model calibration. It enhances both across-task accuracy and prediction reliability.

Conclusion: SAMix represents a promising advancement for robust continual learning systems by simultaneously improving performance and calibration through neural collapse-aware adaptive mixup.

Abstract: While most continual learning methods focus on mitigating forgetting and
improving accuracy, they often overlook the critical aspect of network
calibration, despite its importance. Neural collapse, a phenomenon where
last-layer features collapse to their class means, has demonstrated advantages
in continual learning by reducing feature-classifier misalignment. Few works
aim to improve the calibration of continual models for more reliable
predictions. Our work goes a step further by proposing a novel method that not
only enhances calibration but also improves performance by reducing
overconfidence, mitigating forgetting, and increasing accuracy. We introduce
Sphere-Adaptive Mixup (SAMix), an adaptive mixup strategy tailored for neural
collapse-based methods. SAMix adapts the mixing process to the geometric
properties of feature spaces under neural collapse, ensuring more robust
regularization and alignment. Experiments show that SAMix significantly boosts
performance, surpassing SOTA methods in continual learning while also improving
model calibration. SAMix enhances both across-task accuracy and the broader
reliability of predictions, making it a promising advancement for robust
continual learning systems.

</details>


### [211] [SNOO: Step-K Nesterov Outer Optimizer - The Surprising Effectiveness of Nesterov Momentum Applied to Pseudo-Gradients](https://arxiv.org/abs/2510.15830)
*Dominik Kallusky,Vinay Rao,Vishal Nandavanam,Hao-Jun Michael Shi*

Main category: cs.LG

TL;DR: The paper introduces SNOO, a Lookahead optimizer variant that applies Nesterov momentum to pseudo-gradients, achieving 1.5-2.5× compute gains in non-distributed training with minimal overhead.


<details>
  <summary>Details</summary>
Motivation: To understand why DiLoCo's distributed optimization approach works surprisingly well in non-distributed settings and to develop a more efficient optimization technique for large language models.

Method: Proposes Step-K Nesterov Outer Optimizer (SNOO), which applies Nesterov momentum to the pseudo-gradient generated from multiple inner optimizer steps on fast weights, within a two-loop Lookahead framework.

Result: SNOO achieves compute factor gains of 1.5-2.5× in non-distributed training up to 1e23 FLOPs scale, with improvements increasing with model size, while maintaining minimal compute/memory overhead and compatibility with model sharding.

Conclusion: SNOO is a practical enhancement for various inner optimizers that provides significant training efficiency improvements through Nesterov momentum applied to pseudo-gradients.

Abstract: The rapid development of large language models (LLMs) has driven the demand
for more efficient optimization techniques. Among these, the Lookahead family
of optimizers employs a two-loop framework, maintaining fast and slow sets of
model weights. Multiple inner optimizer steps on the fast weights produce a
trajectory - the pseudo-gradient - that is used to update the slow weights.
DiLoCo, a notable example originally designed for distributed training, applies
Nesterov momentum to the averaged pseudo-gradient from multiple workers,
claiming to even outperform AdamW in a non-distributed setup. In this paper, we
empirically show that DiLoCo's surprising effectiveness stems primarily from
applying Nesterov momentum to the pseudo-gradient, which improves training in a
non-distributed setting. We call this Lookahead variant the Step-$K$ Nesterov
Outer Optimizer (SNOO). We demonstrate that SNOO achieves compute factor gains
of 1.5 - 2.5$\times$ in a non-distributed setting up to a scale of 1e23
training FLOPs, with improvements that increase with model size. Because of its
minimal compute and memory overhead and compatibility with model sharding, SNOO
is a practical enhancement for a variety of inner optimizers, including AdamW
and Muon.

</details>


### [212] [Poultry Farm Intelligence: An Integrated Multi-Sensor AI Platform for Enhanced Welfare and Productivity](https://arxiv.org/abs/2510.15757)
*Pieris Panagi,Savvas Karatsiolis,Kyriacos Mosphilis,Nicholas Hadjisavvas,Andreas Kamilaris,Nicolas Nicolaou,Efstathios Stavrakis,Vassilis Vassiliades*

Main category: cs.LG

TL;DR: PoultryFI is a modular AI platform that integrates six modules for continuous monitoring, prediction, and optimization in poultry farming, achieving 100% egg-count accuracy and reliable forecasting.


<details>
  <summary>Details</summary>
Motivation: Small and medium-sized poultry farms lack affordable, integrated tools for continuous monitoring and decision-making, relying on manual inspections instead.

Method: Uses evolutionary algorithms for camera placement optimization, synchronized audio-visual monitoring, edge vision models for egg counting, forecasting models, and recommendation modules integrated with weather data.

Result: Field trials show 100% egg-count accuracy on Raspberry Pi 5, robust anomaly detection, and reliable short-term forecasting up to 10 days in advance.

Conclusion: PoultryFI bridges the gap between isolated pilot tools and scalable farm-wide intelligence, enabling proactive welfare and profitability management.

Abstract: Poultry farming faces increasing pressure to meet productivity targets while
ensuring animal welfare and environmental compliance. Yet many small and
medium-sized farms lack affordable, integrated tools for continuous monitoring
and decision-making, relying instead on manual, reactive inspections. This
paper presents Poultry Farm Intelligence (PoultryFI) - a modular,
cost-effective platform that integrates six AI-powered modules: Camera
Placement Optimizer, Audio-Visual Monitoring, Analytics & Alerting, Real-Time
Egg Counting, Production & Profitability Forecasting, and a Recommendation
Module.
  Camera layouts are first optimized offline using evolutionary algorithms for
full poultry house coverage with minimal hardware. The Audio-Visual Monitoring
module extracts welfare indicators from synchronized video, audio, and feeding
data. Analytics & Alerting produces daily summaries and real-time
notifications, while Real-Time Egg Counting uses an edge vision model to
automate production tracking. Forecasting models predict egg yield and feed
consumption up to 10 days in advance, and the Recommendation Module integrates
forecasts with weather data to guide environmental and operational adjustments.
  This is among the first systems to combine low-cost sensing, edge analytics,
and prescriptive AI to continuously monitor flocks, predict production, and
optimize performance. Field trials demonstrate 100% egg-count accuracy on
Raspberry Pi 5, robust anomaly detection, and reliable short-term forecasting.
PoultryFI bridges the gap between isolated pilot tools and scalable, farm-wide
intelligence, empowering producers to proactively safeguard welfare and
profitability.

</details>


### [213] [Self-Certifying Primal-Dual Optimization Proxies for Large-Scale Batch Economic Dispatch](https://arxiv.org/abs/2510.15850)
*Michael Klamkin,Mathieu Tanneau,Pascal Van Hentenryck*

Main category: cs.LG

TL;DR: Hybrid solver combining optimization proxies with classical solvers to guarantee optimality gaps while achieving significant speedups.


<details>
  <summary>Details</summary>
Motivation: Optimization proxies achieve good average performance but have high worst-case optimality gaps, making them untrustworthy for practical deployment.

Method: Proposes a hybrid solver using duality theory to bound optimality gaps, with fallback to classical solvers when certification fails. Also introduces combined primal-dual proxy training to improve speedup.

Result: Achieves speedups over 1000x compared to parallelized simplex solver while guaranteeing maximum 2% optimality gap on large-scale transmission systems.

Conclusion: Hybrid approach enables trustworthy deployment of optimization proxies with interpretable speed-optimality tradeoffs based on user-defined thresholds.

Abstract: Recent research has shown that optimization proxies can be trained to high
fidelity, achieving average optimality gaps under 1% for large-scale problems.
However, worst-case analyses show that there exist in-distribution queries that
result in orders of magnitude higher optimality gap, making it difficult to
trust the predictions in practice. This paper aims at striking a balance
between classical solvers and optimization proxies in order to enable
trustworthy deployments with interpretable speed-optimality tradeoffs based on
a user-defined optimality threshold. To this end, the paper proposes a hybrid
solver that leverages duality theory to efficiently bound the optimality gap of
predictions, falling back to a classical solver for queries where optimality
cannot be certified. To improve the achieved speedup of the hybrid solver, the
paper proposes an alternative training procedure that combines the primal and
dual proxy training. Experiments on large-scale transmission systems show that
the hybrid solver is highly scalable. The proposed hybrid solver achieves
speedups of over 1000x compared to a parallelized simplex-based solver while
guaranteeing a maximum optimality gap of 2%.

</details>


### [214] [Cavity Duplexer Tuning with 1d Resnet-like Neural Networks](https://arxiv.org/abs/2510.15796)
*Anton Raskovalov*

Main category: cs.LG

TL;DR: Machine learning method for tuning cavity duplexers with many adjustment screws using supervised learning with 1D ResNet-like architecture and S-parameter feature processing.


<details>
  <summary>Details</summary>
Motivation: To efficiently tune cavity duplexers with large numbers of adjustment screws, avoiding the limitations of conventional reinforcement learning approaches.

Method: Supervised learning setup with neural network architecture featuring 1D ResNet-like backbone and processing of S-parameter features including curve shape, peak positions and amplitudes, combined with external control algorithm.

Result: The system can achieve nearly tuned state of the duplexer within 4-5 rotations per screw.

Conclusion: The proposed supervised learning approach with specialized neural network architecture is effective for cavity duplexer tuning, outperforming conventional reinforcement learning methods.

Abstract: This paper presents machine learning method for tuning of cavity duplexer
with a large amount of adjustment screws. After testing we declined
conventional reinforcement learning approach and reformulated our task in the
supervised learning setup. The suggested neural network architecture includes
1d ResNet-like backbone and processing of some additional information about
S-parameters, like the shape of curve and peaks positions and amplitudes. This
neural network with external control algorithm is capable to reach almost the
tuned state of the duplexer within 4-5 rotations per screw.

</details>


### [215] [FIDDLE: Reinforcement Learning for Quantum Fidelity Enhancement](https://arxiv.org/abs/2510.15833)
*Hoang M. Ngo,Tamer Kahveci,My T. Thai*

Main category: cs.LG

TL;DR: FIDDLE is a learning framework that directly maximizes quantum circuit process fidelity during routing using Gaussian Process-based fidelity estimation and reinforcement learning for optimization.


<details>
  <summary>Details</summary>
Motivation: Current quantum devices suffer from noise that reduces reliability, and traditional transpilation methods use indirect metrics like circuit depth rather than directly optimizing process fidelity.

Method: Two-module framework: 1) Gaussian Process surrogate model for process fidelity estimation with limited samples, 2) Reinforcement learning module for routing optimization.

Result: FIDDLE provides better fidelity estimation than existing techniques and significantly improves process fidelity across various noise models compared to state-of-the-art methods.

Conclusion: Directly optimizing process fidelity during routing outperforms traditional indirect approaches, enabling more reliable quantum circuits on noisy devices.

Abstract: Quantum computing has the potential to revolutionize fields like quantum
optimization and quantum machine learning. However, current quantum devices are
hindered by noise, reducing their reliability. A key challenge in gate-based
quantum computing is improving the reliability of quantum circuits, measured by
process fidelity, during the transpilation process, particularly in the routing
stage. In this paper, we address the Fidelity Maximization in Routing Stage
(FMRS) problem by introducing FIDDLE, a novel learning framework comprising two
modules: a Gaussian Process-based surrogate model to estimate process fidelity
with limited training samples and a reinforcement learning module to optimize
routing. Our approach is the first to directly maximize process fidelity,
outperforming traditional methods that rely on indirect metrics such as circuit
depth or gate count. We rigorously evaluate FIDDLE by comparing it with
state-of-the-art fidelity estimation techniques and routing optimization
methods. The results demonstrate that our proposed surrogate model is able to
provide a better estimation on the process fidelity compared to existing
learning techniques, and our end-to-end framework significantly improves the
process fidelity of quantum circuits across various noise models.

</details>


### [216] [Transfer Orthology Networks](https://arxiv.org/abs/2510.15837)
*Vikash Singh*

Main category: cs.LG

TL;DR: TRON is a neural network architecture for cross-species transfer learning that uses orthologous relationships to guide knowledge transfer between species through a species conversion layer.


<details>
  <summary>Details</summary>
Motivation: To enable effective cross-species transfer learning by leveraging biological orthology relationships, allowing better utilization of available transcriptomic data across different species.

Method: Uses a bipartite graph of orthologous relationships between species, with a learned species conversion layer whose weights are masked by the biadjacency matrix, prepended to a pre-trained feedforward neural network for phenotype prediction.

Result: The architecture allows efficient knowledge transfer by learning linear transformations that map gene expression from source to target species, with learned weights providing interpretable insights into functional orthology.

Conclusion: TRON provides a biologically grounded and interpretable approach to cross-species transfer learning, with ongoing work to collect experimental validation data.

Abstract: We present Transfer Orthology Networks (TRON), a novel neural network
architecture designed for cross-species transfer learning. TRON leverages
orthologous relationships, represented as a bipartite graph between species, to
guide knowledge transfer. Specifically, we prepend a learned species conversion
layer, whose weights are masked by the biadjacency matrix of this bipartite
graph, to a pre-trained feedforward neural network that predicts a phenotype
from gene expression data in a source species. This allows for efficient
transfer of knowledge to a target species by learning a linear transformation
that maps gene expression from the source to the target species' gene space.
The learned weights of this conversion layer offer a potential avenue for
interpreting functional orthology, providing insights into how genes across
species contribute to the phenotype of interest. TRON offers a biologically
grounded and interpretable approach to cross-species transfer learning, paving
the way for more effective utilization of available transcriptomic data. We are
in the process of collecting cross-species transcriptomic/phenotypic data to
gain experimental validation of the TRON architecture.

</details>


### [217] [Learning Correlated Reward Models: Statistical Barriers and Opportunities](https://arxiv.org/abs/2510.15839)
*Yeshwanth Cherapanamjeri,Constantinos Daskalakis,Gabriele Farina,Sobhan Mohammadpour*

Main category: cs.LG

TL;DR: The paper shows that pairwise preference data is insufficient for learning correlated probit models, but best-of-three preference data enables efficient estimation of correlated utilities with improved personalization.


<details>
  <summary>Details</summary>
Motivation: Random Utility Models (RUMs) are crucial for RLHF but suffer from the Independence of Irrelevant Alternatives (IIA) assumption, which oversimplifies human preferences. Current methods lack guarantees for models that avoid IIA.

Method: The authors investigate correlated probit models and prove that pairwise preference data cannot capture correlations. They propose using best-of-three preference data and develop an efficient estimator for this setting.

Result: Theoretical analysis shows best-of-three data enables statistically and computationally efficient estimation with near-optimal performance. Experiments on real-world datasets demonstrate improved personalization of human preferences.

Conclusion: Higher-order preference data (specifically best-of-three) is essential for learning correlated utilities, overcoming limitations of pairwise comparisons and enabling more accurate modeling of diverse human preferences.

Abstract: Random Utility Models (RUMs) are a classical framework for modeling user
preferences and play a key role in reward modeling for Reinforcement Learning
from Human Feedback (RLHF). However, a crucial shortcoming of many of these
techniques is the Independence of Irrelevant Alternatives (IIA) assumption,
which collapses \emph{all} human preferences to a universal underlying utility
function, yielding a coarse approximation of the range of human preferences. On
the other hand, statistical and computational guarantees for models avoiding
this assumption are scarce. In this paper, we investigate the statistical and
computational challenges of learning a \emph{correlated} probit model, a
fundamental RUM that avoids the IIA assumption. First, we establish that the
classical data collection paradigm of pairwise preference data is
\emph{fundamentally insufficient} to learn correlational information,
explaining the lack of statistical and computational guarantees in this
setting. Next, we demonstrate that \emph{best-of-three} preference data
provably overcomes these shortcomings, and devise a statistically and
computationally efficient estimator with near-optimal performance. These
results highlight the benefits of higher-order preference data in learning
correlated utilities, allowing for more fine-grained modeling of human
preferences. Finally, we validate these theoretical guarantees on several
real-world datasets, demonstrating improved personalization of human
preferences.

</details>


<div id='cs.IT'></div>

# cs.IT [[Back]](#toc)

### [218] [Adaptive Base Representation Theorem: An Alternative to Binary Number System](https://arxiv.org/abs/2510.15099)
*Ravin Kumar*

Main category: cs.IT

TL;DR: The paper introduces the Adaptive Base Representation (ABR) Theorem and a novel number system that serves as a structured alternative to binary, offering unique decimal representations with the same bit length as binary encoding while maintaining compatibility with existing compression and error correction methods.


<details>
  <summary>Details</summary>
Motivation: To provide a structured alternative to the binary number system for digital computers that maintains the same bit efficiency while offering unique decimal representations and compatibility with existing digital infrastructure.

Method: Proposes the ABR number system based on the ABR Theorem, with theoretical foundations and mathematical formulations demonstrating its ability to encode the same integer range as binary using the same number of bits.

Result: The ABR number system can uniquely represent decimal numbers using the same number of bits as binary encoding while maintaining compatibility with data compression algorithms (Huffman coding, arithmetic coding) and error detection/correction mechanisms (Hamming codes).

Conclusion: The ABR number system presents a viable alternative to binary that could inspire new approaches in digital data representation, computational design, and has practical applications in areas like digital steganography.

Abstract: This paper introduces the Adaptive Base Representation (ABR) Theorem and
proposes a novel number system that offers a structured alternative to the
binary number system for digital computers. The ABR number system enables each
decimal number to be represented uniquely and using the same number of bits,
$n$, as the binary encoding. Theoretical foundations and mathematical
formulations demonstrate that ABR can encode the same integer range as binary,
validating its potential as a viable alternative. Additionally, the ABR number
system is compatible with existing data compression algorithms like Huffman
coding and arithmetic coding, as well as error detection and correction
mechanisms such as Hamming codes. We further explore practical applications,
including digital steganography, to illustrate the utility of ABR in
information theory and digital encoding, suggesting that the ABR number system
could inspire new approaches in digital data representation and computational
design.

</details>


### [219] [Outage-Aware Sum Rate Maximization in Movable Antennas-Enabled Systems](https://arxiv.org/abs/2510.15292)
*Guojie Hu,Qingqing Wu,Ming-Min Zhao,Wen Chen,Zhenyu Xiao,Kui Xu,Jiangbo Si*

Main category: cs.IT

TL;DR: This paper proposes movable antennas (MAs) for MISO systems to maximize outage-aware sum rate using only statistical CSI, avoiding training latency in delay-sensitive scenarios.


<details>
  <summary>Details</summary>
Motivation: To address delay-sensitive communication scenarios where periodic channel training causes additional latency, and to leverage the flexibility of movable antennas for improved performance without requiring instantaneous CSI.

Method: Joint optimization of antenna positions and transmit beamforming using statistical CSI-based zero-forcing beamforming, Laguerre series approximation for SINR CDF derivation, and projected gradient ascent for antenna position optimization.

Result: Numerical results show the proposed MA scheme outperforms conventional fixed-position antennas and other benchmarks in outage-aware sum rate performance.

Conclusion: Movable antennas with statistical CSI can effectively improve system performance in delay-sensitive scenarios while avoiding training latency, with the proposed optimization framework achieving significant gains over traditional approaches.

Abstract: In this paper, we investigate the movable antennas (MAs)-enabled
multiple-input-single-output (MISO) systems, where the base station (BS)
equipped with multiple MAs serves multiple single-antenna user. The
delay-sensitive scenario is considered, where users refrain from periodically
sending training signals to the BS for channel estimations to avoid additional
latency. As a result, the BS relies solely on the statistical channel state
information (CSI) to transmit data with a fixed rate. Under this setup, we aim
to maximize the outage-aware sum rate of all users, by jointly optimizing
antenna positions and the transmit beamforming at the BS, while satisfying the
given target outage probability requirement at each user. The problem is highly
non-convex, primarily because the exact cumulative distribution function (CDF)
of the received signal-to-interference-plus-noise ratio (SINR) of each user is
difficult to derive. To simplify analysis and without comprising performance,
we adopt the statistical CSI based zero-forcing beamforming design. We then
introduce one important lemma to derive the tight mean and variance of the
SINR. Leveraging these results, we further exploit the Laguerre series
approximation to successfully derive the closedform and tight CDF of the SINR.
Subsequently, the outageaware sum rate expression is presented but still
includes complex structure with respect to antenna positions. Facing this
challenge, the projected gradient ascent (PGA) method is developed to
iteratively update antenna positions until convergence. Numerical results
demonstrate the effectiveness of our proposed schemes compared to conventional
fixed-position antenna (FPA) and other competitive benchmarks.

</details>


### [220] [Rotatable Antenna Meets UAV: Towards Dual-Level Channel Reconfiguration Paradigm for ISAC](https://arxiv.org/abs/2510.15295)
*Shiying Chen,Guangji Chen,Long Shi,Qingqing Wu,Kang Wei*

Main category: cs.IT

TL;DR: Proposes a dual-level channel reconfiguration framework for ISAC using rotatable antennas on UAVs to optimize trade-off between sensing and communication performance.


<details>
  <summary>Details</summary>
Motivation: ISAC faces challenges in balancing sensing and communication due to shared wireless resources. Need flexible control over both functionalities.

Method: Deploy rotatable antennas on UAV to control path loss and channel correlation. Joint optimization of RA rotation, beamforming, and UAV trajectory with sensing constraints.

Result: Derived closed-form solutions for static UAV case. Proved HFH structure for mobile UAV trajectory. Simulation shows significant improvement in S&C trade-off region.

Conclusion: The proposed dual-level channel reconfiguration effectively balances sensing and communication performance in ISAC systems.

Abstract: Integrated sensing and communication (ISAC) is viewed as a key enabler for
future wireless networks by sharing the hardware and wireless resources between
the functionalities of sensing and communication (S&C). Due to the shared
wireless resources for both S&C, it is challenging to achieve a critical
trade-off between these two integrated functionalities. To address this issue,
this paper proposes a novel dual-level channel reconfiguration framework for
ISAC by deploying rotatable antennas at an unmanned aerial vehicle (UAV), where
both the large-scale path loss and the correlation of S&C channels can be
proactively controlled, thereby allowing a flexible trade-off between S&C
performance. To characterize the S&C tradeoff, we aim to maximize the
communication rate by jointly optimizing the RA rotation, the transmit
beamforming, and the UAV trajectory, subject to the given requirement of
sensing performance. For the typical scenario of static UAV deployment, we
introduce the concept of subspace correlation coefficient to derive closed-form
solutions for the optimal RA rotation, transmit beamforming, and UAV hovering
location. For the scenario of a fully mobile UAV, we prove that the optimal
trajectory of a UAV follows a hover-fly-hover (HFH) structure, thereby
obtaining its global optimal solution. Simulation results show that the
proposed design significantly improves the achievable S&C trade-off region
compared to benchmark schemes.

</details>


### [221] [Subverting Flexible Multiuser Communications via Movable Antenna-Enabled Jammer](https://arxiv.org/abs/2510.15298)
*Guojie Hu,Qingqing Wu,Lipeng Zhu,Kui Xu,Guoxin Li,Jiangbo Si,Jian Ouyang,Tong-Xing Zheng*

Main category: cs.IT

TL;DR: This paper proposes using movable antenna (MA) technology in legitimate jammers to disrupt suspicious multiuser communications by optimizing antenna positions and jamming beamforming to minimize the benefit of suspicious transmissions.


<details>
  <summary>Details</summary>
Motivation: Movable antenna technology provides additional spatial degrees of freedom that can be exploited for security purposes to disrupt suspicious wireless communications, offering advantages over conventional fixed-position antennas.

Method: The authors develop alternating optimization algorithms to jointly optimize antenna positions and jamming beamforming at the MAJ, considering the reactive power allocation adjustments by the suspicious transmitter. They analyze the ST's optimal behavior and solve simplified problems accordingly.

Result: Numerical results demonstrate that the proposed MA-enabled legitimate jammer schemes outperform conventional fixed-position antenna systems and other benchmarks in disrupting suspicious communications.

Conclusion: Movable antenna technology significantly enhances the effectiveness of legitimate jammers for security applications, providing better performance than traditional fixed antennas through optimized spatial positioning and beamforming strategies.

Abstract: Movable antenna (MA) is an emerging technology which can reconfigure wireless
channels via adaptive antenna position adjustments at transceivers, thereby
bringing additional spatial degrees of freedom for improving system
performance. In this paper, from a security perspective, we exploit the
MAenabled legitimate jammer (MAJ) to subvert suspicious multiuser downlink
communications consisting of one suspicious transmitter (ST) and multiple
suspicious receivers (SRs). Specifically, our objective is to minimize the
benefit (the sum rate of all SRs or the minimum rate among all SRs) of such
suspicious communications, by jointly optimizing antenna positions and the
jamming beamforming at the MAJ. However, the key challenge lies in that given
the MAJ's actions, the ST can reactively adjust its power allocations to
instead maximize its benefit for mitigating the unfavorable interference. Such
flexible behavior of the ST confuses the optimization design of the MAJ to a
certain extent. Facing this difficulty, corresponding to the above two
different benefits: i) we respectively determine the optimal behavior of the ST
given the MAJ's actions; ii) armed with these, we arrive at two simplified
problems and then develop effective alternating optimization based algorithms
to iteratively solve them. In addition to these, we also focus on the special
case of two SRs, and reveal insightful conclusions about the deployment rule of
antenna positions at the MAJ. Furthermore, we analyze the ideal antenna
deployment scheme at the MAJ for achieving the globally performance lower
bound. Numerical results demonstrate the effectiveness of our proposed schemes
compared to conventional fixed-position antenna (FPA) and other competitive
benchmarks.

</details>


### [222] [ProxySelect: Frequency Selectivity-Aware Scheduling for Joint OFDMA and MU-MIMO in 802.11ax WiFi](https://arxiv.org/abs/2510.15452)
*Xiang Zhang,Michail Palaiologos,Christian Bluemm,Giuseppe Caire*

Main category: cs.IT

TL;DR: ProxySelect is a scalable user scheduling algorithm for WiFi 802.11ax that addresses frequency selectivity challenges in joint OFDMA and MU-MIMO systems using proxy rates and sampling-based candidate generation.


<details>
  <summary>Details</summary>
Motivation: WiFi 802.11ax introduces OFDMA to support concurrent transmissions, but increased frequency selectivity makes conventional narrowband user selection algorithms inefficient as optimal user sets vary across frequency subbands.

Method: Formulate scheduling as an integer linear program with binary variables for user-RU associations. Introduce proxy rates to approximate ZFBF rates without matrix inversion, and use sampling-based candidate group generation to select near-orthogonal user groups.

Result: Simulations using realistic ray-tracing channel models show ProxySelect achieves near-optimal rate performance with significantly lower complexity compared to conventional approaches.

Conclusion: ProxySelect provides an efficient solution for frequency selectivity-aware user scheduling in 802.11ax WiFi systems, balancing performance and computational complexity through proxy rates and bounded ILP formulation.

Abstract: IEEE 802.11ax introduces orthogonal frequency division multiple access
(OFDMA) to WiFi to support concurrent transmissions to a larger number of
users. As bandwidth continues to grow, WiFi channels exhibit increased
frequency selectivity, which poses new challenges for MU-MIMO user selection:
the optimal user set varies across frequency and is interleaved over subbands
(called resource units, or RUs). This frequency selectivity, coupled with the
complex subband allocation pattern, renders conventional narrowband user
selection algorithms inefficient for 802.11ax. In this paper, we propose
\emph{ProxySelect}, a scalable and frequency selectivity-aware user scheduling
algorithm for joint OFDMA and MU-MIMO usage in 802.11ax under zero-forcing
beamforming (ZFBF). The scheduling task is formulated as an integer linear
program (ILP) with binary variables indicating user (group)-RU associations,
and linear constraints ensuring standard compatibility. To reduce complexity,
we introduce a novel proxy rate--a function of individual channel strengths and
their correlations--that approximates the ZFBF rate without requiring
cubic-complexity matrix inversion. Additionally, we develop a sampling-based
candidate group generation scheme that selects up to $T$ near-orthogonal user
groups for each RU, thereby bounding the ILP size and ensuring scalability.
Simulations using realistic ray-tracing-based channel models show that
ProxySelect achieves near-optimal rate performance with significantly lower
complexity.

</details>


### [223] [Near-Field Imaging by Exploiting Frequency Correlation in Wireless Communication Networks](https://arxiv.org/abs/2510.15459)
*Tianyu Yang,Kangda Zhi,Shuangyang Li,Giuseppe Caire*

Main category: cs.IT

TL;DR: This paper proposes a sparse Bayesian learning approach for near-field imaging in wideband wireless networks, leveraging frequency-domain image correlation and designing optimal illumination patterns.


<details>
  <summary>Details</summary>
Motivation: To address near-field imaging challenges in wideband wireless communication networks by exploiting both near-field channel characteristics and image correlation across frequencies.

Method: Formulates image recovery as a multiple measurement vector compressed sensing problem with varying sensing matrices across frequencies. Uses sparse Bayesian learning to simultaneously estimate image coefficients and their correlation. Designs two illumination patterns: one minimizing sensing matrix coherence and another maximizing illumination power.

Result: Numerical results demonstrate the effectiveness of the proposed SBL-based method and show superiority of the illumination designs in improving imaging performance.

Conclusion: The proposed SBL-based solution combined with optimized illumination patterns provides an effective approach for near-field imaging in wideband wireless networks, successfully handling frequency-varying sensing matrices and correlated image coefficients.

Abstract: In this work, we address the near-field imaging under a wideband wireless
communication network by exploiting both the near-field channel of a uniform
linear array (ULA) and the image correlation in the frequency domain. We first
formulate the image recovery as a special multiple measurement vector (MMV)
compressed sensing (CS) problem, where at various frequencies the sensing
matrices can be different, and the image coefficients are correlated. To solve
such an MMV problem with various sensing matrices and correlated coefficients,
we propose a sparse Bayesian learning (SBL)-based solution to simultaneously
estimate all image coefficients and their correlation on multiple frequencies.
Moreover, to enhance estimation performance, we design two illumination
patterns following two different criteria. From the CS perspective, the first
design minimizes the total coherence of the sensing matrix to increase the
mutual orthogonality of the basis vectors. Alternatively, to improve SNR, the
second design maximizes the illumination power of the imaging area. Numerical
results demonstrate the effectiveness of the proposed SBL-based method and the
superiority of the illumination designs.

</details>


### [224] [New generalizations of circular complex fuzzy sets and Gaussian weighted aggregation operators](https://arxiv.org/abs/2510.15605)
*Yelda Gülfırat,Mehmet Ünver*

Main category: cs.IT

TL;DR: The paper introduces circular complex q-rung orthopair fuzzy sets (CCq-ROFS) as a unified framework that generalizes existing circular complex intuitionistic fuzzy sets and complex q-rung orthopair fuzzy sets, with Gaussian-based aggregation operators for uncertainty representation.


<details>
  <summary>Details</summary>
Motivation: To create a unified framework that generalizes existing circular complex fuzzy set structures and provides a smoother, statistically meaningful representation of uncertainty using Gaussian-based approaches.

Method: Extends Gaussian-based framework to CCq-ROFSs, constructs new Gaussian-based aggregation operators using Gaussian triangular norm and conorm, and formulates Gaussian-weighted arithmetic and geometric aggregation operators.

Result: Developed a comprehensive framework that unifies CCIFSs and complex q-ROFSs, with specific cases for q=2 (circular complex Pythagorean fuzzy sets) and q=3 (circular complex Fermatean fuzzy sets).

Conclusion: The proposed CCq-ROFS framework successfully generalizes existing fuzzy set structures and provides enhanced Gaussian-based aggregation operators for improved uncertainty modeling and decision-making applications.

Abstract: In this paper, we introduce the concept of the circular complex $q$-rung
orthopair fuzzy set (CC$q$-ROFS) as a novel generalization that unifies the
existing frameworks of circular complex intuitionistic fuzzy sets (CCIFSs) and
complex $q$-rung orthopair fuzzy sets. If $q = 2$, the structure is referred to
as a circular complex Pythagorean fuzzy set, and if $q = 3$, it is called a
circular complex Fermatean fuzzy set. The proposed approach extends the
Gaussian-based framework to the CC$q$-ROFSs, aiming to achieve a smoother and
statistically meaningful representation of uncertainty. Within this setting,
new Gaussian-based aggregation operators for CC$q$-ROFSs are constructed by
employing the Gaussian triangular norm and conorm. Furthermore,
Gaussian-weighted arithmetic and Gaussian-weighted geometric aggregation
operators are formulated to enable consistent integration of membership and
non-membership information for fuzzy modeling and decision-making.

</details>


### [225] [Beyond-Diagonal RIS Under Non-Idealities: Learning-Based Architecture Discovery and Optimization](https://arxiv.org/abs/2510.15701)
*Binggui Zhou,Bruno Clerckx*

Main category: cs.IT

TL;DR: Proposes a learning-based framework (LTTADF) to discover optimal architectures for non-ideal beyond-diagonal reconfigurable intelligent surfaces (BD-RIS) that balance performance and circuit complexity tradeoffs.


<details>
  <summary>Details</summary>
Motivation: Traditional BD-RIS design faces challenges with non-idealities and circuit complexity tradeoffs. Architecture discovery for non-ideal BD-RIS is unexplored, making it difficult to achieve optimal performance-complexity balance in real-world deployments.

Method: Developed a two-tier learning framework (LTTADF) with an architecture generator and performance optimizer to jointly discover optimal non-ideal BD-RIS architectures given specific circuit complexities, enabling efficient exploration of large architecture spaces while avoiding poor local optima.

Result: The framework achieves near-optimal solutions for performance optimization and provides valuable insights for deploying non-ideal BD-RIS considering the performance-circuit complexity tradeoff.

Conclusion: LTTADF effectively addresses the challenges of architecture discovery for non-ideal BD-RIS, enabling practical deployment with optimized performance-complexity tradeoffs in next-generation wireless networks.

Abstract: Beyond-diagonal reconfigurable intelligent surface (BD-RIS) has recently been
introduced to enable advanced control over electromagnetic waves to further
increase the benefits of traditional RIS in enhancing signal quality and
improving spectral and energy efficiency for next-generation wireless networks.
A significant issue in designing and deploying BD-RIS is the tradeoff between
its performance and circuit complexity. Despite some efforts in exploring
optimal architectures with the lowest circuit complexities for ideal BD-RIS,
architecture discovery for non-ideal BD-RIS remains uninvestigated. Therefore,
how non-idealities and circuit complexity jointly affect the performance of
BD-RIS remains unclear, making it difficult to achieve the performance -
circuit complexity tradeoff in the presence of non-idealities. Essentially,
architecture discovery for non-ideal BD-RIS faces challenges from both the
computational complexity of global architecture search and the difficulty in
achieving global optima. To tackle these challenges, we propose a
learning-based two-tier architecture discovery framework (LTTADF) consisting of
an architecture generator and a performance optimizer to jointly discover
optimal architectures of non-ideal BD-RIS given specific circuit complexities,
which can effectively explore over a large architecture space while avoiding
getting trapped in poor local optima and thus achieving near-optimal solutions
for the performance optimization. Numerical results provide valuable insights
for deploying non-ideal BD-RIS considering the performance - circuit complexity
tradeoff.

</details>


<div id='eess.IV'></div>

# eess.IV [[Back]](#toc)

### [226] [Symmetric Entropy-Constrained Video Coding for Machines](https://arxiv.org/abs/2510.15347)
*Yuxiao Sun,Yao Zhao,Meiqin Liu,Chao Yao,Jian Jin,Weisi Lin*

Main category: eess.IV

TL;DR: SEC-VCM is a symmetric entropy-constrained video coding framework for machines that aligns video codec with visual backbones to preserve semantics and discard irrelevant information for machine vision systems, achieving significant bitrate savings across multiple video understanding tasks.


<details>
  <summary>Details</summary>
Motivation: Existing VCM methods bind codecs to specific downstream models, requiring retraining and limiting generalization in multi-task scenarios. Current unified frameworks using visual backbones/foundation models mainly maintain semantic consistency but don't directly link video coding with understanding under VB/VFM guidance.

Method: Proposes SEC-VCM with symmetric alignment between video codec and VB, using bi-directional entropy-constraint (BiEC) mechanism to ensure symmetry between video decoding and VB encoding by suppressing conditional entropy. Also includes semantic-pixel dual-path fusion (SPDF) module to inject pixel-level priors into reconstruction.

Result: Achieves state-of-the-art rate-task performance with significant bitrate savings over VTM: 37.41% on video instance segmentation, 29.83% on video object segmentation, 46.22% on object detection, and 44.94% on multiple object tracking.

Conclusion: SEC-VCM effectively establishes direct linkage between video coding and understanding under VB guidance, enabling the codec to leverage VB's representation capabilities for preserving semantics while discarding MVS-irrelevant information, with superior performance across multiple machine vision tasks.

Abstract: As video transmission increasingly serves machine vision systems (MVS)
instead of human vision systems (HVS), video coding for machines (VCM) has
become a critical research topic. Existing VCM methods often bind codecs to
specific downstream models, requiring retraining or supervised data and thus
limiting generalization in multi-task scenarios. Recently, unified VCM
frameworks have employed visual backbones (VB) and visual foundation models
(VFM) to support multiple video understanding tasks with a single codec. They
mainly utilize VB/VFM to maintain semantic consistency or suppress non-semantic
information, but seldom explore how to directly link video coding with
understanding under VB/VFM guidance. Hence, we propose a Symmetric
Entropy-Constrained Video Coding framework for Machines (SEC-VCM). It
establishes a symmetric alignment between the video codec and VB, allowing the
codec to leverage VB's representation capabilities to preserve semantics and
discard MVS-irrelevant information. Specifically, a bi-directional
entropy-constraint (BiEC) mechanism ensures symmetry between the process of
video decoding and VB encoding by suppressing conditional entropy. This helps
the codec to explicitly handle semantic information beneficial for MVS while
squeezing useless information. Furthermore, a semantic-pixel dual-path fusion
(SPDF) module injects pixel-level priors into the final reconstruction. Through
semantic-pixel fusion, it suppresses artifacts harmful to MVS and improves
machine-oriented reconstruction quality. Experimental results show our
framework achieves state-of-the-art (SOTA) in rate-task performance, with
significant bitrate savings over VTM on video instance segmentation (37.41%),
video object segmentation (29.83%), object detection (46.22%), and multiple
object tracking (44.94%). We will release our code.

</details>


### [227] [Confidence-Weighted Semi-Supervised Learning for Skin Lesion Segmentation Using Hybrid CNN-Transformer Networks](https://arxiv.org/abs/2510.15354)
*Saqib Qamar*

Main category: eess.IV

TL;DR: MIRA-U is a semi-supervised framework for skin lesion segmentation that combines uncertainty-aware teacher-student pseudo-labeling with a hybrid CNN-Transformer architecture, achieving state-of-the-art performance with limited labeled data.


<details>
  <summary>Details</summary>
Motivation: Automated skin lesion segmentation is crucial for early skin cancer detection but faces challenges due to limited annotated training data, requiring effective semi-supervised approaches.

Method: Uses uncertainty-aware teacher-student pseudo-labeling with a hybrid CNN-Transformer architecture, featuring a teacher network pre-trained via masked image modeling and a U-shaped student network with cross-attention skip connections.

Result: Achieves superior performance with Dice Similarity Coefficient of 0.9153 and Intersection over Union of 0.8552 using only 50% labeled data on ISIC-2016 and PH2 datasets.

Conclusion: The proposed framework effectively addresses limited annotation challenges in skin lesion segmentation through improved pseudo-label quality and boundary delineation, outperforming existing methods in low-annotation scenarios.

Abstract: Automated skin lesion segmentation through dermoscopic analysis is essential
for early skin cancer detection, yet remains challenging due to limited
annotated training data. We present MIRA-U, a semi-supervised framework that
combines uncertainty-aware teacher-student pseudo-labeling with a hybrid
CNN-Transformer architecture. Our approach employs a teacher network
pre-trained via masked image modeling to generate confidence-weighted soft
pseudo-labels, which guide a U-shaped CNN-Transformer student network featuring
cross-attention skip connections. This design enhances pseudo-label quality and
boundary delineation, surpassing reconstruction-based and CNN-only baselines,
particularly in low-annotation regimes. Extensive evaluation on ISIC-2016 and
PH2 datasets demonstrates superior performance, achieving a Dice Similarity
Coefficient (DSC) of 0.9153 and Intersection over Union (IoU) of 0.8552 using
only 50% labeled data. Code is publicly available on GitHub.

</details>


### [228] [A Cross-Framework Study of Temporal Information Buffering Strategies for Learned Video Compression](https://arxiv.org/abs/2510.15426)
*Kuan-Wei Ho,Yi-Hsin Chen,Martin Benjak,Jörn Ostermann,Wen-Hsiao Peng*

Main category: eess.IV

TL;DR: Systematic evaluation of temporal buffering strategies (explicit, implicit, hybrid) across four inter-frame coding frameworks for learned video compression.


<details>
  <summary>Details</summary>
Motivation: Lack of comprehensive studies comparing all combinations of inter-frame coding frameworks and temporal propagation strategies in learned video codecs.

Method: Systematic evaluation of explicit, implicit, and hybrid buffering across four inter-frame coding frameworks (residual coding, conditional coding, conditional residual coding, masked conditional residual coding) under unified experimental setup.

Result: Provides thorough understanding of effectiveness of different temporal buffering strategies across various inter-frame coding frameworks.

Conclusion: Comprehensive analysis reveals the performance impact of temporal propagation methods on different learned video compression frameworks.

Abstract: Recent advances in learned video codecs have demonstrated remarkable
compression efficiency. Two fundamental design aspects are critical: the choice
of inter-frame coding framework and the temporal information propagation
strategy. Inter-frame coding frameworks include residual coding, conditional
coding, conditional residual coding, and masked conditional residual coding,
each with distinct mechanisms for utilizing temporal predictions. Temporal
propagation methods can be categorized as explicit, implicit, or hybrid
buffering, differing in how past decoded information is stored and used.
However, a comprehensive study covering all possible combinations is still
lacking. This work systematically evaluates the impact of explicit, implicit,
and hybrid buffering on coding performance across four inter-frame coding
frameworks under a unified experimental setup, providing a thorough
understanding of their effectiveness.

</details>


### [229] [SANR: Scene-Aware Neural Representation for Light Field Image Compression with Rate-Distortion Optimization](https://arxiv.org/abs/2510.15775)
*Gai Zhang,Xinfeng Zhang,Lv Tang,Hongyu An,Li Zhang,Qingming Huang*

Main category: eess.IV

TL;DR: SANR is a Scene-Aware Neural Representation framework for light field image compression that introduces hierarchical scene modeling and end-to-end rate-distortion optimization, achieving 65.62% BD-rate saving against HEVC.


<details>
  <summary>Details</summary>
Motivation: Light field images have high-dimensional data volumes that challenge efficient compression. Existing neural representation methods neglect explicit scene structure modeling and lack end-to-end rate-distortion optimization, limiting compression efficiency.

Method: Proposes SANR with hierarchical scene modeling using multi-scale latent codes to capture scene structures, and incorporates entropy-constrained quantization-aware training for end-to-end rate-distortion optimization.

Result: SANR significantly outperforms state-of-the-art techniques with 65.62% BD-rate saving against HEVC, demonstrating superior rate-distortion performance.

Conclusion: The proposed SANR framework effectively addresses limitations in neural representation-based light field compression by combining scene-aware modeling with end-to-end rate-distortion optimization.

Abstract: Light field images capture multi-view scene information and play a crucial
role in 3D scene reconstruction. However, their high-dimensional nature results
in enormous data volumes, posing a significant challenge for efficient
compression in practical storage and transmission scenarios. Although neural
representation-based methods have shown promise in light field image
compression, most approaches rely on direct coordinate-to-pixel mapping through
implicit neural representation (INR), often neglecting the explicit modeling of
scene structure. Moreover, they typically lack end-to-end rate-distortion
optimization, limiting their compression efficiency. To address these
limitations, we propose SANR, a Scene-Aware Neural Representation framework for
light field image compression with end-to-end rate-distortion optimization. For
scene awareness, SANR introduces a hierarchical scene modeling block that
leverages multi-scale latent codes to capture intrinsic scene structures,
thereby reducing the information gap between INR input coordinates and the
target light field image. From a compression perspective, SANR is the first to
incorporate entropy-constrained quantization-aware training (QAT) into neural
representation-based light field image compression, enabling end-to-end
rate-distortion optimization. Extensive experiment results demonstrate that
SANR significantly outperforms state-of-the-art techniques regarding
rate-distortion performance with a 65.62\% BD-rate saving against HEVC.

</details>
